{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Software Engineering Study Guide \u00b6 A hands-on, fill-in-as-you-learn framework for systems design and algorithms How This Works \u00b6 Numbered sequences you work through at your own pace. Each topic follows the same structure: ELI5 \u2192 Quiz \u2192 Implementation \u2192 Decision Framework \u2192 Practice \u2192 Review Checklist Two Paths \u00b6 Systems Design (17 Topics) \u00b6 Build real implementations to understand how systems work: Storage Engines - Implement B+Trees & LSM Trees Row vs Column Storage - OLTP vs OLAP, columnar formats, compression Networking Fundamentals - TCP/UDP, HTTP versions, WebSockets, DNS, TLS Search & Indexing - Inverted indexes, full-text search, ranking algorithms Caching Patterns - LRU, LFU, cache-aside, write-through API Design - REST principles, versioning, pagination Security Patterns - JWT, RBAC, API keys, secrets management Rate Limiting - Token bucket, sliding window algorithms Load Balancing - Consistent hashing, health checks Concurrency Patterns - Locks, producer-consumer, thread safety Database Scaling - Replication, sharding, partitioning Message Queues - Queue vs pub/sub, delivery guarantees Stream Processing - Windowing, watermarks, exactly-once semantics Observability - Metrics, logging, tracing, SLOs Distributed Transactions - Saga pattern, idempotency Consensus Patterns - Raft, leader election, distributed locks DSA (17 Topics) \u00b6 Pattern-based approach from easy to advanced: Two Pointers - Opposite directions, same direction, different speeds Sliding Window - Fixed and variable window sizes for subarray problems Hash Tables - Fast lookups, grouping, frequency counting Linked Lists - Reversal, cycle detection, fast/slow pointers Stacks & Queues - LIFO/FIFO, monotonic stacks, deque operations Trees - Traversals - Inorder, preorder, postorder, level-order (BFS) Trees - Recursion - Height, diameter, LCA, path problems Binary Search - Classic search, rotated arrays, 2D matrices Heaps - Priority queues, top K problems (prerequisite for Dijkstra) Graphs (DFS/BFS) - Traversals, cycle detection, connected components Union-Find - Disjoint sets for dynamic connectivity, Kruskal's MST Advanced Graphs - Topological Sort, Dijkstra, MST Backtracking - Permutations, combinations, constraint satisfaction Dynamic Programming - 1D - Fibonacci, house robber, coin change Dynamic Programming - 2D - Knapsack, LCS, edit distance Tries - Prefix trees for string problems Advanced Topics - Bit manipulation, intervals, prefix sums Learning Process \u00b6 Implement code stubs \u2192 Run examples \u2192 Fill in explanations Build decision trees for when to use (and NOT use) each pattern Complete practice problems \u2192 Check off review checklist Key Principles \u00b6 Implement First: Build it to understand it, then explain it simply Debug to Learn: Fix broken implementations to understand edge cases Practice Scenarios: Apply concepts to real-world design problems Getting Started \u00b6 Systems Design: 01. Storage Engines \u2192 - Implement B+Tree and LSM Tree, benchmark performance DSA: 01. Two Pointers \u2192 - Learn three pointer patterns, build pattern recognition No pressure, no timelines. Just learn \u2192 implement \u2192 fill in \u2192 move on.","title":"Home"},{"location":"#software-engineering-study-guide","text":"A hands-on, fill-in-as-you-learn framework for systems design and algorithms","title":"Software Engineering Study Guide"},{"location":"#how-this-works","text":"Numbered sequences you work through at your own pace. Each topic follows the same structure: ELI5 \u2192 Quiz \u2192 Implementation \u2192 Decision Framework \u2192 Practice \u2192 Review Checklist","title":"How This Works"},{"location":"#two-paths","text":"","title":"Two Paths"},{"location":"#learning-process","text":"Implement code stubs \u2192 Run examples \u2192 Fill in explanations Build decision trees for when to use (and NOT use) each pattern Complete practice problems \u2192 Check off review checklist","title":"Learning Process"},{"location":"#key-principles","text":"Implement First: Build it to understand it, then explain it simply Debug to Learn: Fix broken implementations to understand edge cases Practice Scenarios: Apply concepts to real-world design problems","title":"Key Principles"},{"location":"#getting-started","text":"Systems Design: 01. Storage Engines \u2192 - Implement B+Tree and LSM Tree, benchmark performance DSA: 01. Two Pointers \u2192 - Learn three pointer patterns, build pattern recognition No pressure, no timelines. Just learn \u2192 implement \u2192 fill in \u2192 move on.","title":"Getting Started"},{"location":"dsa/01-two-pointers/","text":"Two Pointers \u00b6 Reduce O(n\u00b2) to O(n) by using two indices moving through data Important info \u00b6 Can be same/different direction/speed Same direction cursor conventions Write Cursor: Points to the first available space where the next piece of incoming data will be stored Read Cursor: Points to the first piece of available data that has not been read yet. ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all three patterns, explain them simply. Prompts to guide you: What is the two pointers pattern in one sentence? Your answer: Avoiding the need to exhaustively \"check every pair\" by keeping track of two pointers and exploiting some property of the input Why is it faster than nested loops? Your answer: You don't need to check all combinations leading to fewer ops Real-world analogy: Same direction partitioning: Using your left arm to keep the good apples in your sweater and your right arm to sort new ones in or out. When you get a good one, your left arm moves a little to make room for the good apple Opposite direction matching: You and your brother have 20 chuck-e-cheese tickets for 2 prizes. You want to use all 20 tickets and all the prizes are sorted by ticket price. Different speeds exploring: You're lost and walking along a rushing river. You throw a message in a bottle, wait a bit, then keep walking. If you eventually see the bottle again, you know you're on a circular river. Importantly, this wouldn't work if you started running the same speed as the river current! When does this pattern work? Your answer: When you can eliminate multiple possibilities with each pointer movement or when trying to explore paths and you can't store state. When does this pattern fail? Your answer: when you truly need to check every combination or when the problem requires random access to all elements simultaneously. Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Two nested loops searching for a sum pair: Time complexity: O(n^2) Verified after learning: [Actual: O(?)] Two pointers searching for a sum pair in sorted array: Time complexity: O(n) Space complexity: [Your guess: O(?)] Verified: [Actual] Speedup calculation: If n = 1,000, nested loops = n\u00b2 = 1,000,000 operations Two pointers = n = 1000 operations Speedup factor: 1,000 times faster Scenario Predictions \u00b6 Scenario 1: Find pair that sums to 10 in [1, 3, 5, 7, 9] Can you use two pointers? Yes, input is sorted Starting positions: left = 0 , right = 4 If sum = 8 (too small), which pointer moves? Left, since this makes the sum bigger If sum = 12 (too big), which pointer moves? Right, since this makes the sum smaller Scenario 2: Find pair that sums to 10 in [9, 3, 1, 7, 5] (unsorted) Can you use two pointers directly? No, input is not sorted What must you do first? Sort the input Scenario 3: Remove duplicates from [1, 1, 2, 2, 3] Which pattern applies? Same Why that pattern? [Fill in your reasoning] Trade-off Quiz \u00b6 Question: When would HashSet be BETTER than two pointers for finding pairs? Your answer: When you don't care about space complexity Verified answer: [Fill in after learning] Question: What's the MAIN requirement for opposite-direction two pointers? Array must be sorted Array must have even length Array must contain unique elements Array must be positive integers Verify after implementation: [Which one(s)?] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example: Find Pair Sum \u00b6 Problem: Find two numbers in a sorted array that sum to a target. Approach 1: Brute Force (Nested Loops) \u00b6 // Naive approach - Check all possible pairs public static boolean hasPairSum_BruteForce(int[] nums, int target) { for (int i = 0; i < nums.length; i++) { for (int j = i + 1; j < nums.length; j++) { if (nums[i] + nums[j] == target) { return true; } } } return false; } Analysis: Time: O(n\u00b2) - For each element, check all remaining elements Space: O(1) - No extra space For n = 10,000: ~100,000,000 operations Approach 2: Two Pointers (Optimized) \u00b6 // Optimized approach - Use two pointers from opposite ends public static boolean hasPairSum_TwoPointers(int[] nums, int target) { int left = 0; int right = nums.length - 1; while (left < right) { int sum = nums[left] + nums[right]; if (sum == target) return true; if (sum < target) left++; // Need larger sum else right--; // Need smaller sum } return false; } Analysis: Time: O(n) - Each pointer moves at most n/2 steps Space: O(1) - No extra space For n = 10,000: ~10,000 operations Performance Comparison \u00b6 Array Size Brute Force (O(n\u00b2)) Two Pointers (O(n)) Speedup n = 100 10,000 ops 100 ops 100x n = 1,000 1,000,000 ops 1,000 ops 1,000x n = 10,000 100,000,000 ops 10,000 ops 10,000x Your calculation: For n = 5,000, the speedup is approximately 7000 times faster. Why Does Two Pointers Work? \u00b6 Key insight to understand: In a sorted array [1, 3, 5, 7, 9] looking for sum = 10: Step 1: left=0 (val=1), right=4 (val=9), sum=10 \u2192 FOUND! If we were looking for sum = 12: Step 1: left=0 (val=1), right=4 (val=9), sum=10 (too small) \u2192 Move left++ because we need a LARGER sum Step 2: left=1 (val=3), right=4 (val=9), sum=12 \u2192 FOUND! Why can we skip pairs? When sum is too small, moving right-- makes it even smaller (not helpful) When sum is too large, moving left++ makes it even larger (not helpful) So each move eliminates multiple pairs in one step! After implementing, explain in your own words: Why does sorted order matter? It means we know for sure that moving a pointer isn't skipping valid pairs What pairs do we skip and why is it safe? Any pairs whose sum is greater/lesser than the current Core Implementation \u00b6 Pattern 1: Opposite Direction Pointers \u00b6 Concept: Start from both ends, move toward each other. Use case: Palindromes, pair sum in sorted array. public class OppositeDirectionPointers { /** * Problem: Check if string is a palindrome * Time: O(n), Space: O(1) */ public static boolean isPalindrome(String s) { int l = 0; int r = s.length() - 1; while (l < r) { if (s.charAt(l) != s.charAt(r)) { return false; } l++; r--; } return true; // Replace with implementation } /** * Problem: Find pair in sorted array that sums to target * Time: O(n), Space: O(1) * <p> */ public static int[] twoSum(int[] nums, int target) { int l = 0; int r = nums.length - 1; while (l < r) { int sum = nums[l] + nums[r]; if (sum == target) { return new int[]{l, r}; } else if (sum < target) { l++; } else { r--; } } throw new IllegalArgumentException(\"No two sum solution\"); } /** * Problem: Reverse array in-place * Time: O(n), Space: O(1) * <p> */ public static void reverseArray(int[] arr) { int l = 0; int r = arr.length - 1; while (l < r) { int tmp = arr[l]; arr[l] = arr[r]; arr[r] = tmp; l++; r--; } } } Runnable Client Code: public class OppositeDirectionClient { public static void main(String[] args) { System.out.println(\"=== Opposite Direction Two Pointers ===\\n\"); // Test 1: Palindrome check System.out.println(\"--- Test 1: Palindrome ---\"); String[] testStrings = {\"racecar\", \"hello\", \"noon\", \"a\", \"\"}; for (String s : testStrings) { boolean result = OppositeDirectionPointers.isPalindrome(s); System.out.printf(\"isPalindrome(\\\"%s\\\") = %b%n\", s, result); } // Test 2: Two sum in sorted array System.out.println(\"\\n--- Test 2: Two Sum ---\"); int[] sortedArray = {1, 3, 5, 7, 9, 11}; int target = 12; int[] result = OppositeDirectionPointers.twoSum(sortedArray, target); System.out.printf(\"Array: %s%n\", Arrays.toString(sortedArray)); System.out.printf(\"Target: %d%n\", target); System.out.printf(\"Pair indices: %s%n\", Arrays.toString(result)); if (result[0] != -1) { System.out.printf(\"Values: %d + %d = %d%n\", sortedArray[result[0]], sortedArray[result[1]], target); } // Test 3: Reverse array System.out.println(\"\\n--- Test 3: Reverse Array ---\"); int[] arr = {1, 2, 3, 4, 5}; System.out.println(\"Before: \" + Arrays.toString(arr)); OppositeDirectionPointers.reverseArray(arr); System.out.println(\"After: \" + Arrays.toString(arr)); } } Pattern 2: Same Direction Pointers (Slow/Fast) \u00b6 Concept: Both pointers move left to right, at different speeds. Use case: Remove duplicates, partition array, in-place modifications. public class SameDirectionPointers { /** * Problem: Remove duplicates from sorted array in-place * Return new length * Time: O(n), Space: O(1) * <p> */ public static int removeDuplicates(int[] nums) { if (nums.length == 0) return 0; int write = 1; for (int read = 1; read < nums.length; read++) { // duplicate, skip this write if (nums[read] == nums[write - 1]) { continue; } nums[write] = nums[read]; write++; } return write; // new length } /** * Problem: Move all zeros to end, maintain order of non-zeros * Time: O(n), Space: O(1) * <p> */ public static void moveZeroes(int[] nums) { int write = 0; for (int read = 0; read < nums.length; read++) { if (nums[read] == 0) { continue; } nums[write] = nums[read]; write++; } while (write < nums.length) { nums[write] = 0; write++; } } /** * Problem: Partition array - all elements < pivot go left * Time: O(n), Space: O(1) * <p> * <pre> * [ Condition Met | Unmet/Mixed | Unprocessed ] * 0 slow-1 slow fast n-1 * \u2193 \u2193 \u2193 \u2193 \u2193 * Array: [ 2 1 4 3 0 | 9 8 7 6 5 | ? ? ? ? ] * \u2191 \u2191 \u2191 \u2191 * \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518 * Elements \u2264 Pivot Current Element * (The \"Good\" Zone) Being Evaluated * </pre> */ public static int partition(int[] arr, int pivot) { int wall = 0; // \"good zone indices < wall\" for (int fast = 0; fast < arr.length; fast++) { if (arr[fast] >= pivot) { continue; } int tmp = arr[fast]; arr[fast] = arr[wall]; arr[wall] = tmp; wall++; } return wall; // Replace with implementation } } Runnable Client Code: public class SameDirectionClient { public static void main(String[] args) { System.out.println(\"=== Same Direction Two Pointers ===\\n\"); // Test 1: Remove duplicates System.out.println(\"--- Test 1: Remove Duplicates ---\"); int[] arr1 = {1, 1, 2, 2, 2, 3, 4, 4, 5}; System.out.println(\"Before: \" + Arrays.toString(arr1)); int newLength = SameDirectionPointers.removeDuplicates(arr1); System.out.println(\"After: \" + Arrays.toString(Arrays.copyOf(arr1, newLength))); System.out.println(\"New length: \" + newLength); // Test 2: Move zeros System.out.println(\"\\n--- Test 2: Move Zeros ---\"); int[] arr2 = {0, 1, 0, 3, 12, 0, 5}; System.out.println(\"Before: \" + Arrays.toString(arr2)); SameDirectionPointers.moveZeroes(arr2); System.out.println(\"After: \" + Arrays.toString(arr2)); // Test 3: Partition System.out.println(\"\\n--- Test 3: Partition ---\"); int[] arr3 = {7, 2, 9, 1, 5, 3, 8}; int pivot = 5; System.out.println(\"Before: \" + Arrays.toString(arr3)); System.out.println(\"Pivot: \" + pivot); int partitionIdx = SameDirectionPointers.partition(arr3, pivot); System.out.println(\"After: \" + Arrays.toString(arr3)); System.out.println(\"Partition index: \" + partitionIdx); System.out.println(\"(All elements before index \" + partitionIdx + \" are < \" + pivot + \")\"); } } Pattern 3: Different Speed Pointers \u00b6 Concept: One pointer moves faster than the other. Use case: Linked list cycle detection, finding middle element. public class DifferentSpeedPointers { // Simple ListNode definition static class ListNode { int val; ListNode next; ListNode(int val) { this.val = val; } } /** * Problem: Detect cycle in linked list * Time: O(n), Space: O(1) */ public static boolean hasCycle(ListNode head) { if (head == null) return false; ListNode slow = head; ListNode fast = head; while (fast != null) { slow = slow.next; fast = fast.next; if (fast != null) { fast = fast.next; } if (slow == fast) return true; } return false; } /** * Problem: Find middle of linked list * If even length, return second middle node * Time: O(n), Space: O(1) */ public static ListNode findMiddle(ListNode head) { // this handles the \"rounding up\" behavior ListNode sentinel = new ListNode(-1); sentinel.next = head; ListNode slow = sentinel; ListNode fast = sentinel; while (fast != null) { slow = slow.next; fast = fast.next; if (fast != null) { fast = fast.next; } } return slow ; } /** * Problem: Find kth node from end * Time: O(n), Space: O(1) */ public static ListNode findKthFromEnd(ListNode head, int k) { if (head == null) throw new IllegalArgumentException(); if (k <= 0) throw new IllegalArgumentException(); ListNode boat = head; for (int i = 0; i <= k; i++) { boat = boat.next; } ListNode waterskiier = head; while (boat != null) { boat = boat.next; waterskiier = waterskiier.next; } return waterskiier; } // Helper: Create linked list from array static ListNode createList(int[] values) { if (values.length == 0) return null; ListNode head = new ListNode(values[0]); ListNode current = head; for (int i = 1; i < values.length; i++) { current.next = new ListNode(values[i]); current = current.next; } return head; } // Helper: Print linked list static void printList(ListNode head) { ListNode current = head; while (current != null) { System.out.print(current.val); if (current.next != null) System.out.print(\" -> \"); current = current.next; } System.out.println(); } } Runnable Client Code: public class DifferentSpeedClient { public static void main(String[] args) { System.out.println(\"=== Different Speed Two Pointers ===\\n\"); // Test 1: Cycle detection System.out.println(\"--- Test 1: Cycle Detection ---\"); // List without cycle: 1 -> 2 -> 3 -> 4 -> 5 ListNode list1 = DifferentSpeedPointers.createList(new int[]{1, 2, 3, 4, 5}); System.out.print(\"List: \"); DifferentSpeedPointers.printList(list1); System.out.println(\"Has cycle: \" + DifferentSpeedPointers.hasCycle(list1)); // List with cycle: 1 -> 2 -> 3 -> 4 -> 5 -> (back to 3) ListNode list2 = DifferentSpeedPointers.createList(new int[]{1, 2, 3, 4, 5}); ListNode node3 = list2.next.next; // Node with value 3 ListNode tail = list2.next.next.next.next; // Node with value 5 tail.next = node3; // Create cycle System.out.println(\"\\nList with cycle (5 -> 3):\"); System.out.println(\"Has cycle: \" + DifferentSpeedPointers.hasCycle(list2)); // Test 2: Find middle System.out.println(\"\\n--- Test 2: Find Middle ---\"); ListNode list3 = DifferentSpeedPointers.createList(new int[]{1, 2, 3, 4, 5}); System.out.print(\"List (odd length): \"); DifferentSpeedPointers.printList(list3); ListNode middle = DifferentSpeedPointers.findMiddle(list3); System.out.println(\"Middle value: \" + middle.val); ListNode list4 = DifferentSpeedPointers.createList(new int[]{1, 2, 3, 4, 5, 6}); System.out.print(\"List (even length): \"); DifferentSpeedPointers.printList(list4); middle = DifferentSpeedPointers.findMiddle(list4); System.out.println(\"Middle value: \" + middle.val); // Test 3: Kth from end System.out.println(\"\\n--- Test 3: Kth From End ---\"); ListNode list5 = DifferentSpeedPointers.createList(new int[]{1, 2, 3, 4, 5}); System.out.print(\"List: \"); DifferentSpeedPointers.printList(list5); for (int k = 1; k <= 3; k++) { ListNode kthNode = DifferentSpeedPointers.findKthFromEnd(list5, k); System.out.printf(\"%dth from end: %d%n\", k, kthNode.val); } } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken implementations. This tests your understanding. Challenge 1: Broken Palindrome Checker \u00b6 /** * This code is supposed to check if a string is a palindrome. * It has 1 BUG. Find it! */ public static boolean isPalindrome_Buggy(String s) { int left = 0; int right = s.length(); while (left < right) { if (s.charAt(left) != s.charAt(right)) { return false; } left++; right--; } return true; } Your debugging: Bug 1: right needs to start at length-1 or charAt will OOBE Click to verify your answers Bug 1 (Line 7): right should be s.length() - 1 , not s.length() . Array indices are 0-based. Challenge 2: Broken Remove Duplicates \u00b6 /** * Remove duplicates from sorted array. * This has 1 CRITICAL BUG and 1 EDGE CASE BUG. */ public static int removeDuplicates_Buggy(int[] nums) { int slow = 0; int fast = 1; while (fast < nums.length) { if (nums[fast] != nums[slow]) { nums[slow] = nums[fast]; slow++; } fast++; } return slow; } Your debugging: Bug 1: the 0th index gets overwritten accidentally Bug 1 fix: First increment slow pointer then write Bug 2: The length of the fixed array Bug 2 fix: slow+1 Test case to expose the bug: Input: [1, 1, 2, 2, 3] Expected output: [1, 2, 3, ?, ?] and return length = 3 Actual output with buggy code: [Trace through manually] Click to verify your answers Bug 1: Should be slow++ BEFORE nums[slow] = nums[fast] . Current code overwrites the unique element before advancing. Correct: if (nums[fast] != nums[slow]) { slow++; nums[slow] = nums[fast]; } Bug 2: Should return slow + 1 , not slow . The length is one more than the index. Challenge 3: Broken Cycle Detection \u00b6 /** * Detect cycle in linked list. * This has 1 SUBTLE BUG that causes infinite loop. */ public static boolean hasCycle_Buggy(ListNode head) { if (head == null) return false; ListNode slow = head; ListNode fast = head; while (fast != null && fast.next != null) { slow = slow.next; fast = fast.next.next; } if (slow == fast) return true; return false; } Your debugging: Bug: slow==fast needs to be inside the loop Trace through example: List with cycle: 1 \u2192 2 \u2192 3 \u2192 4 \u2192 2 (cycle back to 2) Expected: true Actual: [What happens?] Click to verify your answer Bug: The check if (slow == fast) should be INSIDE the while loop, not after! Correct: while (fast != null && fast.next != null) { slow = slow.next; fast = fast.next.next; if (slow == fast) return true; // Check inside loop! } return false; // No cycle found Why: If there's a cycle, slow and fast will meet inside the loop. Checking after means we exit the loop (which only happens when there's NO cycle), so we'd never detect the cycle. Challenge 4: Move Zeroes Logic Error \u00b6 /** * Move all zeros to the end while maintaining order of non-zeros. * This code compiles but produces WRONG output. */ public static void moveZeroes_Buggy(int[] nums) { int slow = 0; for (int fast = 0; fast < nums.length; fast++) { if (nums[fast] != 0) { nums[slow] = nums[fast]; slow++; } } } Your debugging: Bug: We don't wipe the end of the array afterwards Example: Input [0, 1, 0, 3, 12] , output is [1,3,12,3,12] Expected: [1, 3, 12, 0, 0] Actual: [What do you get?] Fix: Wipe with zero's at the end Click to verify your answer Bug: When slow == fast , we're copying a number onto itself. Then we increment slow, leaving the original position unchanged. We need to ZERO OUT the original position OR use a swap. Fix Option 1 - Use swap: if (nums[fast] != 0) { // Swap nums[slow] and nums[fast] int temp = nums[slow]; nums[slow] = nums[fast]; nums[fast] = temp; slow++; } Fix Option 2 - Zero out after first pass: // ... existing code ... // After the loop, fill remaining with zeros for (int i = slow; i < nums.length; i++) { nums[i] = 0; } Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 6+ bugs across 4 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common two-pointer mistakes to avoid Common mistakes you discovered: [List the patterns you noticed] [Fill in] [Fill in] Decision Framework \u00b6 Your task: Build decision trees for when to use two pointers. Question 1: Is the data sorted? \u00b6 Answer after solving problems: Why does sorting matter? Two pointers eliminates possibilities - we can only be sure we're not eliminating valid possibilities with sorted input. Sorting establishes a predictable relationship between elements and their positions. Can two pointers work on unsorted arrays? Yes but only when the array value is irrelevant, e.g. you're working with linkedlist pointers Question 2: What are you looking for? \u00b6 Answer for each pattern: Opposite direction when: Looking for: when you need to consider combinations from both ends of a sorted array Movement rule: Start at ends, conditionally move one of the pointers in each iter Example problems: Two Sum (sorted), Valid Palindrome, Container With Most Water, Trapping Rain Water, 3Sum/4Sum variants Same direction when: Looking for: In-place array modification, partitioning (lomuto) Movement rule: Read and write pointer. Do a write each iteration but conditionally move pointers Example problems: Remove Duplicates from Sorted Array, Move Zeroes, Remove Element, Partition Array, Sort Colors (Dutch National Flag) Different speed when: - Looking for: Linked list structural properties - Movement rule: Slow moves 1 step per iteration, fast moves 2 steps - Example problems: Linked List Cycle I & II, Find Middle of Linked List, Kth Node From End, Happy Number, Reorder List Your Decision Tree \u00b6 Build this after solving practice problems: flowchart LR Start[[\"Two Pointers?\"]] Start --> Q1{Linked List?} Q1 -->|YES| DiffSpeed([Different Speed<br/>cycle, middle, kth]) Q1 -->|NO| Q2{Goal?} Q2 -->|In-place modify<br/>partition, filter| SameDir([Same Direction<br/>slow/fast write/read]) Q2 -->|Find pairs<br/>palindrome| Q3{Sorted or<br/>sortable?} Q3 -->|YES| OppDir([Opposite Direction<br/>left++, right--]) Q3 -->|NO| Other([Use Hash Table]) Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 3): 125. Valid Palindrome Pattern: [Which one?] Your solution time: ___ Key insight: [Fill in after solving] 26. Remove Duplicates from Sorted Array Pattern: [Which one?] Your solution time: ___ Key insight: [Fill in] 283. Move Zeroes Pattern: [Which one?] Your solution time: ___ Key insight: [Fill in] Medium (Complete 2-3): 15. 3Sum Pattern: [Extension of which pattern?] Difficulty: [Rate 1-10] Key insight: [Fill in] Mistake made: [Fill in if any] 11. Container With Most Water Pattern: [Which one?] Difficulty: [Rate 1-10] Key insight: [Fill in] 167. Two Sum II Pattern: [Which one?] Comparison to Two Sum I: [How is it different?] Hard (Optional): 42. Trapping Rain Water Pattern: [Which variant?] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Opposite direction: palindrome, two sum, reverse all work Same direction: remove duplicates, move zeros, partition all work Different speed: cycle detection, find middle, kth from end all work All client code runs successfully Pattern Recognition Can identify which pattern to use for new problems Understand when each pattern applies Know the movement rules for each variant Problem Solving Solved 3 easy problems Solved 2-3 medium problems Analyzed time/space complexity Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use two pointers Can explain trade-offs vs other approaches Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Mastery Certification \u00b6 I certify that I can: Implement all three two-pointer patterns from memory Explain when and why to use each pattern Identify the correct pattern for new problems Analyze time and space complexity Compare trade-offs with alternative approaches Debug common two-pointer mistakes Teach this concept to someone else","title":"01. Two Pointers"},{"location":"dsa/01-two-pointers/#two-pointers","text":"Reduce O(n\u00b2) to O(n) by using two indices moving through data","title":"Two Pointers"},{"location":"dsa/01-two-pointers/#important-info","text":"Can be same/different direction/speed Same direction cursor conventions Write Cursor: Points to the first available space where the next piece of incoming data will be stored Read Cursor: Points to the first piece of available data that has not been read yet.","title":"Important info"},{"location":"dsa/01-two-pointers/#eli5-explain-like-im-5","text":"Your task: After implementing all three patterns, explain them simply. Prompts to guide you: What is the two pointers pattern in one sentence? Your answer: Avoiding the need to exhaustively \"check every pair\" by keeping track of two pointers and exploiting some property of the input Why is it faster than nested loops? Your answer: You don't need to check all combinations leading to fewer ops Real-world analogy: Same direction partitioning: Using your left arm to keep the good apples in your sweater and your right arm to sort new ones in or out. When you get a good one, your left arm moves a little to make room for the good apple Opposite direction matching: You and your brother have 20 chuck-e-cheese tickets for 2 prizes. You want to use all 20 tickets and all the prizes are sorted by ticket price. Different speeds exploring: You're lost and walking along a rushing river. You throw a message in a bottle, wait a bit, then keep walking. If you eventually see the bottle again, you know you're on a circular river. Importantly, this wouldn't work if you started running the same speed as the river current! When does this pattern work? Your answer: When you can eliminate multiple possibilities with each pointer movement or when trying to explore paths and you can't store state. When does this pattern fail? Your answer: when you truly need to check every combination or when the problem requires random access to all elements simultaneously.","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/01-two-pointers/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/01-two-pointers/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/01-two-pointers/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/01-two-pointers/#debugging-challenges","text":"Your task: Find and fix bugs in broken implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"dsa/01-two-pointers/#decision-framework","text":"Your task: Build decision trees for when to use two pointers.","title":"Decision Framework"},{"location":"dsa/01-two-pointers/#practice","text":"","title":"Practice"},{"location":"dsa/01-two-pointers/#review-checklist","text":"Before moving to the next topic: Implementation Opposite direction: palindrome, two sum, reverse all work Same direction: remove duplicates, move zeros, partition all work Different speed: cycle detection, find middle, kth from end all work All client code runs successfully Pattern Recognition Can identify which pattern to use for new problems Understand when each pattern applies Know the movement rules for each variant Problem Solving Solved 3 easy problems Solved 2-3 medium problems Analyzed time/space complexity Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use two pointers Can explain trade-offs vs other approaches Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else","title":"Review Checklist"},{"location":"dsa/02-sliding-window/","text":"Sliding Window \u00b6 Optimize subarray/substring problems from O(n\u00b2) to O(n) ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is the sliding window pattern in one sentence? Your answer: [Fill in after implementation] How is it different from two pointers? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Sliding window is like a camera viewfinder moving across a scene...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] When does this pattern fail? Your answer: [Fill in after trying non-contiguous problems] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Two nested loops finding max sum of k elements: Time complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Sliding window finding max sum of k elements: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Speedup calculation: If n = 1,000 and k = 100, nested loops = n \u00d7 k = _____ operations Sliding window = n = _____ operations Speedup factor: _____ times faster Scenario Predictions \u00b6 Scenario 1: Find maximum sum of 3 consecutive elements in [1, 4, 2, 10, 2, 3, 1, 0, 20] Can you use sliding window? [Yes/No - Why?] Window type: [Fixed/Dynamic - Why?] What do you track in the window? [Sum? Elements? Other?] When you slide from index 0-2 to 1-3, what changes? [Fill in] Scenario 2: Find longest substring without repeating characters in \"abcabcbb\" Can you use sliding window? [Yes/No - Why?] Window type: [Fixed/Dynamic - Why?] What happens when you encounter a duplicate? [Expand/Shrink window?] What data structure tracks window state? [Fill in] Scenario 3: Find subarray with sum = 10 in [1, 2, 3, 7, 5] (can be non-contiguous) Can you use sliding window? [Yes/No - Why?] What's the key requirement that breaks sliding window? [Fill in] Trade-off Quiz \u00b6 Question: When would brute force be SIMPLER than sliding window for max sum of k elements? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN requirement for sliding window to work? Array must be sorted Problem involves contiguous subarray/substring Window size must be constant Array must contain positive integers Verify after implementation: [Which one(s)?] Question: Fixed vs Dynamic window - which applies when? Fixed window when: [Fill in] Dynamic window when: [Fill in] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example: Maximum Sum of K Consecutive Elements \u00b6 Problem: Find the maximum sum of any k consecutive elements in an array. Approach 1: Brute Force (Nested Loops) \u00b6 // Naive approach - Recalculate sum for each window public static int maxSum_BruteForce(int[] nums, int k) { if (nums.length < k) return 0; int maxSum = Integer.MIN_VALUE; // For each possible starting position for (int i = 0; i <= nums.length - k; i++) { int windowSum = 0; // Calculate sum of k elements starting at i for (int j = i; j < i + k; j++) { windowSum += nums[j]; } maxSum = Math.max(maxSum, windowSum); } return maxSum; } Analysis: Time: O(n \u00d7 k) - For each of n-k positions, sum k elements Space: O(1) - No extra space For n = 10,000, k = 100: ~1,000,000 operations Approach 2: Sliding Window (Optimized) \u00b6 // Optimized approach - Reuse previous sum by sliding public static int maxSum_SlidingWindow(int[] nums, int k) { if (nums.length < k) return 0; // Calculate sum of first window int windowSum = 0; for (int i = 0; i < k; i++) { windowSum += nums[i]; } int maxSum = windowSum; // Slide window: remove left, add right for (int i = k; i < nums.length; i++) { windowSum = windowSum - nums[i - k] + nums[i]; // KEY: Reuse previous sum! maxSum = Math.max(maxSum, windowSum); } return maxSum; } Analysis: Time: O(n) - One pass to build first window, one pass to slide Space: O(1) - Only track window sum For n = 10,000, k = 100: ~10,000 operations Performance Comparison \u00b6 Array Size (n) Window Size (k) Brute Force (O(n\u00d7k)) Sliding Window (O(n)) Speedup n = 100 k = 10 1,000 ops 100 ops 10x n = 1,000 k = 100 100,000 ops 1,000 ops 100x n = 10,000 k = 100 1,000,000 ops 10,000 ops 100x Your calculation: For n = 5,000 and k = 50, the speedup is approximately _____ times faster. Why Does Sliding Window Work? \u00b6 Key insight to understand: In array [1, 4, 2, 10, 2, 3] with k = 3: Window 1: [1, 4, 2] sum = 7 Window 2: [4, 2, 10] sum = 16 Brute force: Calculate 4 + 2 + 10 = 16 (3 operations) Sliding window: Previous sum (7) - 1 + 10 = 16 (2 operations) Why can we reuse the sum? Window 2 shares elements [4, 2] with Window 1 Only difference: remove leftmost (1), add rightmost (10) No need to recalculate the shared elements! Visualization: [1, 4, 2, 10, 2, 3] ^-----^ Window 1: sum = 7 ^-----^ Window 2: Remove 1, Add 10 \u2192 sum = 7 - 1 + 10 = 16 ^-----^ Window 3: Remove 4, Add 2 \u2192 sum = 16 - 4 + 2 = 14 After implementing, explain in your own words: Why does the window \"slide\" instead of \"jump\"? [Your answer] What would happen if the window wasn't contiguous? [Your answer] How is this different from two pointers? [Your answer] Core Implementation \u00b6 Pattern 1: Fixed Window Size \u00b6 Concept: Window size is constant, slide one position at a time. Use case: Maximum/minimum of k consecutive elements, average of subarrays. public class FixedWindow { /** * Problem: Maximum sum of K consecutive elements * Time: O(n), Space: O(1) * * TODO: Implement fixed window */ public static double maxAverageSubarray(int[] nums, int k) { if (nums.length < k) return 0.0; // TODO: Calculate initial window sum // Slide the window and update max as you go return 0.0; // Replace with implementation } /** * Problem: Contains nearby duplicate within k distance * Time: O(n), Space: O(k) * * TODO: Implement using HashSet as fixed window */ public static boolean containsNearbyDuplicate(int[] nums, int k) { // TODO: Use a set to track elements in current window return false; // Replace with implementation } /** * Problem: Maximum of all subarrays of size k * Time: O(n), Space: O(k) using deque * * TODO: Implement using deque for efficient max tracking */ public static int[] maxSlidingWindow(int[] nums, int k) { // TODO: Use a deque to maintain useful elements in window // Keep elements in decreasing order for easy max access return new int[0]; // Replace with implementation } } Runnable Client Code: import java.util.*; public class FixedWindowClient { public static void main(String[] args) { System.out.println(\"=== Fixed Window Size ===\\n\"); // Test 1: Max average subarray System.out.println(\"--- Test 1: Max Average ---\"); int[] nums1 = {1, 12, -5, -6, 50, 3}; int k1 = 4; double maxAvg = FixedWindow.maxAverageSubarray(nums1, k1); System.out.printf(\"Array: %s%n\", Arrays.toString(nums1)); System.out.printf(\"k = %d%n\", k1); System.out.printf(\"Max average: %.2f%n\", maxAvg); // Test 2: Contains nearby duplicate System.out.println(\"\\n--- Test 2: Nearby Duplicate ---\"); int[][] dupTests = { {1, 2, 3, 1}, // k=3, should be true {1, 0, 1, 1}, // k=1, should be true {1, 2, 3, 1, 2, 3} // k=2, should be false }; int[] kValues = {3, 1, 2}; for (int i = 0; i < dupTests.length; i++) { boolean hasDup = FixedWindow.containsNearbyDuplicate(dupTests[i], kValues[i]); System.out.printf(\"Array: %s, k=%d -> %b%n\", Arrays.toString(dupTests[i]), kValues[i], hasDup); } // Test 3: Max sliding window System.out.println(\"\\n--- Test 3: Max Sliding Window ---\"); int[] nums3 = {1, 3, -1, -3, 5, 3, 6, 7}; int k3 = 3; int[] maxes = FixedWindow.maxSlidingWindow(nums3, k3); System.out.printf(\"Array: %s%n\", Arrays.toString(nums3)); System.out.printf(\"k = %d%n\", k3); System.out.printf(\"Maximums: %s%n\", Arrays.toString(maxes)); } } Pattern 2: Dynamic Window Size \u00b6 Concept: Window expands and contracts based on condition. Use case: Longest/shortest substring with constraint, subarray sum. import java.util.*; public class DynamicWindow { /** * Problem: Longest substring without repeating characters * Time: O(n), Space: O(k) where k = unique chars * * TODO: Implement dynamic window with HashSet */ public static int lengthOfLongestSubstring(String s) { Set<Character> window = new HashSet<>(); int left = 0, maxLen = 0; // TODO: Implement iteration/conditional logic return 0; // Replace with implementation } /** * Problem: Longest substring with at most K distinct characters * Time: O(n), Space: O(k) * * TODO: Implement with HashMap for frequency counting */ public static int lengthOfLongestSubstringKDistinct(String s, int k) { if (k == 0) return 0; Map<Character, Integer> window = new HashMap<>(); int left = 0, maxLen = 0; // TODO: Implement iteration/conditional logic return 0; // Replace with implementation } /** * Problem: Minimum size subarray sum >= target * Time: O(n), Space: O(1) * * TODO: Implement shrinking window */ public static int minSubArrayLen(int target, int[] nums) { int left = 0, sum = 0, minLen = Integer.MAX_VALUE; // TODO: Implement iteration/conditional logic // TODO: Return minLen == Integer.MAX_VALUE ? 0 : minLen return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class DynamicWindowClient { public static void main(String[] args) { System.out.println(\"=== Dynamic Window Size ===\\n\"); // Test 1: Longest substring without repeating System.out.println(\"--- Test 1: Longest Substring (No Repeats) ---\"); String[] test1 = {\"abcabcbb\", \"bbbbb\", \"pwwkew\", \"\"}; for (String s : test1) { int len = DynamicWindow.lengthOfLongestSubstring(s); System.out.printf(\"\\\"%s\\\" -> %d%n\", s, len); } // Test 2: Longest with K distinct System.out.println(\"\\n--- Test 2: K Distinct Characters ---\"); String[] test2 = {\"eceba\", \"aa\", \"aaabbccd\"}; int[] kValues = {2, 1, 2}; for (int i = 0; i < test2.length; i++) { int len = DynamicWindow.lengthOfLongestSubstringKDistinct(test2[i], kValues[i]); System.out.printf(\"\\\"%s\\\", k=%d -> %d%n\", test2[i], kValues[i], len); } // Test 3: Minimum subarray sum System.out.println(\"\\n--- Test 3: Min Subarray Sum >= Target ---\"); int[][] arrays = { {2, 3, 1, 2, 4, 3}, {1, 4, 4}, {1, 1, 1, 1, 1, 1, 1, 1} }; int[] targets = {7, 4, 11}; for (int i = 0; i < arrays.length; i++) { int minLen = DynamicWindow.minSubArrayLen(targets[i], arrays[i]); System.out.printf(\"Array: %s, target=%d -> %d%n\", Arrays.toString(arrays[i]), targets[i], minLen); } } } Pattern 3: String Problems with Window \u00b6 Concept: Track character frequencies in window for pattern matching. Use case: Anagram problems, substring with all characters. import java.util.*; public class StringWindow { /** * Problem: Find all anagrams of pattern in string * Time: O(n), Space: O(1) - only 26 letters * * TODO: Implement using frequency arrays */ public static List<Integer> findAnagrams(String s, String p) { List<Integer> result = new ArrayList<>(); if (s.length() < p.length()) return result; // TODO: Create frequency array for pattern p // TODO: Create frequency array for current window // TODO: Fixed window of size p.length() return result; // Replace with implementation } /** * Problem: Permutation in string (s2 contains permutation of s1) * Time: O(n), Space: O(1) * * TODO: Implement using sliding window comparison */ public static boolean checkInclusion(String s1, String s2) { if (s1.length() > s2.length()) return false; // TODO: Similar to findAnagrams but return true on first match return false; // Replace with implementation } /** * Problem: Minimum window substring containing all chars of t * Time: O(n + m), Space: O(k) where k = unique chars * * TODO: Implement using two frequency maps */ public static String minWindow(String s, String t) { if (s.isEmpty() || t.isEmpty()) return \"\"; // TODO: Create frequency map for t // TODO: Track matched characters // TODO: Expand right, shrink left when valid return \"\"; // Replace with implementation } } Runnable Client Code: import java.util.*; public class StringWindowClient { public static void main(String[] args) { System.out.println(\"=== String Window Problems ===\\n\"); // Test 1: Find anagrams System.out.println(\"--- Test 1: Find Anagrams ---\"); String[][] anagramTests = { {\"cbaebabacd\", \"abc\"}, {\"abab\", \"ab\"} }; for (String[] test : anagramTests) { List<Integer> indices = StringWindow.findAnagrams(test[0], test[1]); System.out.printf(\"s=\\\"%s\\\", p=\\\"%s\\\" -> %s%n\", test[0], test[1], indices); } // Test 2: Check inclusion System.out.println(\"\\n--- Test 2: Permutation In String ---\"); String[][] inclusionTests = { {\"ab\", \"eidbaooo\"}, {\"ab\", \"eidboaoo\"}, {\"abc\", \"bbbca\"} }; for (String[] test : inclusionTests) { boolean found = StringWindow.checkInclusion(test[0], test[1]); System.out.printf(\"s1=\\\"%s\\\", s2=\\\"%s\\\" -> %b%n\", test[0], test[1], found); } // Test 3: Minimum window System.out.println(\"\\n--- Test 3: Minimum Window Substring ---\"); String[][] windowTests = { {\"ADOBECODEBANC\", \"ABC\"}, {\"a\", \"a\"}, {\"a\", \"aa\"} }; for (String[] test : windowTests) { String result = StringWindow.minWindow(test[0], test[1]); System.out.printf(\"s=\\\"%s\\\", t=\\\"%s\\\" -> \\\"%s\\\"%n\", test[0], test[1], result); } } } Pattern 4: Two Pointers + Window Hybrid \u00b6 Concept: Combine sliding window with two-pointer techniques. Use case: Character replacement, fruit baskets, longest repeating replacement. import java.util.*; public class HybridWindow { /** * Problem: Longest repeating character replacement * Time: O(n), Space: O(1) - only 26 letters * * TODO: Implement window with character replacement */ public static int characterReplacement(String s, int k) { int[] count = new int[26]; int left = 0, maxCount = 0, maxLen = 0; // TODO: Implement iteration/conditional logic return 0; // Replace with implementation } /** * Problem: Max consecutive ones with k flips * Time: O(n), Space: O(1) * * TODO: Implement window tracking flips */ public static int longestOnes(int[] nums, int k) { int left = 0, zeros = 0, maxLen = 0; // TODO: Implement iteration/conditional logic return 0; // Replace with implementation } /** * Problem: Fruits into baskets (at most 2 types) * Time: O(n), Space: O(1) * * TODO: Implement window with at most 2 distinct elements */ public static int totalFruit(int[] fruits) { Map<Integer, Integer> basket = new HashMap<>(); int left = 0, maxFruits = 0; // TODO: Implement iteration/conditional logic return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class HybridWindowClient { public static void main(String[] args) { System.out.println(\"=== Two Pointers + Window Hybrid ===\\n\"); // Test 1: Character replacement System.out.println(\"--- Test 1: Character Replacement ---\"); String[] strings = {\"ABAB\", \"AABABBA\", \"AAAA\"}; int[] kValues = {2, 1, 2}; for (int i = 0; i < strings.length; i++) { int len = HybridWindow.characterReplacement(strings[i], kValues[i]); System.out.printf(\"s=\\\"%s\\\", k=%d -> %d%n\", strings[i], kValues[i], len); } // Test 2: Max consecutive ones System.out.println(\"\\n--- Test 2: Max Consecutive Ones ---\"); int[][] arrays = { {1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0}, {0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1} }; int[] flips = {2, 3}; for (int i = 0; i < arrays.length; i++) { int len = HybridWindow.longestOnes(arrays[i], flips[i]); System.out.printf(\"Array: %s%n\", Arrays.toString(arrays[i])); System.out.printf(\"k=%d -> %d%n%n\", flips[i], len); } // Test 3: Fruits into baskets System.out.println(\"--- Test 3: Fruits Into Baskets ---\"); int[][] fruitArrays = { {1, 2, 1}, {0, 1, 2, 2}, {1, 2, 3, 2, 2} }; for (int[] fruits : fruitArrays) { int total = HybridWindow.totalFruit(fruits); System.out.printf(\"Fruits: %s -> %d%n\", Arrays.toString(fruits), total); } } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken implementations. This tests your understanding of sliding window mechanics. Challenge 1: Broken Fixed Window (Max Average) \u00b6 /** * Find maximum average of k consecutive elements. * This has 2 BUGS. Find them! */ public static double maxAverage_Buggy(int[] nums, int k) { int windowSum = 0; // Build first window for (int i = 0; i <= k; i++) { windowSum += nums[i]; } double maxAvg = windowSum / k; // Slide window for (int i = k; i < nums.length; i++) { windowSum = windowSum - nums[i - k] + nums[i]; maxAvg = Math.max(maxAvg, windowSum / k); } return maxAvg; } Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Test case to expose bugs: Input: nums = [1, 12, -5, -6, 50, 3] , k = 4 Expected: Maximum average should be 12.75 (for subarray [12, -5, -6, 50] ) Actual with bugs: [Trace through manually] Click to verify your answers Bug 1 (Line 5): Loop should be i < k , not i <= k . We want k elements (indices 0 to k-1), not k+1 elements. Bug 2 (Line 15): windowSum / k performs integer division. Should be windowSum / (double) k to get accurate average. Correct code: for (int i = 0; i < k; i++) { // Fixed: i < k windowSum += nums[i]; } double maxAvg = windowSum / (double) k; // Fixed: cast to double for (int i = k; i < nums.length; i++) { windowSum = windowSum - nums[i - k] + nums[i]; maxAvg = Math.max(maxAvg, windowSum / (double) k); // Fixed: cast to double } Challenge 2: Broken Dynamic Window (Longest Substring) \u00b6 /** * Find longest substring without repeating characters. * This has 1 CRITICAL BUG. Find it! */ public static int longestSubstring_Buggy(String s) { Set<Character> window = new HashSet<>(); int left = 0, maxLen = 0; for (int right = 0; right < s.length(); right++) { char c = s.charAt(right); while (window.contains(c)) { window.remove(s.charAt(left)); left++; } window.add(c); maxLen = Math.max(maxLen, window.size()); } return maxLen; } Your debugging: Bug: [What\\'s the bug?] Trace through example: Input: \"abcabcbb\" Expected: 3 (substring \"abc\") Trace the window and maxLen calculation at each step: [Fill in] Click to verify your answer Bug (Line 16): Using window.size() seems correct but could fail in edge cases. The proper calculation is right - left + 1 to get the current window length. Why the bug is subtle: In this specific implementation, window.size() and right - left + 1 are usually the same because we maintain a set. However, using indices is the standard approach and more explicit. Correct: maxLen = Math.max(maxLen, right - left + 1); // Proper window length calculation Note: This is a subtle bug because the code might work in many cases, but using index-based calculation is clearer and more maintainable. Challenge 3: Broken Window Shrinking Logic \u00b6 /** * Find minimum subarray length with sum >= target. * This has 1 CRITICAL BUG in shrinking logic. */ public static int minSubArrayLen_Buggy(int target, int[] nums) { int left = 0, sum = 0, minLen = Integer.MAX_VALUE; for (int right = 0; right < nums.length; right++) { sum += nums[right]; if (sum >= target) { minLen = Math.min(minLen, right - left + 1); sum -= nums[left]; left++; } } return minLen == Integer.MAX_VALUE ? 0 : minLen; } Your debugging: Bug location: [What's wrong with the condition?] Bug explanation: [Why does IF fail but WHILE works?] Test case to expose bug: Input: nums = [2, 3, 1, 2, 4, 3] , target = 7 Expected: 2 (subarray [4, 3] ) Actual with buggy code: [Trace through and predict] Trace manually: right=0: sum=2, sum < 7, skip right=1: sum=5, sum < 7, skip right=2: sum=6, sum < 7, skip right=3: sum=8, sum >= 7, minLen=4, left=1, sum=6 right=4: sum=10, sum >= 7, minLen=4 (not updated!), left=2, sum=8 ... Click to verify your answer Bug (Line 7): Should be while (sum >= target) instead of if (sum >= target) . Why: When we find a valid window, we should shrink it as much as possible to find the minimum length. Using if only shrinks once, but we might be able to shrink multiple times and still maintain sum >= target . Correct: while (sum >= target) { // Keep shrinking while valid minLen = Math.min(minLen, right - left + 1); sum -= nums[left]; left++; } Key insight: Dynamic windows often need while loops for shrinking, not if statements, because you want to shrink as much as possible while maintaining the constraint. Challenge 4: Not Shrinking Window at All \u00b6 /** * Longest substring with at most K distinct characters. * This code expands the window but NEVER SHRINKS it properly! */ public static int longestKDistinct_Buggy(String s, int k) { Map<Character, Integer> window = new HashMap<>(); int left = 0, maxLen = 0; for (int right = 0; right < s.length(); right++) { char c = s.charAt(right); window.put(c, window.getOrDefault(c, 0) + 1); if (window.size() > k) { char leftChar = s.charAt(left); window.put(leftChar, window.get(leftChar) - 1); left++; } maxLen = Math.max(maxLen, right - left + 1); } return maxLen; } Your debugging: Bug 1: [Why is checking size > k once not enough?] Bug 2: [What happens if frequency becomes 0?] Fixes: [Fill in both fixes] Example to trace: Input: s = \"eceba\" , k = 2 Expected: 3 (substring \"ece\" or \"eba\") Trace window map state at each step: [Fill in] Click to verify your answers Bug 1: Should be while (window.size() > k) instead of if . We need to keep shrinking until we have at most k distinct characters. Bug 2: After decrementing frequency, we must remove the character from the map if frequency becomes 0. Otherwise, window.size() will never decrease! Correct: while (window.size() > k) { // Keep shrinking while invalid char leftChar = s.charAt(left); window.put(leftChar, window.get(leftChar) - 1); if (window.get(leftChar) == 0) { // Remove if frequency is 0 window.remove(leftChar); } left++; } Key mistake: Not removing keys from HashMap when their frequency reaches 0 is a common bug in sliding window problems with frequency tracking. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 6+ bugs across 4 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common sliding window mistakes to avoid Common mistakes you discovered: Off-by-one errors: [In loop bounds, especially building first window] Using IF instead of WHILE for shrinking: [Window doesn't shrink enough] Not removing from HashMap when frequency = 0: [Window size never decreases] Integer division errors: [Forgetting to cast to double for averages] [Add any other patterns you noticed] Decision Framework \u00b6 Your task: Build decision trees for when to use sliding window. Question 1: Is the subarray/substring contiguous? \u00b6 Answer after solving problems: Why contiguous matters? [Sliding window only works on contiguous data] Can sliding window work on non-contiguous? [No - need other techniques] Your observation: [Fill in based on testing] Question 2: Fixed vs Dynamic window? \u00b6 Answer for each pattern: Fixed window when: Window size: [Known constant k] Movement rule: [Always move both pointers together] Example problems: [Max average, nearby duplicate] Dynamic window when: Window size: [Varies based on constraint] Movement rule: [Expand right, shrink left when needed] Example problems: [Longest substring, min subarray sum] Question 3: What state to track? \u00b6 Answer for different scenarios: For sum/count problems: Track: [Running sum, count] Data structure: [Variables, no extra space] For unique elements: Track: [Set of current elements] Data structure: [HashSet] For frequency: Track: [Count of each element] Data structure: [HashMap or frequency array] Your Decision Tree \u00b6 Build this after solving practice problems: flowchart LR Start[\"Sliding Window Pattern Selection\"] Q1{\"Is subarray/substring contiguous?\"} Start --> Q1 N2[\"Use other technique<br/>(DP, backtracking)\"] Q1 -->|\"NO\"| N2 N3[\"Continue\"] Q1 -->|\"YES\"| N3 Q4{\"Is window size known?\"} Start --> Q4 N5([\"Use fixed window \u2713\"]) Q4 -->|\"YES (fixed k)\"| N5 N6([\"Use dynamic window \u2713\"]) Q4 -->|\"NO (find optimal)\"| N6 Q7{\"What to track in window?\"} Start --> Q7 N8([\"Variables O(1) space \u2713\"]) Q7 -->|\"Sum/Count\"| N8 N9([\"HashSet O(k) space \u2713\"]) Q7 -->|\"Unique elements\"| N9 N10([\"HashMap/Array O(k) space \u2713\"]) Q7 -->|\"Frequencies\"| N10 N11([\"Deque O(k) space \u2713\"]) Q7 -->|\"Maximum\"| N11 Q12{\"Shrink condition?\"} Start --> Q12 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 4): 643. Maximum Average Subarray I Pattern: [Fixed window] Your solution time: ___ Key insight: [Fill in after solving] 219. Contains Duplicate II Pattern: [Fixed window with HashSet] Your solution time: ___ Key insight: [Fill in] 1876. Substrings of Size Three with Distinct Characters Pattern: [Fixed window] Your solution time: ___ Key insight: [Fill in] 1456. Maximum Number of Vowels Pattern: [Fixed window] Your solution time: ___ Key insight: [Fill in] Medium (Complete 3-4): 3. Longest Substring Without Repeating Characters Pattern: [Dynamic window] Difficulty: [Rate 1-10] Key insight: [Fill in] Mistake made: [Fill in if any] 424. Longest Repeating Character Replacement Pattern: [Dynamic window with replacement] Difficulty: [Rate 1-10] Key insight: [Fill in] 438. Find All Anagrams in a String Pattern: [Fixed window with frequency] Difficulty: [Rate 1-10] Key insight: [Fill in] 567. Permutation in String Pattern: [Fixed window with frequency] Comparison to 438: [How similar?] Hard (Optional): 76. Minimum Window Substring Pattern: [Dynamic window with frequency] Key insight: [Fill in after solving] 239. Sliding Window Maximum Pattern: [Fixed window with deque] Key insight: [Monotonic deque technique] Review Checklist \u00b6 Before moving to the next topic: Implementation Fixed window: max average, nearby duplicate, max sliding window all work Dynamic window: longest substring, k distinct, min subarray sum all work String window: find anagrams, check inclusion, min window all work Hybrid: character replacement, max ones, fruit baskets all work All client code runs successfully Pattern Recognition Can identify fixed vs dynamic window Understand when to expand vs shrink Know what state to track in window Understand the contiguous requirement Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (empty, k > length) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use sliding window Can explain trade-offs vs other approaches Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand difference from two pointers Mastery Certification \u00b6 I certify that I can: Implement both fixed and dynamic window patterns from memory Explain when and why to use each pattern Identify the correct window type for new problems Track appropriate state (sum, frequency, set) in the window Analyze time and space complexity Debug common sliding window mistakes (if/while, HashMap cleanup, off-by-one) Compare trade-offs with brute force approaches Teach this concept to someone else","title":"02. Sliding Window"},{"location":"dsa/02-sliding-window/#sliding-window","text":"Optimize subarray/substring problems from O(n\u00b2) to O(n)","title":"Sliding Window"},{"location":"dsa/02-sliding-window/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is the sliding window pattern in one sentence? Your answer: [Fill in after implementation] How is it different from two pointers? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Sliding window is like a camera viewfinder moving across a scene...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] When does this pattern fail? Your answer: [Fill in after trying non-contiguous problems]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/02-sliding-window/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/02-sliding-window/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/02-sliding-window/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/02-sliding-window/#debugging-challenges","text":"Your task: Find and fix bugs in broken implementations. This tests your understanding of sliding window mechanics.","title":"Debugging Challenges"},{"location":"dsa/02-sliding-window/#decision-framework","text":"Your task: Build decision trees for when to use sliding window.","title":"Decision Framework"},{"location":"dsa/02-sliding-window/#practice","text":"","title":"Practice"},{"location":"dsa/02-sliding-window/#review-checklist","text":"Before moving to the next topic: Implementation Fixed window: max average, nearby duplicate, max sliding window all work Dynamic window: longest substring, k distinct, min subarray sum all work String window: find anagrams, check inclusion, min window all work Hybrid: character replacement, max ones, fruit baskets all work All client code runs successfully Pattern Recognition Can identify fixed vs dynamic window Understand when to expand vs shrink Know what state to track in window Understand the contiguous requirement Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (empty, k > length) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use sliding window Can explain trade-offs vs other approaches Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand difference from two pointers","title":"Review Checklist"},{"location":"dsa/03-hash-tables/","text":"Hash Tables \u00b6 O(1) average lookup, insertion, and deletion using key-value mapping ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is a hash table in one sentence? Your answer: [Fill in after implementation] Why is O(1) lookup possible? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Hash tables are like a library card catalog...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What happens when two keys hash to the same location? Your answer: [Fill in after learning about collisions] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Linear search through array to find if element exists: Time complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Hash table lookup to find if element exists: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Speedup calculation: If n = 1,000, linear search = n = _____ operations Hash table lookup = _____ operations (average case) Speedup factor: _____ times faster Scenario Predictions \u00b6 Scenario 1: Count frequency of each word in [\"cat\", \"dog\", \"cat\", \"bird\", \"dog\", \"cat\"] Can you use a hash map? [Yes/No - Why?] What would be the key? [Fill in] What would be the value? [Fill in] What's the final map for \"cat\"? [Key: \"cat\", Value: ?] Scenario 2: Find two numbers that sum to 10 in [2, 7, 11, 15] Can you use a hash map? [Yes/No - Why?] What would you store in the map? [Fill in] How does hash map help vs nested loops? [Fill in] Scenario 3: Group anagrams: [\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"] What makes two strings anagrams? [Fill in] What should be the map key? [Fill in your idea] How to create the key from \"eat\"? [Fill in] Hash Collision Quiz \u00b6 Question: What happens when two different keys hash to the same location? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: Why is HashMap lookup O(1) average but O(n) worst case? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Trade-off Quiz \u00b6 Question: When would sorting + binary search be BETTER than using a HashMap? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN advantage of HashMap over arrays? Always uses less memory Can use non-integer keys Maintains sorted order Always faster for all operations Verify after implementation: [Which one(s)?] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example: Two Sum Problem \u00b6 Problem: Find two numbers in an array that sum to a target value. Approach 1: Brute Force (Nested Loops) \u00b6 // Naive approach - Check all possible pairs public static int[] twoSum_BruteForce(int[] nums, int target) { for (int i = 0; i < nums.length; i++) { for (int j = i + 1; j < nums.length; j++) { if (nums[i] + nums[j] == target) { return new int[] {i, j}; } } } return new int[] {-1, -1}; } Analysis: Time: O(n\u00b2) - For each element, check all remaining elements Space: O(1) - No extra space For n = 10,000: ~100,000,000 comparisons Approach 2: HashMap (Optimized) \u00b6 // Optimized approach - Use HashMap for O(1) lookup public static int[] twoSum_HashMap(int[] nums, int target) { Map<Integer, Integer> map = new HashMap<>(); for (int i = 0; i < nums.length; i++) { int complement = target - nums[i]; if (map.containsKey(complement)) { return new int[] {map.get(complement), i}; } map.put(nums[i], i); } return new int[] {-1, -1}; } Analysis: Time: O(n) - Single pass through array, O(1) lookups Space: O(n) - Store up to n elements in map For n = 10,000: ~10,000 operations Performance Comparison \u00b6 Array Size Brute Force (O(n\u00b2)) HashMap (O(n)) Speedup n = 100 10,000 ops 100 ops 100x n = 1,000 1,000,000 ops 1,000 ops 1,000x n = 10,000 100,000,000 ops 10,000 ops 10,000x Your calculation: For n = 5,000, the speedup is approximately _____ times faster. Why Does HashMap Work? \u00b6 Key insight to understand: In array [2, 7, 11, 15] looking for sum = 9: Step 1: num=2, complement=7, map={} \u2192 not found, add 2\u21920 Step 2: num=7, complement=2, map={2\u21920} \u2192 FOUND! Return [0, 1] Why is this faster? Instead of checking every pair (2,7), (2,11), (2,15), (7,11), (7,15), (11,15) We check each number once and use O(1) lookup to find its complement HashMap eliminates the need for the inner loop! After implementing, explain in your own words: Why does O(1) lookup matter? [Your answer] What's the space/time trade-off? [Your answer] Example: Finding Duplicates \u00b6 Problem: Check if an array contains any duplicate values. Approach 1: Linear Search for Each Element \u00b6 // Naive approach - For each element, search rest of array public static boolean containsDuplicate_LinearSearch(int[] nums) { for (int i = 0; i < nums.length; i++) { for (int j = i + 1; j < nums.length; j++) { if (nums[i] == nums[j]) { return true; } } } return false; } Analysis: Time: O(n\u00b2) - Nested loops Space: O(1) - No extra space Approach 2: HashSet (Optimized) \u00b6 // Optimized approach - Use HashSet for O(1) membership test public static boolean containsDuplicate_HashSet(int[] nums) { Set<Integer> seen = new HashSet<>(); for (int num : nums) { if (seen.contains(num)) { return true; // Found duplicate! } seen.add(num); } return false; } Analysis: Time: O(n) - Single pass, O(1) contains/add operations Space: O(n) - Store up to n elements Performance Comparison \u00b6 Array Size Linear Search (O(n\u00b2)) HashSet (O(n)) Speedup n = 100 10,000 ops 100 ops 100x n = 1,000 1,000,000 ops 1,000 ops 1,000x n = 10,000 100,000,000 ops 10,000 ops 10,000x Key insight: HashSet remembers what we've seen in O(1) time No need to repeatedly search through previous elements Trade memory for speed! After implementing, answer: When is the space trade-off worth it? [Your answer] When might you prefer the O(1) space solution? [Your answer] Core Implementation \u00b6 Pattern 1: Basic Hash Map Operations \u00b6 Concept: Fast lookups using key-value pairs. Use case: Frequency counting, two-sum problems, duplicate detection. public class BasicHashMap { /** * Problem: Two Sum - find indices of two numbers that sum to target * Time: O(n), Space: O(n) * * TODO: Implement using HashMap for O(1) lookup */ public static int[] twoSum(int[] nums, int target) { // TODO: Use a map to remember what you've seen // What should be stored as the key? As the value? return new int[] {-1, -1}; // Replace with implementation } /** * Problem: Count frequency of each character * Time: O(n), Space: O(k) where k = unique characters * * TODO: Implement frequency counter */ public static Map<Character, Integer> countCharacters(String s) { // TODO: Track how many times each character appears // Consider using getOrDefault for cleaner code return new HashMap<>(); // Replace with implementation } /** * Problem: Contains duplicate - check if array has duplicates * Time: O(n), Space: O(n) * * TODO: Implement using HashSet */ public static boolean containsDuplicate(int[] nums) { // TODO: Use a set to track seen elements // What indicates a duplicate has been found? return false; // Replace with implementation } } Runnable Client Code: import java.util.*; public class BasicHashMapClient { public static void main(String[] args) { System.out.println(\"=== Basic Hash Map Operations ===\\n\"); // Test 1: Two Sum System.out.println(\"--- Test 1: Two Sum ---\"); int[] nums = {2, 7, 11, 15}; int target = 9; int[] result = BasicHashMap.twoSum(nums, target); System.out.printf(\"Array: %s%n\", Arrays.toString(nums)); System.out.printf(\"Target: %d%n\", target); System.out.printf(\"Indices: %s%n\", Arrays.toString(result)); if (result[0] != -1) { System.out.printf(\"Values: %d + %d = %d%n\", nums[result[0]], nums[result[1]], target); } // Test 2: Character frequency System.out.println(\"\\n--- Test 2: Character Frequency ---\"); String[] testStrings = {\"hello\", \"mississippi\", \"aabbcc\"}; for (String s : testStrings) { Map<Character, Integer> freq = BasicHashMap.countCharacters(s); System.out.printf(\"String: \\\"%s\\\"%n\", s); System.out.println(\"Frequency: \" + freq); System.out.println(); } // Test 3: Contains duplicate System.out.println(\"--- Test 3: Contains Duplicate ---\"); int[][] testArrays = { {1, 2, 3, 4, 5}, {1, 2, 3, 1}, {1, 1, 1, 3, 3, 4, 3, 2, 4, 2} }; for (int[] arr : testArrays) { boolean hasDup = BasicHashMap.containsDuplicate(arr); System.out.printf(\"Array: %s -> %s%n\", Arrays.toString(arr), hasDup ? \"HAS duplicates\" : \"NO duplicates\"); } } } Pattern 2: Hash Set for Fast Membership Testing \u00b6 Concept: Use HashSet for O(1) membership checks. Use case: Finding missing numbers, intersection/union operations. import java.util.*; public class HashSetOperations { /** * Problem: Find intersection of two arrays * Time: O(n + m), Space: O(min(n, m)) * * TODO: Implement using HashSet */ public static int[] intersection(int[] nums1, int[] nums2) { // TODO: Store one array in a set for fast lookup // How do you find common elements? return new int[0]; // Replace with implementation } /** * Problem: Find missing number from 0 to n * Time: O(n), Space: O(n) * * TODO: Implement using HashSet */ public static int missingNumber(int[] nums) { // TODO: Store all present numbers // How do you check which number is missing? return -1; // Replace with implementation } /** * Problem: Longest consecutive sequence * Time: O(n), Space: O(n) * * TODO: Implement using HashSet */ public static int longestConsecutive(int[] nums) { // TODO: Store all numbers in a set for O(1) lookup // How do you identify the start of a sequence? // How do you count consecutive numbers? return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class HashSetOperationsClient { public static void main(String[] args) { System.out.println(\"=== Hash Set Operations ===\\n\"); // Test 1: Intersection System.out.println(\"--- Test 1: Intersection ---\"); int[] arr1 = {1, 2, 2, 1}; int[] arr2 = {2, 2}; int[] intersection = HashSetOperations.intersection(arr1, arr2); System.out.printf(\"Array 1: %s%n\", Arrays.toString(arr1)); System.out.printf(\"Array 2: %s%n\", Arrays.toString(arr2)); System.out.printf(\"Intersection: %s%n\", Arrays.toString(intersection)); int[] arr3 = {4, 9, 5}; int[] arr4 = {9, 4, 9, 8, 4}; int[] intersection2 = HashSetOperations.intersection(arr3, arr4); System.out.printf(\"\\nArray 1: %s%n\", Arrays.toString(arr3)); System.out.printf(\"Array 2: %s%n\", Arrays.toString(arr4)); System.out.printf(\"Intersection: %s%n\", Arrays.toString(intersection2)); // Test 2: Missing number System.out.println(\"\\n--- Test 2: Missing Number ---\"); int[][] testArrays = { {3, 0, 1}, {0, 1}, {9, 6, 4, 2, 3, 5, 7, 0, 1} }; for (int[] arr : testArrays) { int missing = HashSetOperations.missingNumber(arr); System.out.printf(\"Array: %s -> Missing: %d%n\", Arrays.toString(arr), missing); } // Test 3: Longest consecutive sequence System.out.println(\"\\n--- Test 3: Longest Consecutive ---\"); int[][] sequenceArrays = { {100, 4, 200, 1, 3, 2}, {0, 3, 7, 2, 5, 8, 4, 6, 0, 1}, {9, 1, -3, 2, 4, 8, 3, -1, 6, -2, -4, 7} }; for (int[] arr : sequenceArrays) { int length = HashSetOperations.longestConsecutive(arr); System.out.printf(\"Array: %s%n\", Arrays.toString(arr)); System.out.printf(\"Longest consecutive: %d%n%n\", length); } } } Pattern 3: Hash Map for Grouping \u00b6 Concept: Group elements by a computed key. Use case: Anagrams, group by property, categorization. import java.util.*; public class HashMapGrouping { /** * Problem: Group anagrams together * Time: O(n * k log k) where k = max word length, Space: O(n * k) * * TODO: Implement using HashMap with sorted string as key */ public static List<List<String>> groupAnagrams(String[] strs) { // TODO: What makes anagrams have the same key? // How can you transform each string into a unique key? return new ArrayList<>(); // Replace with implementation } /** * Problem: Group numbers by digit sum * Time: O(n * d) where d = digits, Space: O(n) * * TODO: Implement custom grouping */ public static Map<Integer, List<Integer>> groupByDigitSum(int[] nums) { // TODO: Compute a key for each number based on its digits // Group numbers with the same key together return new HashMap<>(); // Replace with implementation } /** * Problem: Find all strings that start with same character * Time: O(n), Space: O(n) * * TODO: Implement grouping by first character */ public static Map<Character, List<String>> groupByFirstChar(String[] words) { // TODO: Extract the grouping criterion from each word // Store words with the same criterion together return new HashMap<>(); // Replace with implementation } // Helper: Calculate digit sum private static int digitSum(int n) { int sum = 0; n = Math.abs(n); while (n > 0) { sum += n % 10; n /= 10; } return sum; } } Runnable Client Code: import java.util.*; public class HashMapGroupingClient { public static void main(String[] args) { System.out.println(\"=== Hash Map Grouping ===\\n\"); // Test 1: Group anagrams System.out.println(\"--- Test 1: Group Anagrams ---\"); String[] words = {\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"}; List<List<String>> groups = HashMapGrouping.groupAnagrams(words); System.out.println(\"Words: \" + Arrays.toString(words)); System.out.println(\"Grouped:\"); for (List<String> group : groups) { System.out.println(\" \" + group); } // Test 2: Group by digit sum System.out.println(\"\\n--- Test 2: Group by Digit Sum ---\"); int[] numbers = {12, 21, 13, 31, 100, 10, 1, 23, 32}; Map<Integer, List<Integer>> digitGroups = HashMapGrouping.groupByDigitSum(numbers); System.out.println(\"Numbers: \" + Arrays.toString(numbers)); System.out.println(\"Grouped by digit sum:\"); for (Map.Entry<Integer, List<Integer>> entry : digitGroups.entrySet()) { System.out.printf(\" Sum %d: %s%n\", entry.getKey(), entry.getValue()); } // Test 3: Group by first character System.out.println(\"\\n--- Test 3: Group by First Character ---\"); String[] dictionary = {\"apple\", \"ant\", \"ball\", \"bear\", \"cat\", \"car\", \"dog\"}; Map<Character, List<String>> charGroups = HashMapGrouping.groupByFirstChar(dictionary); System.out.println(\"Words: \" + Arrays.toString(dictionary)); System.out.println(\"Grouped by first character:\"); for (Map.Entry<Character, List<String>> entry : charGroups.entrySet()) { System.out.printf(\" %c: %s%n\", entry.getKey(), entry.getValue()); } } } Pattern 4: Hash Map for Sliding Window with Constraints \u00b6 Concept: Track window state using frequency map. Use case: Substring problems with character constraints. import java.util.*; public class HashMapWindow { /** * Problem: Minimum window substring containing all chars of target * Time: O(n + m), Space: O(k) where k = unique chars * * TODO: Implement using HashMap to track frequencies */ public static String minWindow(String s, String t) { // TODO: Track character frequencies in the target string // Use a sliding window to find the minimum valid window return \"\"; // Replace with implementation } /** * Problem: Check if s2 contains permutation of s1 * Time: O(n), Space: O(1) - only 26 chars * * TODO: Implement using frequency comparison */ public static boolean checkInclusion(String s1, String s2) { // TODO: How can you detect a permutation using frequencies? // Consider using a fixed-size window return false; // Replace with implementation } } Runnable Client Code: import java.util.*; public class HashMapWindowClient { public static void main(String[] args) { System.out.println(\"=== Hash Map Sliding Window ===\\n\"); // Test 1: Minimum window substring System.out.println(\"--- Test 1: Minimum Window ---\"); String[][] testCases = { {\"ADOBECODEBANC\", \"ABC\"}, {\"a\", \"a\"}, {\"a\", \"aa\"} }; for (String[] test : testCases) { String s = test[0]; String t = test[1]; String result = HashMapWindow.minWindow(s, t); System.out.printf(\"s=\\\"%s\\\", t=\\\"%s\\\" -> \\\"%s\\\"%n\", s, t, result); } // Test 2: Check inclusion (permutation) System.out.println(\"\\n--- Test 2: Check Inclusion ---\"); String[][] inclusionTests = { {\"ab\", \"eidbaooo\"}, {\"ab\", \"eidboaoo\"}, {\"abc\", \"ccccbbbbaaaa\"} }; for (String[] test : inclusionTests) { String s1 = test[0]; String s2 = test[1]; boolean result = HashMapWindow.checkInclusion(s1, s2); System.out.printf(\"s1=\\\"%s\\\", s2=\\\"%s\\\" -> %b%n\", s1, s2, result); } } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken implementations. This tests your understanding of hash tables. Challenge 1: Broken Two Sum with HashMap \u00b6 /** * This code is supposed to find two numbers that sum to target. * It has 2 BUGS. Find them! */ public static int[] twoSum_Buggy(int[] nums, int target) { Map<Integer, Integer> map = new HashMap<>(); for (int i = 0; i < nums.length; i++) { map.put(nums[i], i); } for (int i = 0; i < nums.length; i++) { int complement = target - nums[i]; if (map.containsKey(complement)) { return new int[] {map.get(complement), i}; } } return new int[] {-1, -1}; } Your debugging: Bug 1: [What\\'s the bug?] Bug 2 location: [Which line?] Bug 2 explanation: [What if nums[i] + nums[i] = target? Same index used twice!] Bug 2 fix: [How to check indices are different?] Test case to expose Bug 2: Input: nums = [3, 2, 4], target = 6 Expected: [1, 2] (indices of 2 and 4) Buggy output: [What happens if we check 3+3?] Click to verify your answers Bug 1: Two separate loops are inefficient (though not technically wrong). Better to check and add in single loop. Bug 2: Could return [i, i] if the same element appears twice. Fix: if (map.containsKey(complement) && map.get(complement) != i) { return new int[] {map.get(complement), i}; } map.put(nums[i], i); Better solution - check BEFORE adding: for (int i = 0; i < nums.length; i++) { int complement = target - nums[i]; if (map.containsKey(complement)) { return new int[] {map.get(complement), i}; } map.put(nums[i], i); // Add after checking } Challenge 2: Broken Frequency Counter \u00b6 /** * Count character frequencies. * This has 1 NULL POINTER BUG. */ public static Map<Character, Integer> countChars_Buggy(String s) { Map<Character, Integer> freq = new HashMap<>(); for (char c : s.toCharArray()) { int count = freq.get(c); freq.put(c, count + 1); } return freq; } Your debugging: Bug: [What\\'s the bug?] Test case: Input: \"hello\" Expected: {h=1, e=1, l=2, o=1} Buggy output: [What exception?] Click to verify your answers Bug: freq.get(c) returns null for first occurrence, causing NullPointerException when adding 1. Fix Option 1 - Use getOrDefault: int count = freq.getOrDefault(c, 0); freq.put(c, count + 1); Fix Option 2 - Check containsKey: if (freq.containsKey(c)) { freq.put(c, freq.get(c) + 1); } else { freq.put(c, 1); } Fix Option 3 - Use compute: freq.compute(c, (key, val) -> val == null ? 1 : val + 1); Challenge 3: Broken Group Anagrams \u00b6 /** * Group anagrams together. * This has a LOGIC BUG with the key generation. */ public static List<List<String>> groupAnagrams_Buggy(String[] strs) { Map<String, List<String>> groups = new HashMap<>(); for (String s : strs) { String key = s.toLowerCase(); if (!groups.containsKey(key)) { groups.put(key, new ArrayList<>()); } groups.get(key).add(s); } return new ArrayList<>(groups.values()); } Your debugging: Bug: [What\\'s the bug?] Test case: Input: [\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"] Expected: [[\"eat\", \"tea\", \"ate\"], [\"tan\", \"nat\"], [\"bat\"]] Buggy output: [Each word in its own group!] Click to verify your answer Bug: Lowercase doesn't make anagrams have the same key. \"eat\" and \"tea\" are different when lowercased. Fix - Sort characters: char[] chars = s.toCharArray(); Arrays.sort(chars); String key = new String(chars); Now \"eat\", \"tea\", and \"ate\" all become \"aet\" when sorted! Challenge 4: Hash Collision Awareness \u00b6 /** * This code works but has PERFORMANCE issues due to collisions. * Identify the problem. */ public static class BadHashCode { private String name; private int age; @Override public int hashCode() { return 42; } @Override public boolean equals(Object o) { if (!(o instanceof BadHashCode)) return false; BadHashCode other = (BadHashCode) o; return this.name.equals(other.name) && this.age == other.age; } } Your debugging: Bug: [What's wrong with always returning 42?] Performance impact: [What's the time complexity now?] Explanation: [How does HashMap work with this hashCode?] Real-world scenario: HashMap with 10,000 BadHashCode objects Expected lookup: O(1) Actual lookup: [What happens?] Click to verify your understanding Problem: All objects hash to the same bucket (42), creating a massive collision. Performance: HashMap degrades to O(n) for all operations because all entries are in one linked list/tree. Why it's bad: HashMap has many buckets (default 16, grows to thousands) All 10,000 objects go into ONE bucket Lookup requires scanning through all 10,000 objects Defeats the entire purpose of hashing! Correct implementation: @Override public int hashCode() { return Objects.hash(name, age); } Key lesson: Good hash functions distribute objects evenly across buckets. Challenge 5: Missing Null Checks \u00b6 /** * Find intersection of two arrays. * This has MULTIPLE null-safety bugs. */ public static int[] intersection_Buggy(int[] nums1, int[] nums2) { Set<Integer> set1 = new HashSet<>(); for (int num : nums1) { set1.add(num); } Set<Integer> result = new HashSet<>(); for (int num : nums2) { if (set1.contains(num)) { result.add(num); } } return result.stream().mapToInt(i -> i).toArray();} Your debugging: Bug 1: [What happens if nums1 is null?] Bug 2: [What happens if nums2 is null?] Bug 3: [Is this actually a bug? Empty result valid?] Fixes: Should you check for null? Return empty array? Throw exception? [Your answer] [Your defensive programming strategy] Click to verify your approach Bugs 1 & 2: NullPointerException if either array is null. Fix - Add null checks: public static int[] intersection_Fixed(int[] nums1, int[] nums2) { if (nums1 == null || nums2 == null) { return new int[0]; // or throw IllegalArgumentException } // ... rest of implementation } Bug 3: Not actually a bug - empty result is valid when there's no intersection. Design decision: Return empty array: Easier for caller, no exception handling Throw exception: Fail fast, makes null input a programmer error Which is better? Depends on your API design philosophy! Challenge 6: Longest Consecutive Sequence - Off By One \u00b6 /** * Find longest consecutive sequence. * This has a SUBTLE off-by-one bug. */ public static int longestConsecutive_Buggy(int[] nums) { Set<Integer> numSet = new HashSet<>(); for (int num : nums) { numSet.add(num); } int maxLength = 0; for (int num : numSet) { if (!numSet.contains(num - 1)) { // Start of sequence int currentNum = num; int currentLength = 1; while (numSet.contains(currentNum + 1)) { currentNum++; currentLength++; } maxLength = Math.max(maxLength, currentLength); } } return maxLength;} Your debugging: Bug: [What's returned for empty array?] Expected: [What should be returned?] Fix: [Is 0 correct for empty array? Or should it be different?] Test cases: Input: [] \u2192 Expected: 0 or -1? [Your decision] Input: [100, 4, 200, 1, 3, 2] \u2192 Expected: 4 (sequence: 1,2,3,4) Click to verify Bug: For empty array, returns 0. Is this a bug? Answer: Depends on problem specification! Returning 0 is often correct (no elements = sequence length 0) Some problems might expect -1 or throw exception The code is actually CORRECT! This was a trick question to make you think about edge cases. Real lesson: Always verify edge case behavior matches problem requirements. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found the two-sum same-index bug Fixed null pointer in frequency counter (knew 2+ solutions) Corrected anagram key generation (sorted characters) Understood hash collision performance impact Added null checks for defensive programming Analyzed edge cases (empty array behavior) Common hash table mistakes you discovered: [List the patterns you noticed] [Fill in] [Fill in] Best practices learned: [When to use getOrDefault vs containsKey?] [Why check before adding to map in some cases?] [How to handle null inputs?] Decision Framework \u00b6 Your task: Build decision trees for when to use hash tables. Question 1: What operation do you need? \u00b6 Answer after solving problems: Fast lookup by key? [When to use HashMap vs array?] Fast membership test? [When to use HashSet?] Frequency counting? [Why is HashMap ideal?] Your observation: [Fill in based on testing] Question 2: What are the time/space trade-offs? \u00b6 Answer for each pattern: HashMap for lookups: Time complexity: [Average case? Worst case?] Space complexity: [How much extra space?] Best use cases: [List problems you solved] HashSet for membership: Time complexity: [Compare to linear search] Space complexity: [Trade-off worth it when?] Best use cases: [List problems you solved] HashMap for grouping: Time complexity: [Depends on what?] Space complexity: [How to estimate?] Best use cases: [List problems you solved] Your Decision Tree \u00b6 Build this after solving practice problems: flowchart LR Start[\"Hash Table Pattern Selection\"] Q1{\"What do you need?\"} Start --> Q1 Q2{\"Fast lookup by key?\"} Q3{\"Fast membership test?\"} Q4{\"Count frequencies?\"} Q5{\"Group by property?\"} Q6{\"Track window state?\"} Q7{\"Space constraint?\"} Start --> Q7 N8[\"Consider alternatives\"] Q7 -->|\"Yes\"| N8 N9[\"Hash table is usually best choice\"] Q7 -->|\"No\"| N9 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 4): 1. Two Sum Pattern: [Which one?] Your solution time: ___ Key insight: [Fill in after solving] 217. Contains Duplicate Pattern: [Which one?] Your solution time: ___ Key insight: [Fill in] 242. Valid Anagram Pattern: [Which one?] Your solution time: ___ Key insight: [Fill in] 349. Intersection of Two Arrays Pattern: [Which one?] Your solution time: ___ Key insight: [Fill in] Medium (Complete 3-4): 49. Group Anagrams Pattern: [Which one?] Difficulty: [Rate 1-10] Key insight: [Fill in] Mistake made: [Fill in if any] 128. Longest Consecutive Sequence Pattern: [Which one?] Difficulty: [Rate 1-10] Key insight: [Fill in] 560. Subarray Sum Equals K Pattern: [Which one?] Difficulty: [Rate 1-10] Key insight: [Prefix sum + HashMap] 387. First Unique Character in a String Pattern: [Which one?] Comparison: [Two-pass vs one-pass?] Hard (Optional): 76. Minimum Window Substring Pattern: [Sliding window + HashMap] Key insight: [Fill in after solving] 30. Substring with Concatenation of All Words Pattern: [Which variant?] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Basic HashMap: two sum, frequency counting, contains duplicate all work HashSet: intersection, missing number, longest consecutive all work Grouping: group anagrams, custom grouping all work Window: minimum window substring, check inclusion work All client code runs successfully Pattern Recognition Can identify when to use HashMap vs HashSet Understand frequency counting pattern Understand grouping pattern Know when hash table beats other approaches Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Understood collision handling (conceptually) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use hash tables Can explain trade-offs vs other approaches Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand hash table internals (basic level) Mastery Certification \u00b6 I certify that I can: Implement HashMap and HashSet patterns from memory Explain when and why to use each data structure Identify the correct pattern for new problems Analyze time and space complexity (including collision impact) Compare trade-offs with alternative approaches Debug common hash table mistakes (null checks, collisions, key design) Teach this concept to someone else using analogies Design real-world systems using hash tables","title":"03. Hash Tables"},{"location":"dsa/03-hash-tables/#hash-tables","text":"O(1) average lookup, insertion, and deletion using key-value mapping","title":"Hash Tables"},{"location":"dsa/03-hash-tables/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is a hash table in one sentence? Your answer: [Fill in after implementation] Why is O(1) lookup possible? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Hash tables are like a library card catalog...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What happens when two keys hash to the same location? Your answer: [Fill in after learning about collisions]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/03-hash-tables/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/03-hash-tables/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/03-hash-tables/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/03-hash-tables/#debugging-challenges","text":"Your task: Find and fix bugs in broken implementations. This tests your understanding of hash tables.","title":"Debugging Challenges"},{"location":"dsa/03-hash-tables/#decision-framework","text":"Your task: Build decision trees for when to use hash tables.","title":"Decision Framework"},{"location":"dsa/03-hash-tables/#practice","text":"","title":"Practice"},{"location":"dsa/03-hash-tables/#review-checklist","text":"Before moving to the next topic: Implementation Basic HashMap: two sum, frequency counting, contains duplicate all work HashSet: intersection, missing number, longest consecutive all work Grouping: group anagrams, custom grouping all work Window: minimum window substring, check inclusion work All client code runs successfully Pattern Recognition Can identify when to use HashMap vs HashSet Understand frequency counting pattern Understand grouping pattern Know when hash table beats other approaches Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Understood collision handling (conceptually) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use hash tables Can explain trade-offs vs other approaches Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand hash table internals (basic level)","title":"Review Checklist"},{"location":"dsa/04-linked-lists/","text":"Linked Lists \u00b6 Pointer manipulation for reversing, detecting cycles, and merging lists ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is a linked list in one sentence? Your answer: [Fill in after implementation] Why can't we use array indices? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Linked lists are like a treasure hunt where each clue leads to the next...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What's the key difference between singly and doubly linked lists? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Reversing a linked list iteratively: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified after learning: [Actual] Reversing a linked list recursively: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual - why different from iterative?] Finding middle of linked list: If you traverse once to count, then again to middle: [O(?)] Using slow/fast pointers: [O(?)] Speedup: [How much better?] Scenario Predictions \u00b6 Scenario 1: Reverse list 1 -> 2 -> 3 -> 4 -> 5 How many pointers needed? [Fill in: prev, curr, next - why each?] Initial state: prev = , curr = , next = ___ After first iteration: List becomes [Draw it: ? -> ? -> ? -> ?] What's the new head? [Which pointer points to it?] Scenario 2: Detect cycle in 1 -> 2 -> 3 -> 4 -> 2 (cycle back to 2) Can you use fast/slow pointers? [Yes/No - Why?] Starting positions: slow = , fast = Will they ever meet? [Yes/No - Why/Why not?] Where will they meet? [At which node?] Scenario 3: Remove 2nd node from end in 1 -> 2 -> 3 -> 4 -> 5 Which pattern applies? [Two pointers with gap - why?] Gap size needed: [How far apart should pointers be?] Result should be: [Which nodes remain?] Trade-off Quiz \u00b6 Question: When would recursion be WORSE than iteration for reversing? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning - consider stack space] Question: What's the MAIN advantage of linked lists over arrays? Faster random access Less memory usage O(1) insertion/deletion at known position Better cache locality Verify after implementation: [Which one(s)?] Before/After: Why This Pattern Matters \u00b6 Your task: Compare array operations vs linked list operations to understand trade-offs. Example 1: Insertion at Beginning \u00b6 Problem: Insert element at the beginning of a collection. Approach 1: Array (Dynamic) \u00b6 // Array approach - Shift all elements public static int[] insertAtBeginning_Array(int[] arr, int value) { int[] newArr = new int[arr.length + 1]; newArr[0] = value; // Copy all existing elements (shifted right) for (int i = 0; i < arr.length; i++) { newArr[i + 1] = arr[i]; } return newArr; } Analysis: Time: O(n) - Must shift all elements Space: O(n) - Need new array For n = 10,000: ~10,000 copy operations Approach 2: Linked List \u00b6 // Linked list approach - Just update pointers public static ListNode insertAtBeginning_List(ListNode head, int value) { ListNode newNode = new ListNode(value); newNode.next = head; return newNode; // New head } Analysis: Time: O(1) - Just two pointer assignments Space: O(1) - Single new node For n = 10,000: Always 2 operations (constant!) Performance Comparison \u00b6 Collection Size Array (O(n)) Linked List (O(1)) Speedup n = 100 100 ops 2 ops 50x n = 1,000 1,000 ops 2 ops 500x n = 10,000 10,000 ops 2 ops 5,000x Your calculation: For n = 5,000, inserting at beginning is _____ times faster with linked list. Example 2: Access Middle Element \u00b6 Problem: Access the middle element of a collection. Approach 1: Array \u00b6 // Array approach - Direct access public static int getMiddle_Array(int[] arr) { return arr[arr.length / 2]; // O(1) access! } Analysis: Time: O(1) - Direct index access Space: O(1) Operations: 1 (always!) Approach 2: Linked List (Naive) \u00b6 // Linked list approach - Must traverse public static int getMiddle_List_Naive(ListNode head) { // First pass: count nodes int count = 0; ListNode curr = head; while (curr != null) { count++; curr = curr.next; } // Second pass: traverse to middle curr = head; for (int i = 0; i < count / 2; i++) { curr = curr.next; } return curr.val; } Analysis: Time: O(n) - Must traverse to middle Space: O(1) For n = 10,000: ~15,000 steps (count + traverse to middle) Approach 3: Linked List (Optimized with Fast/Slow) \u00b6 // Optimized: slow/fast pointers - single pass public static int getMiddle_List_Optimized(ListNode head) { ListNode slow = head, fast = head; while (fast != null && fast.next != null) { slow = slow.next; // Move 1 step fast = fast.next.next; // Move 2 steps } return slow.val; // Slow is at middle when fast reaches end } Analysis: Time: O(n) - Single pass Space: O(1) For n = 10,000: ~5,000 steps (half as many as naive) Performance Comparison \u00b6 Collection Size Array (O(1)) List Naive (O(n)) List Optimized (O(n)) n = 100 1 op 150 ops 50 ops n = 1,000 1 op 1,500 ops 500 ops n = 10,000 1 op 15,000 ops 5,000 ops Key insight: Arrays win for access, linked lists win for insertion/deletion. Why Does Pointer Manipulation Work? \u00b6 Key insight to understand: When reversing 1 -> 2 -> 3 -> null : Initial: 1 -> 2 -> 3 -> null ^ head Goal: null <- 1 <- 2 <- 3 ^ new head How? Change each 'next' pointer to point backwards! Step-by-step visualization: Step 0: prev = null, curr = 1 null 1 -> 2 -> 3 -> null ^ ^ prev curr Step 1: Save next, reverse curr.next, move forward null <- 1 2 -> 3 -> null ^ ^ prev curr Step 2: Continue null <- 1 <- 2 3 -> null ^ ^ prev curr Step 3: Continue null <- 1 <- 2 <- 3 null ^ prev (new head!) After implementing, explain in your own words: Why do we need three pointers (prev, curr, next)? [Your answer] What happens if we skip saving 'next'? [Your answer] Why does prev end up as the new head? [Your answer] Core Implementation \u00b6 Pattern 1: Reverse Linked List \u00b6 Concept: Change direction of next pointers. Use case: Reverse entire list, reverse in groups, reverse between positions. public class ReverseLinkedList { static class ListNode { int val; ListNode next; ListNode(int val) { this.val = val; } } /** * Problem: Reverse entire linked list * Time: O(n), Space: O(1) * * TODO: Implement iteratively using three pointers */ public static ListNode reverseList(ListNode head) { // TODO: Initialize pointers/variables // TODO: Implement iteration/conditional logic // TODO: Return result return null; // Replace with implementation } /** * Problem: Reverse linked list recursively * Time: O(n), Space: O(n) due to recursion stack * * TODO: Implement recursive reversal */ public static ListNode reverseListRecursive(ListNode head) { // TODO: Handle base case // TODO: Recursively reverse rest: newHead = reverse(head.next) // TODO: Update state // TODO: Set head.next = null // TODO: Return newHead return null; // Replace with implementation } /** * Problem: Reverse first K nodes * Time: O(k), Space: O(1) * * TODO: Implement partial reversal */ public static ListNode reverseFirstK(ListNode head, int k) { // TODO: Similar to reverseList but count k steps // TODO: Return new head and keep rest unchanged return null; // Replace with implementation } // Helper: Create list from array static ListNode createList(int[] values) { if (values.length == 0) return null; ListNode head = new ListNode(values[0]); ListNode current = head; for (int i = 1; i < values.length; i++) { current.next = new ListNode(values[i]); current = current.next; } return head; } // Helper: Print list static void printList(ListNode head) { ListNode current = head; while (current != null) { System.out.print(current.val); if (current.next != null) System.out.print(\" -> \"); current = current.next; } System.out.println(); } } Runnable Client Code: public class ReverseLinkedListClient { public static void main(String[] args) { System.out.println(\"=== Reverse Linked List ===\\n\"); // Test 1: Reverse entire list System.out.println(\"--- Test 1: Reverse Entire List ---\"); int[] values = {1, 2, 3, 4, 5}; ListNode list = ReverseLinkedList.createList(values); System.out.print(\"Original: \"); ReverseLinkedList.printList(list); ListNode reversed = ReverseLinkedList.reverseList(list); System.out.print(\"Reversed: \"); ReverseLinkedList.printList(reversed); // Test 2: Reverse recursively System.out.println(\"\\n--- Test 2: Reverse Recursively ---\"); ListNode list2 = ReverseLinkedList.createList(new int[]{1, 2, 3, 4, 5}); System.out.print(\"Original: \"); ReverseLinkedList.printList(list2); ListNode reversedRec = ReverseLinkedList.reverseListRecursive(list2); System.out.print(\"Reversed: \"); ReverseLinkedList.printList(reversedRec); // Test 3: Reverse first K System.out.println(\"\\n--- Test 3: Reverse First K Nodes ---\"); ListNode list3 = ReverseLinkedList.createList(new int[]{1, 2, 3, 4, 5, 6, 7}); int k = 3; System.out.print(\"Original: \"); ReverseLinkedList.printList(list3); System.out.println(\"K = \" + k); ListNode reversedK = ReverseLinkedList.reverseFirstK(list3, k); System.out.print(\"Result: \"); ReverseLinkedList.printList(reversedK); } } Pattern 2: Cycle Detection \u00b6 Concept: Use Floyd's cycle detection (slow/fast pointers). Use case: Detect cycle, find cycle start, remove cycle. public class CycleDetection { static class ListNode { int val; ListNode next; ListNode(int val) { this.val = val; } } /** * Problem: Detect if linked list has a cycle * Time: O(n), Space: O(1) * * TODO: Implement Floyd's cycle detection */ public static boolean hasCycle(ListNode head) { // TODO: Initialize pointers/variables // TODO: Implement iteration/conditional logic // TODO: Return false return false; // Replace with implementation } /** * Problem: Find the node where cycle begins * Time: O(n), Space: O(1) * * TODO: Implement cycle start detection */ public static ListNode detectCycle(ListNode head) { // TODO: First detect if cycle exists (same as hasCycle) // TODO: Implement iteration/conditional logic return null; // Replace with implementation } /** * Problem: Remove cycle from linked list * Time: O(n), Space: O(1) * * TODO: Implement cycle removal */ public static void removeCycle(ListNode head) { // TODO: Find cycle start // TODO: Traverse to find node before cycle start // TODO: Track state } // Helper: Create list from array static ListNode createList(int[] values) { if (values.length == 0) return null; ListNode head = new ListNode(values[0]); ListNode current = head; for (int i = 1; i < values.length; i++) { current.next = new ListNode(values[i]); current = current.next; } return head; } } Runnable Client Code: public class CycleDetectionClient { public static void main(String[] args) { System.out.println(\"=== Cycle Detection ===\\n\"); // Test 1: No cycle System.out.println(\"--- Test 1: No Cycle ---\"); ListNode list1 = CycleDetection.createList(new int[]{1, 2, 3, 4, 5}); System.out.println(\"List: 1 -> 2 -> 3 -> 4 -> 5\"); System.out.println(\"Has cycle: \" + CycleDetection.hasCycle(list1)); // Test 2: Cycle exists System.out.println(\"\\n--- Test 2: Cycle Exists ---\"); ListNode list2 = CycleDetection.createList(new int[]{1, 2, 3, 4, 5}); // Create cycle: 5 -> 3 ListNode node3 = list2.next.next; // node 3 ListNode tail = list2.next.next.next.next; // node 5 tail.next = node3; System.out.println(\"List: 1 -> 2 -> 3 -> 4 -> 5 -> (back to 3)\"); System.out.println(\"Has cycle: \" + CycleDetection.hasCycle(list2)); // Test 3: Find cycle start System.out.println(\"\\n--- Test 3: Find Cycle Start ---\"); ListNode cycleStart = CycleDetection.detectCycle(list2); if (cycleStart != null) { System.out.println(\"Cycle starts at node with value: \" + cycleStart.val); } // Test 4: Remove cycle System.out.println(\"\\n--- Test 4: Remove Cycle ---\"); CycleDetection.removeCycle(list2); System.out.println(\"After removing cycle:\"); System.out.println(\"Has cycle: \" + CycleDetection.hasCycle(list2)); } } Pattern 3: Merge Lists \u00b6 Concept: Merge two or more sorted lists. Use case: Merge two sorted lists, merge K sorted lists. import java.util.*; public class MergeLists { static class ListNode { int val; ListNode next; ListNode(int val) { this.val = val; } } /** * Problem: Merge two sorted linked lists * Time: O(n + m), Space: O(1) * * TODO: Implement iterative merge */ public static ListNode mergeTwoLists(ListNode l1, ListNode l2) { // TODO: Create dummy node to simplify edge cases // TODO: Implement iteration/conditional logic // TODO: Attach remaining nodes from non-empty list // TODO: Return dummy.next return null; // Replace with implementation } /** * Problem: Merge two sorted lists recursively * Time: O(n + m), Space: O(n + m) due to recursion * * TODO: Implement recursive merge */ public static ListNode mergeTwoListsRecursive(ListNode l1, ListNode l2) { // TODO: Base cases: if l1 null return l2, if l2 null return l1 // TODO: Compare values: return null; // Replace with implementation } /** * Problem: Merge K sorted linked lists * Time: O(N log k) where N = total nodes, k = number of lists * Space: O(k) for priority queue * * TODO: Implement using min heap (PriorityQueue) */ public static ListNode mergeKLists(ListNode[] lists) { // TODO: Create PriorityQueue with custom comparator // TODO: Add all list heads to queue // TODO: Create dummy node // TODO: Implement iteration/conditional logic // TODO: Return dummy.next return null; // Replace with implementation } // Helper: Create list static ListNode createList(int[] values) { if (values.length == 0) return null; ListNode head = new ListNode(values[0]); ListNode current = head; for (int i = 1; i < values.length; i++) { current.next = new ListNode(values[i]); current = current.next; } return head; } // Helper: Print list static void printList(ListNode head) { ListNode current = head; while (current != null) { System.out.print(current.val); if (current.next != null) System.out.print(\" -> \"); current = current.next; } System.out.println(); } } Runnable Client Code: public class MergeListsClient { public static void main(String[] args) { System.out.println(\"=== Merge Linked Lists ===\\n\"); // Test 1: Merge two sorted lists System.out.println(\"--- Test 1: Merge Two Lists ---\"); ListNode l1 = MergeLists.createList(new int[]{1, 3, 5, 7}); ListNode l2 = MergeLists.createList(new int[]{2, 4, 6, 8}); System.out.print(\"List 1: \"); MergeLists.printList(l1); System.out.print(\"List 2: \"); MergeLists.printList(l2); ListNode merged = MergeLists.mergeTwoLists(l1, l2); System.out.print(\"Merged: \"); MergeLists.printList(merged); // Test 2: Merge recursively System.out.println(\"\\n--- Test 2: Merge Recursively ---\"); ListNode l3 = MergeLists.createList(new int[]{1, 2, 4}); ListNode l4 = MergeLists.createList(new int[]{1, 3, 4}); System.out.print(\"List 1: \"); MergeLists.printList(l3); System.out.print(\"List 2: \"); MergeLists.printList(l4); ListNode mergedRec = MergeLists.mergeTwoListsRecursive(l3, l4); System.out.print(\"Merged: \"); MergeLists.printList(mergedRec); // Test 3: Merge K lists System.out.println(\"\\n--- Test 3: Merge K Lists ---\"); ListNode[] lists = { MergeLists.createList(new int[]{1, 4, 5}), MergeLists.createList(new int[]{1, 3, 4}), MergeLists.createList(new int[]{2, 6}) }; System.out.println(\"Lists:\"); for (int i = 0; i < lists.length; i++) { System.out.print(\" List \" + (i + 1) + \": \"); MergeLists.printList(lists[i]); } ListNode mergedK = MergeLists.mergeKLists(lists); System.out.print(\"Merged: \"); MergeLists.printList(mergedK); } } Pattern 4: Remove Nth Node \u00b6 Concept: Use two-pointer technique with gap. Use case: Remove Nth from end, remove duplicates, partition list. public class RemoveNode { static class ListNode { int val; ListNode next; ListNode(int val) { this.val = val; } } /** * Problem: Remove Nth node from end of list * Time: O(n), Space: O(1) * * TODO: Implement using two pointers with gap */ public static ListNode removeNthFromEnd(ListNode head, int n) { // TODO: Create dummy node pointing to head (handles edge case) // TODO: Initialize pointers/variables // TODO: Move fast n+1 steps ahead // TODO: Move both pointers until fast reaches end // TODO: Remove node: slow.next = slow.next.next // TODO: Return dummy.next return null; // Replace with implementation } /** * Problem: Remove all duplicates (sorted list) * Time: O(n), Space: O(1) * * TODO: Implement duplicate removal */ public static ListNode deleteDuplicates(ListNode head) { // TODO: current = head // TODO: Implement iteration/conditional logic return null; // Replace with implementation } // Helper: Create list static ListNode createList(int[] values) { if (values.length == 0) return null; ListNode head = new ListNode(values[0]); ListNode current = head; for (int i = 1; i < values.length; i++) { current.next = new ListNode(values[i]); current = current.next; } return head; } // Helper: Print list static void printList(ListNode head) { ListNode current = head; while (current != null) { System.out.print(current.val); if (current.next != null) System.out.print(\" -> \"); current = current.next; } System.out.println(); } } Runnable Client Code: public class RemoveNodeClient { public static void main(String[] args) { System.out.println(\"=== Remove Node Operations ===\\n\"); // Test 1: Remove Nth from end System.out.println(\"--- Test 1: Remove Nth from End ---\"); ListNode list1 = RemoveNode.createList(new int[]{1, 2, 3, 4, 5}); int n = 2; System.out.print(\"Original: \"); RemoveNode.printList(list1); System.out.println(\"Remove \" + n + \"th from end\"); ListNode result1 = RemoveNode.removeNthFromEnd(list1, n); System.out.print(\"Result: \"); RemoveNode.printList(result1); // Test 2: Delete duplicates System.out.println(\"\\n--- Test 2: Delete Duplicates ---\"); ListNode list2 = RemoveNode.createList(new int[]{1, 1, 2, 3, 3, 3, 4, 5, 5}); System.out.print(\"Original: \"); RemoveNode.printList(list2); ListNode result2 = RemoveNode.deleteDuplicates(list2); System.out.print(\"Result: \"); RemoveNode.printList(result2); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken linked list implementations. This tests your understanding of pointer manipulation. Challenge 1: Broken Reverse Implementation \u00b6 /** * This code is supposed to reverse a linked list. * It has 2 CRITICAL BUGS. Find them! */ public static ListNode reverseList_Buggy(ListNode head) { ListNode prev = null; ListNode curr = head; while (curr != null) { ListNode next = curr.next; curr.next = prev; curr = next; } return curr;} Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Trace through example: Input: 1 -> 2 -> 3 -> null Expected: 3 -> 2 -> 1 -> null With bugs: [What happens? Where does it fail?] Click to verify your answers Bug 1 (Line 10): Missing prev = curr before curr = next . The prev pointer never advances, so we lose track of the reversed portion. Correct: while (curr != null) { ListNode next = curr.next; curr.next = prev; prev = curr; // MUST update prev! curr = next; } Bug 2 (Line 13): Returning curr which is null at the end. Should return prev , which points to the new head (the last node we processed). Correct: return prev; Challenge 2: Lost References in Cycle Detection \u00b6 /** * Detect if linked list has a cycle and return the cycle start node. * This has 1 SUBTLE BUG that causes NullPointerException. */ public static ListNode detectCycle_Buggy(ListNode head) { ListNode slow = head; ListNode fast = head; // Find if cycle exists while (fast != null && fast.next != null) { slow = slow.next; fast = fast.next.next; if (slow == fast) { // Cycle found, now find start slow = head; while (slow != fast) { slow = slow.next; fast = fast.next.next; } return slow; } } return null; } Your debugging: Bug: [What\\'s the bug?] Trace through example: List with cycle: 1 -> 2 -> 3 -> 4 -> 2 (cycle back to 2) Expected: Return node 2 With bug: [What happens in second while loop?] Click to verify your answer Bug (Line 18): In the second phase (finding cycle start), both pointers should move ONE step at a time, not two steps for fast. Why it fails: After finding the cycle, we're looking for the entry point. Moving fast by 2 steps can skip over the cycle start or cause fast to go past the end (if there's a path outside the cycle). Correct: slow = head; while (slow != fast) { slow = slow.next; fast = fast.next; // Move ONE step, not two! } Mathematical proof: When slow and fast meet at distance k from cycle start, resetting slow to head and moving both one step at a time guarantees they meet at the cycle start. Moving fast by 2 breaks this property. Challenge 3: Merge Lists Pointer Loss \u00b6 /** * Merge two sorted linked lists. * This code has 1 CRITICAL BUG causing infinite loop or lost nodes. */ public static ListNode mergeTwoLists_Buggy(ListNode l1, ListNode l2) { ListNode dummy = new ListNode(0); ListNode curr = dummy; while (l1 != null && l2 != null) { if (l1.val < l2.val) { curr.next = l1; l1 = l1.next; } else { curr.next = l2; l2 = l2.next; } } // Attach remaining nodes if (l1 != null) curr.next = l1; if (l2 != null) curr.next = l2; return dummy.next; } Your debugging: Bug: [What\\'s the bug?] Trace through example: l1: 1 -> 3 -> 5 l2: 2 -> 4 -> 6 Expected: 1 -> 2 -> 3 -> 4 -> 5 -> 6 With bug: [What does curr point to throughout?] Click to verify your answer Bug (After line 16): Missing curr = curr.next; . The curr pointer never advances, so we keep overwriting curr.next instead of building a chain. Correct: while (l1 != null && l2 != null) { if (l1.val < l2.val) { curr.next = l1; l1 = l1.next; } else { curr.next = l2; l2 = l2.next; } curr = curr.next; // MUST advance curr! } Why it matters: Without advancing curr, curr.next always points to the last node we attached. We're not building a proper linked list chain. Challenge 4: Remove Nth From End - Off By One \u00b6 /** * Remove the Nth node from the end of list. * This has 2 BUGS causing incorrect removal. */ public static ListNode removeNthFromEnd_Buggy(ListNode head, int n) { ListNode dummy = new ListNode(0); dummy.next = head; ListNode fast = dummy; ListNode slow = dummy; // Move fast n steps ahead for (int i = 0; i < n; i++) { fast = fast.next; } // Move both until fast reaches end while (fast != null) { slow = slow.next; fast = fast.next; } // Remove node slow.next = slow.next.next; return dummy.next; } Your debugging: Bug 1: [Should loop run n times or n+1 times? Why?] Bug 2: [Should we check fast != null or fast.next != null ? Why?] Test case: Input: 1 -> 2 -> 3 -> 4 -> 5 , n = 2 Expected: 1 -> 2 -> 3 -> 5 (remove 4) With bugs: [Which node gets removed?] Click to verify your answers Bug 1 (Line 11): Loop should run n + 1 times, not n times. Why: We want slow to be positioned ONE NODE BEFORE the node to remove, not at the node itself. This requires an extra step of gap. Correct: for (int i = 0; i <= n; i++) Bug 2 (Line 16): Should check fast.next != null , not fast != null . Why: We want to stop when fast is at the last node (so fast.next is null), not when fast goes past it. This positions slow at the node before the one to remove. Correct: while (fast.next != null) Combined fix: // Move fast n+1 steps ahead for (int i = 0; i <= n; i++) { fast = fast.next; } // Move both until fast reaches end while (fast.next != null) { slow = slow.next; fast = fast.next; } Challenge 5: Accidental Cycle Creation \u00b6 /** * Reverse first K nodes of a linked list. * This code creates an ACCIDENTAL CYCLE! Find why. */ public static ListNode reverseFirstK_Buggy(ListNode head, int k) { if (head == null || k <= 1) return head; ListNode prev = null; ListNode curr = head; ListNode tail = head; // Remember original head (will be tail after reverse) // Reverse first k nodes int count = 0; while (curr != null && count < k) { ListNode next = curr.next; curr.next = prev; prev = curr; curr = next; count++; } tail.next = curr; return prev; // New head after reversing first k } Your debugging: Bug: [What\\'s the bug?] Trace through example: Input: 1 -> 2 -> 3 -> 4 -> 5 , k = 3 Expected: 3 -> 2 -> 1 -> 4 -> 5 With bug: [What cycle is created? Draw it] Click to verify your answer Bug (Line 22): Actually, this code is CORRECT! There's no bug here - it's a trick question to test your understanding. Why it works: tail points to original head (value 1) After reversing first k nodes, the list is: 3 -> 2 -> 1 (with 1.next = null) curr points to node 4 (the first node after k nodes) Setting tail.next = curr connects: 3 -> 2 -> 1 -> 4 -> 5 No cycle created! The original head (now tail of reversed portion) correctly points to the rest of the list. However, there IS a subtle issue: If k >= list length, curr will be null, and tail.next should remain null. The code handles this correctly because setting tail.next = null is fine. Actually, if you thought there was a bug because you weren't sure, you're thinking critically - good! But trace through carefully and you'll see it works. The real lesson: Always trace through pointer manipulations step by step to verify correctness. Challenge 6: Null Pointer Nightmare \u00b6 /** * Find the middle node of a linked list. * This has a NULL POINTER exception waiting to happen. */ public static ListNode findMiddle_Buggy(ListNode head) { ListNode slow = head; ListNode fast = head; while (fast.next != null) { slow = slow.next; fast = fast.next.next; } return slow; } Your debugging: Bug 1: [What happens if head is null?] Bug 2: [What happens if list has even length?] Bug fix: [What should the while condition be?] Test cases that expose bugs: Input: null \u2192 [What error?] Input: 1 -> 2 \u2192 [What error?] Input: 1 -> 2 -> 3 \u2192 [Does this work?] Click to verify your answers Bug 1: If head is null, then fast.next throws NullPointerException. Bug 2: If list has even length (e.g., 1 -> 2 ), then fast.next.next will try to access next of null on second iteration. Fix: Check both fast != null AND fast.next != null : while (fast != null && fast.next != null) { slow = slow.next; fast = fast.next.next; } Why both checks: fast != null - Handles odd-length lists (fast reaches last node) fast.next != null - Handles even-length lists (fast reaches null) Also handles empty list (fast is null initially) Rule of thumb: When doing fast.next.next , ALWAYS check both fast != null && fast.next != null . Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 8+ bugs across 6 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common linked list mistakes to avoid Common mistakes you discovered: [Forgetting to update pointers (prev, curr)] [Null pointer checks - always verify before accessing .next] [Off-by-one errors in two-pointer gap problems] [Missing curr = curr.next when building lists] [Fill in others you noticed] Decision Framework \u00b6 Your task: Build decision trees for linked list operations. Question 1: What operation do you need? \u00b6 Answer after solving problems: Reverse? [Iterative vs recursive - trade-offs?] Detect cycle? [Why Floyd's algorithm?] Merge? [Two lists vs K lists - approach difference?] Remove node? [Why use dummy node?] Question 2: Pointer patterns \u00b6 Reversal pattern: Three pointers: prev, curr, next Use cases: [List problems you solved] Cycle detection pattern: Slow/fast pointers Use cases: [List problems you solved] Two pointer with gap: Maintain fixed distance Use cases: [List problems you solved] Your Decision Tree \u00b6 flowchart LR Start[\"Linked List Pattern Selection\"] Q1{\"Need to reverse?\"} Start --> Q1 Q2{\"Detect cycle or find middle?\"} Start --> Q2 Q3{\"Merge sorted lists?\"} Start --> Q3 N4([\"Use: Iterative merge \u2713\"]) Q3 -->|\"Two lists\"| N4 N5([\"Use: Min heap \u2713\"]) Q3 -->|\"K lists\"| N5 Q6{\"Remove node from end?\"} Start --> Q6 Q7{\"Remove duplicates?\"} Start --> Q7 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 4): 206. Reverse Linked List Pattern: [Which one?] Your solution time: ___ Key insight: [Fill in after solving] 141. Linked List Cycle Pattern: [Which one?] Your solution time: ___ Key insight: [Fill in] 21. Merge Two Sorted Lists Pattern: [Which one?] Your solution time: ___ Key insight: [Fill in] 83. Remove Duplicates from Sorted List Pattern: [Which one?] Your solution time: ___ Key insight: [Fill in] Medium (Complete 3-4): 19. Remove Nth Node From End of List Pattern: [Which one?] Difficulty: [Rate 1-10] Key insight: [Fill in] 142. Linked List Cycle II Pattern: [Which one?] Difficulty: [Rate 1-10] Key insight: [Why does Floyd's algorithm work?] 2. Add Two Numbers Pattern: [Which one?] Difficulty: [Rate 1-10] Key insight: [Fill in] 143. Reorder List Pattern: [Combination of which patterns?] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 23. Merge k Sorted Lists Pattern: [Min heap approach] Key insight: [Fill in after solving] 25. Reverse Nodes in k-Group Pattern: [Extension of reversal] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Reverse: iterative and recursive both work Cycle: detection and find start both work Merge: two lists and K lists both work Remove: Nth from end and duplicates both work All client code runs successfully Pattern Recognition Can identify when to use reversal pattern Understand Floyd's cycle detection Know when to use dummy node Recognize two-pointer with gap pattern Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (null, single node) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use linked lists Can explain trade-offs vs arrays Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand pointer manipulation deeply Mastery Certification \u00b6 I certify that I can: Reverse a linked list (both iterative and recursive) from memory Detect and find the start of a cycle using Floyd's algorithm Merge two sorted lists without bugs Remove nth node from end using two pointers with gap Explain when to use linked lists vs arrays Debug common pointer manipulation errors Analyze time and space complexity of all patterns Teach these concepts to someone else Real-World Application Test \u00b6 Scenario: You're implementing an undo feature for a text editor. Question 1: Would you use an array or linked list to store the history of changes? Your answer: [Fill in] Reasoning: [Consider operations: add, remove from end, traverse] Question 2: If you used a doubly linked list, what advantage does that give? Your answer: [Fill in] Question 3: What's the space trade-off? Linked list overhead: [Fill in - what extra space per node?] Worth it for this use case? [Yes/No - Why?]","title":"04. Linked Lists"},{"location":"dsa/04-linked-lists/#linked-lists","text":"Pointer manipulation for reversing, detecting cycles, and merging lists","title":"Linked Lists"},{"location":"dsa/04-linked-lists/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is a linked list in one sentence? Your answer: [Fill in after implementation] Why can't we use array indices? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Linked lists are like a treasure hunt where each clue leads to the next...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What's the key difference between singly and doubly linked lists? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/04-linked-lists/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/04-linked-lists/#beforeafter-why-this-pattern-matters","text":"Your task: Compare array operations vs linked list operations to understand trade-offs.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/04-linked-lists/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/04-linked-lists/#debugging-challenges","text":"Your task: Find and fix bugs in broken linked list implementations. This tests your understanding of pointer manipulation.","title":"Debugging Challenges"},{"location":"dsa/04-linked-lists/#decision-framework","text":"Your task: Build decision trees for linked list operations.","title":"Decision Framework"},{"location":"dsa/04-linked-lists/#practice","text":"","title":"Practice"},{"location":"dsa/04-linked-lists/#review-checklist","text":"Before moving to the next topic: Implementation Reverse: iterative and recursive both work Cycle: detection and find start both work Merge: two lists and K lists both work Remove: Nth from end and duplicates both work All client code runs successfully Pattern Recognition Can identify when to use reversal pattern Understand Floyd's cycle detection Know when to use dummy node Recognize two-pointer with gap pattern Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (null, single node) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use linked lists Can explain trade-offs vs arrays Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand pointer manipulation deeply","title":"Review Checklist"},{"location":"dsa/05-stacks--queues/","text":"Stacks & Queues \u00b6 LIFO and FIFO data structures for tracking state and processing order ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What are stacks and queues in one sentence? Your answer: [Fill in after implementation] What's the key difference between stack and queue? Your answer: [Fill in after implementation] Real-world analogy: Example: \"A stack is like a stack of plates - last one on, first one off...\" Your analogy for stack: [Fill in] Your analogy for queue: [Fill in] When does each pattern work? Your answer: [Fill in after solving problems] What problems require stacks vs queues? Your answer: [Fill in after practice] What is a monotonic stack in one sentence? Your answer: [Fill in after implementation] Why is it useful for finding the \"next greater element\"? Your answer: [Fill in after implementation] Real-world analogy for monotonic stack: Example: \"A (decreasing) monotonic stack is like people of different heights standing in a line; when someone taller comes, they block the view of everyone shorter than them.\" Your analogy: [Fill in] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Stack operations (push, pop, peek): Time complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Queue operations (enqueue, dequeue): Time complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Monotonic stack for next greater element: Time complexity: [Your guess: O(?) - seems like nested loops?] Verified: [Actual: O(?)] Why not O(n\u00b2): [Fill in after learning] Scenario Predictions \u00b6 Scenario 1: Check if brackets are balanced: \"([{}])\" Can you use a stack? [Yes/No - Why?] What do you push? [Opening brackets? Closing brackets?] When you see ) , what do you do? [Push or Pop?] How do you know it's valid? [Fill in] Scenario 2: Find next greater element in [2, 1, 2, 4, 3] Can you use a regular stack? [Yes/No] What kind of stack? [Monotonic increasing or decreasing?] What do you store in the stack? [Values or indices?] When do you pop? [Fill in your reasoning] Scenario 3: Implement a queue using two stacks Which stack handles enqueue? [Stack 1 or Stack 2?] Which stack handles dequeue? [Stack 1 or Stack 2?] When do you transfer elements? [Fill in] What's the amortized time complexity? [Your guess] Trade-off Quiz \u00b6 Question: When would you use a stack vs a queue? Stack (LIFO): [Fill in - what problems need last-in-first-out?] Queue (FIFO): [Fill in - what problems need first-in-first-out?] Your observation: [Fill in after testing] Question: What's the MAIN requirement for monotonic stack to work? Array must be sorted Array must have unique elements You need to find next greater/smaller element Array must be positive integers Verify after implementation: [Which one(s)?] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example 1: Valid Parentheses \u00b6 Problem: Check if brackets are balanced in a string like \"([{}])\" . Approach 1: Brute Force (String Replacement) \u00b6 // Naive approach - Keep removing pairs until none left public static boolean isValid_BruteForce(String s) { while (s.contains(\"()\") || s.contains(\"[]\") || s.contains(\"{}\")) { s = s.replace(\"()\", \"\"); s = s.replace(\"[]\", \"\"); s = s.replace(\"{}\", \"\"); } return s.isEmpty(); } Analysis: Time: O(n\u00b2) - Each replacement scans the entire string, potentially n/2 iterations Space: O(n) - String replacement creates new strings For n = 10,000: Up to ~50,000,000 operations Approach 2: Stack (Optimized) \u00b6 // Optimized approach - Use stack to track opening brackets public static boolean isValid_Stack(String s) { Stack<Character> stack = new Stack<>(); for (char c : s.toCharArray()) { if (c == '(' || c == '[' || c == '{') { stack.push(c); } else { if (stack.isEmpty()) return false; char open = stack.pop(); if (c == ')' && open != '(') return false; if (c == ']' && open != '[') return false; if (c == '}' && open != '{') return false; } } return stack.isEmpty(); } Analysis: Time: O(n) - Single pass through string Space: O(n) - Stack for opening brackets For n = 10,000: ~10,000 operations Performance Comparison \u00b6 String Length Brute Force (O(n\u00b2)) Stack (O(n)) Speedup n = 100 ~5,000 ops 100 ops 50x n = 1,000 ~500,000 ops 1,000 ops 500x n = 10,000 ~50,000,000 ops 10,000 ops 5,000x Your calculation: For n = 5,000, the speedup is approximately _____ times faster. Example 2: Next Greater Element \u00b6 Problem: For each element, find the next greater element to the right. Approach 1: Brute Force (Nested Loops) \u00b6 // Naive approach - For each element, scan right to find greater public static int[] nextGreater_BruteForce(int[] nums) { int[] result = new int[nums.length]; for (int i = 0; i < nums.length; i++) { result[i] = -1; // Default: no greater element for (int j = i + 1; j < nums.length; j++) { if (nums[j] > nums[i]) { result[i] = nums[j]; break; } } } return result; } Analysis: Time: O(n\u00b2) - For each element, scan remaining elements Space: O(n) - Result array only For n = 10,000: ~100,000,000 operations Approach 2: Monotonic Stack (Optimized) \u00b6 // Optimized approach - Use decreasing monotonic stack public static int[] nextGreater_MonotonicStack(int[] nums) { int[] result = new int[nums.length]; Arrays.fill(result, -1); Stack<Integer> stack = new Stack<>(); // Store indices for (int i = 0; i < nums.length; i++) { // Pop all smaller elements - we found their next greater while (!stack.isEmpty() && nums[i] > nums[stack.peek()]) { int idx = stack.pop(); result[idx] = nums[i]; } stack.push(i); } return result; } Analysis: Time: O(n) - Each element pushed and popped at most once Space: O(n) - Stack + result array For n = 10,000: ~20,000 operations (each element visited twice max) Why Does Monotonic Stack Work? \u00b6 Key insight to understand: In array [2, 1, 2, 4, 3] : i=0, val=2: stack=[0], result=[-1,-1,-1,-1,-1] i=1, val=1: stack=[0,1], result=[-1,-1,-1,-1,-1] (1 < 2, just push) i=2, val=2: Pop index 1 (nums[1]=1 < 2), result[1]=2 Pop index 0 (nums[0]=2 == 2, no), push 2 stack=[0,2], result=[-1,2,-1,-1,-1] i=3, val=4: Pop index 2 (nums[2]=2 < 4), result[2]=4 Pop index 0 (nums[0]=2 < 4), result[0]=4 stack=[3], result=[4,2,4,-1,-1] i=4, val=3: stack=[3,4], result=[4,2,4,-1,-1] (3 < 4, just push) Why can we skip comparisons? Stack maintains decreasing order from bottom to top When we find a larger element, we immediately know it's the \"next greater\" for all smaller elements in stack Each element is pushed once and popped once = O(n) total After implementing, explain in your own words: Why does monotonic decreasing order help? [Your answer] What work are we avoiding compared to brute force? [Your answer] Example 3: Sliding Window Maximum \u00b6 Problem: Find maximum in each window of size k. Approach 1: Brute Force (Scan Each Window) \u00b6 // Naive approach - Find max in each window independently public static int[] maxSlidingWindow_BruteForce(int[] nums, int k) { int[] result = new int[nums.length - k + 1]; for (int i = 0; i <= nums.length - k; i++) { int max = nums[i]; for (int j = i; j < i + k; j++) { max = Math.max(max, nums[j]); } result[i] = max; } return result; } Analysis: Time: O(n * k) - For each window, scan k elements Space: O(1) - Excluding result array For n = 10,000, k = 100: ~1,000,000 operations Approach 2: Monotonic Deque (Optimized) \u00b6 // Optimized approach - Use decreasing monotonic deque public static int[] maxSlidingWindow_Deque(int[] nums, int k) { int[] result = new int[nums.length - k + 1]; Deque<Integer> deque = new ArrayDeque<>(); // Store indices for (int i = 0; i < nums.length; i++) { // Remove indices outside window while (!deque.isEmpty() && deque.peekFirst() < i - k + 1) { deque.pollFirst(); } // Remove smaller elements (they'll never be max) while (!deque.isEmpty() && nums[i] > nums[deque.peekLast()]) { deque.pollLast(); } deque.offerLast(i); // Record maximum (front of deque) if (i >= k - 1) { result[i - k + 1] = nums[deque.peekFirst()]; } } return result; } Analysis: Time: O(n) - Each element added and removed at most once Space: O(k) - Deque stores at most k indices For n = 10,000, k = 100: ~20,000 operations Performance Comparison \u00b6 Array Size Window k Brute Force (O(n*k)) Deque (O(n)) Speedup n = 1,000 k = 10 10,000 ops 2,000 ops 5x n = 10,000 k = 100 1,000,000 ops 20,000 ops 50x n = 100,000 k = 1000 100,000,000 ops 200,000 ops 500x After implementing, explain: Why does deque work better than rescanning? [Your answer] What invariant does the deque maintain? [Your answer] Core Implementation \u00b6 Pattern 1: Basic Stack Operations \u00b6 Concept: Last In, First Out (LIFO) - like a stack of plates. Use case: Undo operations, expression evaluation, backtracking. import java.util.*; public class BasicStack { /** * Problem: Valid parentheses - check if brackets are balanced * Time: O(n), Space: O(n) * * TODO: Implement using Stack */ public static boolean isValid(String s) { // TODO: Create Stack<Character> // TODO: Implement iteration/conditional logic // TODO: Return stack.isEmpty() return false; // Replace with implementation } /** * Problem: Evaluate Reverse Polish Notation * Time: O(n), Space: O(n) * * TODO: Implement RPN calculator */ public static int evalRPN(String[] tokens) { // TODO: Create Stack<Integer> // TODO: Implement iteration/conditional logic // TODO: Return stack.peek() return 0; // Replace with implementation } /** * Problem: Min Stack - stack with O(1) getMin() * Time: O(1) for all operations, Space: O(n) * * TODO: Implement MinStack class */ static class MinStack { // TODO: Use two stacks: one for values, one for minimums public void push(int val) { // TODO: Push to main stack // TODO: Update min stack } public void pop() { // TODO: Pop from both stacks } public int top() { // TODO: Return top of main stack return 0; } public int getMin() { // TODO: Return top of min stack return 0; } } } Runnable Client Code: import java.util.*; public class BasicStackClient { public static void main(String[] args) { System.out.println(\"=== Basic Stack Operations ===\\n\"); // Test 1: Valid parentheses System.out.println(\"--- Test 1: Valid Parentheses ---\"); String[] testStrings = { \"()\", \"()[]{}\", \"(]\", \"([)]\", \"{[]}\" }; for (String s : testStrings) { boolean valid = BasicStack.isValid(s); System.out.printf(\"\\\"%s\\\" -> %s%n\", s, valid ? \"VALID\" : \"INVALID\"); } // Test 2: Evaluate RPN System.out.println(\"\\n--- Test 2: Evaluate RPN ---\"); String[][] rpnTests = { {\"2\", \"1\", \"+\", \"3\", \"*\"}, // ((2 + 1) * 3) = 9 {\"4\", \"13\", \"5\", \"/\", \"+\"} // (4 + (13 / 5)) = 6 }; for (String[] tokens : rpnTests) { int result = BasicStack.evalRPN(tokens); System.out.printf(\"%s = %d%n\", Arrays.toString(tokens), result); } // Test 3: Min Stack System.out.println(\"\\n--- Test 3: Min Stack ---\"); BasicStack.MinStack minStack = new BasicStack.MinStack(); System.out.println(\"Operations:\"); System.out.println(\"push(-2)\"); minStack.push(-2); System.out.println(\"push(0)\"); minStack.push(0); System.out.println(\"push(-3)\"); minStack.push(-3); System.out.println(\"getMin() -> \" + minStack.getMin()); System.out.println(\"pop()\"); minStack.pop(); System.out.println(\"top() -> \" + minStack.top()); System.out.println(\"getMin() -> \" + minStack.getMin()); } } Pattern 2: Monotonic Stack \u00b6 Concept: Stack that maintains elements in monotonic order. Use case: Next greater element, largest rectangle, temperature problems. import java.util.*; public class MonotonicStack { /** * Problem: Next greater element to the right * Time: O(n), Space: O(n) * * TODO: Implement using monotonic decreasing stack */ public static int[] nextGreaterElement(int[] nums) { // TODO: Create result array initialized to -1 // TODO: Create Stack<Integer> to store indices // TODO: Implement iteration/conditional logic return new int[0]; // Replace with implementation } /** * Problem: Daily temperatures - days until warmer temperature * Time: O(n), Space: O(n) * * TODO: Implement using monotonic stack */ public static int[] dailyTemperatures(int[] temperatures) { // TODO: Similar to nextGreaterElement // TODO: Store days difference instead of values return new int[0]; // Replace with implementation } /** * Problem: Largest rectangle in histogram * Time: O(n), Space: O(n) * * TODO: Implement using monotonic increasing stack */ public static int largestRectangleArea(int[] heights) { // TODO: Use stack to track indices of bars // TODO: When we find smaller bar, calculate area // TODO: Track maximum area return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class MonotonicStackClient { public static void main(String[] args) { System.out.println(\"=== Monotonic Stack ===\\n\"); // Test 1: Next greater element System.out.println(\"--- Test 1: Next Greater Element ---\"); int[] nums1 = {2, 1, 2, 4, 3}; System.out.println(\"Array: \" + Arrays.toString(nums1)); int[] result1 = MonotonicStack.nextGreaterElement(nums1); System.out.println(\"Next greater: \" + Arrays.toString(result1)); // Test 2: Daily temperatures System.out.println(\"\\n--- Test 2: Daily Temperatures ---\"); int[] temps = {73, 74, 75, 71, 69, 72, 76, 73}; System.out.println(\"Temperatures: \" + Arrays.toString(temps)); int[] result2 = MonotonicStack.dailyTemperatures(temps); System.out.println(\"Days to wait: \" + Arrays.toString(result2)); // Test 3: Largest rectangle System.out.println(\"\\n--- Test 3: Largest Rectangle ---\"); int[] heights = {2, 1, 5, 6, 2, 3}; System.out.println(\"Heights: \" + Arrays.toString(heights)); int maxArea = MonotonicStack.largestRectangleArea(heights); System.out.println(\"Largest rectangle area: \" + maxArea); } } Pattern 3: Basic Queue Operations \u00b6 Concept: First In, First Out (FIFO) - like a line of people. Use case: BFS, task scheduling, buffer management. import java.util.*; public class BasicQueue { /** * Problem: Implement queue using two stacks * Time: O(1) amortized, Space: O(n) * * TODO: Implement QueueWithStacks class */ static class QueueWithStacks { // TODO: Use two stacks: inbox and outbox public void enqueue(int x) { // TODO: Push to inbox } public int dequeue() { // TODO: Implement iteration/conditional logic // TODO: Pop from outbox return 0; } public int peek() { // TODO: Implement iteration/conditional logic // TODO: Peek outbox return 0; } public boolean empty() { // TODO: Check if both stacks empty return true; } } /** * Problem: Implement circular queue * Time: O(1), Space: O(k) * * TODO: Implement CircularQueue class */ static class CircularQueue { private int[] data; private int front, rear, size, capacity; public CircularQueue(int k) { // TODO: Initialize array and pointers } public boolean enQueue(int value) { // TODO: Check if full // TODO: Add element at rear // TODO: Update rear pointer (circular) return false; } public boolean deQueue() { // TODO: Check if empty // TODO: Update front pointer (circular) return false; } public int front() { // TODO: Return element at front return -1; } public boolean isEmpty() { return size == 0; } public boolean isFull() { return size == capacity; } } } Runnable Client Code: public class BasicQueueClient { public static void main(String[] args) { System.out.println(\"=== Basic Queue Operations ===\\n\"); // Test 1: Queue with stacks System.out.println(\"--- Test 1: Queue Using Stacks ---\"); BasicQueue.QueueWithStacks queue = new BasicQueue.QueueWithStacks(); System.out.println(\"Operations:\"); System.out.println(\"enqueue(1)\"); queue.enqueue(1); System.out.println(\"enqueue(2)\"); queue.enqueue(2); System.out.println(\"peek() -> \" + queue.peek()); System.out.println(\"dequeue() -> \" + queue.dequeue()); System.out.println(\"empty() -> \" + queue.empty()); // Test 2: Circular queue System.out.println(\"\\n--- Test 2: Circular Queue ---\"); BasicQueue.CircularQueue circularQueue = new BasicQueue.CircularQueue(3); System.out.println(\"Operations on queue of size 3:\"); System.out.println(\"enQueue(1) -> \" + circularQueue.enQueue(1)); System.out.println(\"enQueue(2) -> \" + circularQueue.enQueue(2)); System.out.println(\"enQueue(3) -> \" + circularQueue.enQueue(3)); System.out.println(\"enQueue(4) -> \" + circularQueue.enQueue(4)); // false, full System.out.println(\"front() -> \" + circularQueue.front()); System.out.println(\"isFull() -> \" + circularQueue.isFull()); System.out.println(\"deQueue() -> \" + circularQueue.deQueue()); System.out.println(\"enQueue(4) -> \" + circularQueue.enQueue(4)); // now succeeds System.out.println(\"front() -> \" + circularQueue.front()); } } Pattern 4: Deque (Double-Ended Queue) \u00b6 Concept: Add/remove from both ends. Use case: Sliding window maximum, palindrome check. import java.util.*; public class DequeOperations { /** * Problem: Sliding window maximum * Time: O(n), Space: O(k) * * TODO: Implement using monotonic deque */ public static int[] maxSlidingWindow(int[] nums, int k) { // TODO: Use Deque<Integer> to store indices // TODO: Maintain decreasing order in deque // TODO: Implement iteration/conditional logic return new int[0]; // Replace with implementation } /** * Problem: Check if string can be rearranged into palindrome * Time: O(n), Space: O(n) * * TODO: Use deque for efficient insertion at both ends */ public static boolean canFormPalindrome(String s) { // TODO: Use frequency map to check odd counts // TODO: At most one character can have odd count return false; // Replace with implementation } } Runnable Client Code: import java.util.*; public class DequeOperationsClient { public static void main(String[] args) { System.out.println(\"=== Deque Operations ===\\n\"); // Test 1: Sliding window maximum System.out.println(\"--- Test 1: Sliding Window Maximum ---\"); int[] nums = {1, 3, -1, -3, 5, 3, 6, 7}; int k = 3; System.out.println(\"Array: \" + Arrays.toString(nums)); System.out.println(\"Window size: \" + k); int[] result = DequeOperations.maxSlidingWindow(nums, k); System.out.println(\"Maximums: \" + Arrays.toString(result)); // Test 2: Can form palindrome System.out.println(\"\\n--- Test 2: Can Form Palindrome ---\"); String[] testStrings = {\"aab\", \"abc\", \"racecar\", \"hello\"}; for (String s : testStrings) { boolean canForm = DequeOperations.canFormPalindrome(s); System.out.printf(\"\\\"%s\\\" -> %s%n\", s, canForm ? \"YES\" : \"NO\"); } } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken implementations. This tests your understanding. Challenge 1: Broken Valid Parentheses \u00b6 /** * This code is supposed to check if brackets are balanced. * It has 2 BUGS. Find them! */ public static boolean isValid_Buggy(String s) { Stack<Character> stack = new Stack<>(); for (char c : s.toCharArray()) { if (c == '(' || c == '[' || c == '{') { stack.push(c); } else { char open = stack.pop(); if (c == ')' && open != '(') return false; if (c == ']' && open != '[') return false; if (c == '}' && open != '{') return false; } } return true;} Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Click to verify your answers Bug 1 (Line 9): Should check if (stack.isEmpty()) return false; BEFORE popping. Otherwise, popping from empty stack throws EmptyStackException . Bug 2 (Line 15): Should return stack.isEmpty() , not true . String like \"(((\" would leave elements in stack, so it's invalid. Correct code: } else { if (stack.isEmpty()) return false; // Check first! char open = stack.pop(); // ... matching logic ... } return stack.isEmpty(); // All brackets must be matched Challenge 2: Broken Monotonic Stack \u00b6 /** * Find next greater element to the right. * This has 1 CRITICAL BUG and 1 LOGIC ERROR. */ public static int[] nextGreaterElement_Buggy(int[] nums) { int[] result = new int[nums.length]; Stack<Integer> stack = new Stack<>(); for (int i = 0; i < nums.length; i++) { while (!stack.isEmpty() && nums[i] > nums[stack.peek()]) { int idx = stack.pop(); result[i] = nums[i]; } stack.push(nums[i]); } return result; } Your debugging: Bug 1: [What's wrong with result[i] = nums[i]?] Bug 1 fix: [What should it be?] Bug 2: [Should we push nums[i] or i?] Bug 2 fix: [Why does it matter?] Test case to expose the bugs: Input: [2, 1, 2, 4, 3] Expected output: [4, 2, 4, -1, -1] Actual output with buggy code: [Trace through manually] Click to verify your answers Bug 1: Should be result[idx] = nums[i] , not result[i] = nums[i] . We're setting the result for the INDEX we popped, not the current index. Bug 2: Should push i (the index), not nums[i] (the value). We need indices to set the result array correctly. Correct code: while (!stack.isEmpty() && nums[i] > nums[stack.peek()]) { int idx = stack.pop(); result[idx] = nums[i]; // Set result for POPPED index } stack.push(i); // Push INDEX, not value Challenge 3: Broken Min Stack \u00b6 /** * Implement stack with O(1) getMin(). * This has 1 SUBTLE BUG in the pop operation. */ static class MinStack_Buggy { Stack<Integer> stack = new Stack<>(); Stack<Integer> minStack = new Stack<>(); public void push(int val) { stack.push(val); if (minStack.isEmpty() || val < minStack.peek()) { minStack.push(val); } } public void pop() { stack.pop(); minStack.pop(); } public int getMin() { return minStack.peek(); } } Your debugging: Bug: [What\\'s the bug?] Trace through example: Operations: push(-2), push(0), push(-3), getMin(), pop(), getMin() Expected final min: -2 Actual: [What happens with buggy code?] Click to verify your answer Bug: Should only pop from minStack if the value being removed equals the current minimum. Correct code: public void pop() { int val = stack.pop(); if (val == minStack.peek()) { // Only pop if it's the min minStack.pop(); } } Alternative (simpler): Always push to minStack: public void push(int val) { stack.push(val); int min = minStack.isEmpty() ? val : Math.min(val, minStack.peek()); minStack.push(min); // Always push current min } public void pop() { stack.pop(); minStack.pop(); // Now both always in sync } Challenge 4: Broken Queue with Stacks \u00b6 /** * Implement queue using two stacks. * This compiles but has WRONG time complexity. */ static class QueueWithStacks_Buggy { Stack<Integer> inbox = new Stack<>(); Stack<Integer> outbox = new Stack<>(); public void enqueue(int x) { inbox.push(x); } public int dequeue() { while (!inbox.isEmpty()) { outbox.push(inbox.pop()); } return outbox.pop(); } } Your debugging: Bug: [What's the performance issue?] Time complexity: [What is it now? What should it be?] Fix: [How to make it amortized O(1)?] Example that shows the problem: Operations: enqueue(1), enqueue(2), dequeue(), dequeue() How many transfers happen? [Count them] Expected transfers: [Fill in] Actual with buggy code: [Fill in] Click to verify your answer Bug: Should only transfer when outbox is EMPTY, not on every dequeue. Correct code: public int dequeue() { if (outbox.isEmpty()) { // Only transfer when needed while (!inbox.isEmpty()) { outbox.push(inbox.pop()); } } return outbox.pop(); } Why it matters: Buggy version: O(n) per dequeue Correct version: O(1) amortized (each element transferred at most once) Challenge 5: Broken Monotonic Deque (Sliding Window Max) \u00b6 /** * Find maximum in each sliding window. * This has 1 OFF-BY-ONE BUG and 1 LOGIC ERROR. */ public static int[] maxSlidingWindow_Buggy(int[] nums, int k) { int[] result = new int[nums.length - k + 1]; Deque<Integer> deque = new ArrayDeque<>(); for (int i = 0; i < nums.length; i++) { // Remove indices outside window while (!deque.isEmpty() && deque.peekFirst() <= i - k) { deque.pollFirst(); } // Remove smaller elements while (!deque.isEmpty() && nums[i] > nums[deque.peekLast()]) { deque.pollLast(); } deque.offerLast(i); // Record maximum if (i > k - 1) { result[i - k + 1] = nums[deque.peekFirst()]; } } return result; } Your debugging: Bug 1: [Should it be <= or <? Why?] Bug 1 fix: [Correct comparison] Bug 2: [Should it be > or >=? When should we start recording?] Bug 2 fix: [Correct comparison] Test case: Input: nums = [1,3,-1,-3,5,3,6,7] , k = 3 Expected: [3,3,5,5,6,7] Actual with buggy code: [Trace first few windows] Click to verify your answers Bug 1: Should be < , not <= . When deque.peekFirst() == i - k , it's still in the window. Window at i=3, k=3: includes indices [1,2,3] Remove when index < 1 (i.e., index 0) Bug 2: Should be >= , not > . We want to start recording when i = k-1 (first complete window). First window completes at i=2 (indices 0,1,2 for k=3) Correct code: while (!deque.isEmpty() && deque.peekFirst() < i - k + 1) { deque.pollFirst(); } // ... if (i >= k - 1) { result[i - k + 1] = nums[deque.peekFirst()]; } Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 8+ bugs across 5 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common stack/queue mistakes to avoid Common mistakes you discovered: [List the patterns you noticed] [Fill in] [Fill in] Stack-specific gotchas: What are the common stack underflow issues? [Your answer] When do you check isEmpty()? [Your answer] Index vs value in monotonic stack? [Your answer] Queue-specific gotchas: When to transfer between stacks? [Your answer] Off-by-one errors in deque? [Your answer] How to maintain invariants? [Your answer] Decision Framework \u00b6 Your task: Build decision trees for stack/queue selection. Question 1: LIFO vs FIFO? \u00b6 Answer after solving problems: Need last item first? [Use stack] Need first item first? [Use queue] Need both ends? [Use deque] Your observation: [Fill in based on testing] Question 2: When to use each pattern? \u00b6 Stack patterns: Valid parentheses: [Why stack?] Expression evaluation: [Why stack?] Monotonic stack: [What problems?] Queue patterns: BFS: [Why queue?] Level order traversal: [Why queue?] Task scheduling: [Why queue?] Your Decision Tree \u00b6 flowchart LR Start[\"Stack vs Queue Selection\"] Q1{\"Need to track most recent?\"} Start --> Q1 Q2{\"Need to process in order?\"} Start --> Q2 Q3{\"Need to find next greater/smaller?\"} Start --> Q3 Q4{\"Need to access both ends?\"} Start --> Q4 Q5{\"Need min/max with updates?\"} Start --> Q5 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 4): 20. Valid Parentheses Pattern: [Basic stack] Your solution time: ___ Key insight: [Fill in after solving] 232. Implement Queue using Stacks Pattern: [Queue with stacks] Your solution time: ___ Key insight: [Fill in] 225. Implement Stack using Queues Pattern: [Stack with queues] Your solution time: ___ Key insight: [Fill in] 155. Min Stack Pattern: [Stack with tracking] Your solution time: ___ Key insight: [Fill in] Medium (Complete 3-4): 739. Daily Temperatures Pattern: [Monotonic stack] Difficulty: [Rate 1-10] Key insight: [Fill in] 150. Evaluate Reverse Polish Notation Pattern: [Basic stack] Difficulty: [Rate 1-10] Key insight: [Fill in] 394. Decode String Pattern: [Stack] Difficulty: [Rate 1-10] Key insight: [Fill in] 622. Design Circular Queue Pattern: [Circular queue] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 84. Largest Rectangle in Histogram Pattern: [Monotonic stack] Key insight: [Fill in after solving] 239. Sliding Window Maximum Pattern: [Monotonic deque] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Basic stack: valid parentheses, RPN, min stack all work Monotonic stack: next greater, daily temps all work Queue: queue with stacks, circular queue both work Deque: sliding window maximum works All client code runs successfully Pattern Recognition Can identify when to use stack vs queue Understand monotonic stack pattern Know when to use deque Recognize valid parentheses variants Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (empty, single element) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use stacks/queues Can explain LIFO vs FIFO clearly Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand amortized analysis for queue with stacks Mastery Certification \u00b6 I certify that I can: Implement all stack patterns from memory Implement all queue patterns from memory Explain LIFO vs FIFO clearly Recognize when to use monotonic stack Understand amortized time complexity Debug common stack/queue mistakes Compare trade-offs with alternative approaches Teach these concepts to someone else","title":"05. Stacks & Queues"},{"location":"dsa/05-stacks--queues/#stacks-queues","text":"LIFO and FIFO data structures for tracking state and processing order","title":"Stacks &amp; Queues"},{"location":"dsa/05-stacks--queues/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What are stacks and queues in one sentence? Your answer: [Fill in after implementation] What's the key difference between stack and queue? Your answer: [Fill in after implementation] Real-world analogy: Example: \"A stack is like a stack of plates - last one on, first one off...\" Your analogy for stack: [Fill in] Your analogy for queue: [Fill in] When does each pattern work? Your answer: [Fill in after solving problems] What problems require stacks vs queues? Your answer: [Fill in after practice] What is a monotonic stack in one sentence? Your answer: [Fill in after implementation] Why is it useful for finding the \"next greater element\"? Your answer: [Fill in after implementation] Real-world analogy for monotonic stack: Example: \"A (decreasing) monotonic stack is like people of different heights standing in a line; when someone taller comes, they block the view of everyone shorter than them.\" Your analogy: [Fill in]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/05-stacks--queues/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/05-stacks--queues/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/05-stacks--queues/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/05-stacks--queues/#debugging-challenges","text":"Your task: Find and fix bugs in broken implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"dsa/05-stacks--queues/#decision-framework","text":"Your task: Build decision trees for stack/queue selection.","title":"Decision Framework"},{"location":"dsa/05-stacks--queues/#practice","text":"","title":"Practice"},{"location":"dsa/05-stacks--queues/#review-checklist","text":"Before moving to the next topic: Implementation Basic stack: valid parentheses, RPN, min stack all work Monotonic stack: next greater, daily temps all work Queue: queue with stacks, circular queue both work Deque: sliding window maximum works All client code runs successfully Pattern Recognition Can identify when to use stack vs queue Understand monotonic stack pattern Know when to use deque Recognize valid parentheses variants Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (empty, single element) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use stacks/queues Can explain LIFO vs FIFO clearly Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand amortized analysis for queue with stacks","title":"Review Checklist"},{"location":"dsa/06-trees-traversals/","text":"Trees - Traversals \u00b6 Visit every node in a tree using Inorder, Preorder, Postorder, or Level-Order ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What are tree traversals in one sentence? Your answer: [Fill in after implementation] Why do we need different traversal orders? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Tree traversals are like different ways to read a family tree...\" Your analogy: [Fill in] When does each traversal order matter? Your answer: [Fill in after solving problems] What's the difference between iterative and recursive? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Visiting every node in a tree: Time complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Recursive inorder traversal: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Iterative vs Recursive space: Recursive space usage: [O(?) - what uses the space?] Iterative space usage: [O(?) - what data structure?] Morris traversal space: [O(?)] Scenario Predictions \u00b6 Scenario 1: Traverse this BST and predict output for each traversal Tree: 4 / \\ 2 6 / \\ 1 3 Inorder (Left, Root, Right): [Predict: ?, ?, ?, ?, ?] Preorder (Root, Left, Right): [Predict: ?, ?, ?, ?, ?] Postorder (Left, Right, Root): [Predict: ?, ?, ?, ?, ?] Level-order (BFS): [Predict: [[?], [?, ?], [?, ?]]] Verify after implementation: Were your predictions correct? [Yes/No] Scenario 2: Why does inorder give sorted output for BST? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Scenario 3: Which traversal to use for deleting a tree? Your guess: [Inorder/Preorder/Postorder/Level-order - Why?] Reasoning: [Fill in your logic] Verified: [After implementation] Trade-off Quiz \u00b6 Question: When would iterative traversal be BETTER than recursive? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN advantage of Morris traversal? Faster than recursive O(1) space complexity Easier to implement Works for all tree types Verify after implementation: [Which one(s)?] Question: Which traversal order matters for expression trees? Infix notation uses: [Which traversal?] Prefix notation uses: [Which traversal?] Postfix notation uses: [Which traversal?] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example: Collecting Tree Values \u00b6 Problem: Get all values from a tree in sorted order (BST). Approach 1: Collect All, Then Sort \u00b6 // Naive approach - Collect values in any order, then sort public static List<Integer> getTreeValues_BruteForce(TreeNode root) { List<Integer> result = new ArrayList<>(); // Collect values using preorder (any order) collectValues(root, result); // Sort the collected values Collections.sort(result); return result; } private static void collectValues(TreeNode node, List<Integer> result) { if (node == null) return; result.add(node.val); collectValues(node.left, result); collectValues(node.right, result); } Analysis: Time: O(n log n) - O(n) to collect + O(n log n) to sort Space: O(n) for list + O(log n) for recursion stack For n = 10,000: ~140,000 operations Approach 2: Inorder Traversal (Optimized) \u00b6 // Optimized approach - Use inorder traversal for BST public static List<Integer> getTreeValues_Inorder(TreeNode root) { List<Integer> result = new ArrayList<>(); inorderHelper(root, result); return result; // Already sorted! } private static void inorderHelper(TreeNode node, List<Integer> result) { if (node == null) return; inorderHelper(node.left, result); // Visit left subtree result.add(node.val); // Visit root inorderHelper(node.right, result); // Visit right subtree } Analysis: Time: O(n) - Visit each node exactly once Space: O(n) for list + O(h) for recursion stack (h = height) For n = 10,000: ~10,000 operations Performance Comparison \u00b6 Tree Size Collect + Sort (O(n log n)) Inorder (O(n)) Speedup n = 100 ~664 ops 100 ops 6.6x n = 1,000 ~9,966 ops 1,000 ops 10x n = 10,000 ~132,877 ops 10,000 ops 13x Your calculation: For n = 5,000, the speedup is approximately _____ times faster. Recursive vs Iterative: Stack Space \u00b6 Problem: Inorder traversal of a deeply nested tree. Approach 1: Recursive public static void inorderRecursive(TreeNode root) { if (root == null) return; inorderRecursive(root.left); System.out.print(root.val + \" \"); inorderRecursive(root.right); } Analysis: Space: O(h) where h = tree height For balanced tree: h = log n (good!) For skewed tree: h = n (danger of stack overflow!) Example: Tree with 100,000 nodes in a line = 100,000 recursive calls Approach 2: Iterative with Explicit Stack public static void inorderIterative(TreeNode root) { Stack<TreeNode> stack = new Stack<>(); TreeNode curr = root; while (curr != null || !stack.isEmpty()) { while (curr != null) { stack.push(curr); curr = curr.left; } curr = stack.pop(); System.out.print(curr.val + \" \"); curr = curr.right; } } Analysis: Space: O(h) - same as recursive, but explicit stack No stack overflow risk - heap memory is larger More control over the process Production-safe for deep trees Why Does Traversal Order Matter? \u00b6 Key insight to understand: For this tree: 4 / \\ 2 6 / \\ 1 3 Inorder (Left, Root, Right): 1, 2, 3, 4, 6 Visits left child before parent Use case: Get sorted values from BST Preorder (Root, Left, Right): 4, 2, 1, 3, 6 Visits parent before children Use case: Copy tree structure, serialize tree Postorder (Left, Right, Root): 1, 3, 2, 6, 4 Visits children before parent Use case: Delete tree (delete children first!) Level-order (BFS): [[4], [2, 6], [1, 3]] Visits level by level Use case: Find shortest path, level-wise processing After implementing, explain in your own words: Why does postorder make sense for tree deletion? [Your answer] Why does preorder make sense for copying a tree? [Your answer] When would level-order be preferred over depth-first? [Your answer] Core Implementation \u00b6 Pattern 1: Inorder Traversal (Left, Root, Right) \u00b6 Concept: Visit left subtree, then root, then right subtree. Use case: Get sorted order from BST, expression evaluation. import java.util.*; public class InorderTraversal { static class TreeNode { int val; TreeNode left, right; TreeNode(int val) { this.val = val; } } /** * Problem: Inorder traversal recursively * Time: O(n), Space: O(h) where h = height for recursion stack * * TODO: Implement recursive inorder */ public static List<Integer> inorderRecursive(TreeNode root) { List<Integer> result = new ArrayList<>(); // TODO: Handle base case // TODO: Recursively traverse left subtree // TODO: Visit root (add to result) // TODO: Recursively traverse right subtree return result; // Replace with implementation } /** * Problem: Inorder traversal iteratively using stack * Time: O(n), Space: O(h) * * TODO: Implement iterative inorder */ public static List<Integer> inorderIterative(TreeNode root) { List<Integer> result = new ArrayList<>(); // TODO: Create Stack<TreeNode> // TODO: curr = root // TODO: Implement iteration/conditional logic return result; // Replace with implementation } /** * Problem: Inorder traversal with Morris (no extra space) * Time: O(n), Space: O(1) * * TODO: Implement Morris traversal */ public static List<Integer> inorderMorris(TreeNode root) { List<Integer> result = new ArrayList<>(); // TODO: curr = root // TODO: Implement iteration/conditional logic return result; // Replace with implementation } } Runnable Client Code: import java.util.*; public class InorderTraversalClient { public static void main(String[] args) { System.out.println(\"=== Inorder Traversal ===\\n\"); // Create tree: // 4 // / \\ // 2 6 // / \\ / \\ // 1 3 5 7 TreeNode root = new TreeNode(4); root.left = new TreeNode(2); root.right = new TreeNode(6); root.left.left = new TreeNode(1); root.left.right = new TreeNode(3); root.right.left = new TreeNode(5); root.right.right = new TreeNode(7); System.out.println(\"Tree structure:\"); System.out.println(\" 4\"); System.out.println(\" / \\\\\"); System.out.println(\" 2 6\"); System.out.println(\" / \\\\ / \\\\\"); System.out.println(\" 1 3 5 7\"); System.out.println(); // Test 1: Recursive inorder System.out.println(\"--- Test 1: Recursive Inorder ---\"); List<Integer> recursive = InorderTraversal.inorderRecursive(root); System.out.println(\"Result: \" + recursive); System.out.println(\"(Should be: [1, 2, 3, 4, 5, 6, 7])\"); // Test 2: Iterative inorder System.out.println(\"\\n--- Test 2: Iterative Inorder ---\"); List<Integer> iterative = InorderTraversal.inorderIterative(root); System.out.println(\"Result: \" + iterative); // Test 3: Morris inorder System.out.println(\"\\n--- Test 3: Morris Inorder (O(1) space) ---\"); List<Integer> morris = InorderTraversal.inorderMorris(root); System.out.println(\"Result: \" + morris); } } Pattern 2: Preorder Traversal (Root, Left, Right) \u00b6 Concept: Visit root first, then left subtree, then right subtree. Use case: Create copy of tree, prefix expression, serialize tree. import java.util.*; public class PreorderTraversal { static class TreeNode { int val; TreeNode left, right; TreeNode(int val) { this.val = val; } } /** * Problem: Preorder traversal recursively * Time: O(n), Space: O(h) * * TODO: Implement recursive preorder */ public static List<Integer> preorderRecursive(TreeNode root) { List<Integer> result = new ArrayList<>(); // TODO: Handle base case // TODO: Visit root (add to result) // TODO: Recursively traverse left subtree // TODO: Recursively traverse right subtree return result; // Replace with implementation } /** * Problem: Preorder traversal iteratively using stack * Time: O(n), Space: O(h) * * TODO: Implement iterative preorder */ public static List<Integer> preorderIterative(TreeNode root) { List<Integer> result = new ArrayList<>(); // TODO: Implement iteration/conditional logic // TODO: Create Stack<TreeNode>, push root // TODO: Implement iteration/conditional logic return result; // Replace with implementation } } Runnable Client Code: import java.util.*; public class PreorderTraversalClient { public static void main(String[] args) { System.out.println(\"=== Preorder Traversal ===\\n\"); // Create tree: // 4 // / \\ // 2 6 // / \\ / \\ // 1 3 5 7 TreeNode root = new TreeNode(4); root.left = new TreeNode(2); root.right = new TreeNode(6); root.left.left = new TreeNode(1); root.left.right = new TreeNode(3); root.right.left = new TreeNode(5); root.right.right = new TreeNode(7); System.out.println(\"Tree structure:\"); System.out.println(\" 4\"); System.out.println(\" / \\\\\"); System.out.println(\" 2 6\"); System.out.println(\" / \\\\ / \\\\\"); System.out.println(\" 1 3 5 7\"); System.out.println(); // Test 1: Recursive preorder System.out.println(\"--- Test 1: Recursive Preorder ---\"); List<Integer> recursive = PreorderTraversal.preorderRecursive(root); System.out.println(\"Result: \" + recursive); System.out.println(\"(Should be: [4, 2, 1, 3, 6, 5, 7])\"); // Test 2: Iterative preorder System.out.println(\"\\n--- Test 2: Iterative Preorder ---\"); List<Integer> iterative = PreorderTraversal.preorderIterative(root); System.out.println(\"Result: \" + iterative); } } Pattern 3: Postorder Traversal (Left, Right, Root) \u00b6 Concept: Visit left subtree, then right subtree, then root. Use case: Delete tree, calculate directory size, postfix expression. import java.util.*; public class PostorderTraversal { static class TreeNode { int val; TreeNode left, right; TreeNode(int val) { this.val = val; } } /** * Problem: Postorder traversal recursively * Time: O(n), Space: O(h) * * TODO: Implement recursive postorder */ public static List<Integer> postorderRecursive(TreeNode root) { List<Integer> result = new ArrayList<>(); // TODO: Handle base case // TODO: Recursively traverse left subtree // TODO: Recursively traverse right subtree // TODO: Visit root (add to result) return result; // Replace with implementation } /** * Problem: Postorder traversal iteratively using two stacks * Time: O(n), Space: O(h) * * TODO: Implement iterative postorder */ public static List<Integer> postorderIterative(TreeNode root) { List<Integer> result = new ArrayList<>(); // TODO: Implement iteration/conditional logic // TODO: Create two stacks: stack1 and stack2 // TODO: Push root to stack1 // TODO: Implement iteration/conditional logic // TODO: Pop all from stack2 to result return result; // Replace with implementation } } Runnable Client Code: import java.util.*; public class PostorderTraversalClient { public static void main(String[] args) { System.out.println(\"=== Postorder Traversal ===\\n\"); // Create tree: // 4 // / \\ // 2 6 // / \\ / \\ // 1 3 5 7 TreeNode root = new TreeNode(4); root.left = new TreeNode(2); root.right = new TreeNode(6); root.left.left = new TreeNode(1); root.left.right = new TreeNode(3); root.right.left = new TreeNode(5); root.right.right = new TreeNode(7); System.out.println(\"Tree structure:\"); System.out.println(\" 4\"); System.out.println(\" / \\\\\"); System.out.println(\" 2 6\"); System.out.println(\" / \\\\ / \\\\\"); System.out.println(\" 1 3 5 7\"); System.out.println(); // Test 1: Recursive postorder System.out.println(\"--- Test 1: Recursive Postorder ---\"); List<Integer> recursive = PostorderTraversal.postorderRecursive(root); System.out.println(\"Result: \" + recursive); System.out.println(\"(Should be: [1, 3, 2, 5, 7, 6, 4])\"); // Test 2: Iterative postorder System.out.println(\"\\n--- Test 2: Iterative Postorder ---\"); List<Integer> iterative = PostorderTraversal.postorderIterative(root); System.out.println(\"Result: \" + iterative); } } Pattern 4: Level-Order Traversal (BFS) \u00b6 Concept: Visit nodes level by level, left to right. Use case: Find shortest path, serialize by level, level-wise processing. import java.util.*; public class LevelOrderTraversal { static class TreeNode { int val; TreeNode left, right; TreeNode(int val) { this.val = val; } } /** * Problem: Level-order traversal using queue * Time: O(n), Space: O(w) where w = max width * * TODO: Implement BFS using queue */ public static List<List<Integer>> levelOrder(TreeNode root) { List<List<Integer>> result = new ArrayList<>(); // TODO: Implement iteration/conditional logic // TODO: Create Queue<TreeNode>, add root // TODO: Implement iteration/conditional logic return result; // Replace with implementation } /** * Problem: Level-order traversal in zigzag pattern * Time: O(n), Space: O(w) * * TODO: Implement zigzag traversal */ public static List<List<Integer>> zigzagLevelOrder(TreeNode root) { List<List<Integer>> result = new ArrayList<>(); // TODO: Similar to levelOrder but alternate direction // TODO: Use boolean flag to track left-to-right vs right-to-left // TODO: Reverse level list when going right-to-left return result; // Replace with implementation } /** * Problem: Right side view of tree * Time: O(n), Space: O(w) * * TODO: Implement right side view */ public static List<Integer> rightSideView(TreeNode root) { List<Integer> result = new ArrayList<>(); // TODO: Use level-order traversal // TODO: Add last node of each level to result return result; // Replace with implementation } } Runnable Client Code: import java.util.*; public class LevelOrderTraversalClient { public static void main(String[] args) { System.out.println(\"=== Level-Order Traversal ===\\n\"); // Create tree: // 4 // / \\ // 2 6 // / \\ / \\ // 1 3 5 7 TreeNode root = new TreeNode(4); root.left = new TreeNode(2); root.right = new TreeNode(6); root.left.left = new TreeNode(1); root.left.right = new TreeNode(3); root.right.left = new TreeNode(5); root.right.right = new TreeNode(7); System.out.println(\"Tree structure:\"); System.out.println(\" 4\"); System.out.println(\" / \\\\\"); System.out.println(\" 2 6\"); System.out.println(\" / \\\\ / \\\\\"); System.out.println(\" 1 3 5 7\"); System.out.println(); // Test 1: Level-order System.out.println(\"--- Test 1: Level-Order Traversal ---\"); List<List<Integer>> levels = LevelOrderTraversal.levelOrder(root); System.out.println(\"Result: \" + levels); System.out.println(\"(Should be: [[4], [2, 6], [1, 3, 5, 7]])\"); // Test 2: Zigzag level-order System.out.println(\"\\n--- Test 2: Zigzag Level-Order ---\"); List<List<Integer>> zigzag = LevelOrderTraversal.zigzagLevelOrder(root); System.out.println(\"Result: \" + zigzag); System.out.println(\"(Should be: [[4], [6, 2], [1, 3, 5, 7]])\"); // Test 3: Right side view System.out.println(\"\\n--- Test 3: Right Side View ---\"); List<Integer> rightView = LevelOrderTraversal.rightSideView(root); System.out.println(\"Result: \" + rightView); System.out.println(\"(Should be: [4, 6, 7])\"); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken implementations. This tests your understanding of traversal mechanics. Challenge 1: Broken Inorder Traversal \u00b6 /** * This recursive inorder is supposed to return [1, 2, 3, 4, 5] * for a BST, but it has 2 BUGS. Find them! */ public static List<Integer> inorderRecursive_Buggy(TreeNode root) { List<Integer> result = new ArrayList<>(); if (root != null) { // Base case check inorderRecursive_Buggy(root.left); result.add(root.val); inorderRecursive_Buggy(root.right); } return result;} Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Test case: Input: Tree with values 1, 2, 3 Expected: [1, 2, 3] Actual with buggy code: [What do you get?] Click to verify your answers Bug 1: The recursive calls don't use the returned result! Each recursive call creates a new empty list. Bug 2: We're creating a new result list in each call, so left and right subtree results are lost. Fix - Need to pass result as parameter: public static List<Integer> inorderRecursive(TreeNode root) { List<Integer> result = new ArrayList<>(); inorderHelper(root, result); return result; } private static void inorderHelper(TreeNode root, List<Integer> result) { if (root == null) return; inorderHelper(root.left, result); result.add(root.val); inorderHelper(root.right, result); } Challenge 2: Broken Iterative Inorder \u00b6 /** * Iterative inorder with stack. * This has 2 CRITICAL BUGS with stack logic. */ public static List<Integer> inorderIterative_Buggy(TreeNode root) { List<Integer> result = new ArrayList<>(); Stack<TreeNode> stack = new Stack<>(); TreeNode curr = root; while (!stack.isEmpty()) { while (curr != null) { stack.push(curr); curr = curr.left; } TreeNode node = stack.pop(); result.add(node.val); curr = curr.left; } return result; } Your debugging: Bug 1: [What's wrong with the while condition?] Bug 1 effect: [What happens? When does loop start/stop?] Bug 1 fix: [Correct condition] Bug 2: [Which direction should curr move?] Bug 2 effect: [What happens? Infinite loop? Wrong order?] Bug 2 fix: [Fill in] Trace through example: Input: Tree with values 1, 2, 3 Expected: [1, 2, 3] With Bug 1: [What happens?] With Bug 2: [What happens?] Click to verify your answers Bug 1: Loop condition should be while (curr != null || !stack.isEmpty()) . Current code won't even start if root is not null. Bug 2: Should be curr = node.right , not curr = curr.left . After visiting a node, we need to go to its right subtree. Correct code: while (curr != null || !stack.isEmpty()) { while (curr != null) { stack.push(curr); curr = curr.left; } TreeNode node = stack.pop(); result.add(node.val); curr = node.right; // Go right after visiting } Challenge 3: Broken Postorder Traversal \u00b6 /** * Postorder using two stacks. * This has 1 LOGIC BUG in the order of operations. */ public static List<Integer> postorderIterative_Buggy(TreeNode root) { List<Integer> result = new ArrayList<>(); if (root == null) return result; Stack<TreeNode> stack1 = new Stack<>(); Stack<TreeNode> stack2 = new Stack<>(); stack1.push(root); while (!stack1.isEmpty()) { TreeNode node = stack1.pop(); stack2.push(node); if (node.right != null) stack1.push(node.right); if (node.left != null) stack1.push(node.left); } while (!stack2.isEmpty()) { result.add(stack2.pop().val); } return result; } Your debugging: Bug location: [Which lines push to stack1?] Bug explanation: [Why does order matter here?] Expected order: [Postorder is Left, Right, Root - so what should we push first?] Think through it: Postorder visits: Left, Right, Root Stack2 reverses the order So stack1 should create what order? [Fill in your reasoning] Click to verify your answer Actually, this code is CORRECT! It's a trick question. Why it works: We want postorder: Left, Right, Root Stack2 reverses the order we put in So we create: Root, Right, Left (which reverses to Left, Right, Root) To create Root, Right, Left in stack2, we push Right then Left to stack1 The code is fine as-is. This tests if you understand the two-stack technique! Challenge 4: Broken Level-Order Traversal \u00b6 /** * Level-order traversal using queue. * This has 2 BUGS with level tracking. */ public static List<List<Integer>> levelOrder_Buggy(TreeNode root) { List<List<Integer>> result = new ArrayList<>(); if (root == null) return result; Queue<TreeNode> queue = new LinkedList<>(); queue.offer(root); while (!queue.isEmpty()) { List<Integer> level = new ArrayList<>(); while (!queue.isEmpty()) { // Wrong loop! TreeNode node = queue.poll(); level.add(node.val); if (node.left != null) queue.offer(node.left); if (node.right != null) queue.offer(node.right); } result.add(level); } return result; } Your debugging: Bug: [What\\'s the bug?] Test case: Tree: 4 / \\ 2 6 / 1 Expected: [[4], [2, 6], [1]] Actual with buggy code: [Trace through - what do you get?] Click to verify your answer Bug: The inner while loop processes until the queue is empty, which means it processes ALL levels at once, not one level at a time. Fix - Capture level size before inner loop: while (!queue.isEmpty()) { int levelSize = queue.size(); // Capture current level size List<Integer> level = new ArrayList<>(); for (int i = 0; i < levelSize; i++) { // Only process current level TreeNode node = queue.poll(); level.add(node.val); if (node.left != null) queue.offer(node.left); if (node.right != null) queue.offer(node.right); } result.add(level); } Why: By capturing queue.size() before the loop, we know exactly how many nodes are in the current level. New nodes added during the loop belong to the NEXT level. Challenge 5: Stack vs Queue Confusion \u00b6 /** * This is supposed to do level-order traversal. * But someone used the WRONG data structure! */ public static List<Integer> traversal_Buggy(TreeNode root) { List<Integer> result = new ArrayList<>(); if (root == null) return result; Stack<TreeNode> stack = new Stack<>(); stack.push(root); while (!stack.isEmpty()) { TreeNode node = stack.pop(); result.add(node.val); if (node.left != null) stack.push(node.left); if (node.right != null) stack.push(node.right); } return result; } Your debugging: What traversal does this actually perform? [Hint: Stack = DFS, Queue = BFS] What would the output be for a simple tree? [Trace through] If we want level-order, what should we use? [Stack/Queue/Other?] Key insight to understand: Stack (LIFO) gives you: [BFS/DFS - which one?] Queue (FIFO) gives you: [BFS/DFS - which one?] Click to verify your answer This performs PREORDER (DFS), not LEVEL-ORDER (BFS)! Why: Stack is LIFO (Last In, First Out) - goes deep first Queue is FIFO (First In, First Out) - goes wide first Fix: Queue<TreeNode> queue = new LinkedList<>(); // Use Queue! queue.offer(root); while (!queue.isEmpty()) { TreeNode node = queue.poll(); result.add(node.val); if (node.left != null) queue.offer(node.left); if (node.right != null) queue.offer(node.right); } Challenge 6: Null Pointer Trap \u00b6 /** * Morris traversal attempt. * This has a CRITICAL null pointer bug! */ public static List<Integer> morrisTraversal_Buggy(TreeNode root) { List<Integer> result = new ArrayList<>(); TreeNode curr = root; while (curr != null) { if (curr.left == null) { result.add(curr.val); curr = curr.right; } else { // Find predecessor TreeNode pred = curr.left; while (pred.right != null) { pred = pred.right; } // Create thread pred.right = curr; curr = curr.left; } } return result; } Your debugging: Bug: [What causes infinite loop?] When does it happen? [When we revisit a threaded node] Fix: [What condition should we check in the while loop?] Click to verify your answer Bug: The while loop while (pred.right != null) will loop forever once we create a thread (pred.right = curr), because we never check if pred.right == curr. Fix - Check for existing thread: TreeNode pred = curr.left; while (pred.right != null && pred.right != curr) { // Check for thread! pred = pred.right; } if (pred.right == null) { // No thread yet - create it pred.right = curr; curr = curr.left; } else { // Thread exists - remove it, visit node, go right pred.right = null; result.add(curr.val); curr = curr.right; } Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 8+ bugs across 6 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common traversal mistakes to avoid Common mistakes you discovered: [List the patterns you noticed] [Fill in] [Fill in] Key insights: Recursive traversals need: [What pattern for combining results?] Iterative inorder needs: [What loop condition?] Level-order needs: [Stack or Queue?] Morris traversal needs: [What check to avoid infinite loops?] Decision Framework \u00b6 Your task: Build decision trees for tree traversal selection. Question 1: Which traversal order do you need? \u00b6 Answer after solving problems: Need sorted order from BST? [Use inorder] Need to copy tree structure? [Use preorder] Need to delete tree safely? [Use postorder] Need level-by-level processing? [Use level-order] Question 2: Recursive vs Iterative? \u00b6 Recursive approach: Pros: [Simpler code, cleaner logic] Cons: [O(h) stack space, risk of stack overflow] Use when: [Tree depth is reasonable] Iterative approach: Pros: [Explicit control, no stack overflow] Cons: [More complex code, need explicit stack/queue] Use when: [Deep trees, production code] Your Decision Tree \u00b6 flowchart LR Start[\"Tree Traversal Selection\"] Q1{\"What order do you need?\"} Start --> Q1 Q2{\"Left, Root, Right<br/>(sorted in BST)?\"} Q3{\"Root, Left, Right<br/>(copy/serialize)?\"} Q4{\"Left, Right, Root<br/>(delete/cleanup)?\"} Q5{\"Level by level<br/>(BFS)?\"} Q6{\"Implementation choice?\"} Start --> Q6 N7[\"Simpler, but O(h) space\"] Q6 -->|\"Recursive\"| N7 N8[\"More code, but safer\"] Q6 -->|\"Iterative\"| N8 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 4): 94. Binary Tree Inorder Traversal Pattern: [Inorder - recursive/iterative/Morris] Your solution time: ___ Key insight: [Fill in after solving] 144. Binary Tree Preorder Traversal Pattern: [Preorder] Your solution time: ___ Key insight: [Fill in] 145. Binary Tree Postorder Traversal Pattern: [Postorder] Your solution time: ___ Key insight: [Fill in] 102. Binary Tree Level Order Traversal Pattern: [Level-order BFS] Your solution time: ___ Key insight: [Fill in] Medium (Complete 3-4): 103. Binary Tree Zigzag Level Order Traversal Pattern: [Level-order with direction alternation] Difficulty: [Rate 1-10] Key insight: [Fill in] 199. Binary Tree Right Side View Pattern: [Level-order, track rightmost] Difficulty: [Rate 1-10] Key insight: [Fill in] 107. Binary Tree Level Order Traversal II Pattern: [Level-order bottom-up] Difficulty: [Rate 1-10] Key insight: [Fill in] 230. Kth Smallest Element in a BST Pattern: [Inorder with counter] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 297. Serialize and Deserialize Binary Tree Pattern: [Preorder or level-order] Key insight: [Fill in after solving] 987. Vertical Order Traversal of a Binary Tree Pattern: [Custom traversal with coordinates] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Inorder: recursive, iterative, Morris all work Preorder: recursive and iterative both work Postorder: recursive and iterative both work Level-order: standard, zigzag, right view all work All client code runs successfully Pattern Recognition Can identify which traversal order to use Understand when to use recursive vs iterative Know BFS vs DFS trade-offs Recognize when Morris traversal helps Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (null, single node) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use traversals Can explain each traversal order's purpose Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand stack space vs heap space trade-offs Mastery Certification \u00b6 I certify that I can: Implement all four traversal patterns (inorder, preorder, postorder, level-order) from memory Implement both recursive and iterative versions Explain when and why to use each traversal order Identify the correct traversal for new problems Analyze time and space complexity for each approach Debug common traversal mistakes (stack/queue confusion, wrong loop conditions) Choose between recursive, iterative, and Morris based on constraints Teach this concept to someone else","title":"06. Trees - Traversals"},{"location":"dsa/06-trees-traversals/#trees-traversals","text":"Visit every node in a tree using Inorder, Preorder, Postorder, or Level-Order","title":"Trees - Traversals"},{"location":"dsa/06-trees-traversals/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What are tree traversals in one sentence? Your answer: [Fill in after implementation] Why do we need different traversal orders? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Tree traversals are like different ways to read a family tree...\" Your analogy: [Fill in] When does each traversal order matter? Your answer: [Fill in after solving problems] What's the difference between iterative and recursive? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/06-trees-traversals/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/06-trees-traversals/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/06-trees-traversals/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/06-trees-traversals/#debugging-challenges","text":"Your task: Find and fix bugs in broken implementations. This tests your understanding of traversal mechanics.","title":"Debugging Challenges"},{"location":"dsa/06-trees-traversals/#decision-framework","text":"Your task: Build decision trees for tree traversal selection.","title":"Decision Framework"},{"location":"dsa/06-trees-traversals/#practice","text":"","title":"Practice"},{"location":"dsa/06-trees-traversals/#review-checklist","text":"Before moving to the next topic: Implementation Inorder: recursive, iterative, Morris all work Preorder: recursive and iterative both work Postorder: recursive and iterative both work Level-order: standard, zigzag, right view all work All client code runs successfully Pattern Recognition Can identify which traversal order to use Understand when to use recursive vs iterative Know BFS vs DFS trade-offs Recognize when Morris traversal helps Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (null, single node) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use traversals Can explain each traversal order's purpose Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand stack space vs heap space trade-offs","title":"Review Checklist"},{"location":"dsa/07-trees-recursion/","text":"Trees - Recursion \u00b6 Solve tree problems using recursive divide-and-conquer approach ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is tree recursion in one sentence? Your answer: [Fill in after implementation] Why is recursion natural for trees? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Tree recursion is like solving a puzzle by breaking it into smaller identical puzzles...\" Your analogy: [Fill in] When does recursion work well for trees? Your answer: [Fill in after solving problems] What's the base case pattern for tree recursion? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Iterative tree height using level-order traversal: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Recursive tree height: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Space usage comparison: For a balanced tree with n = 1,000 nodes, height h = _____ Recursive call stack depth = _____ Level-order queue max size = _____ Scenario Predictions \u00b6 Scenario 1: Calculate height of this tree: 5 / \\ 3 8 / \\ 1 9 Base case returns: [What value for null nodes?] Left subtree height: [Calculate] Right subtree height: [Calculate] Total height: [What formula combines them?] Scenario 2: Find LCA of nodes 1 and 9 in the above tree Which subtrees contain each node? [Fill in] Where will LCA be found? [Node 5, 3, 8, or other?] Why? [Explain your reasoning] Scenario 3: Calculate diameter of the same tree What is diameter? [Define in your words] Diameter through root? [Left height + right height = ?] Could diameter be in a subtree? [Yes/No - Why?] Base Case Quiz \u00b6 Question: What should the base case return for tree height? return 0 when root is null return 1 when root is null return -1 when root is null return 0 when root is a leaf Verify after implementation: [Which one(s)?] Question: For path sum, what makes a valid path? Your answer: [Root to leaf? Any path? Fill in] When is targetSum considered found? [At leaf node? Any node?] Recursion Flow Quiz \u00b6 Question: In tree diameter calculation, why do we need both height AND diameter? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN difference between tree recursion and array recursion? Trees have two recursive calls, arrays have one Trees need base case, arrays don't Trees use divide-and-conquer, arrays use iteration Trees always return values up, arrays modify in-place Verify after implementation: [Which one(s)?] Before/After: Why This Pattern Matters \u00b6 Your task: Compare iterative vs recursive approaches to understand the impact. Example 1: Tree Height \u00b6 Problem: Calculate the height of a binary tree. Approach 1: Iterative (Level-Order Traversal) \u00b6 // Iterative approach - Use queue for level-order traversal public static int height_Iterative(TreeNode root) { if (root == null) return 0; Queue<TreeNode> queue = new LinkedList<>(); queue.offer(root); int height = 0; while (!queue.isEmpty()) { int levelSize = queue.size(); height++; for (int i = 0; i < levelSize; i++) { TreeNode node = queue.poll(); if (node.left != null) queue.offer(node.left); if (node.right != null) queue.offer(node.right); } } return height; } Analysis: Time: O(n) - Visit every node once Space: O(w) - Queue holds max width of tree (could be n/2 for balanced tree) Lines of code: ~15 Complexity: Need to track levels explicitly with queue size Approach 2: Recursive (Divide-and-Conquer) \u00b6 // Recursive approach - Natural tree structure public static int height_Recursive(TreeNode root) { if (root == null) return 0; int leftHeight = height_Recursive(root.left); int rightHeight = height_Recursive(root.right); return 1 + Math.max(leftHeight, rightHeight); } Analysis: Time: O(n) - Visit every node once Space: O(h) - Recursion stack depth equals tree height Lines of code: ~5 Complexity: Natural, elegant, follows tree structure Why Does Recursion Work Better? \u00b6 Key insight to understand: Trees are inherently recursive structures: A tree is either empty (null) OR A root node with left subtree and right subtree This natural recursion makes problems like height trivial: Height of empty tree = 0 (base case) Height of tree = 1 + max(left height, right height) After implementing, explain in your own words: Why is recursion more natural for trees? [Your answer] When would iterative be better? [Your answer] Example 2: Tree Diameter \u00b6 Problem: Find the longest path between any two nodes. Approach 1: Brute Force (Calculate height at every node) \u00b6 // Naive approach - Calculate height at every node separately public static int diameter_BruteForce(TreeNode root) { if (root == null) return 0; // Option 1: Diameter passes through root int leftHeight = height(root.left); int rightHeight = height(root.right); int diameterThroughRoot = leftHeight + rightHeight; // Option 2: Diameter is in left subtree int diameterLeft = diameter_BruteForce(root.left); // Option 3: Diameter is in right subtree int diameterRight = diameter_BruteForce(root.right); return Math.max(diameterThroughRoot, Math.max(diameterLeft, diameterRight)); } private static int height(TreeNode root) { if (root == null) return 0; return 1 + Math.max(height(root.left), height(root.right)); } Analysis: Time: O(n\u00b2) - For each node, calculate height (which visits subtree) Space: O(h) - Recursion stack Problem: Recalculates height multiple times Approach 2: Optimized Recursion (Calculate both at once) \u00b6 // Optimized approach - Calculate height and update diameter in one pass public static int diameter_Optimized(TreeNode root) { int[] maxDiameter = new int[1]; calculateHeight(root, maxDiameter); return maxDiameter[0]; } private static int calculateHeight(TreeNode root, int[] maxDiameter) { if (root == null) return 0; int leftHeight = calculateHeight(root.left, maxDiameter); int rightHeight = calculateHeight(root.right, maxDiameter); // Update diameter while calculating height maxDiameter[0] = Math.max(maxDiameter[0], leftHeight + rightHeight); return 1 + Math.max(leftHeight, rightHeight); } Analysis: Time: O(n) - Visit every node exactly once Space: O(h) - Recursion stack Key insight: Combine height calculation with diameter tracking Performance Comparison \u00b6 Tree Size Brute Force (O(n\u00b2)) Optimized (O(n)) Speedup n = 100 ~10,000 ops 100 ops 100x n = 1,000 ~1,000,000 ops 1,000 ops 1,000x n = 10,000 ~100,000,000 ops 10,000 ops 10,000x Your calculation: For n = 5,000, the speedup is approximately _____ times faster. After implementing, explain in your own words: Why does the optimized version avoid recalculation? [Your answer] What pattern do you see in combining calculations? [Your answer] Example 3: Path Sum \u00b6 Problem: Check if a root-to-leaf path exists with a given sum. Approach 1: Collect All Paths, Then Check \u00b6 // Less efficient - Build all paths, then check public static boolean hasPathSum_Naive(TreeNode root, int targetSum) { List<List<Integer>> allPaths = new ArrayList<>(); collectPaths(root, new ArrayList<>(), allPaths); for (List<Integer> path : allPaths) { int sum = 0; for (int val : path) sum += val; if (sum == targetSum) return true; } return false; } private static void collectPaths(TreeNode root, List<Integer> current, List<List<Integer>> allPaths) { if (root == null) return; current.add(root.val); if (root.left == null && root.right == null) { allPaths.add(new ArrayList<>(current)); } else { collectPaths(root.left, current, allPaths); collectPaths(root.right, current, allPaths); } current.remove(current.size() - 1); } Analysis: Time: O(n) to collect + O(n) to check = O(n) Space: O(n) - Store all paths Problem: Unnecessary space usage Approach 2: Check While Traversing \u00b6 // Efficient - Check sum during traversal public static boolean hasPathSum_Optimized(TreeNode root, int targetSum) { if (root == null) return false; // At leaf: check if remaining sum equals node value if (root.left == null && root.right == null) { return root.val == targetSum; } // Recursively check left and right with reduced sum int remaining = targetSum - root.val; return hasPathSum_Optimized(root.left, remaining) || hasPathSum_Optimized(root.right, remaining); } Analysis: Time: O(n) - May terminate early Space: O(h) - Only recursion stack Key insight: Check condition while traversing, not after Why is the second approach better? Early termination: Returns as soon as a path is found Less space: No need to store all paths Clearer logic: Directly expresses the problem After implementing, explain in your own words: When should you check conditions during recursion vs after? [Your answer] What's the benefit of reducing the target while recursing? [Your answer] Core Implementation \u00b6 Pattern 1: Height and Depth \u00b6 Concept: Calculate tree metrics recursively. Use case: Tree height, depth, balanced tree check. public class TreeHeightDepth { static class TreeNode { int val; TreeNode left, right; TreeNode(int val) { this.val = val; } } /** * Problem: Calculate height of tree * Time: O(n), Space: O(h) for recursion stack * * TODO: Implement recursive height calculation */ public static int height(TreeNode root) { // TODO: Handle base case // TODO: Recursively get left height // TODO: Recursively get right height // TODO: Return 1 + max(leftHeight, rightHeight) return 0; // Replace with implementation } /** * Problem: Check if tree is balanced * Time: O(n), Space: O(h) * * TODO: Implement balanced tree check */ public static boolean isBalanced(TreeNode root) { // TODO: Tree is balanced if: // TODO: Return checkBalance(root) \\!= -1 return false; // Replace with implementation } private static int checkBalance(TreeNode root) { // TODO: Base case: null returns 0 // TODO: Check left subtree balance // TODO: Implement iteration/conditional logic // TODO: Check right subtree balance // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic // TODO: Otherwise return 1 + max(left, right) return 0; // Replace with implementation } /** * Problem: Calculate minimum depth (shortest path to leaf) * Time: O(n), Space: O(h) * * TODO: Implement minimum depth */ public static int minDepth(TreeNode root) { // TODO: Handle base case // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic // TODO: Both children exist: return 1 + min(left, right) return 0; // Replace with implementation } } Runnable Client Code: public class TreeHeightDepthClient { public static void main(String[] args) { System.out.println(\"=== Tree Height and Depth ===\\n\"); // Create balanced tree: // 4 // / \\ // 2 6 // / \\ / \\ // 1 3 5 7 TreeNode balanced = new TreeNode(4); balanced.left = new TreeNode(2); balanced.right = new TreeNode(6); balanced.left.left = new TreeNode(1); balanced.left.right = new TreeNode(3); balanced.right.left = new TreeNode(5); balanced.right.right = new TreeNode(7); System.out.println(\"--- Test 1: Balanced Tree ---\"); System.out.println(\"Height: \" + TreeHeightDepth.height(balanced)); System.out.println(\"Is balanced: \" + TreeHeightDepth.isBalanced(balanced)); System.out.println(\"Min depth: \" + TreeHeightDepth.minDepth(balanced)); // Create unbalanced tree: // 1 // / // 2 // / // 3 TreeNode unbalanced = new TreeNode(1); unbalanced.left = new TreeNode(2); unbalanced.left.left = new TreeNode(3); System.out.println(\"\\n--- Test 2: Unbalanced Tree ---\"); System.out.println(\"Height: \" + TreeHeightDepth.height(unbalanced)); System.out.println(\"Is balanced: \" + TreeHeightDepth.isBalanced(unbalanced)); System.out.println(\"Min depth: \" + TreeHeightDepth.minDepth(unbalanced)); } } Pattern 2: Diameter and Paths \u00b6 Concept: Find longest paths in tree. Use case: Tree diameter, max path sum, all paths. import java.util.*; public class TreeDiameterPaths { static class TreeNode { int val; TreeNode left, right; TreeNode(int val) { this.val = val; } } /** * Problem: Calculate diameter (longest path between any two nodes) * Time: O(n), Space: O(h) * * TODO: Implement diameter calculation */ public static int diameter(TreeNode root) { int[] maxDiameter = new int[1]; // TODO: Helper function to calculate height and update diameter // TODO: Implement iteration/conditional logic // TODO: Track maximum diameter seen calculateHeight(root, maxDiameter); return maxDiameter[0]; } private static int calculateHeight(TreeNode root, int[] maxDiameter) { // TODO: Base case: null returns 0 // TODO: Get left and right heights // TODO: Update maxDiameter: max(current, left + right) // TODO: Return 1 + max(left, right) return 0; // Replace with implementation } /** * Problem: Check if path exists with given sum * Time: O(n), Space: O(h) * * TODO: Implement path sum check */ public static boolean hasPathSum(TreeNode root, int targetSum) { // TODO: Handle base case // TODO: Implement iteration/conditional logic // TODO: Recursively check left and right subtrees // TODO: with targetSum - root.val return false; // Replace with implementation } /** * Problem: Find all root-to-leaf paths with given sum * Time: O(n), Space: O(h) * * TODO: Implement path sum II with backtracking */ public static List<List<Integer>> pathSum(TreeNode root, int targetSum) { List<List<Integer>> result = new ArrayList<>(); // TODO: Use backtracking to explore all paths // TODO: Add node to path, recurse, remove node (backtrack) return result; // Replace with implementation } } Runnable Client Code: import java.util.*; public class TreeDiameterPathsClient { public static void main(String[] args) { System.out.println(\"=== Tree Diameter and Paths ===\\n\"); // Create tree: // 5 // / \\ // 4 8 // / / \\ // 11 13 4 // / \\ / \\ // 7 2 5 1 TreeNode root = new TreeNode(5); root.left = new TreeNode(4); root.right = new TreeNode(8); root.left.left = new TreeNode(11); root.left.left.left = new TreeNode(7); root.left.left.right = new TreeNode(2); root.right.left = new TreeNode(13); root.right.right = new TreeNode(4); root.right.right.left = new TreeNode(5); root.right.right.right = new TreeNode(1); System.out.println(\"--- Test 1: Diameter ---\"); System.out.println(\"Diameter: \" + TreeDiameterPaths.diameter(root)); System.out.println(\"\\n--- Test 2: Has Path Sum (22) ---\"); System.out.println(\"Has path: \" + TreeDiameterPaths.hasPathSum(root, 22)); System.out.println(\"\\n--- Test 3: All Paths with Sum 22 ---\"); List<List<Integer>> paths = TreeDiameterPaths.pathSum(root, 22); System.out.println(\"Paths: \" + paths); } } Pattern 3: Lowest Common Ancestor (LCA) \u00b6 Concept: Find common ancestor of two nodes. Use case: LCA in binary tree, LCA in BST, distance between nodes. public class LowestCommonAncestor { static class TreeNode { int val; TreeNode left, right; TreeNode(int val) { this.val = val; } } /** * Problem: Find LCA in binary tree * Time: O(n), Space: O(h) * * TODO: Implement LCA for binary tree */ public static TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { // TODO: Handle base case // TODO: Recursively search in left and right subtrees // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic return null; // Replace with implementation } /** * Problem: Find LCA in BST (optimized) * Time: O(h), Space: O(h) * * TODO: Implement LCA for BST */ public static TreeNode lowestCommonAncestorBST(TreeNode root, TreeNode p, TreeNode q) { // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic // TODO: Otherwise, root is LCA return null; // Replace with implementation } } Runnable Client Code: public class LowestCommonAncestorClient { public static void main(String[] args) { System.out.println(\"=== Lowest Common Ancestor ===\\n\"); // Create BST: // 6 // / \\ // 2 8 // / \\ / \\ // 0 4 7 9 // / \\ // 3 5 TreeNode root = new TreeNode(6); root.left = new TreeNode(2); root.right = new TreeNode(8); root.left.left = new TreeNode(0); root.left.right = new TreeNode(4); root.left.right.left = new TreeNode(3); root.left.right.right = new TreeNode(5); root.right.left = new TreeNode(7); root.right.right = new TreeNode(9); TreeNode p = root.left; // Node 2 TreeNode q = root.right; // Node 8 System.out.println(\"--- Test 1: LCA of 2 and 8 ---\"); TreeNode lca = LowestCommonAncestor.lowestCommonAncestor(root, p, q); System.out.println(\"LCA: \" + (lca \\!= null ? lca.val : \"null\")); p = root.left; // Node 2 q = root.left.right; // Node 4 System.out.println(\"\\n--- Test 2: LCA of 2 and 4 ---\"); lca = LowestCommonAncestor.lowestCommonAncestor(root, p, q); System.out.println(\"LCA: \" + (lca \\!= null ? lca.val : \"null\")); System.out.println(\"\\n--- Test 3: LCA in BST (optimized) ---\"); lca = LowestCommonAncestor.lowestCommonAncestorBST(root, p, q); System.out.println(\"LCA: \" + (lca \\!= null ? lca.val : \"null\")); } } Pattern 4: Tree Construction \u00b6 Concept: Build tree from traversal arrays. Use case: Construct from inorder/preorder, inorder/postorder. import java.util.*; public class TreeConstruction { static class TreeNode { int val; TreeNode left, right; TreeNode(int val) { this.val = val; } } /** * Problem: Build tree from preorder and inorder traversals * Time: O(n), Space: O(n) * * TODO: Implement tree construction */ public static TreeNode buildTreePreIn(int[] preorder, int[] inorder) { // TODO: Create map of inorder indices for O(1) lookup // TODO: Use helper with preorder index pointer // TODO: Implement iteration/conditional logic return null; // Replace with implementation } /** * Problem: Build tree from postorder and inorder traversals * Time: O(n), Space: O(n) * * TODO: Implement tree construction */ public static TreeNode buildTreePostIn(int[] postorder, int[] inorder) { // TODO: Similar to preorder approach // TODO: But process postorder from right to left // TODO: Build right subtree before left return null; // Replace with implementation } // Helper: Print tree inorder static void printInorder(TreeNode root) { if (root == null) return; printInorder(root.left); System.out.print(root.val + \" \"); printInorder(root.right); } } Runnable Client Code: import java.util.*; public class TreeConstructionClient { public static void main(String[] args) { System.out.println(\"=== Tree Construction ===\\n\"); // Test 1: Build from preorder and inorder System.out.println(\"--- Test 1: Build from Preorder and Inorder ---\"); int[] preorder = {3, 9, 20, 15, 7}; int[] inorder = {9, 3, 15, 20, 7}; System.out.println(\"Preorder: \" + Arrays.toString(preorder)); System.out.println(\"Inorder: \" + Arrays.toString(inorder)); TreeNode root1 = TreeConstruction.buildTreePreIn(preorder, inorder); System.out.print(\"Built tree (inorder): \"); TreeConstruction.printInorder(root1); System.out.println(); // Test 2: Build from postorder and inorder System.out.println(\"\\n--- Test 2: Build from Postorder and Inorder ---\"); int[] postorder = {9, 15, 7, 20, 3}; int[] inorder2 = {9, 3, 15, 20, 7}; System.out.println(\"Postorder: \" + Arrays.toString(postorder)); System.out.println(\"Inorder: \" + Arrays.toString(inorder2)); TreeNode root2 = TreeConstruction.buildTreePostIn(postorder, inorder2); System.out.print(\"Built tree (inorder): \"); TreeConstruction.printInorder(root2); System.out.println(); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken tree recursion implementations. This tests your understanding of recursion flow. Challenge 1: Broken Tree Height \u00b6 /** * Calculate height of binary tree. * This has 2 BUGS. Find them! */ public static int height_Buggy(TreeNode root) { int leftHeight = height_Buggy(root.left); int rightHeight = height_Buggy(root.right); return Math.max(leftHeight, rightHeight);} Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Test case to expose bugs: Tree: Single node (value = 5) Expected height: 1 Actual with bugs: [What do you get? Trace through] Click to verify your answers Bug 1: Missing base case! Should check if (root == null) return 0; at the start. Bug 2: Missing the \"+1\" for the current node. Should be: return 1 + Math.max(leftHeight, rightHeight); Complete correct version: public static int height(TreeNode root) { if (root == null) return 0; int leftHeight = height(root.left); int rightHeight = height(root.right); return 1 + Math.max(leftHeight, rightHeight); } Challenge 2: Broken Diameter Calculation \u00b6 /** * Calculate tree diameter (longest path between any two nodes). * This has 1 CRITICAL BUG. */ public static int diameter_Buggy(TreeNode root) { if (root == null) return 0; int leftHeight = height(root.left); int rightHeight = height(root.right); return leftHeight + rightHeight; } private static int height(TreeNode root) { if (root == null) return 0; return 1 + Math.max(height(root.left), height(root.right)); } Your debugging: Bug explanation: [What case is this code missing?] Example that breaks it: Tree: 1 / 2 / \\ 3 4 Expected diameter: [What is it? Count the edges] Actual with bug: [What does the buggy code return?] Bug fix: [What must we also check?] Click to verify your answer Bug: The diameter might NOT pass through the root! It could be entirely in the left or right subtree. Example: 1 / 2 / \\ 3 4 The longest path is 3 \u2192 2 \u2192 4 (length 2), which is entirely in the left subtree of node 1. Correct approach: public static int diameter(TreeNode root) { if (root == null) return 0; int leftHeight = height(root.left); int rightHeight = height(root.right); int diameterThroughRoot = leftHeight + rightHeight; // MUST also check diameter in subtrees! int diameterLeft = diameter(root.left); int diameterRight = diameter(root.right); return Math.max(diameterThroughRoot, Math.max(diameterLeft, diameterRight)); } Better optimization: Calculate both height and diameter in one pass (as shown in Pattern 2). Challenge 3: Broken Path Sum \u00b6 /** * Check if any root-to-leaf path sums to target. * This has 2 SUBTLE BUGS. */ public static boolean hasPathSum_Buggy(TreeNode root, int targetSum) { if (root == null) return false; if (root.left == null || root.right == null) { return root.val == targetSum; } int remaining = targetSum - root.val; return hasPathSum_Buggy(root.left, remaining) && hasPathSum_Buggy(root.right, remaining); } Your debugging: Bug 1: [What's wrong with the leaf check?] Bug 1 fix: [How should we check if a node is a leaf?] Bug 2: [Should we use && or ||? Why?] Bug 2 fix: [Fill in the correct operator] Test case: Tree: 5 / \\ 4 8 / 11 Target: 9 (path 5 \u2192 4, but 4 is not a leaf!) Expected: false (no root-to-leaf path sums to 9) Actual with bugs: [What happens?] Click to verify your answers Bug 1: Leaf check should use && not || . A leaf has BOTH children null, not just one! Correct: if (root.left == null && root.right == null) Bug 2: Should use || not && . We're checking if EITHER subtree has a valid path, not both! Why: We only need ONE path that sums to target. Using && means BOTH subtrees must have valid paths, which is wrong. Complete correct version: public static boolean hasPathSum(TreeNode root, int targetSum) { if (root == null) return false; // Check if leaf node and sum matches if (root.left == null && root.right == null) { return root.val == targetSum; } int remaining = targetSum - root.val; return hasPathSum(root.left, remaining) || hasPathSum(root.right, remaining); } Challenge 4: Broken LCA \u00b6 /** * Find lowest common ancestor of two nodes. * This has 1 LOGIC ERROR in base case. */ public static TreeNode lowestCommonAncestor_Buggy(TreeNode root, TreeNode p, TreeNode q) { if (root == null) return null; TreeNode left = lowestCommonAncestor_Buggy(root.left, p, q); TreeNode right = lowestCommonAncestor_Buggy(root.right, p, q); if (left != null && right != null) return root; if (left != null) return left; return right; } Your debugging: Bug location: [What's missing in the base case?] Bug explanation: [What should happen when root equals p or q?] Test case: Tree: 3 / \\ 5 1 / \\ 6 2 p = node 5, q = node 1 Expected LCA: node 3 Actual with bug: [Trace through - what happens?] Bug fix: [What condition should be added?] Click to verify your answer Bug: Missing the check for when root == p or root == q ! Why it matters: If we find p or q, we should return it immediately. The LCA logic depends on this! Correct base case: if (root == null || root == p || root == q) return root; Why this works: If root is p, then either: q is in a subtree of p \u2192 LCA is p q is elsewhere \u2192 p will be returned up When both left and right are non-null, current root is LCA When only one side is non-null, LCA is in that subtree Complete correct version: public static TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root == null || root == p || root == q) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if (left != null && right != null) return root; return left != null ? left : right; } Challenge 5: Broken Balanced Tree Check \u00b6 /** * Check if tree is balanced (height difference <= 1 at every node). * This has 1 EFFICIENCY BUG. */ public static boolean isBalanced_Buggy(TreeNode root) { if (root == null) return true; int leftHeight = height(root.left); int rightHeight = height(root.right); return Math.abs(leftHeight - rightHeight) <= 1 && isBalanced_Buggy(root.left) && isBalanced_Buggy(root.right); } private static int height(TreeNode root) { if (root == null) return 0; return 1 + Math.max(height(root.left), height(root.right)); } Your debugging: Bug type: [Correctness or efficiency?] Time complexity: [What is it? O(n)? O(n\u00b2)? O(n log n)?] Bug explanation: [Why is it inefficient?] Better approach: [How can we check balance while calculating height in one pass?] Click to verify your answer Bug: Time complexity is O(n\u00b2)! Same issue as the diameter brute force approach. Why: For each node, we calculate height (O(n) in worst case), and we do this for all n nodes. Better approach: Return -1 to signal imbalance, actual height otherwise. Optimized version: public static boolean isBalanced(TreeNode root) { return checkBalance(root) != -1; } private static int checkBalance(TreeNode root) { if (root == null) return 0; int leftHeight = checkBalance(root.left); if (leftHeight == -1) return -1; // Left subtree unbalanced int rightHeight = checkBalance(root.right); if (rightHeight == -1) return -1; // Right subtree unbalanced // Check current node's balance if (Math.abs(leftHeight - rightHeight) > 1) return -1; return 1 + Math.max(leftHeight, rightHeight); } Time: O(n) - Each node visited once Space: O(h) - Recursion stack Challenge 6: Return Value Confusion \u00b6 /** * Find minimum depth (shortest path to a leaf). * This has 1 SUBTLE BUG with edge cases. */ public static int minDepth_Buggy(TreeNode root) { if (root == null) return 0; int leftDepth = minDepth_Buggy(root.left); int rightDepth = minDepth_Buggy(root.right); return 1 + Math.min(leftDepth, rightDepth); } Your debugging: Edge case that breaks: [What kind of tree structure fails?] Test case: Tree: 1 / 2 / 3 Expected min depth: 3 (only path is 1 \u2192 2 \u2192 3) Actual with bug: [What does the buggy code return?] Bug explanation: [Why does it return the wrong value?] Bug fix: [What special case must we handle?] Click to verify your answer Bug: When a node has only one child, taking the min incorrectly uses 0 from the null child! Example trace: Tree: 1 / 2 / 3 At node 3: leftDepth=0, rightDepth=0, return 1 + min(0,0) = 1 \u2713 At node 2: leftDepth=1, rightDepth=0, return 1 + min(1,0) = 1 \u2717 (WRONG!) At node 1: leftDepth=1, rightDepth=0, return 1 + min(1,0) = 1 \u2717 (WRONG!) Should return 3, but returns 1! Fix: Must handle single-child case specially: public static int minDepth(TreeNode root) { if (root == null) return 0; int leftDepth = minDepth(root.left); int rightDepth = minDepth(root.right); // If one child is null, we must take the other path if (root.left == null) return 1 + rightDepth; if (root.right == null) return 1 + leftDepth; // Both children exist, take minimum return 1 + Math.min(leftDepth, rightDepth); } Key insight: Min depth must be to a LEAF node. A node with one null child is NOT a leaf! Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 8+ bugs across 6 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common tree recursion mistakes to avoid Common recursion mistakes you discovered: [Missing base cases] [Forgetting to add current node in calculation] [Using && instead of || (or vice versa)] [Not considering all cases (diameter in subtrees, single-child nodes)] [Recalculating values inefficiently] Decision Framework \u00b6 Your task: Build decision trees for tree recursion. Question 1: What information flows up the tree? \u00b6 Answer after solving problems: Single value (height, count)? [Simple recursion] Multiple values (balanced + height)? [Use array or class] Path information? [Use backtracking] Global state? [Use instance variable] Question 2: When to use helper functions? \u00b6 Direct recursion: Use when: [Single return value, no extra state] Example: [Height calculation] Helper with extra parameters: Use when: [Need to track state, indices, ranges] Example: [Tree construction from arrays] Helper with global variables: Use when: [Need to update global max/min] Example: [Diameter, max path sum] Your Decision Tree \u00b6 flowchart LR Start[\"Tree Recursion Pattern Selection\"] Q1{\"What are you calculating?\"} Start --> Q1 Q2{\"Single metric<br/>(height, count)?\"} Q3{\"Path-based<br/>(sum, all paths)?\"} Q4{\"Find node(s)<br/>(LCA, search)?\"} Q5{\"Build structure?\"} Q6{\"How to track state?\"} Start --> Q6 N7[\"Direct recursion\"] Q6 -->|\"No extra state\"| N7 N8[\"Helper with array/class\"] Q6 -->|\"Track max/min\"| N8 N9[\"Backtracking with list\"] Q6 -->|\"Track path\"| N9 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 4): 104. Maximum Depth of Binary Tree Pattern: [Height calculation] Your solution time: ___ Key insight: [Fill in after solving] 110. Balanced Binary Tree Pattern: [Balance check] Your solution time: ___ Key insight: [Fill in] 111. Minimum Depth of Binary Tree Pattern: [Min depth] Your solution time: ___ Key insight: [Fill in] 112. Path Sum Pattern: [Path recursion] Your solution time: ___ Key insight: [Fill in] Medium (Complete 3-4): 543. Diameter of Binary Tree Pattern: [Diameter calculation] Difficulty: [Rate 1-10] Key insight: [Fill in] 236. Lowest Common Ancestor of a Binary Tree Pattern: [LCA] Difficulty: [Rate 1-10] Key insight: [Fill in] 105. Construct Binary Tree from Preorder and Inorder Traversal Pattern: [Tree construction] Difficulty: [Rate 1-10] Key insight: [Fill in] 113. Path Sum II Pattern: [Backtracking paths] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 124. Binary Tree Maximum Path Sum Pattern: [Global max tracking] Key insight: [Fill in after solving] 297. Serialize and Deserialize Binary Tree Pattern: [Construction/serialization] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Height and depth: all metrics work Diameter and paths: calculation and search work LCA: binary tree and BST both work Construction: preorder/inorder and postorder/inorder work All client code runs successfully Pattern Recognition Can identify when to use recursion Understand base case patterns Know when to use helper functions Recognize backtracking patterns Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (null, single node, leaf) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use recursion Can explain recursion flow for each pattern Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand recursion stack and space complexity Mastery Certification \u00b6 I certify that I can: Implement all tree recursion patterns from memory Explain when and why to use each pattern Write correct base cases without hesitation Visualize the recursion flow for any tree problem Optimize O(n\u00b2) solutions to O(n) Debug common recursion mistakes Choose between recursion and iteration Teach these concepts to someone else","title":"07. Trees - Recursion"},{"location":"dsa/07-trees-recursion/#trees-recursion","text":"Solve tree problems using recursive divide-and-conquer approach","title":"Trees - Recursion"},{"location":"dsa/07-trees-recursion/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is tree recursion in one sentence? Your answer: [Fill in after implementation] Why is recursion natural for trees? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Tree recursion is like solving a puzzle by breaking it into smaller identical puzzles...\" Your analogy: [Fill in] When does recursion work well for trees? Your answer: [Fill in after solving problems] What's the base case pattern for tree recursion? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/07-trees-recursion/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/07-trees-recursion/#beforeafter-why-this-pattern-matters","text":"Your task: Compare iterative vs recursive approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/07-trees-recursion/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/07-trees-recursion/#debugging-challenges","text":"Your task: Find and fix bugs in broken tree recursion implementations. This tests your understanding of recursion flow.","title":"Debugging Challenges"},{"location":"dsa/07-trees-recursion/#decision-framework","text":"Your task: Build decision trees for tree recursion.","title":"Decision Framework"},{"location":"dsa/07-trees-recursion/#practice","text":"","title":"Practice"},{"location":"dsa/07-trees-recursion/#review-checklist","text":"Before moving to the next topic: Implementation Height and depth: all metrics work Diameter and paths: calculation and search work LCA: binary tree and BST both work Construction: preorder/inorder and postorder/inorder work All client code runs successfully Pattern Recognition Can identify when to use recursion Understand base case patterns Know when to use helper functions Recognize backtracking patterns Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (null, single node, leaf) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use recursion Can explain recursion flow for each pattern Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand recursion stack and space complexity","title":"Review Checklist"},{"location":"dsa/08-binary-search/","text":"Binary Search \u00b6 Reduce O(n) to O(log n) by eliminating half the search space each step ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is binary search in one sentence? Your answer: [Fill in after implementation] Why is O(log n) so fast? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Binary search is like finding a word in a dictionary by opening to the middle...\" Your analogy: [Fill in] When does binary search work? Your answer: [Fill in after solving problems] What breaks binary search? Your answer: [Fill in after testing] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Linear search through entire array: Time complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Binary search in sorted array: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Speedup calculation: If n = 1,024, linear search = n = _____ operations Binary search = log\u2082(n) = _____ operations Speedup factor: _____ times faster Scenario Predictions \u00b6 Scenario 1: Find 7 in [1, 3, 5, 7, 9, 11, 13] Can you use binary search? [Yes/No - Why?] Starting positions: left = , right = First mid calculation: mid = ___ If nums[mid] = 5 (too small), which pointer moves? [Left/Right - Why?] If nums[mid] = 9 (too big), which pointer moves? [Left/Right - Why?] Scenario 2: Find 6 in [1, 3, 5, 7, 9, 11, 13] (not present) What will binary search return? [Fill in] What's the value of left when search ends? [Fill in] Could you use that for insert position? [Yes/No - Why?] Scenario 3: Find 8 in rotated array [6, 7, 8, 1, 2, 3, 4, 5] Can you use classic binary search directly? [Yes/No - Why?] Which half is sorted after first mid? [Fill in your reasoning] How do you determine which half to search? [Fill in] Trade-off Quiz \u00b6 Question: When would linear search be BETTER than binary search? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN requirement for binary search to work? Array must have even length Array must be sorted or have monotonic property Array must contain unique elements Array must be positive integers Verify after implementation: [Which one(s)?] Question: What happens if you calculate mid as (left + right) / 2 with large numbers? Your answer: [Fill in - consider integer overflow] Verified answer: [Fill in after learning] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example: Find Target in Array \u00b6 Problem: Find target value in a sorted array of 1 million elements. Approach 1: Linear Search (Brute Force) \u00b6 // Naive approach - Check every element public static int linearSearch(int[] nums, int target) { for (int i = 0; i < nums.length; i++) { if (nums[i] == target) { return i; } } return -1; } Analysis: Time: O(n) - Worst case: check all elements Space: O(1) - No extra space For n = 1,000,000: up to 1,000,000 comparisons Approach 2: Binary Search (Optimized) \u00b6 // Optimized approach - Eliminate half each step public static int binarySearch(int[] nums, int target) { int left = 0; int right = nums.length - 1; while (left <= right) { int mid = left + (right - left) / 2; // Avoid overflow if (nums[mid] == target) return mid; if (nums[mid] < target) left = mid + 1; // Search right half else right = mid - 1; // Search left half } return -1; // Not found } Analysis: Time: O(log n) - Each step eliminates half Space: O(1) - No extra space For n = 1,000,000: only ~20 comparisons Performance Comparison \u00b6 Array Size Linear Search (O(n)) Binary Search (O(log n)) Speedup n = 100 100 ops 7 ops 14x n = 1,000 1,000 ops 10 ops 100x n = 10,000 10,000 ops 14 ops 714x n = 1,000,000 1,000,000 ops 20 ops 50,000x Your calculation: For n = 16,384, binary search needs _____ comparisons. Why Does Binary Search Work? \u00b6 Key insight to understand: In a sorted array [1, 3, 5, 7, 9, 11, 13] looking for 7: Step 1: left=0, right=6, mid=3, nums[3]=7 \u2192 FOUND! If we were looking for 11: Step 1: left=0, right=6, mid=3, nums[3]=7 (too small) \u2192 Move left=4 because target must be in right half Step 2: left=4, right=6, mid=5, nums[5]=11 \u2192 FOUND! Why can we eliminate half? Array is sorted, so all values to the left of mid are \u2264 nums[mid] All values to the right of mid are \u2265 nums[mid] If target > nums[mid], it CANNOT be in the left half If target < nums[mid], it CANNOT be in the right half So each comparison eliminates half the remaining elements! Logarithmic growth visualization: n = 1,024 \u2192 log\u2082(1,024) = 10 steps n = 2,048 \u2192 log\u2082(2,048) = 11 steps (doubled n, only +1 step!) n = 1,048,576 \u2192 log\u2082(1,048,576) = 20 steps After implementing, explain in your own words: Why is O(log n) so much faster than O(n)? [Your answer] What property of sorted arrays makes this possible? [Your answer] What happens at each step that makes it logarithmic? [Your answer] Aside: Java's Collections.binarySearch \u00b6 Quick reference: Understanding Java's built-in binary search helps with B+Tree implementation. Return Value Convention \u00b6 List<Integer> nums = Arrays.asList(1, 3, 5, 7, 9); // Element FOUND \u2192 returns index Collections.binarySearch(nums, 5); // Returns: 2 // Element NOT FOUND \u2192 returns -(insertion point) - 1 Collections.binarySearch(nums, 6); // Returns: -4 // Decode: insertion point = -(\u22124) - 1 = 3 Why negative encoding? Distinguishes \"not found\" from \"found at index 0\" Encodes where to insert to maintain sorted order Using with Comparators \u00b6 class Person { String name; int age; } // Search by specific field Comparator<Person> byAge = (p1, p2) -> Integer.compare(p1.age, p2.age); int idx = Collections.binarySearch(people, searchKey, byAge); Key insight: You can search by one field without exact object equality. Generic Implementation Pattern \u00b6 public static <T extends Comparable<T>> int binarySearch(List<T> list, T key) { int left = 0, right = list.size() - 1; while (left <= right) { int mid = left + (right - left) / 2; // Avoid overflow int cmp = list.get(mid).compareTo(key); if (cmp == 0) return mid; // Found else if (cmp < 0) left = mid + 1; // Search right else right = mid - 1; // Search left } return -(left + 1); // Not found - encode insertion point } Applying to B+Tree \u00b6 Finding child pointer in internal node: class InternalNode<K extends Comparable<K>> { List<K> keys; // [10, 20, 30] List<Node<K>> children; // [child0, child1, child2, child3] } private int findChildIndex(K searchKey) { int idx = Collections.binarySearch(keys, searchKey); if (idx >= 0) { return idx + 1; // Key found - go to right child } else { return -(idx + 1); // Not found - decode insertion point } } Visual: Given keys [10, 20, 30] , searching for 25 : Children: [<10] [10-20) [20-30) [\u226530] Keys: 10 20 30 Indices: 0 1 2 3 Result: child index 2 (between 20 and 30) Common Pitfalls \u00b6 // \u274c WRONG: Integer overflow int mid = (left + right) / 2; // \u2713 CORRECT: Avoids overflow int mid = left + (right - left) / 2; // \u274c WRONG: Misses single element case while (left < right) { ... } // \u2713 CORRECT: Handles all cases while (left <= right) { ... } // \u274c WRONG: Forgets to decode return Math.abs(idx); // \u2713 CORRECT: Properly decodes insertion point return idx >= 0 ? idx : -(idx + 1); Core Implementation \u00b6 Pattern 1: Classic Binary Search \u00b6 Concept: Search in sorted array by halving search space. Use case: Find target, find insert position, find boundaries. public class ClassicBinarySearch { /** * Problem: Find target in sorted array * Time: O(log n), Space: O(1) * * TODO: Implement classic binary search */ public static int binarySearch(int[] nums, int target) { // TODO: Initialize pointers/variables // TODO: Implement iteration/conditional logic // TODO: Return -1 if not found return -1; // Replace with implementation } /** * Problem: Find insert position for target * Time: O(log n), Space: O(1) * * TODO: Implement search insert position */ public static int searchInsert(int[] nums, int target) { // TODO: Similar to binary search // TODO: Return left pointer when not found return 0; // Replace with implementation } /** * Problem: Find first and last position of target * Time: O(log n), Space: O(1) * * TODO: Implement find range */ public static int[] searchRange(int[] nums, int target) { // TODO: Find leftmost occurrence // TODO: Find rightmost occurrence // TODO: Return [-1, -1] if not found return new int[]{-1, -1}; // Replace with implementation } private static int findFirst(int[] nums, int target) { // TODO: Binary search to find first occurrence // TODO: When found, continue searching left half return -1; // Replace with implementation } private static int findLast(int[] nums, int target) { // TODO: Binary search to find last occurrence // TODO: When found, continue searching right half return -1; // Replace with implementation } } Runnable Client Code: import java.util.*; public class ClassicBinarySearchClient { public static void main(String[] args) { System.out.println(\"=== Classic Binary Search ===\\n\"); // Test 1: Find target System.out.println(\"--- Test 1: Find Target ---\"); int[] arr = {1, 3, 5, 7, 9, 11, 13}; int[] targets = {5, 8, 1, 13}; System.out.println(\"Array: \" + Arrays.toString(arr)); for (int target : targets) { int index = ClassicBinarySearch.binarySearch(arr, target); System.out.printf(\"Search %d: index = %d%n\", target, index); } // Test 2: Search insert position System.out.println(\"\\n--- Test 2: Search Insert Position ---\"); int[] arr2 = {1, 3, 5, 6}; int[] insertTargets = {5, 2, 7, 0}; System.out.println(\"Array: \" + Arrays.toString(arr2)); for (int target : insertTargets) { int pos = ClassicBinarySearch.searchInsert(arr2, target); System.out.printf(\"Insert position for %d: %d%n\", target, pos); } // Test 3: Find range System.out.println(\"\\n--- Test 3: Find First and Last Position ---\"); int[] arr3 = {5, 7, 7, 8, 8, 8, 10}; int rangeTarget = 8; System.out.println(\"Array: \" + Arrays.toString(arr3)); int[] range = ClassicBinarySearch.searchRange(arr3, rangeTarget); System.out.printf(\"Range for %d: [%d, %d]%n\", rangeTarget, range[0], range[1]); } } Pattern 2: Rotated Array Search \u00b6 Concept: Search in rotated sorted array. Use case: Find target in rotated array, find minimum, find rotation point. public class RotatedArraySearch { /** * Problem: Search in rotated sorted array * Time: O(log n), Space: O(1) * * TODO: Implement rotated array search */ public static int search(int[] nums, int target) { // TODO: Initialize pointers/variables // TODO: Implement iteration/conditional logic // // Determine which half is sorted: // If nums[left] <= nums[mid]: left half sorted // Check if target in left half // Else: right half sorted // Check if target in right half return -1; // Replace with implementation } /** * Problem: Find minimum in rotated sorted array * Time: O(log n), Space: O(1) * * TODO: Implement find minimum */ public static int findMin(int[] nums) { // TODO: Initialize pointers/variables // TODO: Implement iteration/conditional logic // TODO: Return nums[left] return 0; // Replace with implementation } /** * Problem: Find rotation count (how many rotations) * Time: O(log n), Space: O(1) * * TODO: Implement rotation count */ public static int findRotationCount(int[] nums) { // TODO: Find index of minimum element // TODO: That index is the rotation count return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class RotatedArraySearchClient { public static void main(String[] args) { System.out.println(\"=== Rotated Array Search ===\\n\"); // Test 1: Search in rotated array System.out.println(\"--- Test 1: Search in Rotated Array ---\"); int[] rotated = {4, 5, 6, 7, 0, 1, 2}; int[] searchTargets = {0, 3, 4, 7}; System.out.println(\"Rotated array: \" + Arrays.toString(rotated)); for (int target : searchTargets) { int index = RotatedArraySearch.search(rotated, target); System.out.printf(\"Search %d: index = %d%n\", target, index); } // Test 2: Find minimum System.out.println(\"\\n--- Test 2: Find Minimum ---\"); int[][] rotatedArrays = { {3, 4, 5, 1, 2}, {4, 5, 6, 7, 0, 1, 2}, {11, 13, 15, 17} }; for (int[] arr : rotatedArrays) { int min = RotatedArraySearch.findMin(arr); System.out.printf(\"Array: %s -> Min: %d%n\", Arrays.toString(arr), min); } // Test 3: Find rotation count System.out.println(\"\\n--- Test 3: Find Rotation Count ---\"); for (int[] arr : rotatedArrays) { int count = RotatedArraySearch.findRotationCount(arr); System.out.printf(\"Array: %s -> Rotations: %d%n\", Arrays.toString(arr), count); } } } Pattern 3: Search in 2D Matrix \u00b6 Concept: Binary search in matrix with sorted properties. Use case: Search in row-sorted matrix, search in fully sorted matrix. public class Search2DMatrix { /** * Problem: Search in matrix where each row is sorted * Time: O(m log n), Space: O(1) * * TODO: Implement 2D matrix search (rows sorted) */ public static boolean searchMatrix1(int[][] matrix, int target) { // TODO: Binary search on each row // TODO: Or: binary search to find row, then binary search in row return false; // Replace with implementation } /** * Problem: Search in matrix sorted like a flat array * Time: O(log(m*n)), Space: O(1) * * TODO: Implement 2D matrix search (fully sorted) */ public static boolean searchMatrix2(int[][] matrix, int target) { // TODO: Treat matrix as 1D array // TODO: Initialize pointers/variables // TODO: Implement logic return false; // Replace with implementation } /** * Problem: Search in matrix sorted row-wise and column-wise * Time: O(m + n), Space: O(1) * * TODO: Implement staircase search */ public static boolean searchMatrixStaircase(int[][] matrix, int target) { // TODO: Start from top-right corner // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic return false; // Replace with implementation } } Runnable Client Code: public class Search2DMatrixClient { public static void main(String[] args) { System.out.println(\"=== Search in 2D Matrix ===\\n\"); // Test 1: Search in row-sorted matrix System.out.println(\"--- Test 1: Row-Sorted Matrix ---\"); int[][] matrix1 = { {1, 3, 5, 7}, {10, 11, 16, 20}, {23, 30, 34, 60} }; int[] targets1 = {3, 13, 60}; for (int target : targets1) { boolean found = Search2DMatrix.searchMatrix1(matrix1, target); System.out.printf(\"Search %d: %s%n\", target, found ? \"FOUND\" : \"NOT FOUND\"); } // Test 2: Search in fully sorted matrix System.out.println(\"\\n--- Test 2: Fully Sorted Matrix ---\"); int[][] matrix2 = { {1, 3, 5, 7}, {10, 11, 16, 20}, {23, 30, 34, 50} }; int[] targets2 = {3, 13, 50}; for (int target : targets2) { boolean found = Search2DMatrix.searchMatrix2(matrix2, target); System.out.printf(\"Search %d: %s%n\", target, found ? \"FOUND\" : \"NOT FOUND\"); } // Test 3: Staircase search System.out.println(\"\\n--- Test 3: Staircase Search ---\"); int[][] matrix3 = { {1, 4, 7, 11}, {2, 5, 8, 12}, {3, 6, 9, 16}, {10, 13, 14, 17} }; int[] targets3 = {5, 20, 14}; for (int target : targets3) { boolean found = Search2DMatrix.searchMatrixStaircase(matrix3, target); System.out.printf(\"Search %d: %s%n\", target, found ? \"FOUND\" : \"NOT FOUND\"); } } } Pattern 4: Binary Search on Answer \u00b6 Concept: Binary search on solution space, not array index. Use case: Find square root, find kth smallest, capacity problems. public class BinarySearchOnAnswer { /** * Problem: Find square root (integer part) * Time: O(log n), Space: O(1) * * TODO: Implement integer square root */ public static int mySqrt(int x) { // TODO: Binary search from 0 to x // TODO: Check if mid * mid <= x // TODO: Return largest mid where mid * mid <= x return 0; // Replace with implementation } /** * Problem: Find minimum capacity to ship packages in D days * Time: O(n log(sum)), Space: O(1) * * TODO: Implement capacity to ship */ public static int shipWithinDays(int[] weights, int days) { // TODO: Binary search on capacity // TODO: Initialize pointers/variables // TODO: Check if capacity allows shipping in D days return 0; // Replace with implementation } private static boolean canShip(int[] weights, int days, int capacity) { // TODO: Simulate shipping with given capacity // TODO: Return true if possible in D days return false; // Replace with implementation } /** * Problem: Find kth missing positive number * Time: O(log n), Space: O(1) * * TODO: Implement kth missing */ public static int findKthPositive(int[] arr, int k) { // TODO: Binary search on array // TODO: Count missing numbers at each position return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class BinarySearchOnAnswerClient { public static void main(String[] args) { System.out.println(\"=== Binary Search on Answer ===\\n\"); // Test 1: Square root System.out.println(\"--- Test 1: Integer Square Root ---\"); int[] sqrtInputs = {4, 8, 16, 27}; for (int x : sqrtInputs) { int sqrt = BinarySearchOnAnswer.mySqrt(x); System.out.printf(\"sqrt(%d) = %d%n\", x, sqrt); } // Test 2: Ship within days System.out.println(\"\\n--- Test 2: Ship Within Days ---\"); int[] weights = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}; int days = 5; System.out.println(\"Weights: \" + Arrays.toString(weights)); System.out.println(\"Days: \" + days); int capacity = BinarySearchOnAnswer.shipWithinDays(weights, days); System.out.println(\"Minimum capacity: \" + capacity); // Test 3: Kth missing positive System.out.println(\"\\n--- Test 3: Kth Missing Positive ---\"); int[] arr = {2, 3, 4, 7, 11}; int k = 5; System.out.println(\"Array: \" + Arrays.toString(arr)); System.out.println(\"k = \" + k); int kthMissing = BinarySearchOnAnswer.findKthPositive(arr, k); System.out.println(\"Kth missing: \" + kthMissing); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken binary search implementations. This tests your understanding. Challenge 1: Broken Classic Binary Search \u00b6 /** * This code is supposed to find target in sorted array. * It has 2 BUGS. Find them! */ public static int binarySearch_Buggy(int[] nums, int target) { int left = 0; int right = nums.length; while (left <= right) { int mid = (left + right) / 2; if (nums[mid] == target) { return mid; } if (nums[mid] < target) { left = mid + 1; } else { right = mid - 1; } } return -1; } Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Click to verify your answers Bug 1 (Line 7): right should be nums.length - 1 , not nums.length . Array indices are 0-based, so valid range is 0 to length-1. Using nums.length will cause ArrayIndexOutOfBoundsException. Bug 2 (Line 10): (left + right) / 2 can overflow when left and right are large integers. Should use left + (right - left) / 2 to avoid overflow. Correct version: int right = nums.length - 1; // Fix Bug 1 int mid = left + (right - left) / 2; // Fix Bug 2 Challenge 2: Broken Infinite Loop \u00b6 /** * Find first occurrence of target. * This has 1 CRITICAL BUG that causes infinite loop. */ public static int findFirst_Buggy(int[] nums, int target) { int left = 0; int right = nums.length - 1; int result = -1; while (left <= right) { int mid = left + (right - left) / 2; if (nums[mid] == target) { result = mid; right = mid; } else if (nums[mid] < target) { left = mid + 1; } else { right = mid - 1; } } return result; } Your debugging: Bug: [What\\'s the bug?] Test case to expose the bug: Input: [5, 7, 7, 8, 8, 8, 10] , target = 8 Expected: index 3 (first occurrence) Actual: [What happens? Trace through manually] Click to verify your answer Bug (Line 12): Should be right = mid - 1 , not right = mid . Why: When nums[mid] == target , we want to search the left half for an earlier occurrence. But if we set right = mid , and if mid happens to equal left (when left and right differ by 1), the loop never terminates because mid stays the same. Example trace with bug: nums = [5, 7, 7, 8, 8, 8, 10], target = 8 left=3, right=5, mid=4, nums[4]=8 \u2192 right=4 left=3, right=4, mid=3, nums[3]=8 \u2192 right=3 left=3, right=3, mid=3, nums[3]=8 \u2192 right=3 \u2190 INFINITE LOOP! Correct version: if (nums[mid] == target) { result = mid; right = mid - 1; // Continue searching left half } Challenge 3: Broken Rotated Array Search \u00b6 /** * Search in rotated sorted array. * This has 1 LOGIC BUG. */ public static int searchRotated_Buggy(int[] nums, int target) { int left = 0; int right = nums.length - 1; while (left <= right) { int mid = left + (right - left) / 2; if (nums[mid] == target) return mid; if (nums[left] < nums[mid]) { // Left half is sorted if (target >= nums[left] && target <= nums[mid]) { right = mid - 1; } else { left = mid + 1; } } else { // Right half is sorted if (target >= nums[mid] && target <= nums[right]) { left = mid + 1; } else { right = mid - 1; } } } return -1; } Your debugging: Bug: [What's the logic error in the left-sorted check?] Example: Test with [3, 1] , target = 1 Step 1: left=0, right=1, mid=0, nums[0]=3, nums[1]=1 nums[left] < nums[mid]? [What happens?] Expected: return 1 Actual: [What do you get?] Fix: [How to correct the condition?] Click to verify your answer Bug: Should be nums[left] <= nums[mid] (with equals), not nums[left] < nums[mid] . Why: When left == mid (happens when there are only 2 elements left and mid rounds down), the condition nums[left] < nums[mid] is false (3 is not < 3), so we incorrectly assume the right half is sorted. Example trace with bug: nums = [3, 1], target = 1 left=0, right=1, mid=0 nums[left]=3, nums[mid]=3 3 < 3? FALSE \u2192 assumes right half sorted target=1 >= nums[mid]=3? FALSE \u2192 right = mid - 1 = -1 \u2192 loop ends, returns -1 (WRONG!) Correct version: if (nums[left] <= nums[mid]) { // Include equals case // Left half is sorted if (target >= nums[left] && target < nums[mid]) { // Also: < not <= right = mid - 1; } else { left = mid + 1; } } Challenge 4: Wrong Mid Calculation in Search Insert \u00b6 /** * Find insert position for target. * This produces WRONG results. */ public static int searchInsert_Buggy(int[] nums, int target) { int left = 0; int right = nums.length - 1; while (left < right) { int mid = left + (right - left) / 2; if (nums[mid] == target) { return mid; } else if (nums[mid] < target) { left = mid + 1; } else { right = mid - 1; } } return left;} Your debugging: Bug 1: [Should loop condition be < or <= ?] Bug 2: [What if target should go after all elements?] Example: Input [1, 3, 5] , target = 4 Expected: 2 (insert between 3 and 5) Trace through: [What do you get?] Example: Input [1, 3, 5] , target = 6 Expected: 3 (insert at end) Trace through: [What do you get?] Fix: [How to handle both cases correctly?] Click to verify your answer Bug: The loop condition and pointer updates don't work together correctly. Two correct approaches: Approach 1: Use left <= right while (left <= right) { int mid = left + (right - left) / 2; if (nums[mid] == target) { return mid; } else if (nums[mid] < target) { left = mid + 1; } else { right = mid - 1; } } return left; // left is the insert position Approach 2: Use left < right with different pointer update while (left < right) { int mid = left + (right - left) / 2; if (nums[mid] < target) { left = mid + 1; } else { right = mid; // Keep right at mid (don't subtract 1) } } return left; The original bug: With left < right and right = mid - 1 , we can miss the correct insert position. Challenge 5: Off-by-One in Find Minimum (Rotated Array) \u00b6 /** * Find minimum in rotated sorted array. * This has OFF-BY-ONE error. */ public static int findMin_Buggy(int[] nums) { int left = 0; int right = nums.length - 1; while (left < right) { int mid = left + (right - left) / 2; if (nums[mid] > nums[right]) { left = mid; } else { right = mid - 1; } } return nums[left]; } Your debugging: Bug 1: [Line with left = mid - what's wrong?] Bug 2: [Line with right = mid - 1 - what's wrong?] Example: Input [3, 4, 5, 1, 2] Expected: 1 Trace through buggy version: [What do you get?] Fix: [Correct both pointer updates] Click to verify your answer Bug 1: left = mid should be left = mid + 1 . When nums[mid] > nums[right] , the minimum is in the right half, but NOT at mid (since nums[mid] is large), so we can safely skip mid. Bug 2: right = mid - 1 should be right = mid . When nums[mid] <= nums[right] , the minimum could BE at mid, so we can't skip it. Trace with bugs: nums = [3, 4, 5, 1, 2] left=0, right=4, mid=2, nums[2]=5, nums[4]=2 5 > 2 \u2192 left=2 (WRONG! Should be 3) left=2, right=4, mid=3, nums[3]=1, nums[4]=2 1 > 2? NO \u2192 right=2 (WRONG! Should be 3) left=2, right=2, loop ends return nums[2]=5 (WRONG! Should be 1) Correct version: if (nums[mid] > nums[right]) { left = mid + 1; // Min is definitely in right half } else { right = mid; // Min could be at mid } Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 8+ bugs across 5 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common binary search mistakes to avoid Common mistakes you discovered: [Off-by-one errors in initialization: right = length vs length-1] [Integer overflow in mid calculation] [Infinite loops from wrong pointer updates] [Wrong loop conditions: < vs <=] [Fill in more patterns you noticed] The three most dangerous binary search bugs: Off-by-one: [When does this happen?] Infinite loop: [What causes this?] Integer overflow: [How to prevent?] Decision Framework \u00b6 Your task: Build decision trees for binary search problems. Question 1: Is the data sorted? \u00b6 Answer after solving problems: Already sorted? [Classic binary search] Rotated? [Find sorted half, adjust search] Partially sorted? [Modified binary search] Not sorted? [Can't use binary search] Question 2: What are you searching for? \u00b6 Search for value: Direct search: [Classic binary search] First/last occurrence: [Modified binary search] Search for position: Insert position: [Binary search with left pointer] Peak element: [Binary search on local property] Search on answer space: Square root, capacity: [Binary search on range] Minimize/maximize: [Binary search with validation] Your Decision Tree \u00b6 flowchart LR Start[\"Binary Search Pattern Selection\"] Q1{\"Standard sorted array?\"} Start --> Q1 Q2{\"Rotated sorted array?\"} Start --> Q2 Q3{\"2D matrix?\"} Start --> Q3 N4([\"Treat as 1D \u2713\"]) Q3 -->|\"Fully sorted\"| N4 N5([\"Search each row \u2713\"]) Q3 -->|\"Row-sorted\"| N5 N6([\"Staircase search \u2713\"]) Q3 -->|\"Row & col sorted\"| N6 Q7{\"Not searching in array?\"} Start --> Q7 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 4): 704. Binary Search Pattern: [Classic binary search] Your solution time: ___ Key insight: [Fill in after solving] 35. Search Insert Position Pattern: [Binary search with insert] Your solution time: ___ Key insight: [Fill in] 69. Sqrt(x) Pattern: [Binary search on answer] Your solution time: ___ Key insight: [Fill in] 278. First Bad Version Pattern: [Find first occurrence] Your solution time: ___ Key insight: [Fill in] Medium (Complete 3-4): 33. Search in Rotated Sorted Array Pattern: [Rotated array] Difficulty: [Rate 1-10] Key insight: [Fill in] 34. Find First and Last Position of Element in Sorted Array Pattern: [Find range] Difficulty: [Rate 1-10] Key insight: [Fill in] 74. Search a 2D Matrix Pattern: [2D binary search] Difficulty: [Rate 1-10] Key insight: [Fill in] 153. Find Minimum in Rotated Sorted Array Pattern: [Find minimum] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 4. Median of Two Sorted Arrays Pattern: [Binary search on two arrays] Key insight: [Fill in after solving] 410. Split Array Largest Sum Pattern: [Binary search on answer] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Classic: search, insert, find range all work Rotated: search and find min both work 2D: all three matrix patterns work On answer: sqrt, capacity, kth missing work All client code runs successfully Pattern Recognition Can identify when to use binary search Understand sorted vs rotated arrays Know when to search on answer space Recognize 2D matrix patterns Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (empty, single element) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use binary search Can explain why O(log n) is fast Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand off-by-one errors and how to avoid them Mastery Certification \u00b6 I certify that I can: Implement classic binary search from memory Explain why O(log n) is fundamentally different from O(n) Identify and fix common binary search bugs Choose the correct binary search variant for new problems Analyze when binary search is better than alternatives Avoid off-by-one errors and infinite loops Explain the sorted/monotonic requirement clearly Teach this concept to someone else","title":"08. Binary Search"},{"location":"dsa/08-binary-search/#binary-search","text":"Reduce O(n) to O(log n) by eliminating half the search space each step","title":"Binary Search"},{"location":"dsa/08-binary-search/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is binary search in one sentence? Your answer: [Fill in after implementation] Why is O(log n) so fast? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Binary search is like finding a word in a dictionary by opening to the middle...\" Your analogy: [Fill in] When does binary search work? Your answer: [Fill in after solving problems] What breaks binary search? Your answer: [Fill in after testing]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/08-binary-search/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/08-binary-search/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/08-binary-search/#aside-javas-collectionsbinarysearch","text":"Quick reference: Understanding Java's built-in binary search helps with B+Tree implementation.","title":"Aside: Java's Collections.binarySearch"},{"location":"dsa/08-binary-search/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/08-binary-search/#debugging-challenges","text":"Your task: Find and fix bugs in broken binary search implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"dsa/08-binary-search/#decision-framework","text":"Your task: Build decision trees for binary search problems.","title":"Decision Framework"},{"location":"dsa/08-binary-search/#practice","text":"","title":"Practice"},{"location":"dsa/08-binary-search/#review-checklist","text":"Before moving to the next topic: Implementation Classic: search, insert, find range all work Rotated: search and find min both work 2D: all three matrix patterns work On answer: sqrt, capacity, kth missing work All client code runs successfully Pattern Recognition Can identify when to use binary search Understand sorted vs rotated arrays Know when to search on answer space Recognize 2D matrix patterns Problem Solving Solved 4 easy problems Solved 3-4 medium problems Analyzed time/space complexity Handled edge cases (empty, single element) Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use binary search Can explain why O(log n) is fast Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand off-by-one errors and how to avoid them","title":"Review Checklist"},{"location":"dsa/09-heaps/","text":"Heaps (Priority Queues) \u00b6 O(log n) insert/delete with O(1) access to min/max element ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is a heap in one sentence? Your answer: [Fill in after implementation] Why is it called a priority queue? Your answer: [Fill in after implementation] Real-world analogy: Example: \"A heap is like a hospital emergency room where patients are seen by urgency...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What's the difference between min-heap and max-heap? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Finding Kth largest using sorting: Time complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Finding Kth largest using min-heap of size K: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Efficiency calculation: If n = 100,000 and k = 10, sorting = n log n = _____ operations Heap approach = n log k = _____ operations Speedup factor: approximately _____ times faster Scenario Predictions \u00b6 Scenario 1: Find 3rd largest in [3, 2, 1, 5, 6, 4] Using min-heap of size 3: Which elements end up in the heap? [Fill in] What is at the top of the heap? [Fill in - is this the answer?] Why min-heap instead of max-heap? [Fill in your reasoning] Scenario 2: Find median of stream [5, 15, 1, 3] Using two heaps: After adding 5: maxHeap = , minHeap = , median = ___ After adding 15: maxHeap = , minHeap = , median = ___ After adding 1: maxHeap = , minHeap = , median = ___ After adding 3: maxHeap = , minHeap = , median = ___ Scenario 3: Merge 3 sorted lists: [1,4,5] , [1,3,4] , [2,6] Heap pattern applies? [Yes/No - Why?] Initial heap state: [Which elements start in heap?] After first extraction: [What gets removed? What gets added?] Trade-off Quiz \u00b6 Question: When would sorting be BETTER than heap for finding Kth largest? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN advantage of heap over Quick Select for Kth largest? Heap is always faster Heap works with data streams Heap uses less space Heap is easier to implement Verify after implementation: [Which one(s)?] Question: For finding top K elements, why use heap of size K instead of size N? Your answer: [Fill in reasoning] Verified: [Fill in after implementation] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example 1: Find Kth Largest Element \u00b6 Problem: Find the Kth largest element in an unsorted array. Approach 1: Sorting (Simple but Inefficient) \u00b6 // Naive approach - Sort entire array public static int findKthLargest_Sorting(int[] nums, int k) { Arrays.sort(nums); // Sort ascending return nums[nums.length - k]; // Kth largest } Analysis: Time: O(n log n) - Must sort all n elements Space: O(1) or O(log n) depending on sort algorithm For n = 100,000: ~1,600,000 operations Approach 2: Min-Heap of Size K (Optimized) \u00b6 // Optimized approach - Maintain heap of K largest elements public static int findKthLargest_Heap(int[] nums, int k) { PriorityQueue<Integer> minHeap = new PriorityQueue<>(); for (int num : nums) { minHeap.offer(num); if (minHeap.size() > k) { minHeap.poll(); // Remove smallest } } return minHeap.peek(); // Kth largest at top } Analysis: Time: O(n log k) - n insertions, each log k operations Space: O(k) - Only store K elements For n = 100,000, k = 10: ~166,000 operations Performance Comparison \u00b6 Array Size (n) k Sorting (O(n log n)) Heap (O(n log k)) Speedup n = 1,000 10 ~10,000 ops ~3,000 ops 3x n = 10,000 10 ~130,000 ops ~33,000 ops 4x n = 100,000 100 ~1,600,000 ops ~660,000 ops 2.4x Your calculation: For n = 50,000 and k = 50, the speedup is approximately _____ times faster. Why Does Min-Heap Work for Kth LARGEST? \u00b6 Key insight to understand: In array [3, 2, 1, 5, 6, 4] looking for k=2 (2nd largest): Step 1: Add 3 \u2192 Heap: [3] Step 2: Add 2 \u2192 Heap: [2, 3] (size=2) Step 3: Add 1 \u2192 Heap: [2, 3], size > k \u2192 remove min \u2192 Heap: [3] Step 4: Add 5 \u2192 Heap: [3, 5] Step 5: Add 6 \u2192 Heap: [3, 5], size > k \u2192 remove min \u2192 Heap: [5, 6] Step 6: Add 4 \u2192 Heap: [4, 5, 6], remove min \u2192 Heap: [5, 6] Answer: heap.peek() = 5 (2nd largest) Why min-heap, not max-heap? Min-heap keeps K largest elements, smallest of them at top When heap size exceeds K, we remove the SMALLEST of the K largest The top element is the Kth largest! After implementing, explain in your own words: Why does removing the minimum preserve the K largest elements? [Your answer] What would happen with a max-heap instead? [Your answer] Example 2: Finding Running Median \u00b6 Problem: Maintain the median as numbers are added one by one. Approach 1: Sort Every Time (Inefficient) \u00b6 // Naive approach - Re-sort after each insertion public static class MedianFinder_Sorting { private List<Integer> list = new ArrayList<>(); public void addNum(int num) { list.add(num); Collections.sort(list); // Re-sort entire list! } public double findMedian() { int n = list.size(); if (n % 2 == 1) { return list.get(n / 2); } else { return (list.get(n / 2 - 1) + list.get(n / 2)) / 2.0; } } } Analysis: Time: O(n log n) per insertion due to sorting Space: O(n) For 10,000 insertions: ~100,000,000 total operations Approach 2: Two Heaps (Optimized) \u00b6 // Optimized approach - Two heaps maintain balance public static class MedianFinder_TwoHeaps { private PriorityQueue<Integer> maxHeap; // Smaller half private PriorityQueue<Integer> minHeap; // Larger half public MedianFinder_TwoHeaps() { maxHeap = new PriorityQueue<>(Collections.reverseOrder()); minHeap = new PriorityQueue<>(); } public void addNum(int num) { maxHeap.offer(num); minHeap.offer(maxHeap.poll()); if (maxHeap.size() < minHeap.size()) { maxHeap.offer(minHeap.poll()); } } public double findMedian() { if (maxHeap.size() > minHeap.size()) { return maxHeap.peek(); } return (maxHeap.peek() + minHeap.peek()) / 2.0; } } Analysis: Time: O(log n) per insertion Space: O(n) For 10,000 insertions: ~130,000 total operations Performance Comparison \u00b6 Number of Elements Sorting (O(n\u00b2log n) total) Two Heaps (O(n log n) total) Speedup n = 100 ~66,000 ops ~700 ops 94x n = 1,000 ~10,000,000 ops ~10,000 ops 1,000x n = 10,000 ~1,300,000,000 ops ~130,000 ops 10,000x Your calculation: For n = 5,000 elements, the speedup is approximately _____ times faster. Why Do Two Heaps Work? \u00b6 Key insight: Stream: [5, 15, 1, 3] Add 5: maxHeap=[5], minHeap=[] \u2192 Median = 5 Add 15: maxHeap=[5], minHeap=[15] \u2192 Median = (5+15)/2 = 10 Add 1: maxHeap=[5,1], minHeap=[15] \u2192 Median = 5 Add 3: maxHeap=[5,3,1], minHeap=[15] \u2192 rebalance \u2192 maxHeap=[5,3], minHeap=[15] \u2192 Median = (5+15)/2 = 10 Invariant maintained: maxHeap contains smaller half (max element on top) minHeap contains larger half (min element on top) Size difference \u2264 1 Median is at the top(s)! After implementing, explain in your own words: Why do we need both heaps instead of just sorting? [Your answer] How does keeping them balanced help find median quickly? [Your answer] Core Implementation \u00b6 Pattern 1: Basic Heap Operations \u00b6 Concept: Maintain heap property through insert and extract operations. Use case: Find kth largest/smallest, top K elements. import java.util.*; public class BasicHeapOperations { /** * Problem: Find Kth largest element in array * Time: O(n log k), Space: O(k) * * TODO: Implement using min-heap of size k */ public static int findKthLargest(int[] nums, int k) { // TODO: Create PriorityQueue (min-heap) of size k // TODO: Implement iteration/conditional logic // TODO: Return heap.peek() (kth largest) return 0; // Replace with implementation } /** * Problem: Find K largest elements * Time: O(n log k), Space: O(k) * * TODO: Implement K largest elements */ public static List<Integer> kLargest(int[] nums, int k) { // TODO: Use min-heap of size k // TODO: Maintain k largest elements // TODO: Return heap as list return new ArrayList<>(); // Replace with implementation } /** * Problem: Sort array using heap * Time: O(n log n), Space: O(n) * * TODO: Implement heap sort */ public static int[] heapSort(int[] nums) { // TODO: Add all elements to max-heap // TODO: Extract max repeatedly to get sorted array return new int[0]; // Replace with implementation } } Runnable Client Code: import java.util.*; public class BasicHeapOperationsClient { public static void main(String[] args) { System.out.println(\"=== Basic Heap Operations ===\\n\"); // Test 1: Kth largest System.out.println(\"--- Test 1: Kth Largest ---\"); int[] arr = {3, 2, 1, 5, 6, 4}; int k = 2; System.out.println(\"Array: \" + Arrays.toString(arr)); System.out.println(\"k = \" + k); int kthLargest = BasicHeapOperations.findKthLargest(arr, k); System.out.println(\"Kth largest: \" + kthLargest); // Test 2: K largest elements System.out.println(\"\\n--- Test 2: K Largest Elements ---\"); int[] arr2 = {3, 2, 3, 1, 2, 4, 5, 5, 6}; k = 4; System.out.println(\"Array: \" + Arrays.toString(arr2)); System.out.println(\"k = \" + k); List<Integer> kLargest = BasicHeapOperations.kLargest(arr2, k); System.out.println(\"K largest: \" + kLargest); // Test 3: Heap sort System.out.println(\"\\n--- Test 3: Heap Sort ---\"); int[] arr3 = {5, 2, 8, 1, 9, 3}; System.out.println(\"Before: \" + Arrays.toString(arr3)); int[] sorted = BasicHeapOperations.heapSort(arr3); System.out.println(\"After: \" + Arrays.toString(sorted)); } } Pattern 2: Merge K Sorted Lists/Arrays \u00b6 Concept: Use min-heap to merge multiple sorted sequences efficiently. Use case: Merge K sorted lists, merge K sorted arrays. import java.util.*; public class MergeKSorted { static class ListNode { int val; ListNode next; ListNode(int val) { this.val = val; } } /** * Problem: Merge K sorted linked lists * Time: O(n log k), Space: O(k) where n = total nodes, k = lists * * TODO: Implement using min-heap */ public static ListNode mergeKLists(ListNode[] lists) { // TODO: Create min-heap with comparator on node.val // TODO: Add first node from each list to heap // TODO: Implement iteration/conditional logic return null; // Replace with implementation } /** * Problem: Merge K sorted arrays * Time: O(n log k), Space: O(k) * * TODO: Implement using min-heap with array indices */ public static List<Integer> mergeKArrays(int[][] arrays) { // TODO: Create heap with [value, arrayIndex, elementIndex] // TODO: Add first element from each array // TODO: Extract min and add next from same array return new ArrayList<>(); // Replace with implementation } /** * Problem: Find smallest range covering elements from K lists * Time: O(n log k), Space: O(k) * * TODO: Implement using heap tracking max */ public static int[] smallestRange(List<List<Integer>> nums) { // TODO: Use min-heap, track current max // TODO: Range = [heap.peek(), currentMax] // TODO: Minimize range size return new int[]{0, 0}; // Replace with implementation } // Helper: Create linked list from array static ListNode createList(int[] values) { if (values.length == 0) return null; ListNode head = new ListNode(values[0]); ListNode current = head; for (int i = 1; i < values.length; i++) { current.next = new ListNode(values[i]); current = current.next; } return head; } // Helper: Print linked list static void printList(ListNode head) { ListNode current = head; while (current != null) { System.out.print(current.val); if (current.next != null) System.out.print(\" -> \"); current = current.next; } System.out.println(); } } Runnable Client Code: import java.util.*; public class MergeKSortedClient { public static void main(String[] args) { System.out.println(\"=== Merge K Sorted ===\\n\"); // Test 1: Merge K lists System.out.println(\"--- Test 1: Merge K Linked Lists ---\"); ListNode[] lists = new ListNode[3]; lists[0] = MergeKSorted.createList(new int[]{1, 4, 5}); lists[1] = MergeKSorted.createList(new int[]{1, 3, 4}); lists[2] = MergeKSorted.createList(new int[]{2, 6}); System.out.println(\"List 1: \"); MergeKSorted.printList(lists[0]); System.out.println(\"List 2: \"); MergeKSorted.printList(lists[1]); System.out.println(\"List 3: \"); MergeKSorted.printList(lists[2]); ListNode merged = MergeKSorted.mergeKLists(lists); System.out.print(\"Merged: \"); MergeKSorted.printList(merged); // Test 2: Merge K arrays System.out.println(\"\\n--- Test 2: Merge K Arrays ---\"); int[][] arrays = { {1, 3, 5, 7}, {2, 4, 6, 8}, {0, 9, 10, 11} }; for (int i = 0; i < arrays.length; i++) { System.out.println(\"Array \" + (i + 1) + \": \" + Arrays.toString(arrays[i])); } List<Integer> mergedArray = MergeKSorted.mergeKArrays(arrays); System.out.println(\"Merged: \" + mergedArray); // Test 3: Smallest range System.out.println(\"\\n--- Test 3: Smallest Range ---\"); List<List<Integer>> nums = Arrays.asList( Arrays.asList(4, 10, 15, 24, 26), Arrays.asList(0, 9, 12, 20), Arrays.asList(5, 18, 22, 30) ); System.out.println(\"Lists: \" + nums); int[] range = MergeKSorted.smallestRange(nums); System.out.println(\"Smallest range: [\" + range[0] + \", \" + range[1] + \"]\"); } } Pattern 3: Top K Frequent Elements \u00b6 Concept: Use heap to find most/least frequent elements. Use case: Top K frequent elements, sort by frequency. import java.util.*; public class TopKFrequent { /** * Problem: Find K most frequent elements * Time: O(n log k), Space: O(n) * * TODO: Implement using frequency map + min-heap */ public static List<Integer> topKFrequent(int[] nums, int k) { // TODO: Count frequencies with HashMap // TODO: Create min-heap of size k, ordered by frequency // TODO: Implement iteration/conditional logic // TODO: Return heap contents return new ArrayList<>(); // Replace with implementation } /** * Problem: Sort array by frequency * Time: O(n log n), Space: O(n) * * TODO: Implement frequency sort */ public static int[] frequencySort(int[] nums) { // TODO: Count frequencies // TODO: Sort by frequency (descending), then by value (ascending) return new int[0]; // Replace with implementation } /** * Problem: K closest points to origin * Time: O(n log k), Space: O(k) * * TODO: Implement using max-heap of size k */ public static int[][] kClosest(int[][] points, int k) { // TODO: Create max-heap ordered by distance // TODO: Maintain k closest points // TODO: Distance = x^2 + y^2 (no need for sqrt) return new int[0][0]; // Replace with implementation } } Runnable Client Code: import java.util.*; public class TopKFrequentClient { public static void main(String[] args) { System.out.println(\"=== Top K Frequent ===\\n\"); // Test 1: Top K frequent System.out.println(\"--- Test 1: Top K Frequent ---\"); int[] arr = {1, 1, 1, 2, 2, 3}; int k = 2; System.out.println(\"Array: \" + Arrays.toString(arr)); System.out.println(\"k = \" + k); List<Integer> topK = TopKFrequent.topKFrequent(arr, k); System.out.println(\"Top K frequent: \" + topK); // Test 2: Frequency sort System.out.println(\"\\n--- Test 2: Frequency Sort ---\"); int[] arr2 = {1, 1, 2, 2, 2, 3}; System.out.println(\"Before: \" + Arrays.toString(arr2)); int[] sorted = TopKFrequent.frequencySort(arr2); System.out.println(\"After: \" + Arrays.toString(sorted)); // Test 3: K closest points System.out.println(\"\\n--- Test 3: K Closest Points ---\"); int[][] points = {{1, 3}, {-2, 2}, {5, 8}, {0, 1}}; k = 2; System.out.println(\"Points:\"); for (int[] point : points) { System.out.println(\" \" + Arrays.toString(point)); } System.out.println(\"k = \" + k); int[][] closest = TopKFrequent.kClosest(points, k); System.out.println(\"K closest points:\"); for (int[] point : closest) { System.out.println(\" \" + Arrays.toString(point)); } } } Pattern 4: Two Heaps (Find Median) \u00b6 Concept: Use two heaps to maintain running median. Use case: Find median from data stream, sliding window median. import java.util.*; public class TwoHeaps { /** * MedianFinder: Maintain running median * Time: O(log n) insert, O(1) find median * Space: O(n) * * TODO: Implement using two heaps */ static class MedianFinder { // TODO: maxHeap stores smaller half (max at top) // TODO: minHeap stores larger half (min at top) // TODO: Keep heaps balanced: |size difference| <= 1 public MedianFinder() { // TODO: Initialize PriorityQueue for max-heap (reverse order) // TODO: Initialize PriorityQueue for min-heap (natural order) } public void addNum(int num) { // TODO: Add to appropriate heap // TODO: Balance heaps if needed // Hint: Always add to maxHeap first, then move to minHeap if needed } public double findMedian() { // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic return 0.0; // Replace with implementation } } /** * Problem: Sliding window median * Time: O(n * k), Space: O(k) * * TODO: Implement sliding window median */ public static double[] medianSlidingWindow(int[] nums, int k) { // TODO: Use two heaps approach // TODO: Handle removal from window // Note: This is complex - use TreeMap or simpler approach return new double[0]; // Replace with implementation } } Runnable Client Code: import java.util.*; public class TwoHeapsClient { public static void main(String[] args) { System.out.println(\"=== Two Heaps ===\\n\"); // Test 1: Median finder System.out.println(\"--- Test 1: Find Median from Data Stream ---\"); TwoHeaps.MedianFinder mf = new TwoHeaps.MedianFinder(); int[] stream = {1, 2, 3, 4, 5}; System.out.println(\"Data stream: \" + Arrays.toString(stream)); System.out.println(\"Median after each insertion:\"); for (int num : stream) { mf.addNum(num); System.out.printf(\" After adding %d: %.1f%n\", num, mf.findMedian()); } // Test 2: Another stream System.out.println(\"\\n--- Test 2: Another Stream ---\"); TwoHeaps.MedianFinder mf2 = new TwoHeaps.MedianFinder(); int[] stream2 = {5, 15, 1, 3}; System.out.println(\"Data stream: \" + Arrays.toString(stream2)); System.out.println(\"Median after each insertion:\"); for (int num : stream2) { mf2.addNum(num); System.out.printf(\" After adding %d: %.1f%n\", num, mf2.findMedian()); } // Test 3: Sliding window median System.out.println(\"\\n--- Test 3: Sliding Window Median ---\"); int[] arr = {1, 3, -1, -3, 5, 3, 6, 7}; int k = 3; System.out.println(\"Array: \" + Arrays.toString(arr)); System.out.println(\"Window size: \" + k); double[] medians = TwoHeaps.medianSlidingWindow(arr, k); System.out.println(\"Medians: \" + Arrays.toString(medians)); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken heap implementations. This tests your understanding. Challenge 1: Broken Kth Largest Finder \u00b6 /** * This code is supposed to find the Kth largest element. * It has 2 BUGS. Find them! */ public static int findKthLargest_Buggy(int[] nums, int k) { PriorityQueue<Integer> maxHeap = new PriorityQueue<>( Collections.reverseOrder() ); for (int num : nums) { maxHeap.offer(num); if (maxHeap.size() > k) { maxHeap.poll(); } } return maxHeap.peek(); } Your debugging: Bug 1: [What\\'s the bug?] Bug 2 location: [Which line?] Bug 2 explanation: [What gets removed? Is this what we want?] Bug 2 impact: [What will the final answer be?] Test case to expose the bug: Input: nums = [3, 2, 1, 5, 6, 4] , k = 2 Expected: 5 (2nd largest) Actual with buggy code: [Trace through manually] Click to verify your answers Bug 1: Should use min-heap , not max-heap! For Kth largest, we want to keep the K largest elements and remove the smallest among them. Correct: PriorityQueue<Integer> minHeap = new PriorityQueue<>(); Bug 2: With max-heap, poll() removes the LARGEST element, which is exactly what we want to KEEP! This defeats the purpose. Why min-heap works: Min-heap of size K keeps K largest elements The smallest of these K largest is at the top That smallest element IS the Kth largest! Trace with correct code: [3, 2, 1, 5, 6, 4], k=2 Add 3: [3] Add 2: [2,3] Add 1: [1,2,3], size>2, poll() removes 1 \u2192 [2,3] Add 5: [2,3,5], poll() removes 2 \u2192 [3,5] Add 6: [3,5,6], poll() removes 3 \u2192 [5,6] Add 4: [4,5,6], poll() removes 4 \u2192 [5,6] peek() = 5 \u2713 Challenge 2: Broken Two-Heap Median Finder \u00b6 /** * Find median from data stream using two heaps. * This has 2 CRITICAL BUGS. */ static class MedianFinder_Buggy { PriorityQueue<Integer> maxHeap = new PriorityQueue<>(Collections.reverseOrder()); PriorityQueue<Integer> minHeap = new PriorityQueue<>(); public void addNum(int num) { if (maxHeap.isEmpty() || num < maxHeap.peek()) { maxHeap.offer(num); } else { minHeap.offer(num); } } public double findMedian() { if (maxHeap.size() > minHeap.size()) { return maxHeap.peek(); } else if (minHeap.size() > maxHeap.size()) { return minHeap.peek(); } else { return (maxHeap.peek() + minHeap.peek()) / 2; } } } Your debugging: Bug 1: [What's missing after adding elements?] Bug 1 explanation: [What happens if sizes become unbalanced?] Bug 1 fix: [What code should be added?] Bug 2: [Which heap should contain the median for even split?] Bug 2 explanation: [Trace through example: add 1, 2, 3] Bug 2 fix: [Fill in] Test case: Stream: [1, 2, 3, 4, 5] Expected medians: [1.0, 1.5, 2.0, 2.5, 3.0] Actual with buggy code: [Trace through] Click to verify your answers Bug 1: Missing rebalancing logic ! Heaps can become severely unbalanced. Correct addNum: public void addNum(int num) { if (maxHeap.isEmpty() || num < maxHeap.peek()) { maxHeap.offer(num); } else { minHeap.offer(num); } // REBALANCE - maintain size difference \u2264 1 if (maxHeap.size() > minHeap.size() + 1) { minHeap.offer(maxHeap.poll()); } else if (minHeap.size() > maxHeap.size()) { maxHeap.offer(minHeap.poll()); } } Bug 2: The else-if case should probably never happen if we maintain maxHeap.size() >= minHeap.size() as invariant. But if it does, the logic is actually correct - we'd return minHeap.peek(). Better approach - Always add to maxHeap first: public void addNum(int num) { maxHeap.offer(num); minHeap.offer(maxHeap.poll()); if (maxHeap.size() < minHeap.size()) { maxHeap.offer(minHeap.poll()); } } This ensures maxHeap.size() is always >= minHeap.size(). Challenge 3: Heap Index Calculation Bugs \u00b6 /** * Manual heap implementation with parent/child index calculations. * This has INDEX CALCULATION BUGS. */ static class MinHeap_Buggy { private List<Integer> heap = new ArrayList<>(); private int parent(int i) { return i / 2; } private int leftChild(int i) { return 2 * i; } private int rightChild(int i) { return 2 * i + 1; } public void insert(int val) { heap.add(val); heapifyUp(heap.size() - 1); } private void heapifyUp(int i) { while (i > 0 && heap.get(i) < heap.get(parent(i))) { swap(i, parent(i)); i = parent(i); } } private void swap(int i, int j) { int temp = heap.get(i); heap.set(i, heap.get(j)); heap.set(j, temp); } } Your debugging: Bug 1: [What's the correct parent formula for 0-indexed array?] Bug 2: [What's the correct leftChild formula?] Bug 3: [What's the correct rightChild formula?] Heap index formulas: For 0-indexed array: Parent of i: [Your formula] Left child of i: [Your formula] Right child of i: [Your formula] Test case: Insert: 5, 3, 7, 1 Expected heap array: [1, 3, 7, 5] (min-heap property) Actual with buggy code: <span class=\"fill-in\">[What happens?]</span> Click to verify your answers All three formulas are wrong for 0-indexed arrays! Correct formulas for 0-indexed: private int parent(int i) { return (i - 1) / 2; // Not i / 2 } private int leftChild(int i) { return 2 * i + 1; // Not 2 * i } private int rightChild(int i) { return 2 * i + 2; // Not 2 * i + 1 } Why these formulas? In 0-indexed array [0, 1, 2, 3, 4, 5, 6] : 0 / \\ 1 2 / \\ / \\ 3 4 5 6 Parent of 1: (1-1)/2 = 0 \u2713 Parent of 2: (2-1)/2 = 0 \u2713 Left of 0: 2*0+1 = 1 \u2713 Right of 0: 2*0+2 = 2 \u2713 Left of 1: 2*1+1 = 3 \u2713 Right of 1: 2*1+2 = 4 \u2713 For 1-indexed arrays (if you stored heap starting at index 1): parent(i) = i / 2 leftChild(i) = 2 * i rightChild(i) = 2 * i + 1 This is why some people prefer 1-indexed - simpler formulas! Challenge 4: Min-Heap vs Max-Heap Confusion \u00b6 /** * Find K closest points to origin. * This code has HEAP TYPE BUG. */ public static int[][] kClosest_Buggy(int[][] points, int k) { PriorityQueue<int[]> minHeap = new PriorityQueue<>( (a, b) -> (a[0]*a[0] + a[1]*a[1]) - (b[0]*b[0] + b[1]*b[1]) ); for (int[] point : points) { minHeap.offer(point); if (minHeap.size() > k) { minHeap.poll(); } } int[][] result = new int[k][2]; for (int i = 0; i < k; i++) { result[i] = minHeap.poll(); } return result; } Your debugging: Bug: [What\\'s the bug?] Think about it: For K largest \u2192 Use [min/max] -heap For K smallest \u2192 Use [min/max] -heap For K closest \u2192 Same as K [largest/smallest] distances Therefore, use [min/max] -heap Test case: Points: [[1,3], [-2,2], [5,8], [0,1]], k=2 Distances: [10, 8, 89, 1] Expected: [[0,1], [-2,2]] (distances 1 and 8) Actual with buggy code: <span class=\"fill-in\">[What do you get?]</span> Click to verify your answers Bug: Using min-heap is WRONG! We need max-heap . Why? K closest = K smallest distances For K smallest distances, use max-heap (same logic as Kth largest) Keep K smallest, remove the largest of them when heap exceeds size K Correct: PriorityQueue<int[]> maxHeap = new PriorityQueue<>( (a, b) -> (b[0]*b[0] + b[1]*b[1]) - (a[0]*a[0] + a[1]*a[1]) // Note: b - a for max-heap (reverse comparison) ); Pattern to remember: K largest values \u2192 min-heap of size K (remove smallest) K smallest values \u2192 max-heap of size K (remove largest) This seems backwards but makes sense: you want to remove the \"outlier\" from your K elements! Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all bugs in Kth largest (min vs max heap) Found rebalancing bug in median finder Fixed all three index calculation formulas Understood when to use min-heap vs max-heap Could explain WHY each bug causes incorrect behavior Common heap mistakes you discovered: [Min vs max confusion - when to use which?] [Index calculation errors in manual implementation] [Forgetting to rebalance in two-heap pattern] [Fill in - what other patterns did you notice?] Decision Framework \u00b6 Your task: Build decision trees for when to use heaps. Question 1: What do you need to track? \u00b6 Answer after solving problems: Need min/max element repeatedly? [Use heap] Need Kth largest/smallest? [Use heap of size K] Need median? [Use two heaps] Your observation: [Fill in based on testing] Question 2: What are the time/space trade-offs? \u00b6 Answer for each pattern: Basic heap operations: Time complexity: [Insert? Extract? Peek?] Space complexity: [How much space?] Best use cases: [List problems you solved] Merge K sorted: Time complexity: [Compare to merge two at a time] Space complexity: [Just heap or output too?] Best use cases: [List problems you solved] Top K frequent: Time complexity: [Why log k not log n?] Space complexity: [Frequency map + heap] Best use cases: [List problems you solved] Two heaps: Time complexity: [Insert? Find median?] Space complexity: [Both heaps needed?] Best use cases: [List problems you solved] Your Decision Tree \u00b6 Build this after solving practice problems: flowchart LR Start[\"Heap Pattern Selection\"] Q1{\"Need Kth largest/smallest?\"} Start --> Q1 Q2{\"Merge K sorted sequences?\"} Start --> Q2 Q3{\"Need top K frequent?\"} Start --> Q3 Q4{\"Need running median?\"} Start --> Q4 Q5{\"Need all elements sorted?\"} Start --> Q5 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 3): 703. Kth Largest Element in a Stream Pattern: [Min-heap of size K] Your solution time: ___ Key insight: [Fill in after solving] 1046. Last Stone Weight Pattern: [Max-heap simulation] Your solution time: ___ Key insight: [Fill in] 1337. The K Weakest Rows in a Matrix Pattern: [Heap with custom comparator] Your solution time: ___ Key insight: [Fill in] Medium (Complete 3-4): 215. Kth Largest Element in an Array Pattern: [Min-heap approach] Difficulty: [Rate 1-10] Key insight: [Fill in] 347. Top K Frequent Elements Pattern: [Frequency + heap] Difficulty: [Rate 1-10] Key insight: [Fill in] 973. K Closest Points to Origin Pattern: [Max-heap of size K] Difficulty: [Rate 1-10] Key insight: [Fill in] 295. Find Median from Data Stream Pattern: [Two heaps] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 23. Merge k Sorted Lists Pattern: [Min-heap with K nodes] Key insight: [Fill in after solving] 480. Sliding Window Median Pattern: [Two heaps with removal] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Basic operations: Kth largest, K largest, heap sort all work Merge K: lists and arrays both work Top K: frequent elements and closest points work Two heaps: median finder works correctly All client code runs successfully Pattern Recognition Can identify when to use min-heap vs max-heap Understand when heap size should be K vs N Know when to use two heaps Recognize merge K sorted pattern Problem Solving Solved 3 easy problems Solved 3-4 medium problems Analyzed time/space complexity Understood heap vs sort trade-offs Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use heaps Can explain heap property and operations Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand PriorityQueue in Java Mastery Certification \u00b6 I certify that I can: Implement all four heap patterns from memory Explain when to use min-heap vs max-heap Identify the correct pattern for new problems Analyze time and space complexity Compare trade-offs with alternative approaches Debug common heap mistakes (min/max confusion, indexing errors) Implement two-heap median finder Calculate parent/child indices correctly Teach this concept to someone else","title":"09. Heaps"},{"location":"dsa/09-heaps/#heaps-priority-queues","text":"O(log n) insert/delete with O(1) access to min/max element","title":"Heaps (Priority Queues)"},{"location":"dsa/09-heaps/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is a heap in one sentence? Your answer: [Fill in after implementation] Why is it called a priority queue? Your answer: [Fill in after implementation] Real-world analogy: Example: \"A heap is like a hospital emergency room where patients are seen by urgency...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What's the difference between min-heap and max-heap? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/09-heaps/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/09-heaps/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/09-heaps/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/09-heaps/#debugging-challenges","text":"Your task: Find and fix bugs in broken heap implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"dsa/09-heaps/#decision-framework","text":"Your task: Build decision trees for when to use heaps.","title":"Decision Framework"},{"location":"dsa/09-heaps/#practice","text":"","title":"Practice"},{"location":"dsa/09-heaps/#review-checklist","text":"Before moving to the next topic: Implementation Basic operations: Kth largest, K largest, heap sort all work Merge K: lists and arrays both work Top K: frequent elements and closest points work Two heaps: median finder works correctly All client code runs successfully Pattern Recognition Can identify when to use min-heap vs max-heap Understand when heap size should be K vs N Know when to use two heaps Recognize merge K sorted pattern Problem Solving Solved 3 easy problems Solved 3-4 medium problems Analyzed time/space complexity Understood heap vs sort trade-offs Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use heaps Can explain heap property and operations Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand PriorityQueue in Java","title":"Review Checklist"},{"location":"dsa/10-graphs/","text":"Graphs: Traversal Patterns \u00b6 Master DFS, BFS, and cycle detection - the foundation for all graph problems ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is a graph in one sentence? Your answer: [Fill in after implementation] Why/when do we use graphs? Your answer: [Fill in after implementation] Real-world analogy: Example: \"A graph is like a social network where people are nodes and friendships are edges...\" Your analogy: [Fill in] What's the difference between DFS and BFS? Your answer: [Fill in after solving problems] When should you use DFS vs BFS? Your answer: [Fill in after practice] How do you detect cycles in graphs? Your answer: [Fill in after learning] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 BFS to find shortest path in unweighted graph: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] DFS to explore all paths in graph: Time complexity: [Your guess: O(?)] Space complexity (recursion stack): [Your guess: O(?)] Verified: [Actual] Scenario Predictions \u00b6 Scenario 1: Find shortest path from A to E in graph: A\u2192B\u2192E, A\u2192C\u2192D\u2192E Which algorithm? [BFS/DFS - Why?] BFS will find path in how many steps? [Fill in] DFS might find which path first? [Fill in] Why does BFS guarantee shortest? [Explain] Scenario 2: Detect if there's a cycle in a graph Which algorithm? [DFS/BFS - Why?] What data structure represents this? [Directed/Undirected graph] How do you detect the cycle? [Fill in your approach] Scenario 3: Count number of islands in 2D grid [['1','1','0'], ['1','0','0'], ['0','0','1']] How many islands? [Your guess] Which algorithm? [DFS/BFS - Why?] What marks a cell as visited? [Fill in] Trade-off Quiz \u00b6 Question 1: When would DFS be BETTER than BFS? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question 2: Adjacency Matrix vs Adjacency List - which is better for sparse graphs? Your answer: [Matrix/List - Why?] Space complexity comparison: [Fill in after learning] Graph Representation Quiz \u00b6 Given graph: 0\u21921, 0\u21922, 1\u21923, 2\u21923 Adjacency List representation: Your answer: <span class=\"fill-in\">[Draw/write the adjacency list structure]</span> Adjacency Matrix representation: Your answer: <span class=\"fill-in\">[Draw the 4x4 matrix]</span> Which uses less space? [Fill in and explain why] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example 1: Find Shortest Path \u00b6 Problem: Find shortest path between two nodes in an unweighted graph. Approach 1: DFS (Suboptimal) \u00b6 // Naive approach - DFS finds A path, not shortest path public static int findPath_DFS(Map<Integer, List<Integer>> graph, int start, int end) { Set<Integer> visited = new HashSet<>(); return dfsPathLength(graph, start, end, visited, 0); } private static int dfsPathLength(Map<Integer, List<Integer>> graph, int current, int end, Set<Integer> visited, int length) { if (current == end) return length; if (visited.contains(current)) return Integer.MAX_VALUE; visited.add(current); int minPath = Integer.MAX_VALUE; for (int neighbor : graph.getOrDefault(current, new ArrayList<>())) { int pathLen = dfsPathLength(graph, neighbor, end, visited, length + 1); minPath = Math.min(minPath, pathLen); } visited.remove(current); // Backtrack return minPath; } Analysis: Time: O(V!) in worst case - explores all possible paths Space: O(V) - recursion stack Problem: Explores unnecessary paths, no guarantee of finding shortest first Approach 2: BFS (Optimized) \u00b6 // Optimized approach - BFS guarantees shortest path public static int findPath_BFS(Map<Integer, List<Integer>> graph, int start, int end) { Queue<Integer> queue = new LinkedList<>(); Set<Integer> visited = new HashSet<>(); queue.offer(start); visited.add(start); int length = 0; while (!queue.isEmpty()) { int size = queue.size(); for (int i = 0; i < size; i++) { int node = queue.poll(); if (node == end) return length; for (int neighbor : graph.getOrDefault(node, new ArrayList<>())) { if (!visited.contains(neighbor)) { queue.offer(neighbor); visited.add(neighbor); } } } length++; } return -1; // Not found } Analysis: Time: O(V + E) - visits each node and edge once Space: O(V) - queue storage Benefit: Explores level-by-level, first path found is shortest Performance Comparison \u00b6 Graph Size DFS (worst case) BFS Speedup V = 10, E = 20 ~3,628,800 ops (10!) 30 ops ~120,000x V = 6, E = 10 ~720 ops (6!) 16 ops 45x V = 100, E = 200 Intractable 300 ops Infinite Your calculation: For a graph with V = 8 nodes, BFS would be approximately _____ times faster in worst case. Why Does BFS Find Shortest Path? \u00b6 Key insight to understand: In graph A\u2192B\u2192D, A\u2192C\u2192D looking for path from A to D: BFS Level 0: [A] BFS Level 1: [B, C] (distance = 1 from A) BFS Level 2: [D, D] (distance = 2 from A, found first time) DFS might go: A \u2192 B \u2192 D (found) but doesn't know if A \u2192 C \u2192 D is shorter without exploring everything. Why BFS guarantees shortest: Explores all nodes at distance k before distance k+1 First time we reach target = minimum distance No need to explore further paths After implementing, explain in your own words: Why does level-order exploration matter? [Your answer] When would DFS accidentally find shortest path? [Your answer] Example 2: Graph Representation \u00b6 Problem: Store graph with 1000 nodes and 5000 edges. Approach 1: Adjacency Matrix \u00b6 // Matrix representation - simple but space-inefficient for sparse graphs public class GraphMatrix { private int[][] matrix; private int V; public GraphMatrix(int vertices) { this.V = vertices; this.matrix = new int[V][V]; } public void addEdge(int src, int dst) { matrix[src][dst] = 1; // O(1) to add } public boolean hasEdge(int src, int dst) { return matrix[src][dst] == 1; // O(1) to check } public List<Integer> getNeighbors(int node) { List<Integer> neighbors = new ArrayList<>(); for (int i = 0; i < V; i++) { if (matrix[node][i] == 1) { neighbors.add(i); } } return neighbors; // O(V) to get all neighbors } } Analysis: Space: O(V\u00b2) = O(1,000,000) for 1000 nodes Add edge: O(1) Check edge: O(1) Get neighbors: O(V) Memory usage: 1000 \u00d7 1000 = 1,000,000 integers \u2248 4 MB Approach 2: Adjacency List \u00b6 // List representation - space-efficient for sparse graphs public class GraphList { private Map<Integer, List<Integer>> adjList; public GraphList() { this.adjList = new HashMap<>(); } public void addEdge(int src, int dst) { adjList.computeIfAbsent(src, k -> new ArrayList<>()).add(dst); // O(1) average } public boolean hasEdge(int src, int dst) { return adjList.getOrDefault(src, new ArrayList<>()).contains(dst); // O(degree) } public List<Integer> getNeighbors(int node) { return adjList.getOrDefault(node, new ArrayList<>()); // O(1) to get list } } Analysis: Space: O(V + E) = O(1000 + 5000) = O(6000) Add edge: O(1) average Check edge: O(degree of node) Get neighbors: O(1) Memory usage: ~6000 integers \u2248 24 KB Space Comparison \u00b6 Graph Type Adjacency Matrix Adjacency List Better Choice Dense (V=1000, E=500,000) 1M ints (4 MB) 501K ints (2 MB) Matrix (similar) Sparse (V=1000, E=5000) 1M ints (4 MB) 6K ints (24 KB) List (167x less) Very Sparse (V=1000, E=100) 1M ints (4 MB) 1.1K ints (4 KB) List (1000x less) Your calculation: For V = 5000 nodes and E = 10,000 edges, adjacency list uses _____ less space than matrix. When to Use Each? \u00b6 Use Adjacency Matrix when: Graph is dense (E \u2248 V\u00b2) Need O(1) edge existence checks frequently All operations need to be simple array lookups Use Adjacency List when: Graph is sparse (E << V\u00b2) Need to iterate through neighbors frequently Memory is limited After implementing, explain in your own words: Why does sparse vs dense matter? [Your answer] What operations are faster with each representation? [Your answer] Core Implementation \u00b6 Pattern 1: DFS (Depth-First Search) \u00b6 Concept: Explore as far as possible along each branch before backtracking. Use case: Detect cycles, find paths, connected components, backtracking problems. import java.util.*; public class DFS { /** * Problem: Number of islands (connected components) * Time: O(m*n), Space: O(m*n) for recursion stack * * TODO: Implement DFS to count islands */ public static int numIslands(char[][] grid) { if (grid == null || grid.length == 0) return 0; int count = 0; // TODO: Implement iteration/conditional logic return 0; // Replace with implementation } private static void dfs(char[][] grid, int i, int j) { // TODO: Base cases // TODO: Implement logic // TODO: Recursively visit 4 neighbors } /** * Problem: Has path in graph (adjacency list) * Time: O(V + E), Space: O(V) * * TODO: Implement DFS path finding */ public static boolean hasPath(Map<Integer, List<Integer>> graph, int start, int end) { Set<Integer> visited = new HashSet<>(); // TODO: Call recursive DFS helper return false; // Replace } private static boolean dfsPath(Map<Integer, List<Integer>> graph, int current, int target, Set<Integer> visited) { // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic // TODO: Mark current as visited // TODO: Implement iteration/conditional logic // TODO: Return false if no path found return false; // Replace } } Runnable Client Code: import java.util.*; public class DFSClient { public static void main(String[] args) { System.out.println(\"=== DFS Pattern ===\\n\"); // Test 1: Number of islands System.out.println(\"--- Test 1: Number of Islands ---\"); char[][] grid1 = { {'1','1','0','0','0'}, {'1','1','0','0','0'}, {'0','0','1','0','0'}, {'0','0','0','1','1'} }; System.out.println(\"Grid:\"); for (char[] row : grid1) { System.out.println(Arrays.toString(row)); } System.out.println(\"Islands: \" + DFS.numIslands(grid1)); // Test 2: Has path System.out.println(\"\\n--- Test 2: Has Path ---\"); Map<Integer, List<Integer>> graph = new HashMap<>(); graph.put(0, Arrays.asList(1, 2)); graph.put(1, Arrays.asList(3)); graph.put(2, Arrays.asList(3)); graph.put(3, Arrays.asList()); System.out.println(\"Graph: \" + graph); System.out.println(\"Path 0 -> 3? \" + DFS.hasPath(graph, 0, 3)); System.out.println(\"Path 0 -> 4? \" + DFS.hasPath(graph, 0, 4)); } } Pattern 2: BFS (Breadth-First Search) \u00b6 Concept: Explore all neighbors at current depth before moving deeper. Use case: Shortest path in unweighted graph, level-order traversal, minimum steps. import java.util.*; public class BFS { /** * Problem: Shortest path in unweighted graph * Time: O(V + E), Space: O(V) * * TODO: Implement BFS shortest path */ public static int shortestPath(Map<Integer, List<Integer>> graph, int start, int end) { Queue<Integer> queue = new LinkedList<>(); Set<Integer> visited = new HashSet<>(); int steps = 0; // TODO: Add start to queue and visited // TODO: Implement iteration/conditional logic return -1; // Not found } /** * Problem: Minimum knight moves to reach target * Time: O(n^2), Space: O(n^2) * * TODO: Implement BFS for chess board */ public static int minKnightMoves(int targetX, int targetY) { int[][] directions = {{2,1}, {2,-1}, {-2,1}, {-2,-1}, {1,2}, {1,-2}, {-1,2}, {-1,-2}}; Queue<int[]> queue = new LinkedList<>(); Set<String> visited = new HashSet<>(); // TODO: Start from (0, 0) int moves = 0; // TODO: Implement iteration/conditional logic return -1; // Replace } /** * Problem: Rotting oranges (multi-source BFS) * Time: O(m*n), Space: O(m*n) * * TODO: Implement multi-source BFS */ public static int orangesRotting(int[][] grid) { Queue<int[]> queue = new LinkedList<>(); int fresh = 0; int minutes = 0; // TODO: Count fresh oranges and add rotten to queue // TODO: BFS to rot adjacent oranges // TODO: Return minutes if all fresh rotted, else -1 return 0; // Replace } } Runnable Client Code: import java.util.*; public class BFSClient { public static void main(String[] args) { System.out.println(\"=== BFS Pattern ===\\n\"); // Test 1: Shortest path System.out.println(\"--- Test 1: Shortest Path ---\"); Map<Integer, List<Integer>> graph = new HashMap<>(); graph.put(0, Arrays.asList(1, 2)); graph.put(1, Arrays.asList(3)); graph.put(2, Arrays.asList(3, 4)); graph.put(3, Arrays.asList(4)); graph.put(4, Arrays.asList()); System.out.println(\"Graph: \" + graph); System.out.println(\"Shortest path 0 -> 4: \" + BFS.shortestPath(graph, 0, 4)); // Test 2: Knight moves System.out.println(\"\\n--- Test 2: Knight Moves ---\"); int[][] targets = {{2, 1}, {5, 5}}; for (int[] target : targets) { int moves = BFS.minKnightMoves(target[0], target[1]); System.out.printf(\"To (%d, %d): %d moves%n\", target[0], target[1], moves); } // Test 3: Rotting oranges System.out.println(\"\\n--- Test 3: Rotting Oranges ---\"); int[][] grid = { {2, 1, 1}, {1, 1, 0}, {0, 1, 1} }; System.out.println(\"Grid:\"); for (int[] row : grid) { System.out.println(Arrays.toString(row)); } System.out.println(\"Minutes to rot all: \" + BFS.orangesRotting(grid)); } } Pattern 3: Cycle Detection \u00b6 Interview Priority: \u2b50\u2b50\u2b50 CRITICAL - Cycle detection is essential for many graph problems Concept: Detect if graph contains a cycle. Use case: Validate DAG, detect deadlocks, dependency resolution. import java.util.*; public class CycleDetection { /** * Problem: Detect cycle in directed graph * Time: O(V + E), Space: O(V) * * TODO: Implement using DFS with states */ public static boolean hasCycleDirected(int n, int[][] edges) { List<List<Integer>> graph = new ArrayList<>(); int[] state = new int[n]; // 0: unvisited, 1: visiting, 2: visited // TODO: Build graph // TODO: DFS from each unvisited node return false; // Replace } private static boolean dfsCycleDirected(List<List<Integer>> graph, int node, int[] state) { // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic // TODO: Mark as visiting // TODO: Visit all neighbors // TODO: Mark as visited return false; // Replace } /** * Problem: Detect cycle in undirected graph * Time: O(V + E), Space: O(V) * * TODO: Implement using DFS with parent tracking */ public static boolean hasCycleUndirected(int n, int[][] edges) { List<List<Integer>> graph = new ArrayList<>(); boolean[] visited = new boolean[n]; // TODO: Build graph (both directions for undirected) // TODO: DFS from each unvisited component return false; // Replace } private static boolean dfsCycleUndirected(List<List<Integer>> graph, int node, int parent, boolean[] visited) { // TODO: Mark as visited // TODO: Visit all neighbors return false; // Replace } } Runnable Client Code: import java.util.*; public class CycleDetectionClient { public static void main(String[] args) { System.out.println(\"=== Cycle Detection ===\\n\"); // Test 1: Directed graph cycle System.out.println(\"--- Test 1: Directed Graph ---\"); int[][] edges1 = {{0, 1}, {1, 2}}; int[][] edges2 = {{0, 1}, {1, 2}, {2, 0}}; System.out.println(\"Edges [[0,1],[1,2]]: \" + CycleDetection.hasCycleDirected(3, edges1)); System.out.println(\"Edges [[0,1],[1,2],[2,0]]: \" + CycleDetection.hasCycleDirected(3, edges2)); // Test 2: Undirected graph cycle System.out.println(\"\\n--- Test 2: Undirected Graph ---\"); int[][] edges3 = {{0, 1}, {1, 2}}; int[][] edges4 = {{0, 1}, {1, 2}, {2, 0}}; System.out.println(\"Edges [[0,1],[1,2]]: \" + CycleDetection.hasCycleUndirected(3, edges3)); System.out.println(\"Edges [[0,1],[1,2],[2,0]]: \" + CycleDetection.hasCycleUndirected(3, edges4)); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken graph implementations. This tests your understanding. Challenge 1: Broken DFS - Visited Array Bug \u00b6 /** * This code is supposed to count connected components using DFS. * It has 2 BUGS. Find them! */ public static int countComponents_Buggy(int n, int[][] edges) { Map<Integer, List<Integer>> graph = new HashMap<>(); // Build graph for (int i = 0; i < n; i++) { graph.put(i, new ArrayList<>()); } for (int[] edge : edges) { graph.get(edge[0]).add(edge[1]); graph.get(edge[1]).add(edge[0]); } boolean[] visited = new boolean[n]; int count = 0; for (int i = 0; i < n; i++) { if (!visited[i]) { dfs(graph, i, visited); } } return count;} private static void dfs(Map<Integer, List<Integer>> graph, int node, boolean[] visited) { visited[node] = true; for (int neighbor : graph.get(node)) { if (!visited[neighbor]) { dfs(graph, neighbor, visited); } } } Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Test case: Input: n = 5, edges = [[0,1], [1,2], [3,4]] Expected: 2 components Actual with buggy code: [What do you get?] Click to verify your answers Bug 1 (Line 18): After calling dfs() , we never increment count ! Should be: if (!visited[i]) { dfs(graph, i, visited); count++; // Increment after exploring component } Bug 2: Actually the same as Bug 1. The count variable is initialized but never incremented, so it always returns 0. Correct fix: for (int i = 0; i < n; i++) { if (!visited[i]) { dfs(graph, i, visited); count++; } } return count; Challenge 2: Broken BFS - Level Tracking Bug \u00b6 /** * Find shortest path length using BFS. * This has 1 CRITICAL BUG that causes wrong results. */ public static int shortestPath_Buggy(Map<Integer, List<Integer>> graph, int start, int end) { Queue<Integer> queue = new LinkedList<>(); Set<Integer> visited = new HashSet<>(); int distance = 0; queue.offer(start); while (!queue.isEmpty()) { int node = queue.poll(); if (node == end) return distance; for (int neighbor : graph.getOrDefault(node, new ArrayList<>())) { if (!visited.contains(neighbor)) { queue.offer(neighbor); visited.add(neighbor); } } distance++; } return -1; } Your debugging: Bug 1: [What's missing after queue.offer(start)?] Bug 2: [How should we process the queue by level?] Why it fails: [Trace through with start=0, end=2, graph: 0\u21921, 1\u21922] Expected distance: 2 Actual with buggy code: [What do you get?] Click to verify your answer Bug 1: Missing visited.add(start) after adding start to queue. Without this, we might revisit the start node. Bug 2: Not processing nodes level-by-level. Should use: while (!queue.isEmpty()) { int size = queue.size(); // Process all nodes at current level for (int i = 0; i < size; i++) { int node = queue.poll(); if (node == end) return distance; for (int neighbor : graph.getOrDefault(node, new ArrayList<>())) { if (!visited.contains(neighbor)) { queue.offer(neighbor); visited.add(neighbor); } } } distance++; // Increment after processing entire level } Why: Without level-by-level processing, distance increments for every node polled, not for each level. Challenge 3: Wrong Adjacency Representation \u00b6 /** * Build graph from edge list for undirected graph. * This has 1 LOGIC BUG. */ public static Map<Integer, List<Integer>> buildGraph_Buggy(int n, int[][] edges) { Map<Integer, List<Integer>> graph = new HashMap<>(); for (int i = 0; i < n; i++) { graph.put(i, new ArrayList<>()); } for (int[] edge : edges) { int u = edge[0], v = edge[1]; graph.get(u).add(v); } return graph; } Your debugging: Bug: [What's missing for undirected graphs?] What happens: [Can you find the path with buggy graph?] Click to verify your answer Bug: For undirected graphs, need to add edge in BOTH directions: for (int[] edge : edges) { int u = edge[0], v = edge[1]; graph.get(u).add(v); graph.get(v).add(u); // Add reverse edge! } Why: In an undirected graph, edge (u,v) means both u\u2192v and v\u2192u. Without the reverse edge, the graph is incorrectly treated as directed. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 5 bugs across 3 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common graph traversal mistakes to avoid Common graph mistakes you discovered: [Forgetting to increment counter after DFS] [Not marking start node as visited in BFS] [Not processing BFS level-by-level] [Not adding both directions for undirected graphs] [Fill in more patterns you noticed] Decision Framework \u00b6 Your task: Build decision trees for when to use each graph algorithm. Question 1: DFS vs BFS - Which to use? \u00b6 Answer after solving problems: Use DFS when: Need to explore all paths: [Backtracking, cycle detection] Memory is limited: [DFS uses less space] Finding any path (not shortest): [Fill in] Use BFS when: Need shortest path: [Unweighted graphs] Level-order traversal: [Process by distance from source] Multi-source problems: [Fill in examples] Question 2: Graph representation? \u00b6 Adjacency List: Use when: [Sparse graphs, need to iterate neighbors] Space: [O(V + E)] Adjacency Matrix: Use when: [Dense graphs, need to check edge existence] Space: [O(V\u00b2)] Your Decision Tree \u00b6 Build this after solving practice problems: flowchart TD Start[\"Graph Traversal Problem\"] Q1{\"Need shortest path<br/>(unweighted)?\"} Start --> Q1 BFS([\"BFS \u2713\"]) Q1 -->|\"Yes\"| BFS Q2{\"Explore all paths<br/>or backtrack?\"} Q1 -->|\"No\"| Q2 DFS1([\"DFS \u2713\"]) Q2 -->|\"Yes\"| DFS1 Q3{\"Detect cycle?\"} Q2 -->|\"No\"| Q3 DFS2([\"DFS with states \u2713\"]) Q3 -->|\"Directed\"| DFS2 DFS3([\"DFS with parent \u2713\"]) Q3 -->|\"Undirected\"| DFS3 Q4{\"Count components?\"} Q3 -->|\"No\"| Q4 DFSBFS([\"DFS or BFS \u2713\"]) Q4 -->|\"Yes\"| DFSBFS Note: For weighted graphs, topological sort, and MST problems, see \"Advanced Graph Algorithms\" Practice \u00b6 LeetCode Problems \u00b6 Interview Priority: Focus on these patterns - they appear in 60% of graph interviews Easy (Complete 2-3): 997. Find the Town Judge \u2b50 Pattern: [Graph properties] Your solution time: ___ Key insight: [Fill in] 1971. Find if Path Exists in Graph \u2b50\u2b50 Pattern: [DFS/BFS] Your solution time: ___ Key insight: [Fill in] Medium (Complete ALL - these are critical): 200. Number of Islands \u2b50\u2b50\u2b50 Pattern: [DFS] Difficulty: [Rate 1-10] Key insight: [Fill in] 133. Clone Graph \u2b50\u2b50\u2b50 Pattern: [DFS/BFS] Difficulty: [Rate 1-10] Key insight: [Fill in] 994. Rotting Oranges \u2b50\u2b50\u2b50 Pattern: [Multi-source BFS] Difficulty: [Rate 1-10] Key insight: [Fill in] 417. Pacific Atlantic Water Flow \u2b50\u2b50 Pattern: [DFS/BFS from multiple sources] Difficulty: [Rate 1-10] Key insight: [Fill in] 261. Graph Valid Tree \u2b50\u2b50 Pattern: [Cycle detection] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 127. Word Ladder \u2b50\u2b50 Pattern: [BFS] Key insight: [Fill in after solving] Next step: After mastering these, move to \"Advanced Graph Algorithms\" for Course Schedule, Dijkstra, MST Review Checklist \u00b6 Before moving to Advanced Graph Algorithms: Implementation DFS: islands, path finding, cycle detection all work BFS: shortest path, multi-source BFS (rotting oranges) work Cycle detection: directed and undirected work Graph representations: can build adjacency list/matrix All client code runs successfully Pattern Recognition Can identify when to use DFS vs BFS Know how to detect cycles in both directed/undirected graphs Understand graph representation trade-offs (list vs matrix) Recognize multi-source BFS patterns Problem Solving Solved 2-3 easy problems Solved 5 core medium problems (Islands, Clone Graph, Rotting Oranges, etc.) Analyzed time/space complexity Handled edge cases (empty graph, disconnected components) Understanding Filled in all ELI5 explanations Built decision tree Can explain DFS vs BFS trade-offs Know when NOT to use each traversal method Understand why BFS finds shortest path in unweighted graphs Mastery Check Could implement DFS/BFS/cycle detection from memory Could recognize traversal pattern in new problem Could explain to someone else Understand why each algorithm works Mastery Certification \u00b6 I certify that I can: Implement DFS and BFS from memory Explain when to use DFS vs BFS Detect cycles in directed and undirected graphs Choose appropriate graph representation (list vs matrix) Identify graph traversal patterns in new problems Analyze time and space complexity Debug common graph traversal mistakes Teach these concepts to someone else","title":"10. Graphs"},{"location":"dsa/10-graphs/#graphs-traversal-patterns","text":"Master DFS, BFS, and cycle detection - the foundation for all graph problems","title":"Graphs: Traversal Patterns"},{"location":"dsa/10-graphs/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is a graph in one sentence? Your answer: [Fill in after implementation] Why/when do we use graphs? Your answer: [Fill in after implementation] Real-world analogy: Example: \"A graph is like a social network where people are nodes and friendships are edges...\" Your analogy: [Fill in] What's the difference between DFS and BFS? Your answer: [Fill in after solving problems] When should you use DFS vs BFS? Your answer: [Fill in after practice] How do you detect cycles in graphs? Your answer: [Fill in after learning]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/10-graphs/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/10-graphs/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/10-graphs/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/10-graphs/#debugging-challenges","text":"Your task: Find and fix bugs in broken graph implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"dsa/10-graphs/#decision-framework","text":"Your task: Build decision trees for when to use each graph algorithm.","title":"Decision Framework"},{"location":"dsa/10-graphs/#practice","text":"","title":"Practice"},{"location":"dsa/10-graphs/#review-checklist","text":"Before moving to Advanced Graph Algorithms: Implementation DFS: islands, path finding, cycle detection all work BFS: shortest path, multi-source BFS (rotting oranges) work Cycle detection: directed and undirected work Graph representations: can build adjacency list/matrix All client code runs successfully Pattern Recognition Can identify when to use DFS vs BFS Know how to detect cycles in both directed/undirected graphs Understand graph representation trade-offs (list vs matrix) Recognize multi-source BFS patterns Problem Solving Solved 2-3 easy problems Solved 5 core medium problems (Islands, Clone Graph, Rotting Oranges, etc.) Analyzed time/space complexity Handled edge cases (empty graph, disconnected components) Understanding Filled in all ELI5 explanations Built decision tree Can explain DFS vs BFS trade-offs Know when NOT to use each traversal method Understand why BFS finds shortest path in unweighted graphs Mastery Check Could implement DFS/BFS/cycle detection from memory Could recognize traversal pattern in new problem Could explain to someone else Understand why each algorithm works","title":"Review Checklist"},{"location":"dsa/11-union-find/","text":"Union-Find (Disjoint Set Union) \u00b6 Efficiently track and merge disjoint sets with near-constant time operations ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is union-find in one sentence? Your answer: [Fill in after implementation] What do \"union\" and \"find\" operations do? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Union-Find is like organizing people into groups where you can quickly check if two people are in the same group...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What makes union-find fast? Your answer: [Fill in after learning optimizations] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Naive connectivity check using DFS/BFS: Time complexity per query: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Union-Find with optimizations (path compression + union by rank): Time complexity per operation: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Speedup calculation: If n = 10,000 nodes with 1,000 connectivity queries DFS approach: _____ operations Union-Find: _____ operations Speedup factor: _____ times faster Scenario Predictions \u00b6 Scenario 1: You have nodes {0, 1, 2, 3, 4}. Perform: union(0,1), union(2,3), union(1,2) After these operations, which nodes are connected? [Fill in] How many disjoint components remain? [Your guess] Are nodes 0 and 3 connected? [Yes/No - Why?] What happens if we call union(0,3) now? [Fill in] Scenario 2: Graph edges: [(0,1), (1,2), (2,3), (3,0)] Can you detect a cycle using union-find? [Yes/No - How?] Which edge creates the cycle? [Fill in your reasoning] What does find(x) return after path compression? [Fill in] Scenario 3: Why path compression? Without path compression: Finding root of deeply nested node costs [O(?)] With path compression: Amortized cost becomes [O(?)] Draw a tree before and after path compression: [Sketch after implementation] Trade-off Quiz \u00b6 Question: When would DFS/BFS be BETTER than union-find for connectivity? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN benefit of union by rank? Saves memory Reduces number of nodes Keeps tree height balanced Makes find operation faster initially Verify after implementation: [Which one(s)?] Question: Can union-find split a component into smaller components? Your answer: [Yes/No - Why or why not?] Implication: [When does this limitation matter?] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example: Connectivity Queries \u00b6 Problem: Check if two nodes are connected in a dynamic graph with union operations. Approach 1: DFS/BFS for Each Query \u00b6 // Naive approach - Traverse graph for every connectivity check public class NaiveConnectivity { private List<Integer>[] graph; public NaiveConnectivity(int n) { graph = new ArrayList[n]; for (int i = 0; i < n; i++) { graph[i] = new ArrayList<>(); } } public void union(int x, int y) { graph[x].add(y); graph[y].add(x); } public boolean connected(int x, int y) { // DFS to check connectivity boolean[] visited = new boolean[graph.length]; return dfs(x, y, visited); } private boolean dfs(int node, int target, boolean[] visited) { if (node == target) return true; visited[node] = true; for (int neighbor : graph[node]) { if (!visited[neighbor]) { if (dfs(neighbor, target, visited)) return true; } } return false; } } Analysis: Time per union: O(1) - Just add edges Time per connected query: O(V + E) - Full DFS/BFS traversal Space: O(V + E) - Store all edges For 1,000 queries on 10,000 nodes: ~10,000,000+ operations per query Approach 2: Union-Find (Optimized) \u00b6 // Optimized approach - Union-Find with path compression and union by rank public class OptimizedConnectivity { private int[] parent; private int[] rank; public OptimizedConnectivity(int n) { parent = new int[n]; rank = new int[n]; for (int i = 0; i < n; i++) { parent[i] = i; rank[i] = 0; } } public int find(int x) { // Path compression: point directly to root if (parent[x] != x) { parent[x] = find(parent[x]); } return parent[x]; } public void union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX == rootY) return; // Union by rank: attach smaller tree under larger if (rank[rootX] < rank[rootY]) { parent[rootX] = rootY; } else if (rank[rootX] > rank[rootY]) { parent[rootY] = rootX; } else { parent[rootY] = rootX; rank[rootX]++; } } public boolean connected(int x, int y) { return find(x) == find(y); } } Analysis: Time per operation: O(\u03b1(n)) \u2248 O(1) - Inverse Ackermann (practically constant) Space: O(n) - Only parent and rank arrays For 1,000 queries: ~1,000 operations total (vs millions) Performance Comparison \u00b6 Operations DFS/BFS (O(V+E)) Union-Find (O(\u03b1(n))) Speedup 100 queries ~100,000 ops ~100 ops 1,000x 1,000 queries ~1,000,000 ops ~1,000 ops 1,000x 10,000 queries ~10,000,000 ops ~10,000 ops 1,000x Your calculation: For 5,000 connectivity queries on a graph with 5,000 nodes, the speedup is approximately _____ times faster. Why Does Union-Find Work? \u00b6 Key insight to understand: Starting with nodes: {0, 1, 2, 3, 4} (all separate) Step 1: union(0, 1) Component structure: 0 2 3 4 / 1 Step 2: union(2, 3) Component structure: 0 2 4 / / 1 3 Step 3: union(1, 2) Component structure: 0 4 / \\ 1 2 / 3 Now find(1) and find(3) both return 0 (same root) \u2192 connected! Path compression in action: Before find(3): 3 \u2192 2 \u2192 0 (must traverse 2 links) After find(3): 3 \u2192 0 (directly points to root) 2 \u2192 0 (also flattened) Why can we skip intermediate nodes? We only care if two nodes share the same root (same component) The path structure doesn't matter, only connectivity Path compression flattens trees without changing connectivity Future operations become faster! After implementing, explain in your own words: Why does union by rank keep trees balanced? [Your answer] How does path compression improve future finds? [Your answer] What's the inverse Ackermann function and why does it matter? [Your answer] Core Implementation \u00b6 Pattern 1: Basic Union-Find with Optimizations \u00b6 Concept: Track connected components with path compression and union by rank. Use case: Dynamic connectivity, detecting cycles, network connections. public class UnionFind { /** * Union-Find Data Structure * Time: O(\u03b1(n)) \u2248 O(1) per operation with optimizations * Space: O(n) * * TODO: Implement with path compression and union by rank */ static class DSU { private int[] parent; private int[] rank; // or size, depending on optimization private int components; // Track number of disjoint sets public DSU(int n) { // TODO: Initialize parent array: parent[i] = i // TODO: Initialize rank array: rank[i] = 0 // TODO: Initialize components } /** * Find with path compression * Time: O(\u03b1(n)) amortized * * TODO: Implement find with path compression */ public int find(int x) { // TODO: Implement iteration/conditional logic // TODO: Path compression: parent[x] = find(parent[x]) // TODO: Return parent[x] return 0; // Replace with implementation } /** * Union by rank * Time: O(\u03b1(n)) amortized * * TODO: Implement union by rank */ public boolean union(int x, int y) { // TODO: Find roots of x and y // TODO: Implement iteration/conditional logic // TODO: Attach smaller rank tree under larger rank tree // TODO: Implement iteration/conditional logic // TODO: Decrement components count // TODO: Return true (successful union) return false; // Replace with implementation } /** * Check if connected * Time: O(\u03b1(n)) */ public boolean connected(int x, int y) { // TODO: Return find(x) == find(y) return false; // Replace with implementation } /** * Get number of disjoint components * Time: O(1) */ public int getComponents() { // TODO: Return components count return 0; // Replace with implementation } /** * Get size of component containing x * Time: O(\u03b1(n)) */ public int getSize(int x) { // TODO: Implement iteration/conditional logic // TODO: Otherwise, count elements with same root return 0; // Replace with implementation } } /** * Problem: Number of connected components in undirected graph * Time: O(E * \u03b1(V)), Space: O(V) * * TODO: Implement using union-find */ public static int countComponents(int n, int[][] edges) { // TODO: Initialize DSU with n nodes // TODO: Implement iteration/conditional logic // TODO: Return number of components return 0; // Replace with implementation } } Runnable Client Code: public class UnionFindClient { public static void main(String[] args) { System.out.println(\"=== Union-Find ===\\n\"); // Test 1: Basic operations System.out.println(\"--- Test 1: Basic Operations ---\"); UnionFind.DSU dsu = new UnionFind.DSU(10); System.out.println(\"Initial components: \" + dsu.getComponents()); // Connect some nodes int[][] connections = {{0, 1}, {1, 2}, {3, 4}, {5, 6}, {6, 7}}; System.out.println(\"\\nConnecting nodes:\"); for (int[] conn : connections) { boolean success = dsu.union(conn[0], conn[1]); System.out.printf(\" union(%d, %d): %s%n\", conn[0], conn[1], success ? \"SUCCESS\" : \"ALREADY CONNECTED\"); } System.out.println(\"\\nFinal components: \" + dsu.getComponents()); // Test connectivity System.out.println(\"\\nConnectivity tests:\"); int[][] tests = {{0, 2}, {0, 3}, {3, 4}, {5, 8}}; for (int[] test : tests) { boolean connected = dsu.connected(test[0], test[1]); System.out.printf(\" connected(%d, %d): %s%n\", test[0], test[1], connected ? \"YES\" : \"NO\"); } // Test 2: Count components System.out.println(\"\\n--- Test 2: Count Components ---\"); int n = 5; int[][] edges = {{0, 1}, {1, 2}, {3, 4}}; System.out.println(\"Nodes: \" + n); System.out.println(\"Edges: \" + java.util.Arrays.deepToString(edges)); int components = UnionFind.countComponents(n, edges); System.out.println(\"Components: \" + components); } } Pattern 2: Cycle Detection \u00b6 Concept: Detect cycles in undirected graphs using union-find. Use case: Redundant connection, graph valid tree. import java.util.*; public class CycleDetection { /** * Problem: Detect if undirected graph has a cycle * Time: O(E * \u03b1(V)), Space: O(V) * * TODO: Implement cycle detection */ public static boolean hasCycle(int n, int[][] edges) { // TODO: Initialize union-find // TODO: Implement iteration/conditional logic // TODO: Return false if no cycle return false; // Replace with implementation } /** * Problem: Find redundant connection (edge that creates cycle) * Time: O(E * \u03b1(V)), Space: O(V) * * TODO: Implement redundant connection */ public static int[] findRedundantConnection(int[][] edges) { // TODO: Initialize union-find // TODO: Implement iteration/conditional logic return new int[]{-1, -1}; // Replace with implementation } /** * Problem: Check if graph is a valid tree * Time: O(E * \u03b1(V)), Space: O(V) * * TODO: Implement tree validation */ public static boolean validTree(int n, int[][] edges) { // TODO: Tree must have exactly n-1 edges // TODO: Must have no cycles // TODO: Must be fully connected (1 component) return false; // Replace with implementation } /** * Problem: Find redundant directed connection * Time: O(E * \u03b1(V)), Space: O(V) * * TODO: Implement for directed graph */ public static int[] findRedundantDirectedConnection(int[][] edges) { // TODO: More complex - need to handle: // TODO: Try removing each candidate edge return new int[]{-1, -1}; // Replace with implementation } } Runnable Client Code: import java.util.*; public class CycleDetectionClient { public static void main(String[] args) { System.out.println(\"=== Cycle Detection ===\\n\"); // Test 1: Has cycle System.out.println(\"--- Test 1: Has Cycle ---\"); int[][] testGraphs = { {{0, 1}, {1, 2}}, // No cycle {{0, 1}, {1, 2}, {2, 0}}, // Cycle {{0, 1}, {0, 2}, {1, 2}} // Cycle }; for (int i = 0; i < testGraphs.length; i++) { int n = 3; boolean cycle = CycleDetection.hasCycle(n, testGraphs[i]); System.out.printf(\"Graph %d: %s -> %s%n\", i + 1, Arrays.deepToString(testGraphs[i]), cycle ? \"HAS CYCLE\" : \"NO CYCLE\"); } // Test 2: Redundant connection System.out.println(\"\\n--- Test 2: Redundant Connection ---\"); int[][] edgeSets = { {{1, 2}, {1, 3}, {2, 3}}, {{1, 2}, {2, 3}, {3, 4}, {1, 4}, {1, 5}} }; for (int[][] edges : edgeSets) { int[] redundant = CycleDetection.findRedundantConnection(edges); System.out.printf(\"Edges: %s%n\", Arrays.deepToString(edges)); System.out.printf(\"Redundant: %s%n%n\", Arrays.toString(redundant)); } // Test 3: Valid tree System.out.println(\"--- Test 3: Valid Tree ---\"); int[][] treeTests = { {{0, 1}, {0, 2}, {0, 3}, {1, 4}}, // Valid tree (5 nodes) {{0, 1}, {1, 2}, {2, 3}, {1, 3}, {1, 4}} // Not tree (cycle) }; for (int i = 0; i < treeTests.length; i++) { int n = 5; boolean isTree = CycleDetection.validTree(n, treeTests[i]); System.out.printf(\"Test %d: %s -> %s%n\", i + 1, Arrays.deepToString(treeTests[i]), isTree ? \"VALID TREE\" : \"NOT TREE\"); } } } Pattern 3: Connected Components Problems \u00b6 Concept: Group elements into connected components. Use case: Number of islands, accounts merge, provinces. import java.util.*; public class ConnectedComponents { /** * Problem: Number of islands (using union-find) * Time: O(m*n * \u03b1(m*n)), Space: O(m*n) * * TODO: Implement using union-find */ public static int numIslands(char[][] grid) { // TODO: Initialize union-find for all cells // TODO: Implement iteration/conditional logic // TODO: Count unique components of land cells return 0; // Replace with implementation } /** * Problem: Number of provinces (friend circles) * Time: O(n^2 * \u03b1(n)), Space: O(n) * * TODO: Implement using union-find */ public static int findCircleNum(int[][] isConnected) { // TODO: Initialize union-find with n people // TODO: Implement iteration/conditional logic // TODO: Return number of components return 0; // Replace with implementation } /** * Problem: Accounts merge (emails belonging to same person) * Time: O(n*k * \u03b1(n*k)), Space: O(n*k) * * TODO: Implement accounts merge */ public static List<List<String>> accountsMerge(List<List<String>> accounts) { // TODO: Map email to account index // TODO: Union accounts that share emails // TODO: Group emails by component // TODO: Sort emails in each group return new ArrayList<>(); // Replace with implementation } /** * Problem: Smallest string with swaps * Time: O(n log n + E * \u03b1(n)), Space: O(n) * * TODO: Implement using union-find */ public static String smallestStringWithSwaps(String s, List<List<Integer>> pairs) { // TODO: Union indices that can be swapped // TODO: Group characters by component // TODO: Sort characters in each component // TODO: Reconstruct string return \"\"; // Replace with implementation } } Runnable Client Code: import java.util.*; public class ConnectedComponentsClient { public static void main(String[] args) { System.out.println(\"=== Connected Components ===\\n\"); // Test 1: Number of islands System.out.println(\"--- Test 1: Number of Islands ---\"); char[][] grid = { {'1','1','0','0','0'}, {'1','1','0','0','0'}, {'0','0','1','0','0'}, {'0','0','0','1','1'} }; System.out.println(\"Grid:\"); for (char[] row : grid) { System.out.println(\" \" + Arrays.toString(row)); } int islands = ConnectedComponents.numIslands(grid); System.out.println(\"Number of islands: \" + islands); // Test 2: Number of provinces System.out.println(\"\\n--- Test 2: Number of Provinces ---\"); int[][] isConnected = { {1, 1, 0}, {1, 1, 0}, {0, 0, 1} }; System.out.println(\"Connections:\"); for (int[] row : isConnected) { System.out.println(\" \" + Arrays.toString(row)); } int provinces = ConnectedComponents.findCircleNum(isConnected); System.out.println(\"Number of provinces: \" + provinces); // Test 3: Accounts merge System.out.println(\"\\n--- Test 3: Accounts Merge ---\"); List<List<String>> accounts = Arrays.asList( Arrays.asList(\"John\", \"johnsmith@mail.com\", \"john00@mail.com\"), Arrays.asList(\"John\", \"johnnybravo@mail.com\"), Arrays.asList(\"John\", \"johnsmith@mail.com\", \"john_newyork@mail.com\"), Arrays.asList(\"Mary\", \"mary@mail.com\") ); System.out.println(\"Accounts:\"); for (List<String> account : accounts) { System.out.println(\" \" + account); } List<List<String>> merged = ConnectedComponents.accountsMerge(accounts); System.out.println(\"\\nMerged accounts:\"); for (List<String> account : merged) { System.out.println(\" \" + account); } // Test 4: Smallest string with swaps System.out.println(\"\\n--- Test 4: Smallest String with Swaps ---\"); String s = \"dcab\"; List<List<Integer>> pairs = Arrays.asList( Arrays.asList(0, 3), Arrays.asList(1, 2) ); System.out.println(\"String: \" + s); System.out.println(\"Swappable pairs: \" + pairs); String result = ConnectedComponents.smallestStringWithSwaps(s, pairs); System.out.println(\"Smallest string: \" + result); } } Pattern 4: Advanced Union-Find Applications \u00b6 Concept: Use union-find with additional constraints or weights. Use case: Satisfiability, equations, sentence similarity. import java.util.*; public class AdvancedUnionFind { /** * Problem: Satisfiability of equality equations * Time: O(n * \u03b1(26)), Space: O(26) * * TODO: Implement equation satisfaction check */ public static boolean equationsPossible(String[] equations) { // TODO: Initialize union-find for 26 letters // TODO: First pass: union all equal variables (==) // TODO: Second pass: check all inequalities (!=) // TODO: Return true if no contradictions return false; // Replace with implementation } /** * Problem: Evaluate division (transitive division) * Time: O(E * \u03b1(V) + Q * V), Space: O(V) * * TODO: Implement with weighted union-find */ public static double[] calcEquation(List<List<String>> equations, double[] values, List<List<String>> queries) { // TODO: Build graph with division relationships // TODO: Implement iteration/conditional logic // TODO: Or use weighted union-find with ratios return new double[0]; // Replace with implementation } /** * Problem: Sentence similarity II (transitive similarity) * Time: O(P * \u03b1(W)), Space: O(W) * * TODO: Implement similarity check */ public static boolean areSentencesSimilar(String[] words1, String[] words2, List<List<String>> pairs) { // TODO: Implement iteration/conditional logic // TODO: Union similar word pairs // TODO: Check if words1[i] and words2[i] in same component return false; // Replace with implementation } /** * Problem: Minimize malware spread * Time: O(n^2 * \u03b1(n)), Space: O(n) * * TODO: Implement using union-find */ public static int minMalwareSpread(int[][] graph, int[] initial) { // TODO: Union all connected nodes // TODO: Implement iteration/conditional logic // TODO: Return node whose removal saves most nodes return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class AdvancedUnionFindClient { public static void main(String[] args) { System.out.println(\"=== Advanced Union-Find ===\\n\"); // Test 1: Equations possible System.out.println(\"--- Test 1: Equations Possible ---\"); String[][] equationSets = { {\"a==b\", \"b!=a\"}, {\"b==a\", \"a==b\"}, {\"a==b\", \"b==c\", \"a==c\"} }; for (String[] equations : equationSets) { boolean possible = AdvancedUnionFind.equationsPossible(equations); System.out.printf(\"Equations: %s%n\", Arrays.toString(equations)); System.out.printf(\"Possible: %s%n%n\", possible ? \"YES\" : \"NO\"); } // Test 2: Evaluate division System.out.println(\"--- Test 2: Evaluate Division ---\"); List<List<String>> equations = Arrays.asList( Arrays.asList(\"a\", \"b\"), Arrays.asList(\"b\", \"c\") ); double[] values = {2.0, 3.0}; List<List<String>> queries = Arrays.asList( Arrays.asList(\"a\", \"c\"), Arrays.asList(\"b\", \"a\"), Arrays.asList(\"a\", \"e\"), Arrays.asList(\"a\", \"a\"), Arrays.asList(\"x\", \"x\") ); System.out.println(\"Equations: \" + equations); System.out.println(\"Values: \" + Arrays.toString(values)); System.out.println(\"Queries: \" + queries); double[] results = AdvancedUnionFind.calcEquation(equations, values, queries); System.out.println(\"Results: \" + Arrays.toString(results)); // Test 3: Sentence similarity System.out.println(\"\\n--- Test 3: Sentence Similarity II ---\"); String[] words1 = {\"great\", \"acting\", \"skills\"}; String[] words2 = {\"fine\", \"drama\", \"talent\"}; List<List<String>> pairs = Arrays.asList( Arrays.asList(\"great\", \"good\"), Arrays.asList(\"fine\", \"good\"), Arrays.asList(\"acting\", \"drama\"), Arrays.asList(\"skills\", \"talent\") ); System.out.println(\"Sentence 1: \" + Arrays.toString(words1)); System.out.println(\"Sentence 2: \" + Arrays.toString(words2)); System.out.println(\"Similar pairs: \" + pairs); boolean similar = AdvancedUnionFind.areSentencesSimilar(words1, words2, pairs); System.out.println(\"Similar: \" + (similar ? \"YES\" : \"NO\")); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken implementations. This tests your understanding. Challenge 1: Broken Find (Missing Path Compression) \u00b6 /** * This find implementation is supposed to use path compression. * It has 1 CRITICAL BUG. Find it! */ public int find_Buggy(int x) { if (parent[x] != x) { return find_Buggy(parent[x]); } return parent[x]; } Your debugging: Bug location: [Which line?] Bug explanation: [What optimization is missing?] Bug fix: [What should the code be?] Test case to measure impact: Chain: 0 \u2192 1 \u2192 2 \u2192 3 \u2192 4 \u2192 5 Call find(5) multiple times Without fix: Each call traverses ___ links With fix: Second call traverses ___ links Click to verify your answers Bug: Missing path compression! Should assign parent[x] = find_Buggy(parent[x]) to flatten the tree. Correct: public int find(int x) { if (parent[x] != x) { parent[x] = find(parent[x]); // Path compression! } return parent[x]; } Impact: Without path compression, find() is O(n) in worst case (long chain). With it, amortized O(\u03b1(n)) \u2248 O(1). Challenge 2: Broken Union (Wrong Parent Update) \u00b6 /** * Union by rank implementation. * This has 1 SUBTLE BUG that breaks the rank optimization. */ public boolean union_Buggy(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX == rootY) return false; if (rank[rootX] < rank[rootY]) { parent[x] = rootY; } else if (rank[rootX] > rank[rootY]) { parent[y] = rootX; } else { parent[rootY] = rootX; rank[rootX]++; } return true; } Your debugging: Bug 1: [What's wrong with parent[x] = rootY ?] Bug 2: [What's wrong with parent[y] = rootX ?] Why this breaks union by rank: [Explain the impact] Correct code: [What should it be?] Trace through example: Nodes: {0, 1, 2, 3}, all separate union(0, 1) \u2192 Works fine union(2, 3) \u2192 Works fine union(0, 2) with buggy code Expected: Attach one root under the other Actual: [What happens?] Click to verify your answer Bug: Should attach ROOTS, not the original nodes! Correct: if (rank[rootX] < rank[rootY]) { parent[rootX] = rootY; // Attach rootX under rootY } else if (rank[rootX] > rank[rootY]) { parent[rootY] = rootX; // Attach rootY under rootX } Why: If you attach x instead of rootX , you're not attaching the entire tree's root, just one node. This breaks the tree structure and defeats the purpose of union by rank. Challenge 3: Broken Cycle Detection \u00b6 /** * Detect cycle in undirected graph. * This has 1 LOGIC ERROR. */ public boolean hasCycle_Buggy(int n, int[][] edges) { UnionFind uf = new UnionFind(n); for (int[] edge : edges) { int u = edge[0]; int v = edge[1]; if (uf.find(u) == uf.find(v)) { uf.union(u, v); return true; } uf.union(u, v); } return false; } Your debugging: Bug: [What's the logic error?] Bug explanation: [Why is this incorrect?] Correct approach: [What should happen when find(u) == find(v)?] Test case: Graph edges: [(0,1), (1,2), (2,0)] Expected: Detect cycle at edge (2,0) Actual with buggy code: [What happens?] Click to verify your answer Bug: Should NOT call union(u, v) when they're already connected! If find(u) == find(v) , they're in the same component, which means adding this edge creates a cycle. Just return true immediately. Correct: if (uf.find(u) == uf.find(v)) { return true; // Cycle detected! Don't union. } uf.union(u, v); Why: Calling union when they're already connected is pointless and wastes an operation. Challenge 4: Missing Component Count Update \u00b6 /** * Union-Find with component counting. * This has 1 MISSING OPERATION. */ public class UnionFind_Buggy { private int[] parent; private int[] rank; private int components; public UnionFind_Buggy(int n) { parent = new int[n]; rank = new int[n]; components = n; for (int i = 0; i < n; i++) { parent[i] = i; } } public boolean union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX == rootY) return false; if (rank[rootX] < rank[rootY]) { parent[rootX] = rootY; } else if (rank[rootX] > rank[rootY]) { parent[rootY] = rootX; } else { parent[rootY] = rootX; rank[rootX]++; } return true; } public int getComponents() { return components; } } Your debugging: Bug: [What's missing in union()?] Impact: [How does this affect getComponents()?] Fix: [What line should be added?] Test case: Initialize with 5 nodes (components = 5) union(0, 1) \u2192 Expected components: 4, Actual: ___ union(2, 3) \u2192 Expected components: 3, Actual: ___ Click to verify your answer Bug: Missing components--; in the union method! Correct: if (rootX == rootY) return false; // ... union logic ... components--; // Decrement when we merge two components! return true; Why: Every successful union merges two disjoint components into one, reducing the total count by 1. Challenge 5: Rank Update Error \u00b6 /** * Union by rank implementation. * This has 1 SUBTLE BUG in rank update. */ public boolean union_Buggy(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX == rootY) return false; if (rank[rootX] < rank[rootY]) { parent[rootX] = rootY; rank[rootY]++; } else if (rank[rootX] > rank[rootY]) { parent[rootY] = rootX; rank[rootX]++; } else { parent[rootY] = rootX; rank[rootX]++; // This one is correct } return true; } Your debugging: Bug 1: [Should we increment rank when rootY is taller?] Bug 2: [Should we increment rank when rootX is taller?] When should rank be incremented? [Fill in the rule] Why rank matters: [Explain the purpose of rank] Click to verify your answer Bug: Only increment rank when ranks are EQUAL! Correct: if (rank[rootX] < rank[rootY]) { parent[rootX] = rootY; // Don't increment - rootY's height doesn't change } else if (rank[rootX] > rank[rootY]) { parent[rootY] = rootX; // Don't increment - rootX's height doesn't change } else { parent[rootY] = rootX; rank[rootX]++; // Only increment when equal! } Why: Rank represents tree height (upper bound). When attaching a shorter tree under a taller one, the height doesn't change. Only when equal-height trees merge does the new root's height increase by 1. Challenge 6: Iterative Find Bug \u00b6 /** * Iterative find with path compression attempt. * This has 1 BUG that prevents path compression from working. */ public int find_Buggy(int x) { // Find root int root = x; while (parent[root] != root) { root = parent[root]; } // Path compression while (parent[x] != x) { int next = parent[x]; parent[x] = root; x = next; } return root; } // Now the BUGGY version - what if we write it like this? public int find_ActualBuggy(int x) { int root = x; while (parent[root] != root) { root = parent[root]; } // Attempted path compression while (parent[x] != root) { parent[x] = root; x = parent[x]; } return root; } Your debugging: Bug: [What's wrong with the compression loop condition?] What happens: [Trace through with chain: 0 \u2192 1 \u2192 2] Correct version: [How to fix it?] Click to verify your answer Bug: In the compression loop, after we set parent[x] = root , we then do x = parent[x] , which now equals root ! This causes the loop to terminate immediately, compressing only the first node. Correct: while (parent[x] != x) { int next = parent[x]; // Save next before modifying parent[x] = root; // Point to root x = next; // Move to saved next } Or use the condition parent[x] != root but save the next pointer first. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 6+ bugs across 6 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common union-find mistakes to avoid Common mistakes you discovered: [Forgetting path compression in find] [Attaching node instead of root in union] [Incorrectly updating rank] [Missing component count decrement] [Calling union when cycle detected] [Path compression iteration bugs] Decision Framework \u00b6 Your task: Build decision trees for union-find problems. Question 1: What do you need to track? \u00b6 Answer after solving problems: Connected components? [Basic union-find] Cycles in graph? [Union-find with cycle detection] Dynamic connectivity? [Union-find with online queries] Weighted relationships? [Weighted union-find] Question 2: What optimizations do you need? \u00b6 Always use: Path compression: [Makes find nearly O(1)] Union by rank/size: [Keeps tree balanced] Additional data: Component size: [Track in size array] Component count: [Decrement on union] Weights/ratios: [For transitive relationships] Your Decision Tree \u00b6 flowchart LR Start[\"Union-Find Pattern Selection\"] Q1{\"Basic connectivity?\"} Start --> Q1 Q2{\"Cycle detection?\"} Start --> Q2 N3([\"Union-find \u2713\"]) Q2 -->|\"Undirected\"| N3 N4[\"DFS<br/>(not union-find)\"] Q2 -->|\"Directed\"| N4 Q5{\"Dynamic components?\"} Start --> Q5 N6([\"Union-find \u2713\"]) Q5 -->|\"Number of islands\"| N6 N7([\"Union-find \u2713\"]) Q5 -->|\"Provinces/groups\"| N7 N8([\"Union-find \u2713\"]) Q5 -->|\"Merging accounts\"| N8 Q9{\"Weighted relationships?\"} Start --> Q9 N10([\"Weighted UF \u2713\"]) Q9 -->|\"Division equations\"| N10 N11([\"Weighted UF \u2713\"]) Q9 -->|\"Distance/ratio\"| N11 N12([\"Basic UF \u2713\"]) Q9 -->|\"Equality constraints\"| N12 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 2): 547. Number of Provinces Pattern: [Connected components] Your solution time: ___ Key insight: [Fill in after solving] 990. Satisfiability of Equality Equations Pattern: [Constraint checking] Your solution time: ___ Key insight: [Fill in] Medium (Complete 3-4): 200. Number of Islands Pattern: [Connected components in grid] Difficulty: [Rate 1-10] Key insight: [Fill in] 684. Redundant Connection Pattern: [Cycle detection] Difficulty: [Rate 1-10] Key insight: [Fill in] 721. Accounts Merge Pattern: [Grouping with union-find] Difficulty: [Rate 1-10] Key insight: [Fill in] 1202. Smallest String With Swaps Pattern: [Components with optimization] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 685. Redundant Connection II Pattern: [Directed graph cycle] Key insight: [Fill in after solving] 399. Evaluate Division Pattern: [Weighted union-find] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Basic union-find with path compression and union by rank works Cycle detection: redundant connection, valid tree work Connected components: islands, provinces, merge accounts work Advanced: equations, weighted relationships work All client code runs successfully Pattern Recognition Can identify when union-find is appropriate Understand path compression and union by rank Know when to track additional data (size, count) Recognize weighted union-find problems Problem Solving Solved 2 easy problems Solved 3-4 medium problems Analyzed time/space complexity Understood amortized analysis Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use union-find Can explain why optimizations work Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand inverse Ackermann function complexity Mastery Certification \u00b6 I certify that I can: Implement union-find with both optimizations from memory Explain path compression and union by rank clearly Identify when union-find is the right tool Analyze time complexity including inverse Ackermann Compare trade-offs with DFS/BFS approaches Debug common union-find mistakes Teach this concept to someone else Solve new union-find problems independently","title":"11. Union-Find"},{"location":"dsa/11-union-find/#union-find-disjoint-set-union","text":"Efficiently track and merge disjoint sets with near-constant time operations","title":"Union-Find (Disjoint Set Union)"},{"location":"dsa/11-union-find/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is union-find in one sentence? Your answer: [Fill in after implementation] What do \"union\" and \"find\" operations do? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Union-Find is like organizing people into groups where you can quickly check if two people are in the same group...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What makes union-find fast? Your answer: [Fill in after learning optimizations]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/11-union-find/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/11-union-find/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/11-union-find/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/11-union-find/#debugging-challenges","text":"Your task: Find and fix bugs in broken implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"dsa/11-union-find/#decision-framework","text":"Your task: Build decision trees for union-find problems.","title":"Decision Framework"},{"location":"dsa/11-union-find/#practice","text":"","title":"Practice"},{"location":"dsa/11-union-find/#review-checklist","text":"Before moving to the next topic: Implementation Basic union-find with path compression and union by rank works Cycle detection: redundant connection, valid tree work Connected components: islands, provinces, merge accounts work Advanced: equations, weighted relationships work All client code runs successfully Pattern Recognition Can identify when union-find is appropriate Understand path compression and union by rank Know when to track additional data (size, count) Recognize weighted union-find problems Problem Solving Solved 2 easy problems Solved 3-4 medium problems Analyzed time/space complexity Understood amortized analysis Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use union-find Can explain why optimizations work Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand inverse Ackermann function complexity","title":"Review Checklist"},{"location":"dsa/12-advanced-graphs/","text":"Graph Algorithms: Optimization & Ordering \u00b6 Apply graph traversal to solve optimization problems: topological sort, shortest paths, and MST ELI5: Explain Like I'm 5 \u00b6 Your task: After learning these algorithms, explain them simply. Prompts to guide you: What does Topological Sort give us? Your answer: [Fill in after learning] When do we need Topological Sort? Your answer: [Course prerequisites, build systems...] What does Dijkstra's algorithm find? Your answer: [Fill in after learning] Real-world analogy for Dijkstra's algorithm: Example: \"Dijkstra's algorithm is like finding the cheapest flight where...\" Your analogy: [Fill in] Why can't we use Dijkstra for negative weights? Your answer: [Fill in after practice] Why do we need Minimum Spanning Trees? Your answer: [Fill in after learning] What problem does Union-Find solve? Your answer: [Dynamic connectivity...] Quick Quiz (Do BEFORE learning) \u00b6 Your task: Test your intuition about these algorithms before diving deep. Complexity Predictions \u00b6 Topological Sort (DFS-based): Time complexity: [Your guess: O(?)] When does it fail: [Your guess] Verified: [Actual: O(V+E)] Dijkstra's algorithm for shortest path: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual: O((V+E)logV)] Minimum Spanning Tree (Kruskal's): Time complexity: [Your guess: O(?)] What data structure: [Union-Find] Verified: [Actual: O(ElogE)] Union-Find operations: Find/Union complexity: [Your guess: O(?)] With optimizations: [Path compression + union by rank] Verified: [Actual: O(\u03b1(n)) \u2248 O(1)] Scenario Predictions \u00b6 Scenario 1: GPS navigation finding fastest route Algorithm: [Dijkstra? A*? Bellman-Ford?] Edge weight: [Distance? Time? Both?] Challenge: [Traffic changes? One-way streets?] Scenario 2: Package dependency resolution (npm, pip) Algorithm: [Topological sort? DFS? BFS?] Failure case: [Circular dependency?] Output: [Install order?] Scenario 3: Network cable installation (connect all offices) Algorithm: [MST? Shortest path?] Goal: [Minimize what?] Constraint: [All connected? No cycles?] Scenario 4: Dynamic friend groups (social network) Algorithm: [Union-Find? DFS?] Operations: [Add friendship, check if connected] Why not DFS: [Union-Find is faster for dynamic updates] Before/After: Why Advanced Graph Algorithms Matter \u00b6 Your task: Compare naive approaches vs optimized algorithms to understand the impact. Example: Network Routing (Shortest Path) \u00b6 Problem: Find shortest path between two cities in a road network with 10,000 intersections Approach 1: Breadth-First Search (BFS) - Unweighted \u00b6 Treats all roads as equal distance Network: A --100km--> B --5km--> C A --50km--> D --50km--> C BFS finds: A \u2192 B \u2192 C (2 hops) Distance: 100 + 5 = 105 km Ignores: A \u2192 D \u2192 C (2 hops) Distance: 50 + 50 = 100 km \u2190 Actually shorter! Problem: BFS optimizes for fewest edges, not shortest distance BFS Execution: Queue: [A] Visited: {} Step 1: Visit A Queue: [B, D] Visited: {A} Step 2: Visit B (first in queue) Queue: [D, C] Visited: {A, B} Step 3: Visit D Queue: [C, C] Visited: {A, B, D} Step 4: Visit C (from B path) Queue: [C] Visited: {A, B, D, C} Result: A \u2192 B \u2192 C (wrong!) Time: O(V + E) but gives wrong answer Approach 2: Dijkstra's Algorithm (Weighted) \u00b6 Same network: A --100km--> B --5km--> C A --50km--> D --50km--> C Dijkstra finds: A \u2192 D \u2192 C Distance: 100 km \u2190 Optimal! How? Explores paths by cumulative distance, not hop count Dijkstra Execution: Priority Queue: [(A, 0)] Distances: {A: 0, others: \u221e} Step 1: Process A (distance 0) Update neighbors: B: 0 + 100 = 100 D: 0 + 50 = 50 PQ: [(D, 50), (B, 100)] Step 2: Process D (distance 50, smallest) Update neighbors: C: 50 + 50 = 100 PQ: [(B, 100), (C, 100)] Step 3: Process B (distance 100) Update neighbors: C: 100 + 5 = 105 (worse than current 100, don't update) PQ: [(C, 100)] Step 4: Process C (distance 100) Done! Path: A \u2192 D \u2192 C Result: 100 km (optimal!) Time: O((V + E) log V) with binary heap Performance Comparison: Metric BFS Dijkstra Improvement Result Wrong (105 km) Correct (100 km) 5% shorter Time O(V + E) O((V + E) log V) Slightly slower Use case Unweighted graphs Weighted graphs Essential! Optimality # of hops Total weight Correct metric Real-world impact: GPS navigation: 5-20% shorter routes with Dijkstra Network routing: Lower latency paths Cost: Minimal (few ms difference for practical graphs) Your calculation: For 1000-node graph, 5000 edges: BFS time: _____ (V + E) Dijkstra time: _____ (E log V) Trade-off: [Worth it?] Core Concepts \u00b6 Recommended study order: \u2b50\u2b50\u2b50 Topological Sort (Topic 3 below) - Most common in interviews \u2b50\u2b50 Dijkstra's Algorithm (Topic 1 below) - Important for weighted graphs \u2b50\u2b50 Union-Find (add after completing above) - Dynamic connectivity \u2b50 MST (Topic 2 below) - Optional, low interview frequency Topic 1: Dijkstra's Shortest Path Algorithm \u00b6 Interview Priority: \u2b50\u2b50 GOOD TO KNOW - Appears in ~15% of graph problems Concept: Find shortest path from source to all other vertices in a weighted graph (non-negative weights). Algorithm Overview: Given graph G = (V, E) and source vertex s: 1. Initialize: - Distance[s] = 0 - Distance[all others] = \u221e - Priority Queue = [(s, 0)] - Visited = {} 2. While PQ not empty: a. Extract vertex u with min distance b. If u in Visited, skip c. Mark u as Visited d. For each neighbor v of u: - new_dist = Distance[u] + weight(u, v) - If new_dist < Distance[v]: Distance[v] = new_dist Parent[v] = u Add (v, new_dist) to PQ 3. Return Distance array and Parent pointers Example: Graph: A --1--> B | | 4 2 | | v v C --1--> D Source: A Iteration 1: Process A (dist=0) Visit B: dist = 0+1 = 1 Visit C: dist = 0+4 = 4 PQ: [(B,1), (C,4)] Distances: {A:0, B:1, C:4, D:\u221e} Iteration 2: Process B (dist=1) Visit D: dist = 1+2 = 3 PQ: [(D,3), (C,4)] Distances: {A:0, B:1, C:4, D:3} Iteration 3: Process D (dist=3) (No unvisited neighbors) PQ: [(C,4)] Distances: {A:0, B:1, C:4, D:3} Iteration 4: Process C (dist=4) Visit D: dist = 4+1 = 5 (worse than 3, ignore) PQ: [] Distances: {A:0, B:1, C:4, D:3} Final shortest paths from A: A \u2192 A: 0 A \u2192 B: 1 (path: A \u2192 B) A \u2192 C: 4 (path: A \u2192 C) A \u2192 D: 3 (path: A \u2192 B \u2192 D) Implementation: class DijkstraShortestPath { static class Edge { int to; int weight; Edge(int to, int weight) { this.to = to; this.weight = weight; } } /** * Dijkstra's algorithm * Time: O((V + E) log V) with binary heap * Space: O(V) */ public int[] dijkstra(List<List<Edge>> graph, int source) { int n = graph.size(); int[] dist = new int[n]; Arrays.fill(dist, Integer.MAX_VALUE); dist[source] = 0; // Priority queue: (vertex, distance) PriorityQueue<int[]> pq = new PriorityQueue<>((a, b) -> a[1] - b[1]); pq.offer(new int[]{source, 0}); boolean[] visited = new boolean[n]; while (!pq.isEmpty()) { int[] curr = pq.poll(); int u = curr[0]; int d = curr[1]; if (visited[u]) continue; visited[u] = true; // Relax edges for (Edge edge : graph.get(u)) { int v = edge.to; int newDist = dist[u] + edge.weight; if (newDist < dist[v]) { dist[v] = newDist; pq.offer(new int[]{v, newDist}); } } } return dist; } // Reconstruct path from source to target public List<Integer> getPath(int[] parent, int target) { List<Integer> path = new ArrayList<>(); for (int v = target; v != -1; v = parent[v]) { path.add(v); } Collections.reverse(path); return path; } } Optimizations: 1. Fibonacci Heap (Theoretical): Time: O(E + V log V) vs Binary Heap: O((V + E) log V) Improvement: Better for dense graphs Practical: Binary heap usually faster due to simpler implementation 2. Bidirectional Dijkstra: Simultaneously search from source and target: Forward: s \u2192 ... \u2192 meet point Backward: t \u2192 ... \u2192 meet point Time: ~2x faster (searches half the graph) Use case: Point-to-point shortest path (GPS navigation) 3. A* Search (Heuristic): Dijkstra: f(n) = g(n) (distance so far) A*: f(n) = g(n) + h(n) (+ estimated distance to goal) h(n) = heuristic (e.g., straight-line distance) Explores fewer nodes by guiding search toward target Optimal if h(n) is admissible (never overestimates) Limitations: Dijkstra FAILS with negative edge weights: Graph: A --1--> B | | 2 -5 | | v v C <------+ Dijkstra from A: 1. Process A: dist[B]=1, dist[C]=2 2. Process B: dist[C] = 1+(-5) = -4 (improvement!) 3. But B already visited, won't update C! Result: dist[C] = 2 (wrong! should be -4) Solution: Use Bellman-Ford for negative weights Use Cases: GPS navigation (road networks) Network routing (OSPF protocol) Robotics path planning Social network analysis (degrees of separation) Topic 2: Minimum Spanning Tree (MST) \u00b6 Interview Priority: \u2b50 OPTIONAL - Appears in <5% of interviews, study if time permits Concept: Subset of edges that connects all vertices with minimum total weight, no cycles. MST Properties: Given graph G = (V, E): - MST has exactly V-1 edges - MST is acyclic (it's a tree) - MST connects all vertices - MST has minimum total edge weight - MST may not be unique (multiple MSTs with same weight) Kruskal's Algorithm: Greedy approach: Add edges in increasing weight order, skip if creates cycle Algorithm: 1. Sort edges by weight (ascending) 2. Initialize Union-Find (each vertex in its own set) 3. For each edge (u, v, weight): - If u and v in different sets: Add edge to MST Union(u, v) - Else: Skip (would create cycle) 4. Return MST Time: O(E log E) for sorting + O(E \u03b1(V)) for Union-Find \u2248 O(E log E) Space: O(V) for Union-Find Example: Graph: A /|\\ 4 2 5 / | \\ B---3---C 6 | D Edges sorted by weight: (A,C): 2 (B,C): 3 (A,B): 4 (A,D): 5 (C,D): 6 Step 1: Add (A,C) weight 2 Sets: {A,C}, {B}, {D} MST: {(A,C)} Step 2: Add (B,C) weight 3 Sets: {A,B,C}, {D} MST: {(A,C), (B,C)} Step 3: Try (A,B) weight 4 A and B already connected \u2192 Skip (would create cycle) Step 4: Add (A,D) weight 5 Sets: {A,B,C,D} MST: {(A,C), (B,C), (A,D)} Step 5: Try (C,D) weight 6 C and D already connected \u2192 Skip MST edges: (A,C), (B,C), (A,D) Total weight: 2 + 3 + 5 = 10 Prim's Algorithm: Greedy approach: Grow MST from a starting vertex Algorithm: 1. Start with arbitrary vertex s 2. Add s to MST 3. Repeat until all vertices in MST: - Find minimum weight edge (u, v) where u in MST, v not in MST - Add v to MST - Add edge (u, v) to MST Time: O(E log V) with binary heap, O(E + V log V) with Fibonacci heap Space: O(V) Similar to Dijkstra but minimizes edge weight instead of path weight Implementation (Kruskal's): class KruskalMST { static class Edge implements Comparable<Edge> { int u, v, weight; Edge(int u, int v, int weight) { this.u = u; this.v = v; this.weight = weight; } public int compareTo(Edge other) { return this.weight - other.weight; } } // Union-Find data structure static class UnionFind { int[] parent, rank; UnionFind(int n) { parent = new int[n]; rank = new int[n]; for (int i = 0; i < n; i++) { parent[i] = i; } } int find(int x) { if (parent[x] != x) { parent[x] = find(parent[x]); // Path compression } return parent[x]; } boolean union(int x, int y) { int px = find(x), py = find(y); if (px == py) return false; // Already in same set // Union by rank if (rank[px] < rank[py]) { parent[px] = py; } else if (rank[px] > rank[py]) { parent[py] = px; } else { parent[py] = px; rank[px]++; } return true; } } public List<Edge> kruskal(int n, List<Edge> edges) { Collections.sort(edges); // O(E log E) UnionFind uf = new UnionFind(n); List<Edge> mst = new ArrayList<>(); for (Edge edge : edges) { if (uf.union(edge.u, edge.v)) { mst.add(edge); if (mst.size() == n - 1) break; // MST complete } } return mst; } } Use Cases: Network design (minimize cable length) Clustering algorithms (single-linkage) Image segmentation Approximation algorithms (TSP) Topic 3: Topological Sort \u00b6 Interview Priority: \u2b50\u2b50\u2b50 CRITICAL - Course Schedule is in top 20 most common problems! Concept: Linear ordering of vertices in a directed acyclic graph (DAG) such that for every edge (u, v), u comes before v. Properties: Valid only for DAGs (Directed Acyclic Graphs): - If graph has cycle \u2192 No topological ordering exists - Multiple valid orderings may exist - Used for dependency resolution, task scheduling DFS-Based Algorithm: Algorithm: 1. Mark all vertices as unvisited 2. For each unvisited vertex: - Perform DFS - After visiting all descendants, add vertex to result (reverse order) 3. Reverse result to get topological order Time: O(V + E) Space: O(V) for recursion stack Key insight: Vertices with no outgoing edges go last Example: DAG (course prerequisites): A (Intro) \u2192 B (Data Structures) \u2192 D (Algorithms) \u2192 C (Databases) \u2192 D Valid topological orders: 1. A, B, C, D 2. A, C, B, D Both satisfy: A before B, A before C, B before D, C before D Detailed Execution: Graph: A \u2192 B \u2192 D \u2193 \u2193 C \u2192 DFS from A: Visit A Visit B Visit D (No outgoing edges, add D to stack) (B done, add B to stack) Visit C Visit D (already visited, skip) (C done, add C to stack) (A done, add A to stack) Stack (reverse order): [D, B, C, A] Reverse: [A, C, B, D] or [A, B, C, D] Implementation: class TopologicalSort { /** * DFS-based topological sort * Time: O(V + E), Space: O(V) */ public List<Integer> topologicalSort(int n, List<List<Integer>> graph) { boolean[] visited = new boolean[n]; Stack<Integer> stack = new Stack<>(); // Visit all vertices for (int i = 0; i < n; i++) { if (!visited[i]) { dfs(i, graph, visited, stack); } } // Build result (reverse of stack) List<Integer> result = new ArrayList<>(); while (!stack.isEmpty()) { result.add(stack.pop()); } return result; } private void dfs(int u, List<List<Integer>> graph, boolean[] visited, Stack<Integer> stack) { visited[u] = true; for (int v : graph.get(u)) { if (!visited[v]) { dfs(v, graph, visited, stack); } } stack.push(u); // Add after visiting all descendants } /** * Kahn's algorithm (BFS-based) * Detects cycles explicitly */ public List<Integer> topologicalSortKahn(int n, List<List<Integer>> graph) { int[] inDegree = new int[n]; // Calculate in-degrees for (int u = 0; u < n; u++) { for (int v : graph.get(u)) { inDegree[v]++; } } // Start with vertices that have no incoming edges Queue<Integer> queue = new LinkedList<>(); for (int i = 0; i < n; i++) { if (inDegree[i] == 0) { queue.offer(i); } } List<Integer> result = new ArrayList<>(); while (!queue.isEmpty()) { int u = queue.poll(); result.add(u); // Remove edges from u for (int v : graph.get(u)) { inDegree[v]--; if (inDegree[v] == 0) { queue.offer(v); } } } // If result doesn't contain all vertices, graph has cycle if (result.size() != n) { throw new IllegalArgumentException(\"Graph has cycle!\"); } return result; } } Cycle Detection: Kahn's algorithm automatically detects cycles: If cycle exists: All vertices in cycle have inDegree > 0 Never added to queue result.size() < n Example: A \u2192 B \u2192 C \u2192 A (cycle) inDegree: A=1, B=1, C=1 Queue: [] (empty! all have incoming edges) Result: [] (empty) Cycle detected! Use Cases: Build systems: Compile dependencies in correct order Task scheduling: Execute tasks respecting dependencies Course prerequisites: Determine valid course order Package managers: Install packages with dependencies Git commit history: Linearize parallel development Topic 4: Union-Find (Disjoint Set Union) \u00b6 Interview Priority: \u2b50\u2b50 IMPORTANT - Key data structure for dynamic connectivity (~10% of graph problems) Concept: Efficiently track and merge disjoint sets, primarily used for dynamic connectivity problems. Core Operations: 1. Find(x): Which set does element x belong to? - Returns representative (root) of the set 2. Union(x, y): Merge the sets containing x and y - Connect roots of both sets 3. Connected(x, y): Are x and y in the same set? - Return Find(x) == Find(y) Optimizations: Path Compression (in Find): Make every node point directly to root Flattens tree structure Time: O(\u03b1(n)) amortized per operation Union by Rank : Always attach smaller tree under larger tree Keeps tree balanced Implementation: class UnionFind { private int[] parent; private int[] rank; public UnionFind(int n) { parent = new int[n]; rank = new int[n]; for (int i = 0; i < n; i++) { parent[i] = i; // Each element is its own parent initially rank[i] = 0; } } // Find with path compression public int find(int x) { if (parent[x] != x) { parent[x] = find(parent[x]); // Path compression } return parent[x]; } // Union by rank public boolean union(int x, int y) { int rootX = find(x); int rootY = find(y); if (rootX == rootY) { return false; // Already in same set } // Union by rank: attach smaller tree under larger if (rank[rootX] < rank[rootY]) { parent[rootX] = rootY; } else if (rank[rootX] > rank[rootY]) { parent[rootY] = rootX; } else { parent[rootY] = rootX; rank[rootX]++; } return true; } public boolean connected(int x, int y) { return find(x) == find(y); } } Common Problems: Number of Connected Components Start with n components Each union decreases count by 1 Final count = n - (number of successful unions) Detect Cycle in Undirected Graph For each edge (u, v): If find(u) == find(v): cycle exists! Else: union(u, v) Accounts Merge (LeetCode 721) Union accounts with common emails Each component = one person Example: Detect Redundant Connection Problem: Find edge that creates cycle in undirected graph Input: edges = [[1,2], [1,3], [2,3]] Output: [2,3] (creates cycle) Solution: UnionFind uf = new UnionFind(n); for (int[] edge : edges) { if (!uf.union(edge[0], edge[1])) { return edge; // This edge creates cycle } } Complexity: Time: O(\u03b1(n)) per operation (inverse Ackermann, effectively O(1)) Space: O(n) for parent and rank arrays When to Use Union-Find: \u2705 Dynamic connectivity (edges added over time) \u2705 Detect cycles in undirected graphs \u2705 Group elements by equivalence relation \u2705 Kruskal's MST algorithm \u274c Need to remove edges (Union-Find doesn't support deletion) \u274c Directed graph cycle detection (use DFS instead) \u274c Shortest path queries (use BFS/Dijkstra) Use Cases: Network connectivity Image segmentation (connected components) Kruskal's MST Social network friend groups Accounts merging Decision Framework \u00b6 Question 1: Which shortest path algorithm? \u00b6 Use Dijkstra when: Non-negative weights: [Road networks, costs] Single source: [From one node to all others] Dense graphs: [Many edges] Use Bellman-Ford when: Negative weights allowed: [Financial arbitrage] Need cycle detection: [Negative cycles] Simple implementation: [No priority queue] Use A* when: Point-to-point search: [GPS navigation] Heuristic available: [Euclidean distance] Want faster search: [Guided by heuristic] Question 2: MST Algorithm Choice? \u00b6 Use Kruskal when: Sparse graph: [E << V\u00b2] Need simple implementation: [Sort + Union-Find] Edge list representation: [Not adjacency list] Use Prim when: Dense graph: [E \u2248 V\u00b2] Adjacency list: [Efficient neighbor access] Want incremental MST: [Grow from one vertex] Question 3: Topological Sort? \u00b6 Use DFS-based when: Simple implementation needed Want to detect cycles during sort Graph fits in memory Use Kahn's (BFS) when: Need explicit cycle detection Want lexicographically smallest ordering Parallel processing possible Question 4: When to use Union-Find? \u00b6 Use Union-Find when: Dynamic connectivity: [Edges added over time] Detect cycles in undirected graphs: [Kruskal's MST] Group by equivalence: [Accounts merge, friend groups] Don't use Union-Find when: Need to remove edges: [UF doesn't support deletion] Directed graph cycles: [Use DFS with states instead] Need shortest paths: [Use BFS/Dijkstra] Debugging Challenges \u00b6 Your task: Find and fix bugs in broken graph algorithm implementations. This tests your understanding of algorithm correctness and edge cases. Challenge 1: Dijkstra's Distance Check Bug \u00b6 /** * Dijkstra's algorithm with a CRITICAL BUG. * Can return incorrect shortest paths! */ public class BuggyDijkstra { public int[] dijkstra(List<List<Edge>> graph, int source) { int n = graph.size(); int[] dist = new int[n]; Arrays.fill(dist, Integer.MAX_VALUE); dist[source] = 0; PriorityQueue<int[]> pq = new PriorityQueue<>((a, b) -> a[1] - b[1]); pq.offer(new int[]{source, 0}); while (!pq.isEmpty()) { int[] curr = pq.poll(); int u = curr[0]; int d = curr[1]; // Missing check here! for (Edge edge : graph.get(u)) { int v = edge.to; int newDist = dist[u] + edge.weight; if (newDist < dist[v]) { dist[v] = newDist; pq.offer(new int[]{v, newDist}); } } } return dist; } } Your debugging: Bug: [What's missing after polling from PQ?] Failure scenario: Graph: 0\u21921(weight=5), 0\u21921(weight=3), 1\u21922(weight=1) Without fix: [How many times do we process node 1?] With fix: [How many times should we process node 1?] Impact: [Correctness? Performance? Both?] Click to verify your answer Bug: Missing distance check after polling. Should verify that we haven't already found a better path to this node. Fix: while (!pq.isEmpty()) { int[] curr = pq.poll(); int u = curr[0]; int d = curr[1]; // Skip if we've already processed this node with better distance if (d > dist[u]) continue; // ... rest of code } Why it matters: We may add the same node to the priority queue multiple times with different distances. Without this check, we process outdated entries, doing unnecessary work and potentially corrupting results. Example trace without fix: Step 1: Process (0, dist=0), add (1, dist=5) Step 2: Find shorter path, add (1, dist=3) Step 3: Process (1, dist=3) \u2713 (correct) Step 4: Process (1, dist=5) \u2717 (outdated entry, wastes time) With fix: Step 4 is skipped because d=5 > dist[1]=3. Challenge 2: Kruskal's MST - Cycle Detection Miss \u00b6 /** * Kruskal's algorithm with MISSING CYCLE CHECK. * Can create cycles in MST! */ public class BuggyKruskalMST { public List<Edge> kruskal(int n, List<Edge> edges) { Collections.sort(edges); // Sort by weight UnionFind uf = new UnionFind(n); List<Edge> mst = new ArrayList<>(); for (Edge edge : edges) { // Missing cycle check! mst.add(edge); if (mst.size() == n - 1) break; } return mst; } } Your debugging: Bug: [What's missing before adding edge to MST?] Failure scenario: Graph: Triangle with edges (0,1,1), (1,2,2), (2,0,3) With bug: MST edges = [Which edges?] Expected: MST edges = [Which edges?] Result: [Valid tree? Contains cycle?] Click to verify your answer Bug: Missing Union-Find check to detect cycles. Must verify that edge doesn't connect two vertices already in the same component. Fix: for (Edge edge : edges) { if (uf.union(edge.u, edge.v)) { // Only add if doesn't create cycle mst.add(edge); if (mst.size() == n - 1) break; } } Why it matters: Kruskal's algorithm relies on Union-Find to detect cycles. Without this check, we'd add all edges in weight order, creating cycles instead of a tree. Correct behavior: Edge (0,1,1): Add (different components) \u2713 Edge (1,2,2): Add (different components) \u2713 Edge (2,0,3): Skip (0 and 2 already connected via 1) \u2717 MST edges: (0,1), (1,2) with total weight = 3 Challenge 3: Topological Sort - Term Handling Bug \u00b6 /** * Topological sort (DFS-based) with CYCLE PROPAGATION BUG. * Fails to detect cycles properly! */ public class BuggyTopologicalSort { public List<Integer> topologicalSort(int n, List<List<Integer>> graph) { int[] visited = new int[n]; // 0: unvisited, 1: visiting, 2: visited Stack<Integer> stack = new Stack<>(); for (int i = 0; i < n; i++) { if (visited[i] == 0) { if (!dfs(i, graph, visited, stack)) { return new ArrayList<>(); // Cycle detected } } } List<Integer> result = new ArrayList<>(); while (!stack.isEmpty()) { result.add(stack.pop()); } return result; } private boolean dfs(int node, List<List<Integer>> graph, int[] visited, Stack<Integer> stack) { visited[node] = 1; // Mark as visiting for (int neighbor : graph.get(node)) { if (visited[neighbor] == 1) { return false; // Cycle detected } if (visited[neighbor] == 0) { dfs(neighbor, graph, visited, stack); // BUG: Missing return check! } } visited[node] = 2; // Mark as visited stack.push(node); return true; } } Your debugging: Bug: [What's missing in the recursive call?] Failure scenario: Graph: 0\u21921, 1\u21922, 2\u21920 (cycle) With bug: [Does it detect the cycle?] Expected: [Should return empty list] Click to verify your answer Bug: Not checking the return value of recursive DFS call. Even if a recursive call detects a cycle (returns false), we ignore it and continue. Fix: if (visited[neighbor] == 0) { if (!dfs(neighbor, graph, visited, stack)) { return false; // Propagate cycle detection } } Why it matters: Cycle detection must propagate back up the call stack. Without checking the return value, we lose the cycle detection signal. Trace with cycle: DFS(0): visits 1 DFS(1): visits 2 DFS(2): sees 0 is visiting \u2192 return false (cycle!) DFS(1): ignores return value, continues \u2192 BUG! DFS(0): completes successfully \u2192 WRONG! Correct: When DFS(2) detects cycle, DFS(1) should return false, then DFS(0) should return false, signaling cycle to main function. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 3 bugs across different algorithms Understood correctness vs performance issues Could explain WHY each bug causes failures Learned common algorithm implementation mistakes Common graph algorithm bugs you discovered: [Missing distance check in Dijkstra after polling] [Not using Union-Find to detect cycles in Kruskal's] [Not propagating cycle detection in TopSort DFS] Practice \u00b6 LeetCode Problems \u00b6 Focus on interview-critical patterns - practice in priority order: Topological Sort (MUST DO - \u2b50\u2b50\u2b50): 207. Course Schedule \u2b50\u2b50\u2b50 Pattern: [Topological Sort / Cycle Detection] Difficulty: [Rate 1-10] Key insight: [Fill in after solving] 210. Course Schedule II \u2b50\u2b50\u2b50 Pattern: [Topological Sort - return order] Difficulty: [Rate 1-10] Key insight: [Fill in] 269. Alien Dictionary (Premium) \u2b50\u2b50 Pattern: [Topological Sort] Difficulty: [Rate 1-10] Union-Find (Important - \u2b50\u2b50): 547. Number of Provinces \u2b50\u2b50 Pattern: [Union-Find / Connected Components] Difficulty: [Rate 1-10] Key insight: [Fill in] 684. Redundant Connection \u2b50\u2b50 Pattern: [Union-Find / Cycle Detection] Difficulty: [Rate 1-10] 721. Accounts Merge \u2b50\u2b50 Pattern: [Union-Find / Grouping] Difficulty: [Rate 1-10] Dijkstra (Good to know - \u2b50\u2b50): 743. Network Delay Time \u2b50\u2b50 Pattern: [Dijkstra's Algorithm] Difficulty: [Rate 1-10] Key insight: [Fill in] 787. Cheapest Flights Within K Stops \u2b50\u2b50 Pattern: [Modified Dijkstra with constraints] Difficulty: [Rate 1-10] MST (Optional - \u2b50): 1584. Min Cost to Connect All Points \u2b50 Pattern: [Prim's MST] Difficulty: [Rate 1-10] Note: Low frequency - only if you have extra time Review Checklist \u00b6 Before moving to next topic, ensure you've mastered: Understanding Understand topological sort (DFS and Kahn's algorithms) \u2b50\u2b50\u2b50 Understand Dijkstra's algorithm \u2b50\u2b50 Know Union-Find with optimizations \u2b50\u2b50 Understand MST algorithms (Kruskal, Prim) \u2b50 Know when each algorithm applies Implementation Can implement topological sort (both methods) Can implement Dijkstra with priority queue Can implement Union-Find with path compression Can implement Kruskal's MST (if time permits) Understand complexity analysis Pattern Recognition Solved Course Schedule I & II (topological sort) Solved 2-3 Union-Find problems Attempted 1-2 Dijkstra problems Understand when to use each algorithm Decision Making Know algorithm trade-offs Can choose correct algorithm for requirements Understand limitations (e.g., Dijkstra + negative weights) Mastery Certification \u00b6 I certify that I can: Implement topological sort from memory (both DFS and Kahn's) \u2b50\u2b50\u2b50 Solve Course Schedule problems confidently Implement Union-Find with path compression and union by rank \u2b50\u2b50 Detect cycles using Union-Find Implement Dijkstra's algorithm \u2b50\u2b50 Explain when Dijkstra fails (negative weights) Understand MST algorithms (Kruskal/Prim) \u2b50 Choose appropriate algorithm for given problem Analyze time/space complexity Debug common algorithm issues Explain these concepts to others","title":"12. Advanced Graphs"},{"location":"dsa/12-advanced-graphs/#graph-algorithms-optimization-ordering","text":"Apply graph traversal to solve optimization problems: topological sort, shortest paths, and MST","title":"Graph Algorithms: Optimization &amp; Ordering"},{"location":"dsa/12-advanced-graphs/#eli5-explain-like-im-5","text":"Your task: After learning these algorithms, explain them simply. Prompts to guide you: What does Topological Sort give us? Your answer: [Fill in after learning] When do we need Topological Sort? Your answer: [Course prerequisites, build systems...] What does Dijkstra's algorithm find? Your answer: [Fill in after learning] Real-world analogy for Dijkstra's algorithm: Example: \"Dijkstra's algorithm is like finding the cheapest flight where...\" Your analogy: [Fill in] Why can't we use Dijkstra for negative weights? Your answer: [Fill in after practice] Why do we need Minimum Spanning Trees? Your answer: [Fill in after learning] What problem does Union-Find solve? Your answer: [Dynamic connectivity...]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/12-advanced-graphs/#quick-quiz-do-before-learning","text":"Your task: Test your intuition about these algorithms before diving deep.","title":"Quick Quiz (Do BEFORE learning)"},{"location":"dsa/12-advanced-graphs/#beforeafter-why-advanced-graph-algorithms-matter","text":"Your task: Compare naive approaches vs optimized algorithms to understand the impact.","title":"Before/After: Why Advanced Graph Algorithms Matter"},{"location":"dsa/12-advanced-graphs/#core-concepts","text":"Recommended study order: \u2b50\u2b50\u2b50 Topological Sort (Topic 3 below) - Most common in interviews \u2b50\u2b50 Dijkstra's Algorithm (Topic 1 below) - Important for weighted graphs \u2b50\u2b50 Union-Find (add after completing above) - Dynamic connectivity \u2b50 MST (Topic 2 below) - Optional, low interview frequency","title":"Core Concepts"},{"location":"dsa/12-advanced-graphs/#decision-framework","text":"","title":"Decision Framework"},{"location":"dsa/12-advanced-graphs/#debugging-challenges","text":"Your task: Find and fix bugs in broken graph algorithm implementations. This tests your understanding of algorithm correctness and edge cases.","title":"Debugging Challenges"},{"location":"dsa/12-advanced-graphs/#practice","text":"","title":"Practice"},{"location":"dsa/12-advanced-graphs/#review-checklist","text":"Before moving to next topic, ensure you've mastered: Understanding Understand topological sort (DFS and Kahn's algorithms) \u2b50\u2b50\u2b50 Understand Dijkstra's algorithm \u2b50\u2b50 Know Union-Find with optimizations \u2b50\u2b50 Understand MST algorithms (Kruskal, Prim) \u2b50 Know when each algorithm applies Implementation Can implement topological sort (both methods) Can implement Dijkstra with priority queue Can implement Union-Find with path compression Can implement Kruskal's MST (if time permits) Understand complexity analysis Pattern Recognition Solved Course Schedule I & II (topological sort) Solved 2-3 Union-Find problems Attempted 1-2 Dijkstra problems Understand when to use each algorithm Decision Making Know algorithm trade-offs Can choose correct algorithm for requirements Understand limitations (e.g., Dijkstra + negative weights)","title":"Review Checklist"},{"location":"dsa/13-backtracking/","text":"Backtracking \u00b6 Explore all possible solutions by building candidates and abandoning them when they fail ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is backtracking in one sentence? Your answer: [Fill in after implementation] How is backtracking different from brute force? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Backtracking is like solving a maze by trying each path and going back when you hit a dead end...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What makes a problem suitable for backtracking? Your answer: [Fill in after learning the pattern] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Brute force generating all permutations of n elements: Time complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Backtracking to find N-Queens solutions: Time complexity: [Your guess: O(?)] Space complexity (excluding output): [Your guess: O(?)] Verified: [Actual] Speedup calculation: For generating subsets, brute force with checking = [Fill in] Backtracking with early pruning = [Fill in] Pruning benefit: [Fill in why it helps] Scenario Predictions \u00b6 Scenario 1: Generate all permutations of [1, 2, 3] How many permutations exist? [Your guess: ___ ] First permutation: [Fill in] When do you backtrack? [Fill in] What state do you restore? [Fill in] Scenario 2: Generate all subsets of [1, 2, 3] How many subsets exist? [Your guess: ___ ] Formula for n elements: [Fill in] How is this different from permutations? [Fill in] Scenario 3: Place 4 queens on a 4x4 board Can you place 2 queens in same column? [Yes/No - Why?] How do you track which columns are under attack? [Fill in your idea] What about diagonals? [Fill in] Trade-off Quiz \u00b6 Question: When would iterative enumeration be BETTER than backtracking? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN difference between permutations and combinations? Permutations care about order, combinations don't Permutations are always larger than combinations Permutations use recursion, combinations use iteration Permutations allow duplicates, combinations don't Verify after implementation: [Which one?] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example: Find All Subsets \u00b6 Problem: Generate all subsets of an array [1, 2, 3] . Approach 1: Brute Force Enumeration \u00b6 // Naive approach - Generate all possible combinations with checking public static List<List<Integer>> subsets_BruteForce(int[] nums) { List<List<Integer>> result = new ArrayList<>(); int n = nums.length; // Generate all possible bit patterns (2^n combinations) for (int mask = 0; mask < (1 << n); mask++) { List<Integer> subset = new ArrayList<>(); // Check each bit to decide if element is included for (int i = 0; i < n; i++) { if ((mask & (1 << i)) != 0) { subset.add(nums[i]); } } result.add(subset); } return result; } Analysis: Time: O(2^n * n) - Generate 2^n combinations, each takes O(n) to build Space: O(2^n * n) - Store all subsets For n = 10: 1,024 subsets * 10 operations = ~10,240 operations Limitation: Cannot easily add pruning or constraints Approach 2: Backtracking with Pruning \u00b6 // Optimized approach - Use backtracking to build subsets public static List<List<Integer>> subsets_Backtracking(int[] nums) { List<List<Integer>> result = new ArrayList<>(); backtrack(nums, 0, new ArrayList<>(), result); return result; } private static void backtrack(int[] nums, int start, List<Integer> current, List<List<Integer>> result) { // Add current subset (valid at every step) result.add(new ArrayList<>(current)); // Explore further choices for (int i = start; i < nums.length; i++) { current.add(nums[i]); // Make choice backtrack(nums, i + 1, current, result); // Explore current.remove(current.size() - 1); // Undo choice (backtrack) } } Analysis: Time: O(2^n * n) - Same asymptotic complexity Space: O(n) - Recursion depth (excluding output) For n = 10: Similar operations BUT can easily add pruning Advantage: Can prune branches early with constraints Performance Comparison \u00b6 Problem Type Brute Force Backtracking Advantage All subsets (n=10) 10,240 ops 10,240 ops Same without pruning Subsets with sum \u2264 K 10,240 ops ~5,000 ops 2x faster with pruning N-Queens (n=8) 16,777,216 ops ~2,000 ops 8,000x faster! Your calculation: For N-Queens with n=4, brute force tries _ placements, backtracking tries approximately ___. Why Does Backtracking with Pruning Work? \u00b6 Key insight to understand: In N-Queens (4x4 board): Brute Force: Try all 4^16 possible placements (>4 billion) Then check if valid Backtracking: Row 1: Try 4 positions Row 2: Only try valid positions (maybe 2 safe) Row 3: Only try valid positions (maybe 1 safe) Row 4: Only try valid positions \u274c Invalid? Backtrack immediately! Pruning eliminates entire branches: Brute force: Check constraints AFTER generating complete solution Backtracking: Check constraints DURING generation Each invalid partial solution eliminates millions of possibilities! After implementing, explain in your own words: Why is backtracking better than brute force enumeration? [Your answer] When does backtracking give the biggest advantage? [Your answer] What problems are NOT improved by backtracking? [Your answer] Core Implementation \u00b6 Pattern 1: Permutations \u00b6 Concept: Generate all possible orderings of elements. Use case: Permutations, permutations with duplicates. import java.util.*; public class PermutationsPattern { /** * Problem: Generate all permutations of distinct integers * Time: O(n! * n), Space: O(n!) * * TODO: Implement using backtracking */ public static List<List<Integer>> permute(int[] nums) { List<List<Integer>> result = new ArrayList<>(); // TODO: Call backtrack with empty list return result; // Replace with implementation } private static void backtrack(int[] nums, List<Integer> current, boolean[] used, List<List<Integer>> result) { // TODO: Handle base case // TODO: Implement iteration/conditional logic } /** * Problem: Permutations with duplicates * Time: O(n! * n), Space: O(n!) * * TODO: Implement with duplicate handling */ public static List<List<Integer>> permuteUnique(int[] nums) { List<List<Integer>> result = new ArrayList<>(); // TODO: Sort array first to handle duplicates // TODO: Use backtracking with duplicate checking return result; // Replace with implementation } /** * Problem: Next permutation * Time: O(n), Space: O(1) * * TODO: Implement next permutation in-place */ public static void nextPermutation(int[] nums) { // TODO: Implement logic // TODO: Find smallest element greater than nums[i] to the right // TODO: Swap them // TODO: Reverse suffix after i } } Runnable Client Code: import java.util.*; public class PermutationsPatternClient { public static void main(String[] args) { System.out.println(\"=== Permutations ===\\n\"); // Test 1: Basic permutations System.out.println(\"--- Test 1: Permutations ---\"); int[] nums1 = {1, 2, 3}; System.out.println(\"Input: \" + Arrays.toString(nums1)); List<List<Integer>> perms = PermutationsPattern.permute(nums1); System.out.println(\"Permutations (\" + perms.size() + \" total):\"); for (List<Integer> perm : perms) { System.out.println(\" \" + perm); } // Test 2: Permutations with duplicates System.out.println(\"\\n--- Test 2: Permutations with Duplicates ---\"); int[] nums2 = {1, 1, 2}; System.out.println(\"Input: \" + Arrays.toString(nums2)); List<List<Integer>> uniquePerms = PermutationsPattern.permuteUnique(nums2); System.out.println(\"Unique permutations (\" + uniquePerms.size() + \" total):\"); for (List<Integer> perm : uniquePerms) { System.out.println(\" \" + perm); } // Test 3: Next permutation System.out.println(\"\\n--- Test 3: Next Permutation ---\"); int[] nums3 = {1, 2, 3}; System.out.println(\"Start: \" + Arrays.toString(nums3)); for (int i = 0; i < 5; i++) { PermutationsPattern.nextPermutation(nums3); System.out.println(\"Next: \" + Arrays.toString(nums3)); } } } Pattern 2: Combinations and Subsets \u00b6 Concept: Generate all possible selections of elements. Use case: Combinations, subsets, subset sum. import java.util.*; public class CombinationsPattern { /** * Problem: Generate all subsets (power set) * Time: O(2^n * n), Space: O(2^n) * * TODO: Implement using backtracking */ public static List<List<Integer>> subsets(int[] nums) { List<List<Integer>> result = new ArrayList<>(); // TODO: Start with empty subset // TODO: Backtrack to generate all subsets return result; // Replace with implementation } private static void backtrackSubsets(int[] nums, int start, List<Integer> current, List<List<Integer>> result) { // TODO: Add current subset to result (valid at every step) // TODO: Implement iteration/conditional logic } /** * Problem: Generate combinations of k elements * Time: O(C(n,k) * k), Space: O(C(n,k)) * * TODO: Implement combinations */ public static List<List<Integer>> combine(int n, int k) { List<List<Integer>> result = new ArrayList<>(); // TODO: Backtrack with size constraint return result; // Replace with implementation } /** * Problem: Combination sum (elements can be reused) * Time: O(2^n), Space: O(n) * * TODO: Implement combination sum */ public static List<List<Integer>> combinationSum(int[] candidates, int target) { List<List<Integer>> result = new ArrayList<>(); // TODO: Backtrack with sum tracking // TODO: Can reuse same element return result; // Replace with implementation } /** * Problem: Subsets with duplicates * Time: O(2^n * n), Space: O(2^n) * * TODO: Implement with duplicate handling */ public static List<List<Integer>> subsetsWithDup(int[] nums) { List<List<Integer>> result = new ArrayList<>(); // TODO: Sort first // TODO: Skip duplicate elements in same level return result; // Replace with implementation } } Runnable Client Code: import java.util.*; public class CombinationsPatternClient { public static void main(String[] args) { System.out.println(\"=== Combinations and Subsets ===\\n\"); // Test 1: Subsets System.out.println(\"--- Test 1: Subsets ---\"); int[] nums1 = {1, 2, 3}; System.out.println(\"Input: \" + Arrays.toString(nums1)); List<List<Integer>> subsets = CombinationsPattern.subsets(nums1); System.out.println(\"Subsets (\" + subsets.size() + \" total):\"); for (List<Integer> subset : subsets) { System.out.println(\" \" + subset); } // Test 2: Combinations System.out.println(\"\\n--- Test 2: Combinations ---\"); int n = 4, k = 2; System.out.println(\"n = \" + n + \", k = \" + k); List<List<Integer>> combinations = CombinationsPattern.combine(n, k); System.out.println(\"Combinations (\" + combinations.size() + \" total):\"); for (List<Integer> comb : combinations) { System.out.println(\" \" + comb); } // Test 3: Combination sum System.out.println(\"\\n--- Test 3: Combination Sum ---\"); int[] candidates = {2, 3, 6, 7}; int target = 7; System.out.println(\"Candidates: \" + Arrays.toString(candidates)); System.out.println(\"Target: \" + target); List<List<Integer>> combSums = CombinationsPattern.combinationSum(candidates, target); System.out.println(\"Combinations:\"); for (List<Integer> comb : combSums) { System.out.println(\" \" + comb); } // Test 4: Subsets with duplicates System.out.println(\"\\n--- Test 4: Subsets with Duplicates ---\"); int[] nums2 = {1, 2, 2}; System.out.println(\"Input: \" + Arrays.toString(nums2)); List<List<Integer>> uniqueSubsets = CombinationsPattern.subsetsWithDup(nums2); System.out.println(\"Unique subsets (\" + uniqueSubsets.size() + \" total):\"); for (List<Integer> subset : uniqueSubsets) { System.out.println(\" \" + subset); } } } Pattern 3: N-Queens and Constraint Satisfaction \u00b6 Concept: Place elements with constraints, backtrack on violations. Use case: N-Queens, Sudoku solver. import java.util.*; public class ConstraintSatisfaction { /** * Problem: N-Queens - place N queens on N\u00d7N board * Time: O(N!), Space: O(N^2) * * TODO: Implement N-Queens using backtracking */ public static List<List<String>> solveNQueens(int n) { List<List<String>> result = new ArrayList<>(); // TODO: Initialize board // TODO: Track columns, diagonals under attack // TODO: Backtrack row by row return result; // Replace with implementation } private static void backtrackQueens(int row, int n, char[][] board, Set<Integer> cols, Set<Integer> diag1, Set<Integer> diag2, List<List<String>> result) { // TODO: Handle base case // TODO: Implement iteration/conditional logic } /** * Problem: Sudoku solver * Time: O(9^m) where m = empty cells, Space: O(1) * * TODO: Implement Sudoku solver */ public static void solveSudoku(char[][] board) { // TODO: Find empty cell // TODO: Try digits 1-9 // TODO: Check row, column, 3\u00d73 box constraints // TODO: Backtrack if no valid digit } private static boolean isValidSudoku(char[][] board, int row, int col, char c) { // TODO: Check row constraint // TODO: Check column constraint // TODO: Check 3\u00d73 box constraint return false; // Replace with implementation } /** * Problem: Count total N-Queens solutions * Time: O(N!), Space: O(N) * * TODO: Implement optimized N-Queens counter */ public static int totalNQueens(int n) { // TODO: Similar to solveNQueens but just count return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class ConstraintSatisfactionClient { public static void main(String[] args) { System.out.println(\"=== Constraint Satisfaction ===\\n\"); // Test 1: N-Queens System.out.println(\"--- Test 1: N-Queens (n=4) ---\"); List<List<String>> solutions = ConstraintSatisfaction.solveNQueens(4); System.out.println(\"Found \" + solutions.size() + \" solutions:\"); for (int i = 0; i < solutions.size(); i++) { System.out.println(\"Solution \" + (i + 1) + \":\"); for (String row : solutions.get(i)) { System.out.println(\" \" + row); } System.out.println(); } // Test 2: Count N-Queens System.out.println(\"--- Test 2: Count N-Queens Solutions ---\"); for (int n = 1; n <= 8; n++) { int count = ConstraintSatisfaction.totalNQueens(n); System.out.printf(\"n=%d: %d solutions%n\", n, count); } // Test 3: Sudoku solver System.out.println(\"\\n--- Test 3: Sudoku Solver ---\"); char[][] sudoku = { {'5','3','.','.','7','.','.','.','.'}, {'6','.','.','1','9','5','.','.','.'}, {'.','9','8','.','.','.','.','6','.'}, {'8','.','.','.','6','.','.','.','3'}, {'4','.','.','8','.','3','.','.','1'}, {'7','.','.','.','2','.','.','.','6'}, {'.','6','.','.','.','.','2','8','.'}, {'.','.','.','4','1','9','.','.','5'}, {'.','.','.','.','8','.','.','7','9'} }; System.out.println(\"Before:\"); printSudoku(sudoku); ConstraintSatisfaction.solveSudoku(sudoku); System.out.println(\"\\nAfter:\"); printSudoku(sudoku); } private static void printSudoku(char[][] board) { for (int i = 0; i < 9; i++) { if (i % 3 == 0 && i != 0) { System.out.println(\"------+-------+------\"); } for (int j = 0; j < 9; j++) { if (j % 3 == 0 && j != 0) { System.out.print(\"| \"); } System.out.print(board[i][j] + \" \"); } System.out.println(); } } } Pattern 4: Grid Search (Word Search) \u00b6 Concept: Explore grid paths with backtracking. Use case: Word search, path finding with constraints. public class GridSearch { /** * Problem: Word search in 2D grid * Time: O(m * n * 4^L) where L = word length, Space: O(L) * * TODO: Implement word search using backtracking */ public static boolean exist(char[][] board, String word) { // TODO: Try starting from each cell // TODO: Use DFS with backtracking return false; // Replace with implementation } private static boolean dfs(char[][] board, String word, int index, int row, int col, boolean[][] visited) { // TODO: Handle base case // TODO: Check bounds and visited // TODO: Check if board[row][col] == word.charAt(index) // TODO: Mark visited // TODO: Explore 4 directions (up, down, left, right) // TODO: Implement iteration/conditional logic // TODO: Unmark visited (backtrack) return false; // Replace with implementation } /** * Problem: Count paths from top-left to bottom-right * Time: O(2^(m+n)), Space: O(m+n) * * TODO: Implement path counter with obstacles */ public static int countPaths(int[][] grid) { // TODO: Backtrack with path counting // TODO: Handle obstacles (grid[i][j] == 1) return 0; // Replace with implementation } /** * Problem: Longest increasing path in matrix * Time: O(m * n), Space: O(m * n) with memoization * * TODO: Implement using DFS with memoization */ public static int longestIncreasingPath(int[][] matrix) { // TODO: DFS from each cell // TODO: Use memo to cache results // TODO: Can only move to strictly greater neighbors return 0; // Replace with implementation } } Runnable Client Code: public class GridSearchClient { public static void main(String[] args) { System.out.println(\"=== Grid Search ===\\n\"); // Test 1: Word search System.out.println(\"--- Test 1: Word Search ---\"); char[][] board = { {'A','B','C','E'}, {'S','F','C','S'}, {'A','D','E','E'} }; String[] words = {\"ABCCED\", \"SEE\", \"ABCB\"}; System.out.println(\"Board:\"); for (char[] row : board) { for (char c : row) { System.out.print(c + \" \"); } System.out.println(); } System.out.println(); for (String word : words) { boolean found = GridSearch.exist(board, word); System.out.printf(\"Word \\\"%s\\\": %s%n\", word, found ? \"FOUND\" : \"NOT FOUND\"); } // Test 2: Count paths System.out.println(\"\\n--- Test 2: Count Paths ---\"); int[][] grid = { {0, 0, 0}, {0, 1, 0}, {0, 0, 0} }; System.out.println(\"Grid (0=path, 1=obstacle):\"); for (int[] row : grid) { for (int cell : row) { System.out.print(cell + \" \"); } System.out.println(); } int paths = GridSearch.countPaths(grid); System.out.println(\"Total paths: \" + paths); // Test 3: Longest increasing path System.out.println(\"\\n--- Test 3: Longest Increasing Path ---\"); int[][] matrix = { {9, 9, 4}, {6, 6, 8}, {2, 1, 1} }; System.out.println(\"Matrix:\"); for (int[] row : matrix) { for (int cell : row) { System.out.print(cell + \" \"); } System.out.println(); } int longest = GridSearch.longestIncreasingPath(matrix); System.out.println(\"Longest increasing path: \" + longest); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken implementations. This tests your understanding of backtracking. Challenge 1: Broken Permutations \u00b6 /** * This code is supposed to generate all permutations. * It has 2 BUGS. Find them! */ public static List<List<Integer>> permute_Buggy(int[] nums) { List<List<Integer>> result = new ArrayList<>(); boolean[] used = new boolean[nums.length]; backtrack(nums, new ArrayList<>(), used, result); return result; } private static void backtrack(int[] nums, List<Integer> current, boolean[] used, List<List<Integer>> result) { // Base case if (current.size() == nums.length) { result.add(current); return; } for (int i = 0; i < nums.length; i++) { if (used[i]) continue; used[i] = true; current.add(nums[i]); backtrack(nums, current, used, result); } } Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Click to verify your answers Bug 1 (Line 11): Should be result.add(new ArrayList<>(current)) , not result.add(current) . Without copying, all results will reference the same list object, which gets modified during backtracking! Bug 2 (After line 20): Missing the backtrack step! Should have: current.remove(current.size() - 1); // Undo choice used[i] = false; // Undo state Without these lines, the algorithm never properly backtracks and explores other branches. Challenge 2: Broken Subsets with Duplicates \u00b6 /** * Generate subsets from array with duplicates [1, 2, 2] * This has 2 BUGS causing duplicate subsets in output. */ public static List<List<Integer>> subsetsWithDup_Buggy(int[] nums) { List<List<Integer>> result = new ArrayList<>(); backtrack(nums, 0, new ArrayList<>(), result); return result; } private static void backtrack(int[] nums, int start, List<Integer> current, List<List<Integer>> result) { result.add(new ArrayList<>(current)); for (int i = start; i < nums.length; i++) { current.add(nums[i]); backtrack(nums, i + 1, current, result); current.remove(current.size() - 1); } } Your debugging: Bug 1: [What must be done to the array first?] Bug 1 fix: [Fill in] Bug 2: [What check is missing in the loop?] Bug 2 fix: [Fill in the condition] Test case to expose the bug: Input: [1, 2, 2] Expected: [[], [1], [1,2], [1,2,2], [2], [2,2]] (6 unique subsets) Actual with buggy code: [How many duplicates?] Click to verify your answers Bug 1: Must sort the array first! Add Arrays.sort(nums); before calling backtrack. Sorting groups duplicates together so we can skip them. Bug 2: Missing duplicate check. After line 15, add: if (i > start && nums[i] == nums[i - 1]) { continue; // Skip duplicates at same level } This skips duplicate elements at the same recursion level, preventing duplicate subsets. Challenge 3: Broken N-Queens \u00b6 /** * N-Queens solver with bugs * This has 1 CRITICAL BUG in the diagonal checking logic. */ public static List<List<String>> solveNQueens_Buggy(int n) { List<List<String>> result = new ArrayList<>(); char[][] board = new char[n][n]; for (char[] row : board) Arrays.fill(row, '.'); Set<Integer> cols = new HashSet<>(); Set<Integer> diag1 = new HashSet<>(); Set<Integer> diag2 = new HashSet<>(); backtrack(0, n, board, cols, diag1, diag2, result); return result; } private static void backtrack(int row, int n, char[][] board, Set<Integer> cols, Set<Integer> diag1, Set<Integer> diag2, List<List<String>> result) { if (row == n) { result.add(constructBoard(board)); return; } for (int col = 0; col < n; col++) { int d1 = row - col; int d2 = row + col; if (cols.contains(col) || diag1.contains(d1) || diag2.contains(d2)) { continue; } // Place queen board[row][col] = 'Q'; cols.add(col); diag1.add(d1); diag2.add(d2); backtrack(row + 1, n, board, cols, diag1, diag2, result); } } Your debugging: Bug: [What\\'s the bug?] Hint: What happens after the recursive call? Click to verify your answer Bug: Missing backtrack cleanup after the recursive call! After line 37, should add: // Remove queen (backtrack) board[row][col] = '.'; cols.remove(col); diag1.remove(d1); diag2.remove(d2); Without this cleanup, the queen placement and constraint tracking aren't properly undone, causing incorrect solutions or missing valid solutions. Challenge 4: Broken Combination Sum with Pruning \u00b6 /** * Find all combinations that sum to target * This has a PRUNING BUG that misses valid solutions. */ public static List<List<Integer>> combinationSum_Buggy(int[] candidates, int target) { List<List<Integer>> result = new ArrayList<>(); Arrays.sort(candidates); // Sorted for pruning backtrack(candidates, target, 0, new ArrayList<>(), result); return result; } private static void backtrack(int[] candidates, int target, int start, List<Integer> current, List<List<Integer>> result) { if (target == 0) { result.add(new ArrayList<>(current)); return; } if (target < 0) { return; } for (int i = start; i < candidates.length; i++) { current.add(candidates[i]); backtrack(candidates, target - candidates[i], i, current, result); current.remove(current.size() - 1); } } Your debugging: Bug: [Where should pruning happen?] Why is it a problem? [What inefficiency does this cause?] Fix: [Add the proper pruning condition] Performance impact: Without fix: Explores many invalid branches With fix: Stops early when candidate exceeds remaining target Click to verify your answer Bug: Pruning should happen BEFORE recursion, not after! In the for loop, add: if (candidates[i] > target) { break; // All remaining candidates are too large (sorted array) } This should be added right after the for loop starts (before line 23). Since the array is sorted, once a candidate exceeds the target, all subsequent candidates will too, so we can break early. Better version of the loop: for (int i = start; i < candidates.length; i++) { if (candidates[i] > target) break; // Prune early! current.add(candidates[i]); backtrack(candidates, target - candidates[i], i, current, result); current.remove(current.size() - 1); } Challenge 5: Duplicate Results Bug \u00b6 /** * Permutations with duplicates [1, 1, 2] * This has a BUG causing duplicate permutations. */ public static List<List<Integer>> permuteUnique_Buggy(int[] nums) { List<List<Integer>> result = new ArrayList<>(); Arrays.sort(nums); // Sorted to handle duplicates boolean[] used = new boolean[nums.length]; backtrack(nums, new ArrayList<>(), used, result); return result; } private static void backtrack(int[] nums, List<Integer> current, boolean[] used, List<List<Integer>> result) { if (current.size() == nums.length) { result.add(new ArrayList<>(current)); return; } for (int i = 0; i < nums.length; i++) { if (used[i]) continue; // Should skip if: same as previous && previous not used used[i] = true; current.add(nums[i]); backtrack(nums, current, used, result); current.remove(current.size() - 1); used[i] = false; } } Your debugging: Bug: [What check is missing?] Why does this cause duplicates? [Explain the logic] Fix: [Fill in the condition] Test case: Input: [1, 1, 2] Expected: [[1,1,2], [1,2,1], [2,1,1]] (3 unique) Actual without fix: [How many duplicates?] Click to verify your answer Bug: Missing duplicate check! After line 20, add: // Skip duplicates: if same as previous element and previous not used if (i > 0 && nums[i] == nums[i - 1] && !used[i - 1]) { continue; } Why this works: If nums[i] == nums[i-1] (duplicates) AND previous not used, we'd generate same permutation By requiring previous to be used first, we ensure unique orderings Example: For [1, 1, 2], we must use first 1 before second 1 at same position Without this check, both 1's get treated as distinct, generating duplicate permutations. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 8+ bugs across 5 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common backtracking mistakes to avoid Common backtracking mistakes you discovered: [Fill in - e.g., forgetting to copy list when adding to result] [Fill in - e.g., missing backtrack cleanup] [Fill in - e.g., wrong pruning condition] [Fill in] Decision Framework \u00b6 Your task: Build decision trees for backtracking problems. Question 1: What are you generating? \u00b6 Answer after solving problems: All permutations? [Use permutation backtracking] All combinations/subsets? [Use combination backtracking] Single valid solution? [Return early when found] Count solutions? [Track count, don't store paths] Question 2: What are the constraints? \u00b6 No duplicates in input: Approach: [Simple backtracking] Duplicates in input: Approach: [Sort first, skip duplicates at same level] Size constraint (k elements): Approach: [Add base case for size] Sum/product constraint: Approach: [Track running sum/product, prune early] Grid constraints: Approach: [Mark visited, unmark on backtrack] Your Decision Tree \u00b6 flowchart LR Start[\"Backtracking Pattern Selection\"] Q1{\"Generating sequences?\"} Start --> Q1 N2([\"Permutations \u2713\"]) Q1 -->|\"All orderings\"| N2 N3([\"Combinations/Subsets \u2713\"]) Q1 -->|\"All selections\"| N3 N4([\"Combinations with constraint \u2713\"]) Q1 -->|\"With size K\"| N4 Q5{\"Constraint satisfaction?\"} Start --> Q5 N6([\"N-Queens pattern \u2713\"]) Q5 -->|\"Board placement\"| N6 N7([\"Try digits with validation \u2713\"]) Q5 -->|\"Sudoku-like\"| N7 N8([\"DFS with visited tracking \u2713\"]) Q5 -->|\"Grid search\"| N8 Q9{\"Optimization?\"} Start --> Q9 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 2): 257. Binary Tree Paths Pattern: [Backtracking with path tracking] Your solution time: ___ Key insight: [Fill in after solving] 401. Binary Watch Pattern: [Generate all combinations] Your solution time: ___ Key insight: [Fill in] Medium (Complete 4-5): 46. Permutations Pattern: [Classic permutations] Difficulty: [Rate 1-10] Key insight: [Fill in] 78. Subsets Pattern: [Classic subsets] Difficulty: [Rate 1-10] Key insight: [Fill in] 39. Combination Sum Pattern: [Combinations with reuse] Difficulty: [Rate 1-10] Key insight: [Fill in] 79. Word Search Pattern: [Grid DFS with backtracking] Difficulty: [Rate 1-10] Key insight: [Fill in] 22. Generate Parentheses Pattern: [Generate valid sequences] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 51. N-Queens Pattern: [Constraint satisfaction] Key insight: [Fill in after solving] 37. Sudoku Solver Pattern: [Grid constraint satisfaction] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Permutations: basic and with duplicates work Combinations: subsets, combinations, combination sum work N-Queens: placement and counting work Word search: grid DFS with backtracking works All client code runs successfully Pattern Recognition Can identify permutation vs combination problems Understand when to use visited array vs set Know when to sort input for duplicates Recognize constraint satisfaction problems Problem Solving Solved 2 easy problems Solved 4-5 medium problems Analyzed time/space complexity Understood pruning strategies Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use backtracking Can explain backtrack step clearly Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand when to prune branches Mastery Certification \u00b6 I certify that I can: Implement all four backtracking patterns from memory Explain when and why to use backtracking Identify the correct pattern for new problems Write proper base cases and backtrack steps Add effective pruning to reduce search space Debug common backtracking mistakes Teach this concept to someone else Analyze when NOT to use backtracking","title":"13. Backtracking"},{"location":"dsa/13-backtracking/#backtracking","text":"Explore all possible solutions by building candidates and abandoning them when they fail","title":"Backtracking"},{"location":"dsa/13-backtracking/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is backtracking in one sentence? Your answer: [Fill in after implementation] How is backtracking different from brute force? Your answer: [Fill in after implementation] Real-world analogy: Example: \"Backtracking is like solving a maze by trying each path and going back when you hit a dead end...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What makes a problem suitable for backtracking? Your answer: [Fill in after learning the pattern]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/13-backtracking/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/13-backtracking/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/13-backtracking/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/13-backtracking/#debugging-challenges","text":"Your task: Find and fix bugs in broken implementations. This tests your understanding of backtracking.","title":"Debugging Challenges"},{"location":"dsa/13-backtracking/#decision-framework","text":"Your task: Build decision trees for backtracking problems.","title":"Decision Framework"},{"location":"dsa/13-backtracking/#practice","text":"","title":"Practice"},{"location":"dsa/13-backtracking/#review-checklist","text":"Before moving to the next topic: Implementation Permutations: basic and with duplicates work Combinations: subsets, combinations, combination sum work N-Queens: placement and counting work Word search: grid DFS with backtracking works All client code runs successfully Pattern Recognition Can identify permutation vs combination problems Understand when to use visited array vs set Know when to sort input for duplicates Recognize constraint satisfaction problems Problem Solving Solved 2 easy problems Solved 4-5 medium problems Analyzed time/space complexity Understood pruning strategies Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use backtracking Can explain backtrack step clearly Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand when to prune branches","title":"Review Checklist"},{"location":"dsa/14-dynamic-programming-1d/","text":"Dynamic Programming (1D) \u00b6 Turn exponential recursion into polynomial iteration by caching subproblem results ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is dynamic programming in one sentence? Your answer: [Fill in after implementation] How is DP different from regular recursion? Your answer: [Fill in after implementation] Real-world analogy: Example: \"DP is like saving your homework answers so you don't have to recalculate the same problems...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What's the difference between top-down and bottom-up? Your answer: [Fill in after learning both approaches] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Pure recursive Fibonacci (no memoization): Time complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Fibonacci with memoization (top-down DP): Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Fibonacci with bottom-up DP: Time complexity: [Your guess: O(?)] Space complexity with array: [Your guess: O(?)] Space complexity optimized: [Your guess: O(?)] Verified: [Actual] Speedup calculation for Fibonacci(40): Recursive (no memo) = 2^40 = _____ operations With memoization = 40 = _____ operations Speedup factor: _____ times faster Scenario Predictions \u00b6 Scenario 1: Climbing stairs - 1 or 2 steps at a time to reach step 5 How many ways without computing? [Your guess: ___ _] Can you see the Fibonacci pattern? [Yes/No - Why?] Recurrence relation: ways(n) = [Fill in formula] Why does memoization help here? [Fill in] Scenario 2: Coin change - coins [1, 2, 5], amount = 11 Minimum coins needed: [Your guess: ___ _] What makes this a DP problem? [Fill in] What are the overlapping subproblems? [Fill in] Scenario 3: House robber - houses [2, 7, 9, 3, 1] Maximum money without adjacents: [Your guess: ___ _] Which pattern applies? [Fibonacci-style/Decision/String/Stock] Recurrence relation: rob(i) = [Fill in formula] Trade-off Quiz \u00b6 Question: When would recursive with memoization be BETTER than bottom-up DP? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN requirement for dynamic programming to work? Problem must involve arrays Must have optimal substructure Must have overlapping subproblems Both optimal substructure AND overlapping subproblems Must be solvable recursively Verify after implementation: [Which one(s)?] Question: Space optimization - when can you reduce O(n) to O(1)? Your answer: [Fill in - what's the pattern?] Verified: [Fill in after learning Fibonacci-style problems] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand exponential to polynomial transformation. Example: Fibonacci Number \u00b6 Problem: Calculate the nth Fibonacci number where F(n) = F(n-1) + F(n-2). Approach 1: Pure Recursion (Exponential) \u00b6 // Naive approach - Recalculates same values many times public static int fib_Recursive(int n) { if (n <= 1) return n; return fib_Recursive(n - 1) + fib_Recursive(n - 2); } Analysis: Time: O(2^n) - Each call branches into two more calls Space: O(n) - Recursion stack depth For n = 40: ~2,000,000,000 operations (takes several seconds) For n = 50: Would take hours or days Why so slow? Tree of recursive calls: fib(5) \u251c\u2500\u2500 fib(4) \u2502 \u251c\u2500\u2500 fib(3) \u2502 \u2502 \u251c\u2500\u2500 fib(2) \u2502 \u2502 \u2502 \u251c\u2500\u2500 fib(1) \u2502 \u2502 \u2502 \u2514\u2500\u2500 fib(0) \u2502 \u2502 \u2514\u2500\u2500 fib(1) \u2502 \u2514\u2500\u2500 fib(2) \u2502 \u251c\u2500\u2500 fib(1) \u2502 \u2514\u2500\u2500 fib(0) \u2514\u2500\u2500 fib(3) \u2190 RECALCULATING same subtree! \u251c\u2500\u2500 fib(2) \u2502 \u251c\u2500\u2500 fib(1) \u2502 \u2514\u2500\u2500 fib(0) \u2514\u2500\u2500 fib(1) fib(3) is calculated TWICE, fib(2) THREE times, fib(1) FIVE times! Approach 2: Memoization - Top-Down DP (Polynomial) \u00b6 // Optimized - Cache results to avoid recalculation public static int fib_Memoization(int n, int[] memo) { if (n <= 1) return n; if (memo[n] != 0) return memo[n]; // Already calculated! memo[n] = fib_Memoization(n - 1, memo) + fib_Memoization(n - 2, memo); return memo[n]; } // Wrapper public static int fib(int n) { return fib_Memoization(n, new int[n + 1]); } Analysis: Time: O(n) - Each fib(i) computed exactly once Space: O(n) - Memoization array + recursion stack For n = 40: ~40 operations (instant) For n = 50: ~50 operations (instant) Approach 3: Bottom-Up DP (Best Space Optimization) \u00b6 // Iterative - Build from bottom, optimize space public static int fib_BottomUp(int n) { if (n <= 1) return n; int prev2 = 0; // F(0) int prev1 = 1; // F(1) for (int i = 2; i <= n; i++) { int current = prev1 + prev2; prev2 = prev1; prev1 = current; } return prev1; } Analysis: Time: O(n) - Single pass Space: O(1) - Only 3 variables (optimized from O(n) array) For n = 40: ~40 operations, minimal memory No recursion stack overhead Performance Comparison \u00b6 Approach n=10 n=20 n=30 n=40 Space Recursive 0.001ms 2ms 200ms 2000ms O(n) stack Memoization 0.001ms 0.001ms 0.001ms 0.001ms O(n) array Bottom-Up 0.001ms 0.001ms 0.001ms 0.001ms O(1) Speedup for n=40: Memoization is ~2,000,000x faster than pure recursion! Why Does DP Work Here? \u00b6 Key properties that enable DP: Optimal Substructure: Solution to F(n) can be built from solutions to F(n-1) and F(n-2) Overlapping Subproblems: Same values calculated repeatedly in naive recursion Visualization of overlapping subproblems: Computing fib(6): fib(5): needs fib(4) + fib(3) fib(4): needs fib(3) + fib(2) fib(3): needs fib(2) + fib(1) \u2191 \u2191 These repeat! These repeat! Without DP: Recalculate everything (exponential waste) With DP: Calculate once, reuse (polynomial efficiency) Example 2: Coin Change (Decision Problem) \u00b6 Problem: Minimum coins needed to make amount = 11 with coins [1, 2, 5]. Approach 1: Brute Force Recursion \u00b6 // Try all combinations - exponential time public static int coinChange_Recursive(int[] coins, int amount) { if (amount == 0) return 0; if (amount < 0) return -1; int min = Integer.MAX_VALUE; for (int coin : coins) { int result = coinChange_Recursive(coins, amount - coin); if (result >= 0 && result < min) { min = result + 1; } } return min == Integer.MAX_VALUE ? -1 : min; } Analysis: Time: O(amount^coins) - Exponential branching Space: O(amount) - Recursion depth For amount = 100, coins = [1, 2, 5]: billions of operations Approach 2: Bottom-Up DP \u00b6 // Build table of minimum coins for each amount public static int coinChange_DP(int[] coins, int amount) { int[] dp = new int[amount + 1]; Arrays.fill(dp, amount + 1); // Initialize with \"infinity\" dp[0] = 0; // Base case for (int i = 1; i <= amount; i++) { for (int coin : coins) { if (i >= coin) { dp[i] = Math.min(dp[i], dp[i - coin] + 1); } } } return dp[amount] > amount ? -1 : dp[amount]; } Analysis: Time: O(amount \u00d7 coins) - Polynomial Space: O(amount) - DP array For amount = 100, coins = [1, 2, 5]: ~300 operations Speedup: From exponential to polynomial! After implementing, explain in your own words: Why does caching subproblem results help? [Your answer] What's the difference between top-down and bottom-up? [Your answer] When would you choose one approach over the other? [Your answer] Core Implementation \u00b6 Pattern 1: Fibonacci-Style Problems \u00b6 Concept: Each state depends on previous 1-2 states. Use case: Climbing stairs, house robber, decode ways. public class FibonacciStyle { /** * Problem: Climbing stairs (1 or 2 steps at a time) * Time: O(n), Space: O(1) * * TODO: Implement using DP */ public static int climbStairs(int n) { // TODO: Base cases: n=1 -> 1, n=2 -> 2 // TODO: DP approach (bottom-up): return 0; // Replace with implementation } /** * Problem: House robber (can't rob adjacent houses) * Time: O(n), Space: O(1) * * TODO: Implement using DP */ public static int rob(int[] nums) { // TODO: dp[i] = max money robbing up to house i // TODO: dp[i] = max(dp[i-1], dp[i-2] + nums[i]) // TODO: Optimize space: only need last 2 values return 0; // Replace with implementation } /** * Problem: House robber II (houses in a circle) * Time: O(n), Space: O(1) * * TODO: Implement circular version */ public static int robCircular(int[] nums) { // TODO: Can't rob both first and last house // TODO: Try two scenarios: // TODO: Return max of both return 0; // Replace with implementation } /** * Problem: Min cost climbing stairs * Time: O(n), Space: O(1) * * TODO: Implement min cost DP */ public static int minCostClimbingStairs(int[] cost) { // TODO: dp[i] = min cost to reach step i // TODO: dp[i] = cost[i] + min(dp[i-1], dp[i-2]) // TODO: Can start from step 0 or 1 return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class FibonacciStyleClient { public static void main(String[] args) { System.out.println(\"=== Fibonacci-Style DP ===\\n\"); // Test 1: Climbing stairs System.out.println(\"--- Test 1: Climbing Stairs ---\"); int[] stairs = {1, 2, 3, 4, 5, 10}; for (int n : stairs) { int ways = FibonacciStyle.climbStairs(n); System.out.printf(\"n=%d: %d ways%n\", n, ways); } // Test 2: House robber System.out.println(\"\\n--- Test 2: House Robber ---\"); int[][] houses = { {1, 2, 3, 1}, {2, 7, 9, 3, 1}, {2, 1, 1, 2} }; for (int[] house : houses) { int maxMoney = FibonacciStyle.rob(house); System.out.printf(\"Houses: %s -> Max: %d%n\", Arrays.toString(house), maxMoney); } // Test 3: House robber II (circular) System.out.println(\"\\n--- Test 3: House Robber II ---\"); int[][] circularHouses = { {2, 3, 2}, {1, 2, 3, 1}, {1, 2, 3} }; for (int[] house : circularHouses) { int maxMoney = FibonacciStyle.robCircular(house); System.out.printf(\"Houses: %s -> Max: %d%n\", Arrays.toString(house), maxMoney); } // Test 4: Min cost climbing System.out.println(\"\\n--- Test 4: Min Cost Climbing Stairs ---\"); int[][] costs = { {10, 15, 20}, {1, 100, 1, 1, 1, 100, 1, 1, 100, 1} }; for (int[] cost : costs) { int minCost = FibonacciStyle.minCostClimbingStairs(cost); System.out.printf(\"Cost: %s -> Min: %d%n\", Arrays.toString(cost), minCost); } } } Pattern 2: Decision Problems (Take/Skip) \u00b6 Concept: At each position, decide to take or skip element. Use case: Coin change, subset sum, partition equal subset. import java.util.*; public class DecisionProblems { /** * Problem: Coin change - minimum coins to make amount * Time: O(amount * n), Space: O(amount) * * TODO: Implement using DP */ public static int coinChange(int[] coins, int amount) { // TODO: dp[i] = min coins to make amount i // TODO: dp[i] = min(dp[i], dp[i - coin] + 1) for each coin // TODO: Initialize dp[0] = 0, rest = infinity // TODO: Return dp[amount] or -1 if impossible return -1; // Replace with implementation } /** * Problem: Coin change II - count ways to make amount * Time: O(amount * n), Space: O(amount) * * TODO: Implement counting ways */ public static int change(int amount, int[] coins) { // TODO: dp[i] = ways to make amount i // TODO: Implement iteration/conditional logic // TODO: dp[i] += dp[i - coin] return 0; // Replace with implementation } /** * Problem: Perfect squares - min perfect squares to sum to n * Time: O(n * sqrt(n)), Space: O(n) * * TODO: Implement using DP */ public static int numSquares(int n) { // TODO: Similar to coin change // TODO: \"Coins\" are perfect squares: 1, 4, 9, 16, ... // TODO: dp[i] = min perfect squares to sum to i return 0; // Replace with implementation } /** * Problem: Partition equal subset sum * Time: O(n * sum), Space: O(sum) * * TODO: Implement subset sum DP */ public static boolean canPartition(int[] nums) { // TODO: Implement iteration/conditional logic // TODO: Problem becomes: can we make sum/2? // TODO: dp[i] = can we make sum i? // TODO: Implement iteration/conditional logic return false; // Replace with implementation } } Runnable Client Code: import java.util.*; public class DecisionProblemsClient { public static void main(String[] args) { System.out.println(\"=== Decision Problems ===\\n\"); // Test 1: Coin change System.out.println(\"--- Test 1: Coin Change ---\"); int[] coins = {1, 2, 5}; int[] amounts = {11, 3, 0, 7}; System.out.println(\"Coins: \" + Arrays.toString(coins)); for (int amount : amounts) { int result = DecisionProblems.coinChange(coins, amount); System.out.printf(\"Amount %d: %d coins%n\", amount, result); } // Test 2: Coin change II (count ways) System.out.println(\"\\n--- Test 2: Coin Change II ---\"); int amount = 5; int[] coins2 = {1, 2, 5}; System.out.println(\"Amount: \" + amount); System.out.println(\"Coins: \" + Arrays.toString(coins2)); int ways = DecisionProblems.change(amount, coins2); System.out.println(\"Ways: \" + ways); // Test 3: Perfect squares System.out.println(\"\\n--- Test 3: Perfect Squares ---\"); int[] numbers = {12, 13, 1, 4, 9, 16}; for (int n : numbers) { int count = DecisionProblems.numSquares(n); System.out.printf(\"n=%d: %d perfect squares%n\", n, count); } // Test 4: Partition equal subset System.out.println(\"\\n--- Test 4: Partition Equal Subset ---\"); int[][] arrays = { {1, 5, 11, 5}, {1, 2, 3, 5}, {1, 2, 3, 4} }; for (int[] arr : arrays) { boolean canPartition = DecisionProblems.canPartition(arr); System.out.printf(\"Array: %s -> %s%n\", Arrays.toString(arr), canPartition ? \"YES\" : \"NO\"); } } } Pattern 3: String DP \u00b6 Concept: Build solution character by character. Use case: Decode ways, word break, palindrome partitioning. import java.util.*; public class StringDP { /** * Problem: Decode ways (1=A, 2=B, ..., 26=Z) * Time: O(n), Space: O(1) * * TODO: Implement decode ways */ public static int numDecodings(String s) { // TODO: dp[i] = ways to decode substring 0..i // TODO: Single digit: if s[i] != '0', dp[i] += dp[i-1] // TODO: Two digits: if 10 <= s[i-1:i] <= 26, dp[i] += dp[i-2] // TODO: Optimize space: only need last 2 values return 0; // Replace with implementation } /** * Problem: Word break - can string be segmented into words * Time: O(n^2 * m) where m = avg word length, Space: O(n) * * TODO: Implement word break */ public static boolean wordBreak(String s, List<String> wordDict) { // TODO: dp[i] = can substring 0..i be segmented? // TODO: dp[i] = true if any dp[j] && s.substring(j,i) in dict // TODO: Use HashSet for O(1) word lookup return false; // Replace with implementation } /** * Problem: Longest increasing subsequence * Time: O(n^2), Space: O(n) * * TODO: Implement LIS using DP */ public static int lengthOfLIS(int[] nums) { // TODO: dp[i] = length of LIS ending at i // TODO: Implement logic // TODO: Return max value in dp array return 0; // Replace with implementation } /** * Problem: Longest palindromic substring * Time: O(n^2), Space: O(n^2) or O(1) with expand from center * * TODO: Implement LPS */ public static String longestPalindrome(String s) { // TODO: Expand around center approach (space O(1)) // TODO: Try each position as center (odd and even length) // TODO: Or use DP: dp[i][j] = is substring i..j palindrome? return \"\"; // Replace with implementation } } Runnable Client Code: import java.util.*; public class StringDPClient { public static void main(String[] args) { System.out.println(\"=== String DP ===\\n\"); // Test 1: Decode ways System.out.println(\"--- Test 1: Decode Ways ---\"); String[] codes = {\"12\", \"226\", \"06\", \"10\"}; for (String code : codes) { int ways = StringDP.numDecodings(code); System.out.printf(\"Code \\\"%s\\\": %d ways%n\", code, ways); } // Test 2: Word break System.out.println(\"\\n--- Test 2: Word Break ---\"); List<String> dict = Arrays.asList(\"leet\", \"code\", \"sand\", \"and\", \"cat\"); String[] testStrings = {\"leetcode\", \"catsand\", \"catsandog\"}; System.out.println(\"Dictionary: \" + dict); for (String s : testStrings) { boolean canBreak = StringDP.wordBreak(s, dict); System.out.printf(\"String \\\"%s\\\": %s%n\", s, canBreak ? \"YES\" : \"NO\"); } // Test 3: Longest increasing subsequence System.out.println(\"\\n--- Test 3: Longest Increasing Subsequence ---\"); int[][] sequences = { {10, 9, 2, 5, 3, 7, 101, 18}, {0, 1, 0, 3, 2, 3}, {7, 7, 7, 7} }; for (int[] seq : sequences) { int length = StringDP.lengthOfLIS(seq); System.out.printf(\"Array: %s -> LIS length: %d%n\", Arrays.toString(seq), length); } // Test 4: Longest palindromic substring System.out.println(\"\\n--- Test 4: Longest Palindromic Substring ---\"); String[] palindromeTests = {\"babad\", \"cbbd\", \"racecar\", \"noon\"}; for (String s : palindromeTests) { String lps = StringDP.longestPalindrome(s); System.out.printf(\"String \\\"%s\\\" -> LPS: \\\"%s\\\"%n\", s, lps); } } } Pattern 4: Buy/Sell Stock Problems \u00b6 Concept: Track states (holding/not holding stock) through array. Use case: Stock trading with various constraints. public class StockProblems { /** * Problem: Best time to buy and sell stock (one transaction) * Time: O(n), Space: O(1) * * TODO: Implement single transaction */ public static int maxProfit(int[] prices) { // TODO: Track minimum price seen so far // TODO: Track maximum profit (price - minPrice) // TODO: One pass solution return 0; // Replace with implementation } /** * Problem: Best time to buy and sell stock II (unlimited transactions) * Time: O(n), Space: O(1) * * TODO: Implement unlimited transactions */ public static int maxProfitUnlimited(int[] prices) { // TODO: Sum all positive differences // TODO: Buy before every increase, sell at peak return 0; // Replace with implementation } /** * Problem: Best time to buy and sell stock with cooldown * Time: O(n), Space: O(1) * * TODO: Implement with cooldown */ public static int maxProfitCooldown(int[] prices) { // TODO: Track three states: // TODO: Transitions: hold -> sold -> rest -> hold return 0; // Replace with implementation } /** * Problem: Best time to buy and sell stock with fee * Time: O(n), Space: O(1) * * TODO: Implement with transaction fee */ public static int maxProfitWithFee(int[] prices, int fee) { // TODO: Similar to cooldown // TODO: Subtract fee when selling return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class StockProblemsClient { public static void main(String[] args) { System.out.println(\"=== Stock Problems ===\\n\"); // Test 1: Single transaction System.out.println(\"--- Test 1: Single Transaction ---\"); int[] prices1 = {7, 1, 5, 3, 6, 4}; System.out.println(\"Prices: \" + Arrays.toString(prices1)); int profit1 = StockProblems.maxProfit(prices1); System.out.println(\"Max profit: \" + profit1); // Test 2: Unlimited transactions System.out.println(\"\\n--- Test 2: Unlimited Transactions ---\"); int[] prices2 = {7, 1, 5, 3, 6, 4}; System.out.println(\"Prices: \" + Arrays.toString(prices2)); int profit2 = StockProblems.maxProfitUnlimited(prices2); System.out.println(\"Max profit: \" + profit2); // Test 3: With cooldown System.out.println(\"\\n--- Test 3: With Cooldown ---\"); int[] prices3 = {1, 2, 3, 0, 2}; System.out.println(\"Prices: \" + Arrays.toString(prices3)); int profit3 = StockProblems.maxProfitCooldown(prices3); System.out.println(\"Max profit: \" + profit3); // Test 4: With fee System.out.println(\"\\n--- Test 4: With Transaction Fee ---\"); int[] prices4 = {1, 3, 2, 8, 4, 9}; int fee = 2; System.out.println(\"Prices: \" + Arrays.toString(prices4)); System.out.println(\"Fee: \" + fee); int profit4 = StockProblems.maxProfitWithFee(prices4, fee); System.out.println(\"Max profit: \" + profit4); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken DP implementations. This tests your understanding of recurrence relations, base cases, and iteration order. Challenge 1: Broken Climbing Stairs \u00b6 /** * This code is supposed to count ways to climb n stairs. * It has 2 BUGS. Find them! */ public static int climbStairs_Buggy(int n) { if (n == 1) return 1; if (n == 2) return 2; int[] dp = new int[n]; dp[0] = 1; dp[1] = 2; for (int i = 2; i <= n; i++) { dp[i] = dp[i - 1] + dp[i - 2]; } return dp[n]; } Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Test case to expose: Input: n = 5 Expected: 8 ways What happens: [Trace through - where does it crash?] Click to verify your answers Bug 1 (Line 6): Array size should be n + 1 , not n . We need indices from 0 to n inclusive, or adjust indexing. Bug 2 (Line 9): Loop condition i <= n will cause ArrayIndexOutOfBoundsException. Either use i < n or fix array size to n + 1 . Correct version: public static int climbStairs(int n) { if (n == 1) return 1; if (n == 2) return 2; int[] dp = new int[n + 1]; // Fix 1: n + 1 size dp[1] = 1; dp[2] = 2; for (int i = 3; i <= n; i++) { // Fix 2: start at 3, or use i < n dp[i] = dp[i - 1] + dp[i - 2]; } return dp[n]; } Alternative fix with O(1) space: public static int climbStairs(int n) { if (n <= 2) return n; int prev2 = 1, prev1 = 2; for (int i = 3; i <= n; i++) { int current = prev1 + prev2; prev2 = prev1; prev1 = current; } return prev1; } Challenge 2: Broken Coin Change \u00b6 /** * Minimum coins to make amount. * This has 2 CRITICAL BUGS - wrong recurrence and wrong base case. */ public static int coinChange_Buggy(int[] coins, int amount) { int[] dp = new int[amount + 1]; Arrays.fill(dp, Integer.MAX_VALUE); dp[0] = 0; // Base case: 0 coins for amount 0 for (int i = 0; i < amount; i++) { for (int coin : coins) { if (i + coin <= amount) { dp[i + coin] = Math.min(dp[i + coin], dp[i] + 1); } } } return dp[amount] == Integer.MAX_VALUE ? -1 : dp[amount]; } Your debugging: Bug 1: [Loop should be i < amount or i <= amount ? Why?] Bug 2: [Is the recurrence relation correct? What if dp[i] is MAX_VALUE?] Test case: coins = [2] , amount = 3 Expected: -1 (impossible) With buggy code: [What do you get?] Trace through manually: i=0: dp[0]=0, trying coin=2 \u2192 dp[2] = min(MAX, 0+1) = 1 i=1: dp[1]=MAX, trying coin=2 \u2192 [What happens here?] Click to verify your answers Bug 1: Loop is actually okay but can be clearer. Better approach is for (int i = 1; i <= amount; i++) and check i - coin >= 0 . Bug 2: The logic has a subtle bug: if dp[i] is MAX_VALUE, then dp[i] + 1 overflows to negative! Need to check before adding. Correct version: public static int coinChange(int[] coins, int amount) { int[] dp = new int[amount + 1]; Arrays.fill(dp, amount + 1); // Use amount + 1 as \"infinity\" (safer) dp[0] = 0; for (int i = 1; i <= amount; i++) { for (int coin : coins) { if (i >= coin) { dp[i] = Math.min(dp[i], dp[i - coin] + 1); } } } return dp[amount] > amount ? -1 : dp[amount]; } Key insight: Use amount + 1 instead of Integer.MAX_VALUE as infinity to avoid overflow. It's impossible to need more than amount coins if you have coin value 1. Challenge 3: Broken House Robber \u00b6 /** * Maximum money robbing houses without adjacent. * This has 1 SUBTLE BUG in recurrence relation. */ public static int rob_Buggy(int[] nums) { if (nums.length == 0) return 0; if (nums.length == 1) return nums[0]; int[] dp = new int[nums.length]; dp[0] = nums[0]; dp[1] = nums[1]; for (int i = 2; i < nums.length; i++) { dp[i] = Math.max(dp[i - 1], dp[i - 2] + nums[i]); } return dp[nums.length - 1]; } Your debugging: Bug location: [Which line has the wrong base case?] Bug explanation: [Why is dp[1] = nums[1] wrong?] Bug fix: [What should dp[1] be?] Test case to expose the bug: Input: [2, 1, 1, 2] Expected: 4 (rob houses 0 and 3) Buggy output: [Trace through - what do you get?] Manual trace with buggy code: dp[0] = 2 dp[1] = 1 (BUG: should this compare something?) dp[2] = max(dp[1], dp[0] + nums[2]) = max(1, 2+1) = 3 dp[3] = max(dp[2], dp[1] + nums[3]) = max(3, 1+2) = 3 Returns 3, but correct answer is 4! Click to verify your answer Bug (Line 7): dp[1] should be Math.max(nums[0], nums[1]) , not just nums[1] . Why: At house 1, you have a choice: rob house 0 OR rob house 1. You should take the maximum of the two, not automatically rob house 1. Correct version: public static int rob(int[] nums) { if (nums.length == 0) return 0; if (nums.length == 1) return nums[0]; int[] dp = new int[nums.length]; dp[0] = nums[0]; dp[1] = Math.max(nums[0], nums[1]); // Fix: choose better of first two for (int i = 2; i < nums.length; i++) { dp[i] = Math.max(dp[i - 1], dp[i - 2] + nums[i]); } return dp[nums.length - 1]; } Space-optimized version: public static int rob(int[] nums) { if (nums.length == 0) return 0; if (nums.length == 1) return nums[0]; int prev2 = nums[0]; int prev1 = Math.max(nums[0], nums[1]); for (int i = 2; i < nums.length; i++) { int current = Math.max(prev1, prev2 + nums[i]); prev2 = prev1; prev1 = current; } return prev1; } Challenge 4: Broken Word Break \u00b6 /** * Check if string can be segmented into dictionary words. * This has 2 BUGS - wrong iteration order and wrong base case check. */ public static boolean wordBreak_Buggy(String s, List<String> wordDict) { Set<String> dict = new HashSet<>(wordDict); boolean[] dp = new boolean[s.length() + 1]; dp[0] = true; for (int i = 1; i <= s.length(); i++) { for (int j = i; j >= 0; j--) { if (dp[j] && dict.contains(s.substring(j, i))) { dp[i] = true; break; } } } return dp[s.length()]; } Your debugging: Bug 1: [Is the inner loop direction correct?] Bug 2: [Is the break statement causing missed solutions?] Test case: s = \"leetcode\" , dict = [\"leet\", \"code\"] Expected: true Trace manually: [Does it find the solution?] Actually... is this code correct? [Carefully trace through the logic] Check: does it explore all possible segmentations? [Your answer] Click to verify your answer Surprise: The code is actually CORRECT! This was a trick question to test your careful analysis. Why it works: Inner loop j from i down to 0 checks all possible last words ending at position i If dp[j] is true (substring 0..j can be segmented) AND substring j..i is in dictionary, then substring 0..i can be segmented The break is an optimization - once we find one valid segmentation, we don't need to find more Common misconception: Students often think the break causes problems, but we only need to know IF segmentation is possible, not find ALL ways. The real learning: Not all complex-looking DP code has bugs! Sometimes the challenge is understanding WHY it's correct. Alternative version without break (slightly less efficient): public static boolean wordBreak(String s, List<String> wordDict) { Set<String> dict = new HashSet<>(wordDict); boolean[] dp = new boolean[s.length() + 1]; dp[0] = true; for (int i = 1; i <= s.length(); i++) { for (int j = 0; j < i; j++) { if (dp[j] && dict.contains(s.substring(j, i))) { dp[i] = true; // No break - checks all j values } } } return dp[s.length()]; } Both versions are correct; the first is slightly faster due to early exit. Challenge 5: Wrong Iteration Order \u00b6 /** * Coin change II - count ways to make amount. * This has 1 CRITICAL BUG that causes wrong answer. */ public static int change_Buggy(int amount, int[] coins) { int[] dp = new int[amount + 1]; dp[0] = 1; // One way to make 0: use no coins for (int i = 1; i <= amount; i++) { for (int coin : coins) { if (i >= coin) { dp[i] += dp[i - coin]; } } } return dp[amount]; } Your debugging: Bug: [Why does loop order matter here?] What does this code actually count? [Permutations or combinations?] How to fix: [Swap loop order? Why?] Test case: amount = 4 , coins = [1, 2] Expected: 3 ways (1+1+1+1, 1+1+2, 2+2) Buggy output: [What do you get? Why?] Think: This buggy version counts [1,1,2] and [1,2,1] and [2,1,1] as different. Should they be? Click to verify your answer Bug: Loop order causes counting permutations instead of combinations! Buggy version counts: {1,1,2}, {1,2,1}, {2,1,1}, {2,2}, {1,1,1,1} = 5 ways Correct should count: {1,1,2}, {2,2}, {1,1,1,1} = 3 ways Fix - swap loop order: public static int change(int amount, int[] coins) { int[] dp = new int[amount + 1]; dp[0] = 1; // Correct: iterate coins in outer loop for (int coin : coins) { for (int i = coin; i <= amount; i++) { dp[i] += dp[i - coin]; } } return dp[amount]; } Why this works: Outer loop on coins ensures we consider coins in order For each coin, we update all amounts that can use it This prevents counting different orders of the same coin set Key insight: In combination problems (Coin Change II, Subset Sum), loop over items in outer loop. In minimum/maximum problems (Coin Change I), loop order doesn't matter. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 7+ bugs across 5 challenges Understood WHY each bug causes incorrect behavior Could explain the difference between combinations and permutations Learned to check: array bounds, base cases, recurrence relations, loop order Common DP mistakes you discovered: Off-by-one errors: [Array size, loop bounds] Wrong base cases: [Initial dp values affect everything] Overflow issues: [Using Integer.MAX_VALUE in arithmetic] Wrong recurrence relation: [Not considering all choices correctly] Wrong iteration order: [Matters for combination vs permutation counting] Your key learnings: What was the most surprising bug? [Your answer] Which bug was hardest to spot? [Your answer] What will you always check now when writing DP code? [Your answer] Decision Framework \u00b6 Your task: Build decision trees for 1D DP problems. Question 1: What defines a state? \u00b6 Answer after solving problems: Single index? [1D DP array] Two indices? [2D DP - next topic] Additional state (holding/not)? [Multiple DP arrays or states] Your observation: [Fill in] Question 2: Top-down or bottom-up? \u00b6 Top-down (Memoization): Pros: [Natural recursion, only compute needed states] Cons: [Stack space, slightly slower] Use when: [Complex recurrence, not all states needed] Bottom-up (Tabulation): Pros: [No stack, often faster, space optimization] Cons: [Must compute all states] Use when: [Simple iteration order, need all states] Your Decision Tree \u00b6 flowchart LR Start[\"\"] Q1{\"Depends on previous 1-2 states?\"} Start --> Q1 Q2{\"Take/skip decision at each element?\"} Start --> Q2 Q3{\"String problem?\"} Start --> Q3 N4([\"String DP \u2713\"]) Q3 -->|\"Build character by character\"| N4 N5[\"2D DP<br/>(next topic)\"] Q3 -->|\"Subsequence\"| N5 N6[\"O(1) space\"] Q3 -->|\"Expand from center\"| N6 Q7{\"State machine<br/>(multiple states)?\"} Start --> Q7 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 3): 70. Climbing Stairs Pattern: [Fibonacci-style] Your solution time: ___ Key insight: [Fill in after solving] 746. Min Cost Climbing Stairs Pattern: [Fibonacci with cost] Your solution time: ___ Key insight: [Fill in] 121. Best Time to Buy and Sell Stock Pattern: [Single transaction] Your solution time: ___ Key insight: [Fill in] Medium (Complete 4-5): 198. House Robber Pattern: [Fibonacci-style decision] Difficulty: [Rate 1-10] Key insight: [Fill in] 322. Coin Change Pattern: [Decision DP] Difficulty: [Rate 1-10] Key insight: [Fill in] 139. Word Break Pattern: [String DP] Difficulty: [Rate 1-10] Key insight: [Fill in] 300. Longest Increasing Subsequence Pattern: [Sequence DP] Difficulty: [Rate 1-10] Key insight: [Fill in] 91. Decode Ways Pattern: [String DP] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 152. Maximum Product Subarray Pattern: [Track min and max] Key insight: [Fill in after solving] 132. Palindrome Partitioning II Pattern: [String DP with cut optimization] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Fibonacci-style: stairs, robber, min cost all work Decision: coin change, partition all work String: decode ways, word break, LIS all work Stock: single, unlimited, cooldown, fee all work All client code runs successfully Pattern Recognition Can identify overlapping subproblems Understand recurrence relations Know when to use top-down vs bottom-up Can optimize space complexity Problem Solving Solved 3 easy problems Solved 4-5 medium problems Analyzed time/space complexity Understood state transitions Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use DP Can explain memoization vs tabulation Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand how to derive recurrence Mastery Certification \u00b6 I certify that I can: Identify when a problem needs DP (optimal substructure + overlapping subproblems) Write correct recurrence relations for all 4 pattern types Implement both top-down (memoization) and bottom-up (tabulation) approaches Optimize space complexity when only previous states are needed Debug common DP bugs (base cases, array bounds, recurrence errors, loop order) Analyze time and space complexity accurately Choose between DP approaches based on problem constraints Explain DP concepts clearly to others","title":"14. Dynamic Programming 1D"},{"location":"dsa/14-dynamic-programming-1d/#dynamic-programming-1d","text":"Turn exponential recursion into polynomial iteration by caching subproblem results","title":"Dynamic Programming (1D)"},{"location":"dsa/14-dynamic-programming-1d/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is dynamic programming in one sentence? Your answer: [Fill in after implementation] How is DP different from regular recursion? Your answer: [Fill in after implementation] Real-world analogy: Example: \"DP is like saving your homework answers so you don't have to recalculate the same problems...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What's the difference between top-down and bottom-up? Your answer: [Fill in after learning both approaches]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/14-dynamic-programming-1d/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/14-dynamic-programming-1d/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand exponential to polynomial transformation.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/14-dynamic-programming-1d/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/14-dynamic-programming-1d/#debugging-challenges","text":"Your task: Find and fix bugs in broken DP implementations. This tests your understanding of recurrence relations, base cases, and iteration order.","title":"Debugging Challenges"},{"location":"dsa/14-dynamic-programming-1d/#decision-framework","text":"Your task: Build decision trees for 1D DP problems.","title":"Decision Framework"},{"location":"dsa/14-dynamic-programming-1d/#practice","text":"","title":"Practice"},{"location":"dsa/14-dynamic-programming-1d/#review-checklist","text":"Before moving to the next topic: Implementation Fibonacci-style: stairs, robber, min cost all work Decision: coin change, partition all work String: decode ways, word break, LIS all work Stock: single, unlimited, cooldown, fee all work All client code runs successfully Pattern Recognition Can identify overlapping subproblems Understand recurrence relations Know when to use top-down vs bottom-up Can optimize space complexity Problem Solving Solved 3 easy problems Solved 4-5 medium problems Analyzed time/space complexity Understood state transitions Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use DP Can explain memoization vs tabulation Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand how to derive recurrence","title":"Review Checklist"},{"location":"dsa/15-dynamic-programming-2d/","text":"Dynamic Programming (2D) \u00b6 Solve problems with two-dimensional state space using tabulation or memoization ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is 2D DP in one sentence? Your answer: [Fill in after implementation] How is 2D DP different from 1D DP? Your answer: [Fill in after implementation] Real-world analogy: Example: \"2D DP is like filling out a grid where each cell depends on cells above and to the left...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] How do you know when you need 2D instead of 1D? Your answer: [Fill in after learning the pattern] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Recursive solution for LCS (no memoization): Time complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] 2D DP table for LCS: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Speedup calculation: If m = 100, n = 100, naive recursion \u2248 _____ operations 2D DP table = m \u00d7 n = _____ operations Speedup factor: _____ times faster Scenario Predictions \u00b6 Scenario 1: Longest Common Subsequence of \"abc\" and \"abc\" What's the answer? [Fill in] Size of DP table? [m+1 \u00d7 n+1 or m \u00d7 n?] Why do we need +1 for dimensions? [Fill in - think about base cases] Scenario 2: Edit distance from \"cat\" to \"dog\" Your guess for minimum edits: [Fill in] What operations are allowed? [Fill in - insert, delete, replace?] If characters match, what happens in DP? [Fill in] Scenario 3: Unique paths in 3\u00d73 grid (can only move right or down) Manual count: [Try to draw and count all paths] DP recurrence: dp[i][j] = [Fill in formula] Starting position value: dp[0][0] = [0 or 1?] Pattern Recognition Quiz \u00b6 Question 1: Which 2D DP pattern applies? Match each problem to its pattern: Longest Common Subsequence \u2192 [String/Grid/Knapsack/Interval?] Unique Paths \u2192 [String/Grid/Knapsack/Interval?] 0/1 Knapsack \u2192 [String/Grid/Knapsack/Interval?] Burst Balloons \u2192 [String/Grid/Knapsack/Interval?] Question 2: When do you need 2D instead of 1D DP? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] State Design Quiz \u00b6 For LCS of strings \"abcde\" and \"ace\": What does dp[3][2] represent? LCS length of \"abc\" and \"ac\" LCS length of first 3 chars of s1 and first 2 chars of s2 LCS length including index 3 and 2 Something else: [Fill in] Verify after implementation: [Which one is correct?] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example 1: Longest Common Subsequence \u00b6 Problem: Find the length of the longest common subsequence of two strings. Approach 1: Brute Force Recursion (No Memoization) \u00b6 // Naive recursive approach - Try all possibilities public static int lcs_Recursive(String s1, String s2, int i, int j) { // Base case: reached end of either string if (i == s1.length() || j == s2.length()) { return 0; } // If characters match, include and move both if (s1.charAt(i) == s2.charAt(j)) { return 1 + lcs_Recursive(s1, s2, i + 1, j + 1); } // Characters don't match - try both options int skipS1 = lcs_Recursive(s1, s2, i + 1, j); int skipS2 = lcs_Recursive(s1, s2, i, j + 1); return Math.max(skipS1, skipS2); } Analysis: Time: O(2^(m+n)) - Exponential! Each call branches into 2 recursive calls Space: O(m+n) - Recursion stack depth For m = n = 20: ~1,000,000,000 operations (over 1 billion!) Why so slow? Recalculates the same subproblems repeatedly. Approach 2: 2D DP Table (Bottom-Up) \u00b6 // Optimized 2D DP - Build table from base cases public static int lcs_DP(String s1, String s2) { int m = s1.length(); int n = s2.length(); int[][] dp = new int[m + 1][n + 1]; // Base case: dp[0][j] = 0, dp[i][0] = 0 (already initialized) for (int i = 1; i <= m; i++) { for (int j = 1; j <= n; j++) { if (s1.charAt(i - 1) == s2.charAt(j - 1)) { dp[i][j] = dp[i - 1][j - 1] + 1; // Match: take both } else { dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]); // No match: try both } } } return dp[m][n]; } Analysis: Time: O(m \u00d7 n) - Fill each cell once Space: O(m \u00d7 n) - Store entire table For m = n = 20: 400 operations (just 400!) Performance Comparison \u00b6 String Lengths Recursive (2^(m+n)) 2D DP (m\u00d7n) Speedup m=10, n=10 ~1,000,000 100 10,000x m=15, n=15 ~1,000,000,000 225 4.4M x m=20, n=20 ~1,000,000,000,000 400 2.5B x Your calculation: For m = 25, n = 25, the speedup is approximately _____ times faster. Why Does 2D DP Work? \u00b6 Key insight to understand: For strings \"ace\" and \"abcde\": \"\" a b c d e \"\" 0 0 0 0 0 0 a 0 1 1 1 1 1 \u2190 'a' matches: dp[1][1] = dp[0][0] + 1 c 0 1 1 2 2 2 \u2190 'c' matches: dp[2][3] = dp[1][2] + 1 e 0 1 1 2 2 3 \u2190 'e' matches: dp[3][5] = dp[2][4] + 1 Answer: dp[3][5] = 3 (LCS = \"ace\") Why do we need the extra row/column of zeros? Fill in after understanding - what do they represent? [Your answer] Why can we skip recomputation? Each cell depends only on: [which cells?] We compute in order: [top-to-bottom, left-to-right] So dependencies are always ready when needed! Example 2: 0/1 Knapsack \u00b6 Problem: Given items with weights and values, maximize value without exceeding capacity. Approach 1: Recursive Exploration \u00b6 // Try all combinations - exponential time public static int knapsack_Recursive(int[] weights, int[] values, int capacity, int index) { // Base case: no items left or no capacity if (index == weights.length || capacity == 0) { return 0; } // Can't take current item - too heavy if (weights[index] > capacity) { return knapsack_Recursive(weights, values, capacity, index + 1); } // Try both: take it or skip it int take = values[index] + knapsack_Recursive(weights, values, capacity - weights[index], index + 1); int skip = knapsack_Recursive(weights, values, capacity, index + 1); return Math.max(take, skip); } Analysis: Time: O(2^n) - For each item, branch into take/skip For n = 30 items: Over 1 billion recursive calls! Approach 2: 2D DP Table \u00b6 // Build table: dp[item][capacity] public static int knapsack_DP(int[] weights, int[] values, int capacity) { int n = weights.length; int[][] dp = new int[n + 1][capacity + 1]; for (int i = 1; i <= n; i++) { for (int w = 0; w <= capacity; w++) { // Skip current item dp[i][w] = dp[i - 1][w]; // Take current item (if it fits) if (weights[i - 1] <= w) { int takeValue = values[i - 1] + dp[i - 1][w - weights[i - 1]]; dp[i][w] = Math.max(dp[i][w], takeValue); } } } return dp[n][capacity]; } Analysis: Time: O(n \u00d7 capacity) For n = 30, capacity = 1000: 30,000 operations vs 1 billion! Visualization: Knapsack Table \u00b6 Items: weights=[1, 2, 3], values=[10, 5, 15], capacity=5 Capacity: 0 1 2 3 4 5 No items (i=0): 0 0 0 0 0 0 Item 1 (w=1,v=10): 0 10 10 10 10 10 \u2190 Take item 1 Item 2 (w=2,v=5): 0 10 10 15 15 15 \u2190 Take items 1+2 or just 1 Item 3 (w=3,v=15): 0 10 10 15 25 25 \u2190 Take items 1+3 Answer: dp[3][5] = 25 (take items 1 and 3) After implementing, explain in your own words: Why does each cell represent \"best value using first i items with capacity w\"? [Your answer] Why do we need to check both \"take\" and \"skip\" options? [Your answer] Core Implementation \u00b6 Pattern 1: Grid Path Problems \u00b6 Concept: Count paths or find optimal path in 2D grid. Use case: Unique paths, minimum path sum, maximal square. public class GridPathProblems { /** * Problem: Unique paths in m\u00d7n grid (can only move right or down) * Time: O(m*n), Space: O(n) optimized * * TODO: Implement using 2D DP */ public static int uniquePaths(int m, int n) { // TODO: dp[i][j] = number of ways to reach cell (i,j) // TODO: dp[i][j] = dp[i-1][j] + dp[i][j-1] // TODO: Base: dp[0][j] = 1, dp[i][0] = 1 // TODO: Optimize to 1D: only need previous row return 0; // Replace with implementation } /** * Problem: Unique paths with obstacles * Time: O(m*n), Space: O(n) * * TODO: Implement with obstacles */ public static int uniquePathsWithObstacles(int[][] obstacleGrid) { // TODO: Similar to uniquePaths // TODO: Implement iteration/conditional logic // TODO: Handle obstacles in first row/column return 0; // Replace with implementation } /** * Problem: Minimum path sum (sum of cell values) * Time: O(m*n), Space: O(n) * * TODO: Implement minimum path sum */ public static int minPathSum(int[][] grid) { // TODO: dp[i][j] = minimum sum to reach (i,j) // TODO: dp[i][j] = grid[i][j] + min(dp[i-1][j], dp[i][j-1]) // TODO: Can modify grid in-place to save space return 0; // Replace with implementation } /** * Problem: Maximum sum path (can move in all 4 directions) * Time: O(m*n), Space: O(m*n) * * TODO: Implement maximum path sum */ public static int maxPathSum(int[][] grid) { // TODO: Use DFS with memoization // TODO: Or: DP with careful ordering return 0; // Replace with implementation } } Runnable Client Code: public class GridPathProblemsClient { public static void main(String[] args) { System.out.println(\"=== Grid Path Problems ===\\n\"); // Test 1: Unique paths System.out.println(\"--- Test 1: Unique Paths ---\"); int[][] grids = {{3, 2}, {3, 7}, {7, 3}}; for (int[] grid : grids) { int paths = GridPathProblems.uniquePaths(grid[0], grid[1]); System.out.printf(\"Grid %d\u00d7%d: %d paths%n\", grid[0], grid[1], paths); } // Test 2: Unique paths with obstacles System.out.println(\"\\n--- Test 2: Unique Paths with Obstacles ---\"); int[][] obstacleGrid = { {0, 0, 0}, {0, 1, 0}, {0, 0, 0} }; System.out.println(\"Grid (0=path, 1=obstacle):\"); printGrid(obstacleGrid); int pathsWithObstacles = GridPathProblems.uniquePathsWithObstacles(obstacleGrid); System.out.println(\"Unique paths: \" + pathsWithObstacles); // Test 3: Minimum path sum System.out.println(\"\\n--- Test 3: Minimum Path Sum ---\"); int[][] grid = { {1, 3, 1}, {1, 5, 1}, {4, 2, 1} }; System.out.println(\"Grid:\"); printGrid(grid); int minSum = GridPathProblems.minPathSum(grid); System.out.println(\"Minimum path sum: \" + minSum); // Test 4: Maximum path sum System.out.println(\"\\n--- Test 4: Maximum Path Sum ---\"); int[][] grid2 = { {1, 2, 3}, {4, 5, 6}, {7, 8, 9} }; System.out.println(\"Grid:\"); printGrid(grid2); int maxSum = GridPathProblems.maxPathSum(grid2); System.out.println(\"Maximum path sum: \" + maxSum); } private static void printGrid(int[][] grid) { for (int[] row : grid) { for (int cell : row) { System.out.print(cell + \" \"); } System.out.println(); } } } Pattern 2: String Matching (LCS, Edit Distance) \u00b6 Concept: Compare two strings character by character. Use case: Longest common subsequence, edit distance, wildcard matching. public class StringMatching { /** * Problem: Longest common subsequence * Time: O(m*n), Space: O(m*n) * * TODO: Implement LCS using 2D DP */ public static int longestCommonSubsequence(String text1, String text2) { // TODO: dp[i][j] = LCS of text1[0..i] and text2[0..j] // TODO: Implement iteration/conditional logic // TODO: Else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) return 0; // Replace with implementation } /** * Problem: Edit distance (insert, delete, replace) * Time: O(m*n), Space: O(m*n) * * TODO: Implement edit distance */ public static int minDistance(String word1, String word2) { // TODO: Implement logic // TODO: Implement iteration/conditional logic // TODO: Else: dp[i][j] = 1 + min(insert, delete, replace) return 0; // Replace with implementation } /** * Problem: Longest palindromic subsequence * Time: O(n^2), Space: O(n^2) * * TODO: Implement LPS using 2D DP */ public static int longestPalindromeSubseq(String s) { // TODO: dp[i][j] = LPS length in s[i..j] // TODO: Implement iteration/conditional logic // TODO: Else: dp[i][j] = max(dp[i+1][j], dp[i][j-1]) // TODO: Fill diagonal first, then expand return 0; // Replace with implementation } /** * Problem: Wildcard matching (* and ?) * Time: O(m*n), Space: O(m*n) * * TODO: Implement wildcard matching */ public static boolean isMatch(String s, String p) { // TODO: dp[i][j] = does s[0..i] match p[0..j]? // TODO: Handle * (matches any sequence) // TODO: Handle ? (matches single char) return false; // Replace with implementation } } Runnable Client Code: public class StringMatchingClient { public static void main(String[] args) { System.out.println(\"=== String Matching ===\\n\"); // Test 1: LCS System.out.println(\"--- Test 1: Longest Common Subsequence ---\"); String[][] lcsTests = { {\"abcde\", \"ace\"}, {\"abc\", \"abc\"}, {\"abc\", \"def\"} }; for (String[] test : lcsTests) { int lcs = StringMatching.longestCommonSubsequence(test[0], test[1]); System.out.printf(\"\\\"%s\\\" and \\\"%s\\\": LCS = %d%n\", test[0], test[1], lcs); } // Test 2: Edit distance System.out.println(\"\\n--- Test 2: Edit Distance ---\"); String[][] editTests = { {\"horse\", \"ros\"}, {\"intention\", \"execution\"}, {\"abc\", \"abc\"} }; for (String[] test : editTests) { int dist = StringMatching.minDistance(test[0], test[1]); System.out.printf(\"\\\"%s\\\" -> \\\"%s\\\": %d edits%n\", test[0], test[1], dist); } // Test 3: Longest palindromic subsequence System.out.println(\"\\n--- Test 3: Longest Palindromic Subsequence ---\"); String[] lpsTests = {\"bbbab\", \"cbbd\", \"racecar\"}; for (String s : lpsTests) { int lps = StringMatching.longestPalindromeSubseq(s); System.out.printf(\"\\\"%s\\\": LPS length = %d%n\", s, lps); } // Test 4: Wildcard matching System.out.println(\"\\n--- Test 4: Wildcard Matching ---\"); String[][] matchTests = { {\"aa\", \"a\"}, {\"aa\", \"*\"}, {\"cb\", \"?a\"}, {\"adceb\", \"*a*b\"} }; for (String[] test : matchTests) { boolean matches = StringMatching.isMatch(test[0], test[1]); System.out.printf(\"s=\\\"%s\\\", p=\\\"%s\\\": %s%n\", test[0], test[1], matches ? \"MATCH\" : \"NO MATCH\"); } } } Pattern 3: Knapsack Problems \u00b6 Concept: Select items with constraints to maximize/minimize value. Use case: 0/1 knapsack, unbounded knapsack, target sum. public class KnapsackProblems { /** * Problem: 0/1 Knapsack * Time: O(n * capacity), Space: O(n * capacity) * * TODO: Implement 0/1 knapsack */ public static int knapsack(int[] weights, int[] values, int capacity) { // TODO: dp[i][w] = max value using first i items with capacity w // TODO: dp[i][w] = max( // ) // TODO: Can optimize to 1D by iterating backwards return 0; // Replace with implementation } /** * Problem: Partition into K equal sum subsets * Time: O(k * n * sum), Space: O(n * sum) * * TODO: Implement partition check */ public static boolean canPartitionKSubsets(int[] nums, int k) { // TODO: Implement iteration/conditional logic // TODO: Target = sum / k // TODO: Use backtracking or DP to check if k subsets possible return false; // Replace with implementation } /** * Problem: Target sum (assign + or - to make target) * Time: O(n * sum), Space: O(sum) * * TODO: Implement target sum */ public static int findTargetSumWays(int[] nums, int target) { // TODO: Transform to subset sum problem // TODO: sum(P) - sum(N) = target where P=positive, N=negative // TODO: sum(P) + sum(N) = sum(all) // TODO: Therefore: sum(P) = (target + sum) / 2 // TODO: Count subsets that sum to (target + sum) / 2 return 0; // Replace with implementation } /** * Problem: Ones and Zeroes (2D knapsack) * Time: O(l * m * n), Space: O(m * n) * * TODO: Implement 2D knapsack */ public static int findMaxForm(String[] strs, int m, int n) { // TODO: dp[i][j] = max strings with i zeros and j ones // TODO: Implement iteration/conditional logic // TODO: Update DP backwards (0/1 knapsack style) return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class KnapsackProblemsClient { public static void main(String[] args) { System.out.println(\"=== Knapsack Problems ===\\n\"); // Test 1: 0/1 Knapsack System.out.println(\"--- Test 1: 0/1 Knapsack ---\"); int[] weights = {1, 2, 3, 5}; int[] values = {10, 5, 15, 7}; int capacity = 7; System.out.println(\"Weights: \" + Arrays.toString(weights)); System.out.println(\"Values: \" + Arrays.toString(values)); System.out.println(\"Capacity: \" + capacity); int maxValue = KnapsackProblems.knapsack(weights, values, capacity); System.out.println(\"Max value: \" + maxValue); // Test 2: Partition K subsets System.out.println(\"\\n--- Test 2: Partition K Subsets ---\"); int[] nums = {4, 3, 2, 3, 5, 2, 1}; int k = 4; System.out.println(\"Array: \" + Arrays.toString(nums)); System.out.println(\"k = \" + k); boolean canPartition = KnapsackProblems.canPartitionKSubsets(nums, k); System.out.println(\"Can partition: \" + (canPartition ? \"YES\" : \"NO\")); // Test 3: Target sum System.out.println(\"\\n--- Test 3: Target Sum ---\"); int[] nums2 = {1, 1, 1, 1, 1}; int target = 3; System.out.println(\"Array: \" + Arrays.toString(nums2)); System.out.println(\"Target: \" + target); int ways = KnapsackProblems.findTargetSumWays(nums2, target); System.out.println(\"Ways: \" + ways); // Test 4: Ones and Zeroes System.out.println(\"\\n--- Test 4: Ones and Zeroes ---\"); String[] strs = {\"10\", \"0001\", \"111001\", \"1\", \"0\"}; int m = 5; // max zeros int n = 3; // max ones System.out.println(\"Strings: \" + Arrays.toString(strs)); System.out.println(\"Max 0s: \" + m + \", Max 1s: \" + n); int maxStrings = KnapsackProblems.findMaxForm(strs, m, n); System.out.println(\"Max strings: \" + maxStrings); } } Pattern 4: Game Theory / Min-Max \u00b6 Concept: Two players making optimal moves. Use case: Stone game, predict winner, burst balloons. public class GameTheory { /** * Problem: Stone game (take from ends, maximize score) * Time: O(n^2), Space: O(n^2) * * TODO: Implement stone game */ public static boolean stoneGame(int[] piles) { // TODO: dp[i][j] = max stones first player gets from piles[i..j] // TODO: Player chooses max of: // TODO: First player wins if dp[0][n-1] > sum/2 return false; // Replace with implementation } /** * Problem: Predict the winner * Time: O(n^2), Space: O(n^2) * * TODO: Implement predict winner */ public static boolean predictWinner(int[] nums) { // TODO: dp[i][j] = max advantage first player has in nums[i..j] // TODO: Advantage = player1 score - player2 score // TODO: Return dp[0][n-1] >= 0 return false; // Replace with implementation } /** * Problem: Burst balloons (maximize coins) * Time: O(n^3), Space: O(n^2) * * TODO: Implement burst balloons */ public static int maxCoins(int[] nums) { // TODO: Add virtual balloons with value 1 at both ends // TODO: dp[i][j] = max coins from bursting balloons (i..j) // TODO: Try each balloon k as last to burst in range [i,j] // TODO: Implement logic return 0; // Replace with implementation } /** * Problem: Minimum score triangulation * Time: O(n^3), Space: O(n^2) * * TODO: Implement triangulation */ public static int minScoreTriangulation(int[] values) { // TODO: Similar to burst balloons // TODO: dp[i][j] = min score triangulating polygon from i to j return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class GameTheoryClient { public static void main(String[] args) { System.out.println(\"=== Game Theory ===\\n\"); // Test 1: Stone game System.out.println(\"--- Test 1: Stone Game ---\"); int[] piles = {5, 3, 4, 5}; System.out.println(\"Piles: \" + Arrays.toString(piles)); boolean firstWins = GameTheory.stoneGame(piles); System.out.println(\"First player wins: \" + (firstWins ? \"YES\" : \"NO\")); // Test 2: Predict winner System.out.println(\"\\n--- Test 2: Predict Winner ---\"); int[][] testArrays = { {1, 5, 2}, {1, 5, 233, 7} }; for (int[] arr : testArrays) { boolean player1Wins = GameTheory.predictWinner(arr); System.out.printf(\"Array: %s -> Player 1 wins: %s%n\", Arrays.toString(arr), player1Wins ? \"YES\" : \"NO\"); } // Test 3: Burst balloons System.out.println(\"\\n--- Test 3: Burst Balloons ---\"); int[] balloons = {3, 1, 5, 8}; System.out.println(\"Balloons: \" + Arrays.toString(balloons)); int maxCoins = GameTheory.maxCoins(balloons); System.out.println(\"Max coins: \" + maxCoins); // Test 4: Triangulation System.out.println(\"\\n--- Test 4: Minimum Score Triangulation ---\"); int[] polygon = {1, 2, 3}; System.out.println(\"Polygon: \" + Arrays.toString(polygon)); int minScore = GameTheory.minScoreTriangulation(polygon); System.out.println(\"Min score: \" + minScore); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken 2D DP implementations. This tests your understanding of state transitions and edge cases. Challenge 1: Broken LCS Implementation \u00b6 /** * Longest Common Subsequence - has 3 BUGS! * Find them all. */ public static int lcs_Buggy(String s1, String s2) { int m = s1.length(); int n = s2.length(); int[][] dp = new int[m][n]; for (int i = 1; i <= m; i++) { for (int j = 1; j <= n; j++) { if (s1.charAt(i) == s2.charAt(j)) { dp[i][j] = dp[i-1][j-1] + 1; } else { dp[i][j] = Math.max(dp[i-1][j], dp[i][j-1]); } } } return dp[m][n];} Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Bug 3: [What\\'s the bug?] Test case to expose bugs: Input: s1 = \"abc\", s2 = \"abc\" Expected: 3 Actual with buggy code: [What happens? Crash or wrong answer?] Click to verify your answers Bug 1 (Line 4): Should be int[][] dp = new int[m + 1][n + 1] . We need extra row/column for the empty string base case. Bug 2 (Line 8): Should be s1.charAt(i - 1) == s2.charAt(j - 1) . The DP indices are 1-based but string indices are 0-based. Bug 3 (Line 15): With Bug 1 unfixed, dp[m][n] is out of bounds. After fixing Bug 1, this is correct. All three bugs are interconnected! The root cause is not allocating space for the base case (empty string). Challenge 2: Broken Edit Distance \u00b6 /** * Edit Distance - has 2 SUBTLE BUGS * One is an off-by-one error, one is a logic error */ public static int editDistance_Buggy(String word1, String word2) { int m = word1.length(); int n = word2.length(); int[][] dp = new int[m + 1][n + 1]; // Initialize base cases for (int i = 0; i <= m; i++) dp[i][0] = i; for (int j = 0; j <= n; j++) dp[0][j] = j; for (int i = 1; i <= m; i++) { for (int j = 1; j <= n; j++) { if (word1.charAt(i) == word2.charAt(j)) { dp[i][j] = dp[i-1][j-1]; } else { int insert = dp[i][j-1]; int delete = dp[i-1][j]; int replace = dp[i-1][j-1]; dp[i][j] = Math.min(insert, Math.min(delete, replace)); } } } return dp[m][n]; } Your debugging: Bug 1: [What's wrong with charAt(i)?] Bug 1 fix: [Correct index?] Bug 2: [What's missing in the min calculation?] Bug 2 fix: [Fill in the correct formula] Test case: Input: \"horse\" \u2192 \"ros\" Expected: 3 (delete 'h', delete 'r', replace 'e' with 's') Actual: [Trace through manually - what do you get?] Click to verify your answers Bug 1 (Line 15): Should be word1.charAt(i - 1) == word2.charAt(j - 1) . DP uses 1-based indexing, strings use 0-based. Bug 2 (Line 21): Each operation (insert, delete, replace) costs 1, so should be: dp[i][j] = 1 + Math.min(insert, Math.min(delete, replace)); Without the + 1 , we're not counting the operation cost! Challenge 3: Broken Unique Paths \u00b6 /** * Unique Paths in m\u00d7n grid * Has 2 BUGS: one initialization, one recurrence */ public static int uniquePaths_Buggy(int m, int n) { int[][] dp = new int[m][n]; // Initialize first row and column for (int i = 0; i < m; i++) dp[i][0] = 0; for (int j = 0; j < n; j++) dp[0][j] = 1; for (int i = 1; i < m; i++) { for (int j = 1; j < n; j++) { dp[i][j] = dp[i-1][j] * dp[i][j-1]; } } return dp[m-1][n-1]; } Your debugging: Bug 1: [Why is dp[i][0] = 0 wrong?] Bug 1 explanation: [What should the first column represent?] Bug 1 fix: [Correct value?] Bug 2: [Why is multiplication wrong?] Bug 2 explanation: [What's the correct recurrence?] Bug 2 fix: [Should be...?] Test case: Input: m = 3, n = 3 Expected: 6 paths Actual with buggy code: [Calculate it] Click to verify your answers Bug 1 (Line 8): Should be dp[i][0] = 1 . There's exactly ONE way to reach any cell in the first column (move down only). Bug 2 (Line 13): Should be dp[i][j] = dp[i-1][j] + dp[i][j-1] (addition, not multiplication). We're COUNTING paths, not multiplying them. Why addition? The number of ways to reach cell (i,j) is the sum of: Ways to reach (i-1,j) [coming from above] Ways to reach (i,j-1) [coming from left] Challenge 4: Broken Knapsack \u00b6 /** * 0/1 Knapsack - has 3 BUGS * Focus on state transition and boundary conditions */ public static int knapsack_Buggy(int[] weights, int[] values, int capacity) { int n = weights.length; int[][] dp = new int[n][capacity + 1]; for (int i = 1; i < n; i++) { for (int w = 1; w <= capacity; w++) { // Don't take item i dp[i][w] = dp[i-1][w]; // Take item i (if it fits) if (weights[i] <= w) { int takeValue = values[i] + dp[i-1][w - weights[i]]; dp[i][w] = Math.max(dp[i][w], takeValue); } } } return dp[n-1][capacity]; } Your debugging: Bug 1: [Why might this cause issues?] Bug 1 fix: [Correct dimensions?] Bug 2: [What gets skipped?] Bug 2 fix: [Should loop start at...?] Bug 3: [Index mismatch between DP and arrays?] Bug 3 explanation: [Fill in] Bug 3 fix: [Correct index?] Test case: weights = [1, 2, 3], values = [10, 5, 15], capacity = 5 Expected: 25 (items 1 and 3) Actual: [Trace through - what happens?] Click to verify your answers Bug 1 (Line 3): Should be int[][] dp = new int[n + 1][capacity + 1] . Need extra row for \"no items\" base case. Bug 2 (Line 5): Should start at i = 1 , which is actually correct! But with Bug 1 fixed, we need the +1 dimension. Bug 3 (Line 11): Should be weights[i - 1] and values[i - 1] . The DP table has n+1 rows but arrays have n elements, so there's an offset. The complete fix: for (int i = 1; i <= n; i++) { // Note: <= n for (int w = 1; w <= capacity; w++) { dp[i][w] = dp[i-1][w]; if (weights[i - 1] <= w) { // i-1 for array index int takeValue = values[i - 1] + dp[i-1][w - weights[i - 1]]; dp[i][w] = Math.max(dp[i][w], takeValue); } } } return dp[n][capacity]; // Not n-1 Challenge 5: Missing Edge Case Initialization \u00b6 /** * Unique Paths with Obstacles * Has 1 CRITICAL EDGE CASE BUG */ public static int uniquePathsWithObstacles_Buggy(int[][] obstacleGrid) { int m = obstacleGrid.length; int n = obstacleGrid[0].length; int[][] dp = new int[m][n]; // Initialize first row for (int j = 0; j < n; j++) { dp[0][j] = 1; } // Initialize first column for (int i = 0; i < m; i++) { dp[i][0] = 1; } for (int i = 1; i < m; i++) { for (int j = 1; j < n; j++) { if (obstacleGrid[i][j] == 1) { dp[i][j] = 0; } else { dp[i][j] = dp[i-1][j] + dp[i][j-1]; } } } return dp[m-1][n-1]; } Your debugging: Bug location: [Lines 10 and 15] Bug explanation: [What happens if first row/column has obstacle?] Why is this critical? [Once blocked, all cells after it are unreachable] Fix: [How to handle obstacles in initialization?] Test case to expose: Grid: 0 0 0 0 1 0 \u2190 Obstacle in first column 0 0 0 Expected: 2 paths Buggy result: <span class=\"fill-in\">[Fill in]</span> Click to verify your answer Bug: Must check for obstacles during initialization. If there's an obstacle in the first row/column, all cells AFTER it are unreachable (can't be reached). Fix: // Initialize first row - stop at first obstacle for (int j = 0; j < n; j++) { if (obstacleGrid[0][j] == 1) { break; // All cells after obstacle are unreachable } dp[0][j] = 1; } // Initialize first column - stop at first obstacle for (int i = 0; i < m; i++) { if (obstacleGrid[i][0] == 1) { break; // All cells after obstacle are unreachable } dp[i][0] = 1; } // Also need to check starting cell! if (obstacleGrid[0][0] == 1) return 0; Key insight: In first row/column, once blocked, everything after is blocked (only one direction to reach them). Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 11+ bugs across 5 challenges Understood WHY each bug causes incorrect behavior Could explain the difference between DP indices and array indices Learned importance of base case initialization Common 2D DP mistakes you discovered: Off-by-one errors: [DP table size vs array size] Index mismatches: [1-based DP vs 0-based strings/arrays] Missing base cases: [Edge initialization forgotten] Wrong recurrence: [Addition vs multiplication, missing +1 cost] Edge case bugs: [Obstacles, empty strings, zero capacity] Your reflection: Which bug was hardest to find? [Fill in] Decision Framework \u00b6 Your task: Build decision trees for 2D DP problems. Question 1: What are your two dimensions? \u00b6 Answer after solving problems: Two strings? [String matching DP] Grid coordinates? [Path DP] Range [i,j]? [Interval DP] Items and capacity? [Knapsack DP] Question 2: What's the recurrence? \u00b6 String matching: Match: [Use both characters] Mismatch: [Try alternatives] Grid paths: Current cell: [Function of neighbors] Direction: [Usually top/left] Interval DP: Try each split point: [Combine subproblems] Knapsack: Take or skip: [Compare options] Your Decision Tree \u00b6 flowchart LR Start[\"2D DP Pattern Selection\"] Q1{\"Two strings?\"} Start --> Q1 N2([\"LCS \u2713\"]) Q1 -->|\"Common subsequence\"| N2 N3([\"Edit distance \u2713\"]) Q1 -->|\"Transform one to other\"| N3 N4([\"Wildcard/regex DP \u2713\"]) Q1 -->|\"Pattern matching\"| N4 Q5{\"Grid problem?\"} Start --> Q5 N6([\"Path counting DP \u2713\"]) Q5 -->|\"Count paths\"| N6 N7([\"Min path sum \u2713\"]) Q5 -->|\"Minimize cost\"| N7 N8([\"Max path DP \u2713\"]) Q5 -->|\"Maximize value\"| N8 Q9{\"Interval/range [i,j]?\"} Start --> Q9 N10([\"Palindrome DP \u2713\"]) Q9 -->|\"Palindrome\"| N10 N11([\"Interval DP \u2713\"]) Q9 -->|\"Burst/merge\"| N11 N12([\"Min-max DP \u2713\"]) Q9 -->|\"Game theory\"| N12 Q13{\"Items with capacity?\"} Start --> Q13 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 2): 62. Unique Paths Pattern: [Grid path counting] Your solution time: ___ Key insight: [Fill in after solving] 64. Minimum Path Sum Pattern: [Grid min path] Your solution time: ___ Key insight: [Fill in] Medium (Complete 4-5): 1143. Longest Common Subsequence Pattern: [String matching] Difficulty: [Rate 1-10] Key insight: [Fill in] 72. Edit Distance Pattern: [String transformation] Difficulty: [Rate 1-10] Key insight: [Fill in] 63. Unique Paths II Pattern: [Grid with obstacles] Difficulty: [Rate 1-10] Key insight: [Fill in] 516. Longest Palindromic Subsequence Pattern: [Interval DP] Difficulty: [Rate 1-10] Key insight: [Fill in] 494. Target Sum Pattern: [Knapsack variant] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 312. Burst Balloons Pattern: [Interval DP] Key insight: [Fill in after solving] 44. Wildcard Matching Pattern: [String matching] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Grid paths: unique, with obstacles, min sum all work String matching: LCS, edit distance, palindrome all work Knapsack: 0/1, target sum, ones-zeroes all work Game theory: stone game, burst balloons work All client code runs successfully Pattern Recognition Can identify when 2D state is needed Understand recurrence in each pattern Know when to optimize space to 1D Recognize interval DP problems Problem Solving Solved 2 easy problems Solved 4-5 medium problems Analyzed time/space complexity Understood state transitions Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use 2D DP Can explain how to derive 2D recurrence Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand space optimization techniques Mastery Certification \u00b6 I certify that I can: Design 2D DP state from problem description Write correct recurrence relations Handle base cases and edge initialization properly Identify and fix off-by-one errors Optimize 2D \u2192 1D space when possible Explain why a problem needs 2D vs 1D Trace through a DP table by hand Debug incorrect implementations quickly Teach 2D DP concepts to someone else","title":"15. Dynamic Programming 2D"},{"location":"dsa/15-dynamic-programming-2d/#dynamic-programming-2d","text":"Solve problems with two-dimensional state space using tabulation or memoization","title":"Dynamic Programming (2D)"},{"location":"dsa/15-dynamic-programming-2d/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is 2D DP in one sentence? Your answer: [Fill in after implementation] How is 2D DP different from 1D DP? Your answer: [Fill in after implementation] Real-world analogy: Example: \"2D DP is like filling out a grid where each cell depends on cells above and to the left...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] How do you know when you need 2D instead of 1D? Your answer: [Fill in after learning the pattern]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/15-dynamic-programming-2d/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/15-dynamic-programming-2d/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/15-dynamic-programming-2d/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/15-dynamic-programming-2d/#debugging-challenges","text":"Your task: Find and fix bugs in broken 2D DP implementations. This tests your understanding of state transitions and edge cases.","title":"Debugging Challenges"},{"location":"dsa/15-dynamic-programming-2d/#decision-framework","text":"Your task: Build decision trees for 2D DP problems.","title":"Decision Framework"},{"location":"dsa/15-dynamic-programming-2d/#practice","text":"","title":"Practice"},{"location":"dsa/15-dynamic-programming-2d/#review-checklist","text":"Before moving to the next topic: Implementation Grid paths: unique, with obstacles, min sum all work String matching: LCS, edit distance, palindrome all work Knapsack: 0/1, target sum, ones-zeroes all work Game theory: stone game, burst balloons work All client code runs successfully Pattern Recognition Can identify when 2D state is needed Understand recurrence in each pattern Know when to optimize space to 1D Recognize interval DP problems Problem Solving Solved 2 easy problems Solved 4-5 medium problems Analyzed time/space complexity Understood state transitions Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use 2D DP Can explain how to derive 2D recurrence Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand space optimization techniques","title":"Review Checklist"},{"location":"dsa/16-tries/","text":"Tries (Prefix Trees) \u00b6 Efficient storage and retrieval of strings with common prefixes using tree structure ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is a trie in one sentence? Your answer: [Fill in after implementation] How is a trie different from a hash table for strings? Your answer: [Fill in after implementation] Real-world analogy: Example: \"A trie is like how you organize words in a dictionary by first letter, then second letter...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What's the space-time tradeoff with tries? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 HashMap search for word in dictionary of n words: Time complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Trie search for word of length m in dictionary of n words: Time complexity: [Your guess: O(?)] Space complexity for entire trie: [Your guess: O(?)] Verified: [Actual] Prefix search comparison: Find all words starting with \"app\" using HashMap: [Time: O(?)] Find all words starting with \"app\" using Trie: [Time: O(?)] Which is faster: [HashMap/Trie - Why?] Scenario Predictions \u00b6 Scenario 1: Dictionary has [\"apple\", \"app\", \"application\", \"apply\"] How many nodes in the trie? [Count them] Which nodes have isEndOfWord = true? [List them] How many nodes share the prefix \"app\"? [Count] Memory saved compared to storing each word separately? [Estimate] Scenario 2: Search for \"appl\" in above dictionary Using search(): returns [true/false - Why?] Using startsWith(): returns [true/false - Why?] What's the difference? [Fill in your reasoning] Scenario 3: Autocomplete with prefix \"ap\" Which words match? [List them] Path through trie: [Describe the traversal] Why is trie better than checking each word? [Fill in] Trade-off Quiz \u00b6 Question: When would HashMap be BETTER than Trie for dictionary? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN advantage of trie over hash table? Faster single word lookup Less space usage Efficient prefix queries Simpler to implement Verify after implementation: [Which one(s)?] Question: What's the space complexity of trie with n words of average length m? O(n) O(m) O(n * m) worst case O(1) Verify: [Which one and why?] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example: Autocomplete System \u00b6 Problem: Find all words in dictionary that start with given prefix. Approach 1: HashMap with Linear Scan \u00b6 // Naive approach - Check every word public static List<String> autocomplete_Naive(Set<String> dictionary, String prefix) { List<String> results = new ArrayList<>(); // Check every word in dictionary for (String word : dictionary) { if (word.startsWith(prefix)) { results.add(word); } } return results; } Analysis: Time: O(n * m) - Check n words, each comparison takes m characters Space: O(1) additional space (not counting results) For dictionary with 100,000 words, prefix length 3: ~300,000 character comparisons Approach 2: Trie (Optimized) \u00b6 // Optimized approach - Navigate to prefix then collect public List<String> autocomplete_Trie(String prefix) { List<String> results = new ArrayList<>(); // Navigate to prefix node: O(p) where p = prefix length TrieNode node = root; for (char c : prefix.toCharArray()) { if (!node.children.containsKey(c)) { return results; // Prefix not found } node = node.children.get(c); } // Collect all words under this node: O(k) where k = results collectWords(node, prefix, results); return results; } Analysis: Time: O(p + k) - p = prefix length, k = number of results Space: O(n * m) for trie structure For same dictionary, prefix length 3: Only 3 comparisons + collecting results Performance Comparison \u00b6 Dictionary Size Prefix Length HashMap (O(n*m)) Trie (O(p+k)) Speedup n = 1,000 m = 5 5,000 ops 5 + k ops 1000x n = 10,000 m = 5 50,000 ops 5 + k ops 10000x n = 100,000 m = 5 500,000 ops 5 + k ops 100000x Your calculation: For n = 50,000 and prefix length 4, the speedup is approximately _____ times faster. Why Does Trie Work? \u00b6 Key insight to understand: Building dictionary: [\"apple\", \"app\", \"application\", \"apply\", \"banana\"] Trie structure: root / \\ a b | | p a | | p* n / \\ | l l a | | | e* y* n | | a a* | t | i | o | n* (* = isEndOfWord) Autocomplete for \"app\": Step 1: Navigate to 'a' \u2192 'p' \u2192 'p' (3 steps only!) Step 2: Collect all words from this subtree Found: \"app\", \"apple\", \"application\", \"apply\" Why can we skip other words? Words not starting with 'a' are in different branch (never visited) Words starting with 'a' but not 'ap' are in different branch (never visited) Only visit nodes relevant to prefix - massive pruning! After implementing, explain in your own words: Why is trie better for prefix queries than hash table? [Your answer] What's the space-time tradeoff? [Your answer] When would hash table still be better? [Your answer] Example: Word Search in Grid \u00b6 Problem: Find all words from dictionary in 2D character grid. Approach 1: Brute Force with Hash Set \u00b6 // Naive approach - DFS from each cell for each word public static List<String> findWords_BruteForce(char[][] board, String[] words) { Set<String> wordSet = new HashSet<>(Arrays.asList(words)); Set<String> found = new HashSet<>(); // Try to find each word starting from each cell for (int i = 0; i < board.length; i++) { for (int j = 0; j < board[0].length; j++) { for (String word : words) { if (dfsSearch(board, i, j, word, 0, new boolean[board.length][board[0].length])) { found.add(word); } } } } return new ArrayList<>(found); } Analysis: Time: O(w * m * n * 4^L) - w words, m*n cells, 4^L DFS For 1000 words, 4x4 grid, word length 10: Extremely slow No early pruning - explores impossible paths Approach 2: Trie with DFS (Optimized) \u00b6 // Optimized approach - Single DFS using trie for pruning public List<String> findWords_Trie(char[][] board, String[] words) { // Build trie once: O(w * L) TrieNode root = buildTrie(words); Set<String> found = new HashSet<>(); // DFS from each cell, trie prunes invalid paths for (int i = 0; i < board.length; i++) { for (int j = 0; j < board[0].length; j++) { dfs(board, i, j, root, found, new boolean[board.length][board[0].length]); } } return new ArrayList<>(found); } // Trie prunes branches early - if prefix doesn't exist in trie, stop! Analysis: Time: O(m * n * 4^L) - Only one DFS per cell Trie pruning: If \"xy\" not in dictionary, never explore \"xyz\", \"xya\", etc. For same problem: 1000x faster with early pruning Performance Comparison \u00b6 Dictionary Size Grid Size Brute Force Trie with Pruning Speedup 100 words 3x3 ~100,000 ops ~1,000 ops 100x 1,000 words 4x4 ~10,000,000 ~10,000 ops 1000x 10,000 words 5x5 Timeout ~100,000 ops 100x+ Key insight: Trie eliminates redundant work by sharing prefixes and enabling early stopping. After implementing, explain in your own words: How does trie enable early pruning in DFS? [Your answer] Why is single DFS with trie better than multiple DFS? [Your answer] Core Implementation \u00b6 Pattern 1: Basic Trie Operations \u00b6 Concept: Build tree where each node represents a character. Use case: Insert, search, prefix search, delete words. public class BasicTrie { /** * TrieNode: Basic node structure */ static class TrieNode { // TODO: Create children array/map for next characters // TODO: Boolean flag for end of word TrieNode[] children; boolean isEndOfWord; TrieNode() { // TODO: Initialize children array (size 26 for a-z) // TODO: Set isEndOfWord = false } } static class Trie { private TrieNode root; public Trie() { // TODO: Initialize root node } /** * Insert word into trie * Time: O(m) where m = word length, Space: O(m) * * TODO: Implement insert */ public void insert(String word) { // TODO: Start from root // TODO: Implement iteration/conditional logic // TODO: Mark last node as end of word } /** * Search if word exists in trie * Time: O(m), Space: O(1) * * TODO: Implement search */ public boolean search(String word) { // TODO: Traverse trie following characters // TODO: Return true only if: return false; // Replace with implementation } /** * Check if any word starts with prefix * Time: O(m), Space: O(1) * * TODO: Implement startsWith */ public boolean startsWith(String prefix) { // TODO: Similar to search // TODO: Return true if all characters found return false; // Replace with implementation } /** * Delete word from trie * Time: O(m), Space: O(m) for recursion * * TODO: Implement delete */ public boolean delete(String word) { // TODO: Use recursive helper // TODO: Only delete nodes that aren't part of other words return false; // Replace with implementation } private boolean deleteHelper(TrieNode node, String word, int index) { // TODO: Base case: reached end of word // TODO: Recursive case: return false; // Replace with implementation } } } Runnable Client Code: public class BasicTrieClient { public static void main(String[] args) { System.out.println(\"=== Basic Trie Operations ===\\n\"); BasicTrie.Trie trie = new BasicTrie.Trie(); // Test 1: Insert and search System.out.println(\"--- Test 1: Insert and Search ---\"); String[] words = {\"apple\", \"app\", \"application\", \"apply\", \"banana\"}; System.out.println(\"Inserting words:\"); for (String word : words) { trie.insert(word); System.out.println(\" Inserted: \" + word); } System.out.println(\"\\nSearching:\"); String[] searchWords = {\"apple\", \"app\", \"appl\", \"ban\", \"banana\"}; for (String word : searchWords) { boolean found = trie.search(word); System.out.printf(\" search(\\\"%s\\\"): %s%n\", word, found ? \"FOUND\" : \"NOT FOUND\"); } // Test 2: Prefix search System.out.println(\"\\n--- Test 2: Prefix Search ---\"); String[] prefixes = {\"app\", \"ban\", \"cat\"}; for (String prefix : prefixes) { boolean hasPrefix = trie.startsWith(prefix); System.out.printf(\" startsWith(\\\"%s\\\"): %s%n\", prefix, hasPrefix ? \"YES\" : \"NO\"); } // Test 3: Delete System.out.println(\"\\n--- Test 3: Delete ---\"); String deleteWord = \"app\"; System.out.println(\"Deleting: \" + deleteWord); trie.delete(deleteWord); System.out.println(\"After deletion:\"); for (String word : new String[]{\"app\", \"apple\", \"application\"}) { boolean found = trie.search(word); System.out.printf(\" search(\\\"%s\\\"): %s%n\", word, found ? \"FOUND\" : \"NOT FOUND\"); } } } Pattern 2: Word Search in Grid with Trie \u00b6 Concept: Use trie to efficiently search multiple words in 2D grid. Use case: Word Search II, Boggle game solver. import java.util.*; public class WordSearchTrie { static class TrieNode { Map<Character, TrieNode> children = new HashMap<>(); String word; // Store word at end node for easy retrieval } /** * Problem: Find all words from dictionary in 2D grid * Time: O(m*n*4^L) where L = max word length, Space: O(k*L) where k = words * * TODO: Implement word search II */ public static List<String> findWords(char[][] board, String[] words) { // TODO: Build trie from words array // TODO: DFS from each cell using trie for pruning // TODO: Mark visited cells during DFS // TODO: Add found words to result (use Set to avoid duplicates) return new ArrayList<>(); // Replace with implementation } private static TrieNode buildTrie(String[] words) { // TODO: Create root node // TODO: Insert each word into trie // TODO: Store complete word at end node return new TrieNode(); // Replace with implementation } private static void dfs(char[][] board, int i, int j, TrieNode node, Set<String> result, boolean[][] visited) { // TODO: Check bounds and visited // TODO: Get character at current position // TODO: Check if character exists in trie children // TODO: Implement iteration/conditional logic // TODO: Mark visited // TODO: DFS in 4 directions // TODO: Unmark visited (backtrack) } /** * Problem: Longest word in dictionary built one char at a time * Time: O(n * L), Space: O(n * L) * * TODO: Implement longest word */ public static String longestWord(String[] words) { // TODO: Build trie // TODO: DFS/BFS to find longest word where all prefixes exist // TODO: Each character must form a valid word return \"\"; // Replace with implementation } } Runnable Client Code: import java.util.*; public class WordSearchTrieClient { public static void main(String[] args) { System.out.println(\"=== Word Search with Trie ===\\n\"); // Test 1: Word Search II System.out.println(\"--- Test 1: Word Search II ---\"); char[][] board = { {'o','a','a','n'}, {'e','t','a','e'}, {'i','h','k','r'}, {'i','f','l','v'} }; String[] words = {\"oath\", \"pea\", \"eat\", \"rain\", \"oat\"}; System.out.println(\"Board:\"); for (char[] row : board) { for (char c : row) { System.out.print(c + \" \"); } System.out.println(); } System.out.println(\"\\nDictionary: \" + Arrays.toString(words)); List<String> found = WordSearchTrie.findWords(board, words); System.out.println(\"Found words: \" + found); // Test 2: Longest word System.out.println(\"\\n--- Test 2: Longest Word ---\"); String[][] wordSets = { {\"w\", \"wo\", \"wor\", \"worl\", \"world\"}, {\"a\", \"banana\", \"app\", \"appl\", \"ap\", \"apply\", \"apple\"} }; for (String[] wordSet : wordSets) { String longest = WordSearchTrie.longestWord(wordSet); System.out.printf(\"Words: %s%n\", Arrays.toString(wordSet)); System.out.printf(\"Longest: \\\"%s\\\"%n%n\", longest); } } } Pattern 3: Autocomplete and Prefix Matching \u00b6 Concept: Find all words with given prefix. Use case: Autocomplete, suggestions, prefix search. import java.util.*; public class Autocomplete { static class TrieNode { Map<Character, TrieNode> children = new HashMap<>(); boolean isEndOfWord; String word; // Store full word for easy retrieval } static class AutocompleteTrie { private TrieNode root; public AutocompleteTrie() { root = new TrieNode(); } /** * Insert word * Time: O(m), Space: O(m) * * TODO: Implement insert */ public void insert(String word) { // TODO: Standard trie insert // TODO: Store word at end node } /** * Find all words with given prefix * Time: O(p + n) where p=prefix length, n=results, Space: O(n) * * TODO: Implement autocomplete */ public List<String> autocomplete(String prefix) { List<String> results = new ArrayList<>(); // TODO: Navigate to end of prefix // TODO: Implement iteration/conditional logic // TODO: DFS from prefix node to collect all words // TODO: Return results return results; // Replace with implementation } private void collectWords(TrieNode node, List<String> results) { // TODO: Implement iteration/conditional logic // TODO: DFS all children } /** * Find top K most frequent words with prefix * Time: O(p + n log k), Space: O(n) * * TODO: Implement top K suggestions */ public List<String> topKSuggestions(String prefix, int k) { // TODO: Get all words with prefix // TODO: Use min-heap to track top K by frequency // TODO: Return top K return new ArrayList<>(); // Replace with implementation } } /** * Problem: Search suggestions system * Time: O(m*n) where m=products, n=searchWord length, Space: O(m*n) * * TODO: Implement search suggestions */ public static List<List<String>> suggestedProducts(String[] products, String searchWord) { // TODO: Build trie from products // TODO: Implement iteration/conditional logic // TODO: Return list of suggestions for each prefix return new ArrayList<>(); // Replace with implementation } } Runnable Client Code: import java.util.*; public class AutocompleteClient { public static void main(String[] args) { System.out.println(\"=== Autocomplete ===\\n\"); // Test 1: Basic autocomplete System.out.println(\"--- Test 1: Basic Autocomplete ---\"); Autocomplete.AutocompleteTrie trie = new Autocomplete.AutocompleteTrie(); String[] dictionary = { \"apple\", \"application\", \"apply\", \"app\", \"banana\", \"band\", \"bandana\", \"ban\" }; System.out.println(\"Dictionary:\"); for (String word : dictionary) { trie.insert(word); System.out.println(\" \" + word); } String[] prefixes = {\"app\", \"ban\", \"cat\"}; System.out.println(\"\\nAutocomplete:\"); for (String prefix : prefixes) { List<String> suggestions = trie.autocomplete(prefix); System.out.printf(\" \\\"%s\\\" -> %s%n\", prefix, suggestions); } // Test 2: Search suggestions system System.out.println(\"\\n--- Test 2: Search Suggestions System ---\"); String[] products = {\"mobile\", \"mouse\", \"moneypot\", \"monitor\", \"mousepad\"}; String searchWord = \"mouse\"; System.out.println(\"Products: \" + Arrays.toString(products)); System.out.println(\"Search word: \" + searchWord); List<List<String>> suggestions = Autocomplete.suggestedProducts(products, searchWord); System.out.println(\"Suggestions for each prefix:\"); for (int i = 0; i < suggestions.size(); i++) { System.out.printf(\" \\\"%s\\\" -> %s%n\", searchWord.substring(0, i + 1), suggestions.get(i)); } } } Pattern 4: Advanced Trie Applications \u00b6 Concept: Use tries for complex string operations. Use case: Longest common prefix, map sum pairs, replace words. import java.util.*; public class AdvancedTrie { /** * Problem: Longest common prefix of all strings * Time: O(S) where S = sum of all characters, Space: O(S) * * TODO: Implement using trie */ public static String longestCommonPrefix(String[] strs) { // TODO: Build trie from all strings // TODO: Traverse trie while: // TODO: Return prefix return \"\"; // Replace with implementation } /** * MapSum: Sum of values with given prefix * * TODO: Implement map sum trie */ static class MapSum { // TODO: TrieNode with value at each node // TODO: Store running sum at each node static class TrieNode { Map<Character, TrieNode> children = new HashMap<>(); int value; // Value of word ending here int sum; // Sum of all words with this prefix } private TrieNode root; public MapSum() { // TODO: Initialize root } /** * Insert key with value * Time: O(m), Space: O(m) */ public void insert(String key, int val) { // TODO: Navigate/create path for key // TODO: Update sum at each node // TODO: Store value at end node } /** * Sum of all values with given prefix * Time: O(m), Space: O(1) */ public int sum(String prefix) { // TODO: Navigate to end of prefix // TODO: Return sum at that node return 0; // Replace with implementation } } /** * Problem: Replace words with shortest root from dictionary * Time: O(N + S), Space: O(N) * * TODO: Implement replace words */ public static String replaceWords(List<String> dictionary, String sentence) { // TODO: Build trie from dictionary // TODO: Implement iteration/conditional logic // TODO: Return modified sentence return \"\"; // Replace with implementation } } Runnable Client Code: import java.util.*; public class AdvancedTrieClient { public static void main(String[] args) { System.out.println(\"=== Advanced Trie ===\\n\"); // Test 1: Longest common prefix System.out.println(\"--- Test 1: Longest Common Prefix ---\"); String[][] stringSets = { {\"flower\", \"flow\", \"flight\"}, {\"dog\", \"racecar\", \"car\"}, {\"interview\", \"internet\", \"interval\"} }; for (String[] strs : stringSets) { String lcp = AdvancedTrie.longestCommonPrefix(strs); System.out.printf(\"Strings: %s%n\", Arrays.toString(strs)); System.out.printf(\"LCP: \\\"%s\\\"%n%n\", lcp); } // Test 2: Map sum System.out.println(\"--- Test 2: Map Sum ---\"); AdvancedTrie.MapSum mapSum = new AdvancedTrie.MapSum(); mapSum.insert(\"apple\", 3); System.out.println(\"insert(\\\"apple\\\", 3)\"); System.out.println(\"sum(\\\"ap\\\"): \" + mapSum.sum(\"ap\")); mapSum.insert(\"app\", 2); System.out.println(\"insert(\\\"app\\\", 2)\"); System.out.println(\"sum(\\\"ap\\\"): \" + mapSum.sum(\"ap\")); // Test 3: Replace words System.out.println(\"\\n--- Test 3: Replace Words ---\"); List<String> dictionary = Arrays.asList(\"cat\", \"bat\", \"rat\"); String sentence = \"the cattle was rattled by the battery\"; System.out.println(\"Dictionary: \" + dictionary); System.out.println(\"Sentence: \" + sentence); String replaced = AdvancedTrie.replaceWords(dictionary, sentence); System.out.println(\"Result: \" + replaced); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken implementations. This tests your understanding. Challenge 1: Broken Insert \u00b6 /** * This insert method is supposed to add words to the trie. * It has 2 BUGS. Find them! */ public void insert_Buggy(String word) { TrieNode current = root; for (char c : word.toCharArray()) { int index = c - 'a'; if (current.children[index] == null) { } current = current.children[index]; } } Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Test case to expose the bug: Insert: \"app\" and \"apple\" Search for \"app\": Expected true, Actual: [Trace through manually] Click to verify your answers Bug 1: Missing node creation! Should be: if (current.children[index] == null) { current.children[index] = new TrieNode(); } Bug 2: Missing current.isEndOfWord = true; after the loop. Without this, search will fail even though word was inserted. Result: Without bug fixes, search(\"app\") would return false because isEndOfWord is never set to true. Challenge 2: Broken Search \u00b6 /** * Search if word exists in trie. * This has 1 CRITICAL BUG. */ public boolean search_Buggy(String word) { TrieNode current = root; for (char c : word.toCharArray()) { int index = c - 'a'; if (current.children[index] == null) { return false; } current = current.children[index]; } return true;} Your debugging: Bug: [What\\'s the bug?] Test case to expose the bug: Dictionary: [\"apple\", \"application\"] Search for \"app\": Expected false, Actual: [What happens?] Search for \"apple\": Expected true, Actual: [What happens?] Click to verify your answer Bug: Should return current.isEndOfWord , not just true . Correct: return current.isEndOfWord; Why: We traversed all characters successfully, but that only means the prefix exists. We must check if it's marked as an actual word ending. Result: Without fix, search(\"app\") returns true even though \"app\" wasn't inserted (only \"apple\" was). Challenge 3: Broken Autocomplete \u00b6 /** * Find all words with given prefix. * This has 2 BUGS causing wrong results. */ public List<String> autocomplete_Buggy(String prefix) { List<String> results = new ArrayList<>(); TrieNode current = root; // Navigate to prefix for (char c : prefix.toCharArray()) { int index = c - 'a'; if (current.children[index] == null) { return results; } current = current.children[index]; } // Collect all words collectWords(current, prefix, results); return results; } private void collectWords(TrieNode node, String currentWord, List<String> results) { for (int i = 0; i < 26; i++) { if (node.children[i] != null) { collectWords(node.children[i], currentWord + (char)('a' + i), results); } } if (node.isEndOfWord) { results.add(currentWord); } } Your debugging: Bug 1: [What check is missing?] Bug 1 fix: [What code to add?] Bug 2: [What's wrong with the order?] Bug 2 fix: [Where should the check go?] Test case: Dictionary: [\"app\", \"apple\", \"application\"] Autocomplete(\"app\"): Expected [\"app\", \"apple\", \"application\"] Actual: [Trace through - what's wrong?] Click to verify your answers Bug 1: Missing check if prefix itself is a word. After navigating to prefix, add: if (current.isEndOfWord) { results.add(prefix); } Bug 2: The isEndOfWord check should come BEFORE recursing to children, not after. This ensures parent words are added before children (proper DFS order). Correct order: private void collectWords(TrieNode node, String currentWord, List<String> results) { if (node.isEndOfWord) { results.add(currentWord); // Add current word first } for (int i = 0; i < 26; i++) { if (node.children[i] != null) { collectWords(node.children[i], currentWord + (char)('a' + i), results); } } } Challenge 4: Memory Leak in Delete \u00b6 /** * Delete word from trie. * This has 1 SUBTLE BUG causing memory waste. */ public boolean delete_Buggy(String word) { return deleteHelper(root, word, 0); } private boolean deleteHelper(TrieNode node, String word, int index) { if (index == word.length()) { if (!node.isEndOfWord) { return false; // Word not in trie } node.isEndOfWord = false; return true; } char c = word.charAt(index); int idx = c - 'a'; TrieNode child = node.children[idx]; if (child == null) { return false; // Word not in trie } boolean shouldDeleteChild = deleteHelper(child, word, index + 1); if (shouldDeleteChild) { node.children[idx] = null; // Delete child } return false; // Don't delete this node } Your debugging: Bug 1: [What's the logic error?] Bug 2: [When should we delete a node?] Bug 3: [What should the final return be?] Test case: Dictionary: [\"app\", \"apple\"] Delete \"apple\" Expected: \"app\" still works, nodes for \"le\" are removed Actual: [What happens? Trace through] Click to verify your answer Bug: The logic for when to delete nodes is wrong. We should only delete a node if: It's not an end of word (after marking false) It has no children Correct implementation: private boolean deleteHelper(TrieNode node, String word, int index) { if (index == word.length()) { if (!node.isEndOfWord) { return false; } node.isEndOfWord = false; // Can delete if no children return isEmpty(node); } char c = word.charAt(index); int idx = c - 'a'; TrieNode child = node.children[idx]; if (child == null) { return false; } boolean shouldDeleteChild = deleteHelper(child, word, index + 1); if (shouldDeleteChild) { node.children[idx] = null; // Delete this node if it's not end of word and has no children return !node.isEndOfWord && isEmpty(node); } return false; } private boolean isEmpty(TrieNode node) { for (int i = 0; i < 26; i++) { if (node.children[i] != null) { return false; } } return true; } Why: Without proper cleanup, deleted word's unique nodes remain in memory even though they're not part of any word. Challenge 5: Off-by-One in Word Search \u00b6 /** * DFS for word search in grid using trie. * This has 1 CRITICAL BUG causing wrong results. */ private void dfs_Buggy(char[][] board, int i, int j, TrieNode node, Set<String> result, boolean[][] visited) { char c = board[i][j]; if (!node.children.containsKey(c)) { return; // Character not in trie } visited[i][j] = true; TrieNode next = node.children.get(c); if (next.word != null) { result.add(next.word); } // Explore 4 directions dfs_Buggy(board, i + 1, j, next, result, visited); dfs_Buggy(board, i - 1, j, next, result, visited); dfs_Buggy(board, i, j + 1, next, result, visited); dfs_Buggy(board, i, j - 1, next, result, visited); visited[i][j] = false; // Backtrack } Your debugging: Bug: [What\\'s the bug?] Test case: Grid: [['a','b'],['c','d']] Dictionary: [\"ab\"] What error occurs? [Fill in] Click to verify your answer Bug: Missing boundary and visited checks at the START of the function. Current code checks after accessing array, causing ArrayIndexOutOfBoundsException. Correct: private void dfs(char[][] board, int i, int j, TrieNode node, Set<String> result, boolean[][] visited) { // Check bounds and visited FIRST if (i < 0 || i >= board.length || j < 0 || j >= board[0].length || visited[i][j]) { return; } char c = board[i][j]; if (!node.children.containsKey(c)) { return; } visited[i][j] = true; TrieNode next = node.children.get(c); if (next.word != null) { result.add(next.word); } dfs(board, i + 1, j, next, result, visited); dfs(board, i - 1, j, next, result, visited); dfs(board, i, j + 1, next, result, visited); dfs(board, i, j - 1, next, result, visited); visited[i][j] = false; } Why: Without boundary checks first, recursive calls will access invalid indices. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 8+ bugs across 5 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common trie mistakes to avoid Common mistakes you discovered: [Forgetting to mark isEndOfWord] [Not creating new nodes during insert] [Not checking isEndOfWord in search] [Wrong order in DFS collection] [Missing boundary checks in grid search] [Incomplete deletion logic] Decision Framework \u00b6 Your task: Build decision trees for trie problems. Question 1: What operations do you need? \u00b6 Answer after solving problems: Insert/search single word? [Basic trie] Prefix search? [Trie with prefix traversal] Find all words with prefix? [Autocomplete trie] Multiple word search in grid? [Trie + DFS] Question 2: What are the constraints? \u00b6 Space critical: Use: [Hash map for children (sparse)] Avoid: [Array for children (dense)] Need frequency/values: Store: [Additional data at nodes] Examples: [MapSum, frequency trie] Need to delete: Implement: [Recursive deletion with cleanup] Your Decision Tree \u00b6 flowchart LR Start[\"Trie Pattern Selection\"] Q1{\"Basic operations?\"} Start --> Q1 N2([\"Basic Trie \u2713\"]) Q1 -->|\"Insert/search/prefix\"| N2 N3([\"Trie with delete \u2713\"]) Q1 -->|\"With deletion\"| N3 Q4{\"Multiple word search?\"} Start --> Q4 N5([\"Trie + DFS \u2713\"]) Q4 -->|\"In grid\"| N5 N6([\"Trie + automaton \u2713\"]) Q4 -->|\"In stream\"| N6 Q7{\"Prefix-based queries?\"} Start --> Q7 N8([\"Autocomplete \u2713\"]) Q7 -->|\"Find all with prefix\"| N8 N9([\"Trie with counts \u2713\"]) Q7 -->|\"Count with prefix\"| N9 N10([\"MapSum trie \u2713\"]) Q7 -->|\"Sum with prefix\"| N10 Q11{\"String operations?\"} Start --> Q11 N12([\"Trie traversal \u2713\"]) Q11 -->|\"Longest common prefix\"| N12 N13([\"Dictionary trie \u2713\"]) Q11 -->|\"Replace with root\"| N13 N14([\"Trie with validation \u2713\"]) Q11 -->|\"Longest word\"| N14 Practice \u00b6 LeetCode Problems \u00b6 Easy (Complete all 2): 208. Implement Trie Pattern: [Basic trie] Your solution time: ___ Key insight: [Fill in after solving] 720. Longest Word in Dictionary Pattern: [Trie with validation] Your solution time: ___ Key insight: [Fill in] Medium (Complete 3-4): 211. Design Add and Search Words Data Structure Pattern: [Trie with wildcard] Difficulty: [Rate 1-10] Key insight: [Fill in] 212. Word Search II Pattern: [Trie + DFS] Difficulty: [Rate 1-10] Key insight: [Fill in] 1268. Search Suggestions System Pattern: [Autocomplete] Difficulty: [Rate 1-10] Key insight: [Fill in] 648. Replace Words Pattern: [Dictionary trie] Difficulty: [Rate 1-10] Key insight: [Fill in] Hard (Optional): 472. Concatenated Words Pattern: [Trie + DP] Key insight: [Fill in after solving] 1707. Maximum XOR With an Element From Array Pattern: [Binary trie] Key insight: [Fill in after solving] Review Checklist \u00b6 Before moving to the next topic: Implementation Basic trie: insert, search, prefix, delete all work Word search: find multiple words in grid works Autocomplete: prefix matching and suggestions work Advanced: LCP, MapSum, replace words work All client code runs successfully Pattern Recognition Can identify when trie is appropriate Understand trie vs hash table tradeoffs Know how to store additional data in nodes Recognize prefix query patterns Problem Solving Solved 2 easy problems Solved 3-4 medium problems Analyzed time/space complexity Understood when to use array vs map for children Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use tries Can explain trie structure and traversal Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand space complexity of tries Mastery Certification \u00b6 I certify that I can: Implement basic trie (insert, search, prefix) from memory Explain when and why to use trie over hash table Implement autocomplete with prefix matching Use trie for word search in 2D grid Implement trie deletion correctly Analyze space and time complexity Debug common trie mistakes Apply tries to real-world problems Teach this concept to someone else","title":"16. Tries"},{"location":"dsa/16-tries/#tries-prefix-trees","text":"Efficient storage and retrieval of strings with common prefixes using tree structure","title":"Tries (Prefix Trees)"},{"location":"dsa/16-tries/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is a trie in one sentence? Your answer: [Fill in after implementation] How is a trie different from a hash table for strings? Your answer: [Fill in after implementation] Real-world analogy: Example: \"A trie is like how you organize words in a dictionary by first letter, then second letter...\" Your analogy: [Fill in] When does this pattern work? Your answer: [Fill in after solving problems] What's the space-time tradeoff with tries? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/16-tries/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/16-tries/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"dsa/16-tries/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/16-tries/#debugging-challenges","text":"Your task: Find and fix bugs in broken implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"dsa/16-tries/#decision-framework","text":"Your task: Build decision trees for trie problems.","title":"Decision Framework"},{"location":"dsa/16-tries/#practice","text":"","title":"Practice"},{"location":"dsa/16-tries/#review-checklist","text":"Before moving to the next topic: Implementation Basic trie: insert, search, prefix, delete all work Word search: find multiple words in grid works Autocomplete: prefix matching and suggestions work Advanced: LCP, MapSum, replace words work All client code runs successfully Pattern Recognition Can identify when trie is appropriate Understand trie vs hash table tradeoffs Know how to store additional data in nodes Recognize prefix query patterns Problem Solving Solved 2 easy problems Solved 3-4 medium problems Analyzed time/space complexity Understood when to use array vs map for children Understanding Filled in all ELI5 explanations Built decision tree Identified when NOT to use tries Can explain trie structure and traversal Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand space complexity of tries","title":"Review Checklist"},{"location":"dsa/17-advanced-topics/","text":"Advanced Topics \u00b6 Master bit manipulation, intervals, and prefix sums ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What are these advanced techniques in one sentence each? Bit manipulation: [Fill in after implementation] Intervals: [Fill in after implementation] Prefix sum: [Fill in after implementation] Real-world analogies: Bit manipulation: \"Like using switches that are either on or off...\" Intervals: \"Like managing calendar appointments...\" Prefix sum: \"Like keeping a running total...\" Your analogies: [Fill in] When does each pattern work? Your answers: [Fill in after solving problems] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 XOR all elements to find single number: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Merge overlapping intervals: Time complexity: [Your guess: O(?)] Why that complexity? [Fill in your reasoning] Verified: [Actual] Scenario Predictions \u00b6 Scenario 1: Find single number in [2, 3, 2, 4, 3] Can you use XOR? [Yes/No - Why?] What property of XOR makes this work? [Fill in] What is 2 XOR 2? [Fill in] What is any number XOR 0? [Fill in] Scenario 2: Merge intervals [[1,3], [2,6], [8,10], [15,18]] Must you sort first? [Yes/No - Why?] How do you check if intervals overlap? [Fill in the condition] **How many intervals in final result? ** [Your prediction: ___ ] Bit Manipulation Quiz \u00b6 Question: What does n & (n-1) do? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: How to check if a number is a power of 2? n % 2 == 0 n > 0 && (n & (n-1)) == 0 n == (n | (n-1)) Math.log(n) % 2 == 0 Verify after implementation: [Which one(s)?] Intervals Quiz \u00b6 Question: For intervals [1,4] and [3,6] , do they overlap? Your answer: [Yes/No] The condition is: start1 <= <span class=\"fill-in\">_____</span> && start2 <= <span class=\"fill-in\">_____</span> Fill in the blanks: [Fill in after learning] Question: What's faster: using a heap vs two sorted arrays for finding meeting rooms? Your prediction: [Fill in] Verified: [Fill in after implementation] Before/After: Why These Patterns Matter \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example 1: Find Single Number \u00b6 Problem: Find the single number when all others appear twice. Approach 1: Hash Set (Naive) \u00b6 // Naive approach - Track seen numbers public static int findSingle_HashSet(int[] nums) { Set<Integer> seen = new HashSet<>(); for (int num : nums) { if (seen.contains(num)) { seen.remove(num); // Seen twice, remove } else { seen.add(num); // First time seeing } } // The remaining element is the single one return seen.iterator().next(); } Analysis: Time: O(n) - Iterate through array once Space: O(n) - Store up to n/2 elements in set For n = 10,000: ~10,000 operations + space for 5,000 integers Approach 2: XOR Bit Manipulation (Optimized) \u00b6 // Optimized approach - Use XOR property public static int findSingle_XOR(int[] nums) { int result = 0; for (int num : nums) { result ^= num; // XOR all numbers } return result; // Pairs cancel out, single remains } Analysis: Time: O(n) - Iterate through array once Space: O(1) - Only one variable For n = 10,000: ~10,000 operations + space for 1 integer Performance Comparison \u00b6 Array Size HashSet Space XOR Space Space Saved n = 1,000 ~500 integers 1 integer 99.8% n = 10,000 ~5,000 integers 1 integer 99.98% n = 1,000,000 ~500,000 integers 1 integer 99.9998% Why Does XOR Work? Key properties: a XOR a = 0 (any number XOR itself is 0) a XOR 0 = a (any number XOR 0 is itself) XOR is commutative and associative Example with [2, 3, 2, 4, 3] : 2 XOR 3 XOR 2 XOR 4 XOR 3 = (2 XOR 2) XOR (3 XOR 3) XOR 4 // Rearrange = 0 XOR 0 XOR 4 // Pairs cancel = 4 // Answer! After implementing, explain in your own words: Why does XOR cancel out duplicates? [Your answer] When would HashSet be better than XOR? [Your answer] Example 2: Merge Intervals \u00b6 Problem: Merge all overlapping intervals in [[1,3], [2,6], [8,10], [15,18]] . Approach 1: Nested Loops (Naive) \u00b6 // Naive approach - Compare all pairs repeatedly public static int[][] merge_BruteForce(int[][] intervals) { if (intervals.length <= 1) return intervals; List<int[]> merged = new ArrayList<>(); boolean[] used = new boolean[intervals.length]; for (int i = 0; i < intervals.length; i++) { if (used[i]) continue; int[] current = intervals[i].clone(); used[i] = true; // Keep merging until no more overlaps found boolean changed = true; while (changed) { changed = false; for (int j = 0; j < intervals.length; j++) { if (used[j]) continue; // Check overlap if (current[0] <= intervals[j][1] && intervals[j][0] <= current[1]) { current[0] = Math.min(current[0], intervals[j][0]); current[1] = Math.max(current[1], intervals[j][1]); used[j] = true; changed = true; } } } merged.add(current); } return merged.toArray(new int[0][]); } Analysis: Time: O(n\u00b2) or worse - Multiple passes needed Space: O(n) - Track used intervals For n = 1,000: Up to ~1,000,000 comparisons Approach 2: Sort + Single Pass (Optimized) \u00b6 // Optimized approach - Sort once, merge in one pass public static int[][] merge_Optimized(int[][] intervals) { if (intervals.length <= 1) return intervals; // Sort by start time Arrays.sort(intervals, (a, b) -> a[0] - b[0]); List<int[]> merged = new ArrayList<>(); int[] current = intervals[0]; merged.add(current); for (int i = 1; i < intervals.length; i++) { if (intervals[i][0] <= current[1]) { // Overlap - merge by extending end current[1] = Math.max(current[1], intervals[i][1]); } else { // No overlap - start new interval current = intervals[i]; merged.add(current); } } return merged.toArray(new int[0][]); } Analysis: Time: O(n log n) - Sorting dominates Space: O(n) - Result array For n = 1,000: ~10,000 comparisons (sort) + 1,000 (merge) Performance Comparison \u00b6 Array Size Brute Force (O(n\u00b2)) Sort + Merge (O(n log n)) Speedup n = 100 ~10,000 ops ~700 ops 14x n = 1,000 ~1,000,000 ops ~10,000 ops 100x n = 10,000 ~100,000,000 ops ~130,000 ops 770x Why Does Sorting Help? After sorting by start time, intervals are in order: Unsorted: [[8,10], [1,3], [15,18], [2,6]] Sorted: [[1,3], [2,6], [8,10], [15,18]] Now you only need to check if current interval end >= next interval start : [1,3] and [2,6] : 3 >= 2 \u2192 Merge to [1,6] [1,6] and [8,10] : 6 < 8 \u2192 Keep separate [8,10] and [15,18] : 10 < 15 \u2192 Keep separate After implementing, explain in your own words: Why can we merge in a single pass after sorting? [Your answer] What intervals do we never need to compare? [Your answer] Core Implementation \u00b6 Pattern 1: Bit Manipulation \u00b6 Concept: Use bitwise operations for efficient computations. Use case: XOR tricks, bit masks, counting bits, power of two. public class BitManipulation { /** * Problem: Single number (all appear twice except one) * Time: O(n), Space: O(1) * * TODO: Implement using XOR */ public static int singleNumber(int[] nums) { // TODO: XOR all numbers // TODO: a XOR a = 0, a XOR 0 = a // TODO: Duplicates cancel out, single remains return 0; // Replace with implementation } /** * Problem: Number of 1 bits (Hamming weight) * Time: O(1), Space: O(1) * * TODO: Implement bit counting */ public static int hammingWeight(int n) { // TODO: Count set bits // TODO: Method 1: Loop and check each bit // TODO: Method 2: n & (n-1) removes rightmost 1 return 0; // Replace with implementation } /** * Problem: Reverse bits * Time: O(1), Space: O(1) * * TODO: Implement bit reversal */ public static int reverseBits(int n) { // TODO: Process bit by bit // TODO: Extract bit: (n >> i) & 1 // TODO: Place bit: result |= (bit << (31 - i)) return 0; // Replace with implementation } /** * Problem: Missing number (0 to n with one missing) * Time: O(n), Space: O(1) * * TODO: Implement using XOR or math */ public static int missingNumber(int[] nums) { // TODO: Method 1: XOR all indices and values // TODO: Method 2: Sum formula - sum(0..n) - sum(nums) return 0; // Replace with implementation } /** * Problem: Power of two * Time: O(1), Space: O(1) * * TODO: Implement power of two check */ public static boolean isPowerOfTwo(int n) { // TODO: Power of 2 has exactly one bit set // TODO: Check: n > 0 && (n & (n-1)) == 0 return false; // Replace with implementation } /** * Problem: Counting bits (0 to n) * Time: O(n), Space: O(n) * * TODO: Implement using DP with bit manipulation */ public static int[] countBits(int n) { // TODO: dp[i] = dp[i >> 1] + (i & 1) // TODO: Bits in i = bits in i/2 + (1 if i is odd) return new int[0]; // Replace with implementation } /** * Problem: Sum of two integers without + operator * Time: O(1), Space: O(1) * * TODO: Implement using bit operations */ public static int getSum(int a, int b) { // TODO: XOR for sum without carry // TODO: AND and shift for carry // TODO: Repeat until no carry return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class BitManipulationClient { public static void main(String[] args) { System.out.println(\"=== Bit Manipulation ===\\n\"); // Test 1: Single number System.out.println(\"--- Test 1: Single Number ---\"); int[] arr1 = {4, 1, 2, 1, 2}; System.out.println(\"Array: \" + Arrays.toString(arr1)); System.out.println(\"Single number: \" + BitManipulation.singleNumber(arr1)); // Test 2: Hamming weight System.out.println(\"\\n--- Test 2: Hamming Weight ---\"); int[] numbers = {11, 128, 255}; for (int n : numbers) { int weight = BitManipulation.hammingWeight(n); System.out.printf(\"%d (binary: %s): %d bits%n\", n, Integer.toBinaryString(n), weight); } // Test 3: Reverse bits System.out.println(\"\\n--- Test 3: Reverse Bits ---\"); int n = 43261596; System.out.printf(\"Original: %d (binary: %s)%n\", n, Integer.toBinaryString(n)); int reversed = BitManipulation.reverseBits(n); System.out.printf(\"Reversed: %d (binary: %s)%n\", reversed, Integer.toBinaryString(reversed)); // Test 4: Missing number System.out.println(\"\\n--- Test 4: Missing Number ---\"); int[] arr2 = {3, 0, 1}; System.out.println(\"Array: \" + Arrays.toString(arr2)); System.out.println(\"Missing: \" + BitManipulation.missingNumber(arr2)); // Test 5: Power of two System.out.println(\"\\n--- Test 5: Power of Two ---\"); int[] testPowers = {1, 2, 3, 4, 16, 18}; for (int num : testPowers) { boolean isPower = BitManipulation.isPowerOfTwo(num); System.out.printf(\"%d: %s%n\", num, isPower ? \"YES\" : \"NO\"); } // Test 6: Counting bits System.out.println(\"\\n--- Test 6: Counting Bits ---\"); int num = 5; int[] bitCounts = BitManipulation.countBits(num); System.out.printf(\"Bit counts from 0 to %d: %s%n\", num, Arrays.toString(bitCounts)); // Test 7: Sum without + operator System.out.println(\"\\n--- Test 7: Sum Without + ---\"); int a = 15, b = 27; int sum = BitManipulation.getSum(a, b); System.out.printf(\"%d + %d = %d%n\", a, b, sum); } } Pattern 2: Intervals \u00b6 Concept: Merge, insert, or manipulate intervals efficiently. Use case: Meeting rooms, merge intervals, interval intersection. import java.util.*; public class Intervals { /** * Problem: Merge overlapping intervals * Time: O(n log n), Space: O(n) * * TODO: Implement merge intervals */ public static int[][] merge(int[][] intervals) { // TODO: Sort by start time // TODO: Iterate and merge overlapping intervals // TODO: Implement iteration/conditional logic // TODO: Otherwise, add previous to result return new int[0][0]; // Replace with implementation } /** * Problem: Insert interval into sorted intervals * Time: O(n), Space: O(n) * * TODO: Implement insert interval */ public static int[][] insert(int[][] intervals, int[] newInterval) { // TODO: Add all intervals before newInterval // TODO: Merge all overlapping intervals // TODO: Add all intervals after newInterval return new int[0][0]; // Replace with implementation } /** * Problem: Interval intersection * Time: O(m + n), Space: O(min(m,n)) * * TODO: Implement interval intersection */ public static int[][] intervalIntersection(int[][] firstList, int[][] secondList) { // TODO: Two pointers on both lists // TODO: Find intersection: max(start1, start2) to min(end1, end2) // TODO: Move pointer of interval that ends first return new int[0][0]; // Replace with implementation } /** * Problem: Minimum number of meeting rooms * Time: O(n log n), Space: O(n) * * TODO: Implement meeting rooms II */ public static int minMeetingRooms(int[][] intervals) { // TODO: Method 1: Sort start and end times separately // TODO: Method 2: Use min-heap for end times return 0; // Replace with implementation } /** * Problem: Remove covered intervals * Time: O(n log n), Space: O(1) * * TODO: Implement remove covered */ public static int removeCoveredIntervals(int[][] intervals) { // TODO: Sort by start (ascending), then end (descending) // TODO: Track current max end // TODO: Implement iteration/conditional logic return 0; // Replace with implementation } /** * Problem: Non-overlapping intervals (min removals) * Time: O(n log n), Space: O(1) * * TODO: Implement min removals */ public static int eraseOverlapIntervals(int[][] intervals) { // TODO: Sort by end time (greedy) // TODO: Keep track of last end time // TODO: Implement iteration/conditional logic return 0; // Replace with implementation } } Runnable Client Code: import java.util.*; public class IntervalsClient { public static void main(String[] args) { System.out.println(\"=== Intervals ===\\n\"); // Test 1: Merge intervals System.out.println(\"--- Test 1: Merge Intervals ---\"); int[][] intervals1 = {{1,3}, {2,6}, {8,10}, {15,18}}; System.out.println(\"Input: \" + Arrays.deepToString(intervals1)); int[][] merged = Intervals.merge(intervals1); System.out.println(\"Merged: \" + Arrays.deepToString(merged)); // Test 2: Insert interval System.out.println(\"\\n--- Test 2: Insert Interval ---\"); int[][] intervals2 = {{1,3}, {6,9}}; int[] newInterval = {2, 5}; System.out.println(\"Intervals: \" + Arrays.deepToString(intervals2)); System.out.println(\"New: \" + Arrays.toString(newInterval)); int[][] inserted = Intervals.insert(intervals2, newInterval); System.out.println(\"Result: \" + Arrays.deepToString(inserted)); // Test 3: Interval intersection System.out.println(\"\\n--- Test 3: Interval Intersection ---\"); int[][] first = {{0,2}, {5,10}, {13,23}, {24,25}}; int[][] second = {{1,5}, {8,12}, {15,24}, {25,26}}; System.out.println(\"First: \" + Arrays.deepToString(first)); System.out.println(\"Second: \" + Arrays.deepToString(second)); int[][] intersection = Intervals.intervalIntersection(first, second); System.out.println(\"Intersection: \" + Arrays.deepToString(intersection)); // Test 4: Meeting rooms System.out.println(\"\\n--- Test 4: Meeting Rooms ---\"); int[][] meetings = {{0,30}, {5,10}, {15,20}}; System.out.println(\"Meetings: \" + Arrays.deepToString(meetings)); int rooms = Intervals.minMeetingRooms(meetings); System.out.println(\"Min rooms needed: \" + rooms); // Test 5: Remove covered intervals System.out.println(\"\\n--- Test 5: Remove Covered Intervals ---\"); int[][] intervals3 = {{1,4}, {3,6}, {2,8}}; System.out.println(\"Intervals: \" + Arrays.deepToString(intervals3)); int remaining = Intervals.removeCoveredIntervals(intervals3); System.out.println(\"Remaining after removing covered: \" + remaining); // Test 6: Erase overlap intervals System.out.println(\"\\n--- Test 6: Erase Overlap Intervals ---\"); int[][] intervals4 = {{1,2}, {2,3}, {3,4}, {1,3}}; System.out.println(\"Intervals: \" + Arrays.deepToString(intervals4)); int removals = Intervals.eraseOverlapIntervals(intervals4); System.out.println(\"Min removals to make non-overlapping: \" + removals); } } Pattern 3: Prefix Sum \u00b6 Concept: Precompute cumulative sums for fast range queries. Use case: Subarray sum, range sum query, contiguous array. import java.util.*; public class PrefixSum { /** * Problem: Range sum query (immutable array) * Time: O(1) query after O(n) preprocessing, Space: O(n) * * TODO: Implement range sum query */ static class NumArray { private int[] prefixSum; public NumArray(int[] nums) { // TODO: Build prefix sum array // TODO: prefixSum[i] = sum of nums[0..i-1] } public int sumRange(int left, int right) { // TODO: Return prefixSum[right+1] - prefixSum[left] return 0; // Replace with implementation } } /** * Problem: Subarray sum equals K * Time: O(n), Space: O(n) * * TODO: Implement using prefix sum + hashmap */ public static int subarraySum(int[] nums, int k) { // TODO: Use HashMap<prefixSum, frequency> // TODO: Implement iteration/conditional logic return 0; // Replace with implementation } /** * Problem: Contiguous array (equal 0s and 1s) * Time: O(n), Space: O(n) * * TODO: Implement using prefix sum */ public static int findMaxLength(int[] nums) { // TODO: Convert 0s to -1s // TODO: Problem becomes: longest subarray with sum 0 // TODO: Use HashMap<prefixSum, firstIndex> // TODO: Implement iteration/conditional logic return 0; // Replace with implementation } /** * Problem: Product of array except self * Time: O(n), Space: O(1) excluding output * * TODO: Implement using prefix/suffix products */ public static int[] productExceptSelf(int[] nums) { // TODO: First pass: compute prefix products // TODO: Second pass: compute suffix products and multiply return new int[0]; // Replace with implementation } /** * Problem: Range sum query 2D (matrix) * Time: O(1) query after O(m*n) preprocessing, Space: O(m*n) * * TODO: Implement 2D prefix sum */ static class NumMatrix { private int[][] prefixSum; public NumMatrix(int[][] matrix) { // TODO: Build 2D prefix sum // TODO: prefixSum[i][j] = sum of submatrix (0,0) to (i-1,j-1) // TODO: Use inclusion-exclusion principle } public int sumRegion(int row1, int col1, int row2, int col2) { // TODO: Use inclusion-exclusion: return 0; // Replace with implementation } } } Runnable Client Code: import java.util.*; public class PrefixSumClient { public static void main(String[] args) { System.out.println(\"=== Prefix Sum ===\\n\"); // Test 1: Range sum query System.out.println(\"--- Test 1: Range Sum Query ---\"); int[] arr = {-2, 0, 3, -5, 2, -1}; PrefixSum.NumArray numArray = new PrefixSum.NumArray(arr); System.out.println(\"Array: \" + Arrays.toString(arr)); int[][] queries = {{0, 2}, {2, 5}, {0, 5}}; for (int[] query : queries) { int sum = numArray.sumRange(query[0], query[1]); System.out.printf(\"sumRange(%d, %d) = %d%n\", query[0], query[1], sum); } // Test 2: Subarray sum equals K System.out.println(\"\\n--- Test 2: Subarray Sum Equals K ---\"); int[] arr2 = {1, 1, 1}; int k = 2; System.out.println(\"Array: \" + Arrays.toString(arr2)); System.out.println(\"k = \" + k); int count = PrefixSum.subarraySum(arr2, k); System.out.println(\"Count of subarrays: \" + count); // Test 3: Contiguous array System.out.println(\"\\n--- Test 3: Contiguous Array ---\"); int[] arr3 = {0, 1, 0, 1, 1, 0}; System.out.println(\"Array: \" + Arrays.toString(arr3)); int maxLen = PrefixSum.findMaxLength(arr3); System.out.println(\"Max length with equal 0s and 1s: \" + maxLen); // Test 4: Product except self System.out.println(\"\\n--- Test 4: Product Except Self ---\"); int[] arr4 = {1, 2, 3, 4}; System.out.println(\"Array: \" + Arrays.toString(arr4)); int[] products = PrefixSum.productExceptSelf(arr4); System.out.println(\"Products: \" + Arrays.toString(products)); // Test 5: 2D range sum query System.out.println(\"\\n--- Test 5: 2D Range Sum Query ---\"); int[][] matrix = { {3, 0, 1, 4, 2}, {5, 6, 3, 2, 1}, {1, 2, 0, 1, 5}, {4, 1, 0, 1, 7}, {1, 0, 3, 0, 5} }; PrefixSum.NumMatrix numMatrix = new PrefixSum.NumMatrix(matrix); System.out.println(\"Matrix:\"); for (int[] row : matrix) { System.out.println(\" \" + Arrays.toString(row)); } int sum = numMatrix.sumRegion(2, 1, 4, 3); System.out.printf(\"sumRegion(2, 1, 4, 3) = %d%n\", sum); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken implementations. This tests your understanding of advanced patterns. Challenge 1: Broken Hamming Weight \u00b6 /** * Count number of 1 bits in an integer. * This has 1 CRITICAL BUG. */ public static int hammingWeight_Buggy(int n) { int count = 0; while (n > 0) { if ((n & 1) == 1) { count++; } n = n >> 1; } return count; } Your debugging: Bug: [What\\'s the bug?] Test case to expose the bug: Input: n = -1 (all 32 bits are 1) Expected output: 32 Actual output with buggy code: [Trace through - what happens?] Click to verify your answers Bug: Using n > 0 stops too early for negative numbers. Signed right shift ( >> ) preserves the sign bit, so negative numbers never become 0. Better fix: Use unsigned right shift ( >>> ) or check all 32 bits explicitly: while (n != 0) { // Check n != 0, not n > 0 if ((n & 1) == 1) { count++; } n = n >>> 1; // Unsigned shift } Alternative: Use n & (n-1) trick that works for any integer: while (n != 0) { count++; n = n & (n - 1); // Remove rightmost 1 bit } Challenge 2: Broken Interval Merge \u00b6 /** * Merge overlapping intervals. * This has 2 BUGS. */ public static int[][] merge_Buggy(int[][] intervals) { if (intervals.length <= 1) return intervals; // Sort by start time Arrays.sort(intervals, (a, b) -> a[0] - b[0]); List<int[]> merged = new ArrayList<>(); int[] current = intervals[0]; for (int i = 1; i < intervals.length; i++) { if (intervals[i][0] <= current[1]) { current[1] = intervals[i][1]; } else { merged.add(current); current = intervals[i]; } } return merged.toArray(new int[0][]);} Your debugging: Bug 1: [Overlap check is correct, but what about the merge?] Bug 2: [Should use Math.max - why?] Bug 3: [What's missing at the end?] Test case to expose bugs: Input: [[1,3], [2,6], [8,10]] Expected: [[1,6], [8,10]] Actual with Bug 1-2: [What do you get?] Actual with Bug 3: [What's missing?] Click to verify your answers Bug 1: Overlap check is actually correct ( intervals[i][0] <= current[1] ). Bug 2: Should be current[1] = Math.max(current[1], intervals[i][1]) . Without max, if the new interval ends before current ends, we'd shrink current incorrectly. Example: [[1,4], [2,3]] should merge to [1,4] , not [1,3] . Bug 3: The last interval ( current ) is never added to the result! Need to add it after the loop: merged.add(current); // Add this line before return return merged.toArray(new int[0][]); Challenge 3: Broken Prefix Sum \u00b6 /** * Count subarrays with sum equals K. * This has 1 SUBTLE BUG. */ public static int subarraySum_Buggy(int[] nums, int k) { Map<Integer, Integer> prefixSumCount = new HashMap<>(); int currentSum = 0; int count = 0; for (int num : nums) { currentSum += num; // Check if currentSum - k exists if (prefixSumCount.containsKey(currentSum - k)) { count += prefixSumCount.get(currentSum - k); } // Update map with current prefix sum prefixSumCount.put(currentSum, prefixSumCount.getOrDefault(currentSum, 0) + 1); } return count; } Your debugging: Bug: [What\\'s the bug?] Test case to expose the bug: Input: nums = [1, 2, 3], k = 3 Expected: 2 (subarrays: [1,2] and [3]) Actual: [Trace through - what do you get?] Click to verify your answer Bug: Missing the check for when currentSum == k . This checks if the entire subarray from index 0 to current equals k. Correct: for (int num : nums) { currentSum += num; // Check if entire subarray from 0 to current equals k if (currentSum == k) { count++; } // Check if currentSum - k exists if (prefixSumCount.containsKey(currentSum - k)) { count += prefixSumCount.get(currentSum - k); } // Update map prefixSumCount.put(currentSum, prefixSumCount.getOrDefault(currentSum, 0) + 1); } Alternative (better): Initialize the map with prefixSumCount.put(0, 1) before the loop. This handles the case where the entire prefix equals k. Challenge 5: Broken Power of Two Check \u00b6 /** * Check if n is a power of 2. * This has 1 EDGE CASE BUG. */ public static boolean isPowerOfTwo_Buggy(int n) { return (n & (n - 1)) == 0;} Your debugging: Bug: [What edge case is missed?] Example: Try with n = 0 , expected false , actual [what?] Example: Try with n = -16 , expected false , actual [what?] Fix: [Add what condition?] Click to verify your answer Bug: Missing check for n > 0 . The expression n & (n-1) == 0 is true for: n = 0 : 0 & -1 = 0 \u2192 returns true (wrong!) n = -16 : Negative powers of 2 also pass (wrong!) Correct: return n > 0 && (n & (n - 1)) == 0; Why it works: Power of 2 has exactly one bit set: 8 = 1000 , 16 = 10000 n - 1 flips all bits after the rightmost 1: 8-1 = 0111 , 16-1 = 01111 n & (n-1) turns off the rightmost 1 bit, resulting in 0 for powers of 2 Challenge 6: Interval Intersection Edge Case \u00b6 /** * Find intersection of two interval lists. * This has 1 BOUNDARY BUG. */ public static int[][] intervalIntersection_Buggy( int[][] firstList, int[][] secondList) { List<int[]> result = new ArrayList<>(); int i = 0, j = 0; while (i < firstList.length && j < secondList.length) { int start = Math.max(firstList[i][0], secondList[j][0]); int end = Math.min(firstList[i][1], secondList[j][1]); result.add(new int[]{start, end}); // Move pointer of interval that ends first if (firstList[i][1] < secondList[j][1]) { i++; } else { j++; } } return result.toArray(new int[0][]); } Your debugging: Bug: [What check is missing?] Example: firstList = [[0,2]] , secondList = [[4,6]] (no overlap) Expected: [] (empty) Actual: [What do you get?] Fix: [Add what condition?] Click to verify your answer Bug: Need to check if start <= end before adding the interval. If there's no overlap, start > end , which is invalid. Correct: int start = Math.max(firstList[i][0], secondList[j][0]); int end = Math.min(firstList[i][1], secondList[j][1]); if (start <= end) { // Only add if valid intersection result.add(new int[]{start, end}); } Why: When intervals don't overlap, start will be greater than end , creating an invalid interval like [4, 2] . Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 8+ bugs across 6 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common mistakes in bit manipulation, intervals, prefix sums, and monotonic stacks Common mistakes you discovered: Bit manipulation: [Sign handling, shift operators] Intervals: [Forgetting final interval, not using Math.max in merge] Prefix sum: [Missing zero-sum check, initial map state] Monotonic stack: [Wrong result index, maintaining stack invariant] Edge cases: [Zero, negatives, empty arrays, non-overlapping intervals] Decision Framework \u00b6 Your task: Build decision trees for advanced patterns. Question 1: Which pattern to use? \u00b6 Bit manipulation when: Need: [Constant space for flags/subsets] Operations: [XOR, AND, OR, shifts] Examples: [Single number, power of 2] Intervals when: Need: [Merge, insert, find overlaps] Input: [Array of [start, end] pairs] Examples: [Meeting rooms, merge intervals] Prefix sum when: Need: [Fast range sum queries] Trade-off: [O(n) space for O(1) queries] Examples: [Subarray sum, range query] Review Checklist \u00b6 Before moving on: Implementation Bit manipulation: XOR, counting bits, power of 2 all work Intervals: merge, insert, intersection all work Prefix sum: range query, subarray sum, 2D all work Monotonic stack: next greater, histogram, temperatures all work All client code runs successfully Pattern Recognition Can identify when to use bit manipulation Recognize interval problems Know when prefix sum helps Identify monotonic stack opportunities Problem Solving Solved 4 easy problems Solved 6-8 medium problems Analyzed time/space complexity Understood when each pattern is optimal Understanding Filled in all ELI5 explanations Built decision trees for each pattern Identified when NOT to use each pattern Can explain trade-offs Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand why these techniques are \"advanced\" Mastery Certification \u00b6 I certify that I can: Implement all four advanced patterns from memory Explain when and why to use each pattern Identify the correct pattern for new problems Analyze time and space complexity Compare trade-offs with alternative approaches Debug common mistakes in bit operations, intervals, prefix sums, and stacks Teach these concepts to someone else Recognize when NOT to use each pattern","title":"17. Advanced Topics"},{"location":"dsa/17-advanced-topics/#advanced-topics","text":"Master bit manipulation, intervals, and prefix sums","title":"Advanced Topics"},{"location":"dsa/17-advanced-topics/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What are these advanced techniques in one sentence each? Bit manipulation: [Fill in after implementation] Intervals: [Fill in after implementation] Prefix sum: [Fill in after implementation] Real-world analogies: Bit manipulation: \"Like using switches that are either on or off...\" Intervals: \"Like managing calendar appointments...\" Prefix sum: \"Like keeping a running total...\" Your analogies: [Fill in] When does each pattern work? Your answers: [Fill in after solving problems]","title":"ELI5: Explain Like I'm 5"},{"location":"dsa/17-advanced-topics/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"dsa/17-advanced-topics/#beforeafter-why-these-patterns-matter","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why These Patterns Matter"},{"location":"dsa/17-advanced-topics/#core-implementation","text":"","title":"Core Implementation"},{"location":"dsa/17-advanced-topics/#debugging-challenges","text":"Your task: Find and fix bugs in broken implementations. This tests your understanding of advanced patterns.","title":"Debugging Challenges"},{"location":"dsa/17-advanced-topics/#decision-framework","text":"Your task: Build decision trees for advanced patterns.","title":"Decision Framework"},{"location":"dsa/17-advanced-topics/#review-checklist","text":"Before moving on: Implementation Bit manipulation: XOR, counting bits, power of 2 all work Intervals: merge, insert, intersection all work Prefix sum: range query, subarray sum, 2D all work Monotonic stack: next greater, histogram, temperatures all work All client code runs successfully Pattern Recognition Can identify when to use bit manipulation Recognize interval problems Know when prefix sum helps Identify monotonic stack opportunities Problem Solving Solved 4 easy problems Solved 6-8 medium problems Analyzed time/space complexity Understood when each pattern is optimal Understanding Filled in all ELI5 explanations Built decision trees for each pattern Identified when NOT to use each pattern Can explain trade-offs Mastery Check Could implement all patterns from memory Could recognize pattern in new problem Could explain to someone else Understand why these techniques are \"advanced\"","title":"Review Checklist"},{"location":"systems/01-storage-engines/","text":"Storage Engines \u00b6 B+Trees vs LSM Trees - The foundation of all database decisions Important notes \u00b6 B+Trees Leaf nodes have equal keys and values, Internal nodes have one extra value since keys are signposts (or \" bouncers\"). Their split logic diffs slightly as a result. LSM Trees Writes are more efficient due to batched sequential writes to SSTables, reducing write amplification ~100x compared to B+Trees (which rewrite entire 4KB pages per insert, even if inserting just 10B) // Binary search result is tricky // When not found: `result = -insertionPoint - 1` // and extraction is: `insertionPoint = -result - 1` int result = Collections.binarySearch(List.of(\"a\", \"b\"), \"c\"); if (result > 0) { int foundIndex = result; } else { int insertionPoint = -result - 1; } ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing and testing both storage engines, explain them simply. Prompts to guide you: What is a B+Tree in one sentence? Your answer: A read-optimised data structure enabling fast lookups (N-ary tree) and range queries (linked leaves) by trading off slower writes (random disk access) Why do databases use B+Trees? Your answer: It enables fast point and range queries Real-world analogy for B+Tree: Your analogy: A B+Tree is like a library - you organise your books into shelves, bays, and stacks. It's fast to find your book but it takes longer to reshelve since you have to randomly walk around What is an LSM Tree in one sentence? Your answer: A write-optimised data structure enabling fast writes (memtable, sstable merge, sequential disk access) by trading off slow reads (sstable iteration) Why do write-heavy databases use LSM Trees? Your answer: To avoid B+Trees' random write bottleneck at high volume Real-world analogy for LSM Tree: Your analogy: An LSM Tree is like a 2nd hand bookstore - you sort a few donations, box them and put them in the back. You can accept lots of books quickly but finding means searching heaps of boxes Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 B+Tree insert operation: Time complexity: O(log(n)) Verified after implementation: O(log(n)) LSM Tree write operation (to MemTable): Time complexity: O(log(n)) Space complexity: O(1) Verified: Time O(1) - technically O(log(memtable_height)) but height is fixed. Space O(levels) Performance calculation: For 100,000 writes, B+Tree = 1,000,000 operations (if log base is 10) For 100,000 writes, LSM Tree = 200,000 operations (before flush) Speedup factor for writes: LSM is approximately 5 times faster Scenario Predictions \u00b6 Scenario 1: Time-series metrics database (1M writes/second, rare reads) Best storage engine? LSM Tree Key consideration: Write amplification Why this choice? LSM's are better at writes I guess Scenario 2: E-commerce inventory system (100k reads/sec, 5k writes/sec) Best storage engine? B+Tree What pattern benefits most? Point lookups Scenario 3: Social media analytics (read historical posts by date range) Which handles range queries better? B+Tree Key data structure feature: Linked leaves Trade-off Quiz \u00b6 Question: When would B+Tree be BETTER than LSM Tree despite slower writes? Your answer: When you need range queries? Verified answer: High read ratio Question: What's the MAIN advantage of LSM Trees for writes? No tree balancing required on each write Better space efficiency Faster range queries Lower read amplification Verify after implementation: Yes no tree balancing Question: What happens if you never compact an LSM Tree? Your prediction: Fragmentation or something Verified: Reads slow to a crawl Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized storage approaches to understand the trade-offs. Example: Write-Heavy Workload \u00b6 Problem: Insert 10,000 key-value pairs as quickly as possible. Approach 1: B+Tree (Immediate Persistence) \u00b6 // Every write requires tree traversal and potential rebalancing BPlusTree<Integer, String> btree = new BPlusTree<>(128); long start = System.nanoTime(); for (int i = 0; i < 10000; i++) { btree.insert(i, \"Value\" + i); // Each insert: O(log N) // Must traverse tree from root to leaf // May trigger node splits (expensive) // Must maintain tree balance property } long duration = System.nanoTime() - start; Analysis: Time: O(N log N) - Each insert is O(log N) Space: O(N) - Tree structure overhead For 10,000 inserts: ~10,000 * log(10,000) = ~130,000 operations Write amplification: High (each insert may split nodes, update parent pointers) Approach 2: LSM Tree (Buffered Writes) \u00b6 // Writes go to in-memory MemTable (just a TreeMap insert) LSMTree<Integer, String> lsm = new LSMTree<>(100); long start = System.nanoTime(); for (int i = 0; i < 10000; i++) { lsm.put(i, \"Value\" + i); // Each put: O(log M), M = memTable size // Only updates in-memory TreeMap // Occasional flush to disk (batched) } long duration = System.nanoTime() - start; Analysis: Time: O(N log M) where M << N (M = MemTable size) Space: O(N) - Eventually flushes to SSTables For 10,000 inserts: ~10,000 * log(100) = ~20,000 operations Write amplification: Lower (batch writes to disk) Performance Comparison \u00b6 Operation Count B+Tree (O(N log N)) LSM Tree (O(N log M)) LSM Advantage N = 1,000 ~10,000 ops ~2,000 ops 5x faster N = 10,000 ~130,000 ops ~20,000 ops 6.5x faster N = 100,000 ~1,700,000 ops ~200,000 ops 8.5x faster Your calculation: For N = 50,000 writes, LSM Tree is approximately 7 times faster. Why Does LSM Win for Writes? \u00b6 Key insight to understand: B+Tree: Every insert = tree traversal + potential split Insert key=50: 1. Traverse root \u2192 internal \u2192 leaf (3 disk seeks) 2. Insert in leaf (sorted position) 3. If leaf full, split node (expensive) 4. Update parent pointers (more writes) Result: 1 logical write = 4-5 physical writes (write amplification!) LSM Tree: Batched sequential writes Insert key=50: 1. Insert into MemTable (in-memory TreeMap) 2. When MemTable full, flush entire batch to SSTable 3. Sequential write to disk (very fast) Result: 1 logical write = 1 in-memory write (occasionally batched to disk) After implementing, explain in your own words: Why does B+Tree require more writes per operation? [Your answer] How does LSM Tree achieve better write throughput? [Your answer] What's the trade-off for read performance? [Your answer] Example: Read-Heavy Workload \u00b6 Problem: Perform 1,000 random lookups after loading 10,000 records. Approach 1: B+Tree (Single Location Read) \u00b6 BPlusTree<Integer, String> btree = new BPlusTree<>(128); // Load data... long start = System.nanoTime(); for (int i = 0; i < 1000; i++) { int key = random.nextInt(10000); String value = btree.search(key); // Single tree traversal: O(log N) // Root \u2192 Internal \u2192 Leaf (3-4 hops) } long duration = System.nanoTime() - start; Analysis: Time: O(log N) per read For 1,000 reads: ~1,000 * log(10,000) = ~13,000 operations Read amplification: Low (single path through tree) Approach 2: LSM Tree (Multiple Location Read) \u00b6 LSMTree<Integer, String> lsm = new LSMTree<>(100); // Load data... (creates multiple SSTables) long start = System.nanoTime(); for (int i = 0; i < 1000; i++) { int key = random.nextInt(10000); String value = lsm.get(key); // Check MemTable + all SSTables // Must check MemTable (O(log M)) // Then check SSTable-5 (O(log S)) // Then check SSTable-4 (O(log S)) // ... continue until found } long duration = System.nanoTime() - start; Analysis: Time: O(log M + K * log S) where K = number of SSTables For 1,000 reads with 10 SSTables: ~1,000 * (10 * log(1000)) = ~100,000 operations Read amplification: High (must check multiple locations) Performance Comparison \u00b6 SSTable Count B+Tree (O(log N)) LSM Tree (O(K * log S)) B+Tree Advantage K = 1 ~13 ops/read ~13 ops/read ~1x (equal) K = 5 ~13 ops/read ~65 ops/read 5x faster K = 10 ~13 ops/read ~130 ops/read 10x faster Your calculation: With 20 SSTables, B+Tree is approximately 20 times faster for reads. Key insight: This is why LSM Trees need compaction - to reduce SSTable count! After benchmarking, fill in: What happens to LSM read performance as SSTables accumulate? [Your answer] Why doesn't B+Tree have this problem? [Your answer] How does compaction help LSM Trees? [Your answer] Case Studies: Storage Engines in the Wild \u00b6 MySQL (InnoDB): The B+Tree Workhorse \u00b6 Engine: InnoDB, the default storage engine for MySQL. Pattern: B+Tree. How it works: InnoDB uses a B+Tree for its primary key index, which is a clustered index . This means the table data itself is stored in the leaf nodes of the B+Tree, physically ordered by the primary key. This makes primary key lookups and range scans extremely fast. Key Takeaway: B+Trees are the default choice for general-purpose OLTP databases like MySQL that require strong consistency, fast point lookups, and efficient range queries (e.g., fetching users in a specific ID range). The trade-off is higher write amplification, as in-place updates can cause page splits. Apache Cassandra: LSM Trees for Write-Heavy Scale \u00b6 Engine: Apache Cassandra. Pattern: Log-Structured Merge-Tree (LSM Tree). How it works: Writes are first appended to a commit log and then written to an in-memory memtable . When the memtable is full, it's flushed to disk as an immutable SSTable . Reads must check the memtable and potentially multiple SSTables . Compaction processes merge SSTables in the background to improve read performance. Key Takeaway: Cassandra is built for massive write throughput and high availability. By turning random writes into sequential appends, LSM Trees are perfect for write-heavy workloads like time-series data, IoT metrics, and logging systems, at the cost of higher read latency and eventual consistency. RocksDB: The Embedded LSM Engine \u00b6 Engine: RocksDB, an embeddable key-value store developed by Facebook. Pattern: LSM Tree. How it works: RocksDB provides an LSM-based storage engine library that other databases can build on top of. It manages memtables , SSTables , and compaction, offering tunable performance for different workloads. Key Takeaway: The LSM Tree pattern is so powerful that it's used as a foundational component in many modern distributed databases like CockroachDB, TiDB, and YugabyteDB. It provides a robust, high-performance engine for handling state in a distributed environment. Core Implementation \u00b6 Part 1: B+Tree \u00b6 Your task: Implement a simplified in-memory B+Tree. import java.util.*; /** * B+Tree: Self-balancing tree optimized for range queries * * Properties: * - All values stored in leaf nodes * - Leaves are linked (for range scans) * - Height kept minimal * - Order K: max K keys per node */ public class BPlusTree<K extends Comparable<K>, V> { private final int order; // Max keys per node (e.g., 4) private Node root; // Base node class abstract class Node { List<K> keys = new ArrayList<>(order); Node parent; } /** * Represents an internal node (hallway) that directs keys to the correct leaf (room). * <h3>The Club Analogy</h3> * Think of this node as a hallway with \"Bouncers\" (Keys) and \"Rooms\" (Children). * <ul> * <li><b>Bouncers (keys):</b> Set the age limit for the room to their right.</li> * <li><b>Rooms (children):</b> There is always one more room than there are bouncers.</li> * </ul> * <pre> * (18) (21) (50) * [ Bouncer 0 ] [ Bouncer 1 ] [ Bouncer 2 ] * / \\ / \\ / \\ * [ Room 0 ] [ Room 1 ] [ Room 2 ] [ Room 3 ] * (Kids) (18+ Only) (21+ Only) (50+ Only) * </pre> * <h3>Navigation via Binary Search</h3> * To find the correct room index, we look for the <b>first bouncer strictly greater</b> * than our search key. * <p>Using {@link java.util.Collections#binarySearch(List, Object)}:</p> * <ul> * <li>If an exact match is found (index {@code i}): You are equal to the bouncer. * Per the \"inclusive-right\" rule, you move to the room to their right: {@code i + 1}.</li> * <li>If no match is found: The method returns {@code -(insertionPoint + 1)}. * The {@code insertionPoint} is the index of the first bouncer strictly greater * than you. You belong in the room at that same index.</li> * </ul> */ class InternalNode extends Node { List<Node> children = new ArrayList<>(order); } // Leaf nodes: have values, no children, linked to prev and next leaf class LeafNode extends Node { List<V> values = new ArrayList<>(order); LeafNode next; // For range scans } public BPlusTree(int order) { this.order = order; this.root = new LeafNode(); } /** * Insert key-value pair * Time: O(log N) * * <pre> * 1. Find correct leaf node * 2. Insert in sorted position * 3. If leaf overflows, split it * 4. Propagate split up the tree * </pre> */ public void insert(K key, V value) { // Start by finding the leaf LeafNode leaf = findLeaf(key); int result = Collections.binarySearch(leaf.keys, key); if (result >= 0) { // key exists, overwrite data int existingKeyIndex = result; leaf.values.set(existingKeyIndex, value); return; } int insertionIndex = -result - 1; leaf.keys.add(insertionIndex, key); leaf.values.add(insertionIndex, value); // If full, split the leaf, then potentially split up index nodes if they're too full if (leaf.keys.size() > order) { splitLeaf(leaf); if (leaf == root) { root = leaf.parent; } // leaf split may have split the parent, so recursively split up InternalNode parent = (InternalNode) leaf.parent; while (parent != null && parent.keys.size() > order) { splitInternal(parent); // If you just split the root and created a new root, update this class state if (parent == root) { root = parent.parent; } parent = (InternalNode) parent.parent; } } } /** * Search for a key * Time: O(log N) * <p> * 1. Start at root * 2. At each internal node, find correct child * 3. At leaf, search for key */ public V search(K key) { LeafNode leaf = findLeaf(key); int result = Collections.binarySearch(leaf.keys, key); if (result < 0) { return null; } return leaf.values.get(result); } /** * Range query: all values where startKey <= key <= endKey * Time: O(log N + results) * <p> * 1. Find leaf containing startKey * 2. Follow leaf.next pointers * 3. Collect values until endKey */ public List<V> rangeQuery(K startKey, K endKey) { List<V> results = new ArrayList<>(); LeafNode leaf = findLeaf(startKey); while (leaf != null) { for (int i = 0; i < leaf.keys.size(); i++) { K key = leaf.keys.get(i); if (key.compareTo(endKey) > 0) return results; // Past end if (key.compareTo(startKey) >= 0) { // In range results.add(leaf.values.get(i)); } } leaf = leaf.next; } return results; } /** * Helper: Find the leaf node where key should be */ private LeafNode findLeaf(K key) { Node current = root; // At each internal node, pick the correct child // Stop when you reach a leaf while (current instanceof InternalNode internalNode) { int result = Collections.binarySearch(internalNode.keys, key); // special java binary search return logic int childIndex; if (result < 0) { // Not found - happy case! /* Given [10,100,1000] 5 has an insertionIndex of 0 50 has an insertionIndex of 1 500 has an insertionIndex of 2 5000 has an insertionIndex of 3 Which is the same as the b+tree child index! */ int insertionIndex = -result - 1; childIndex = insertionIndex; } else { // Found - OOBE case /* Given [10,100,1000] 100 will be found at key index 1. But because of the way b+trees work, we know this key resides in child index 2 */ childIndex = result + 1; } current = internalNode.children.get(childIndex); } return (LeafNode) current; } /** * Helper: Split a full leaf node * and add to the parent (creating a new parent if necessary) */ private void splitLeaf(LeafNode leaf) { // Build the new node and update pointers of siblings int splitIndex = leaf.keys.size() / 2; LeafNode newRightLeaf = new LeafNode(); newRightLeaf.keys.addAll(leaf.keys.subList(splitIndex, leaf.keys.size())); newRightLeaf.values.addAll(leaf.values.subList(splitIndex, leaf.values.size())); newRightLeaf.next = leaf.next; leaf.next = newRightLeaf; // Insert into parent K promotedKey = leaf.keys.get(splitIndex); insertIntoParent(leaf, promotedKey, newRightLeaf); // Clean up old node leaf.keys.subList(splitIndex, leaf.keys.size()).clear(); leaf.values.subList(splitIndex, leaf.values.size()).clear(); } /** * Helper: Split a full internal node * and add to the parent (creating a new parent if necessary) */ private void splitInternal(InternalNode node) { // Build the new node and update pointers of its children int promotedKeyIndex = node.keys.size() / 2; InternalNode newRightNode = new InternalNode(); newRightNode.keys.addAll(node.keys.subList(promotedKeyIndex + 1, node.keys.size())); newRightNode.children.addAll(node.children.subList(promotedKeyIndex + 1, node.children.size())); for (Node child : newRightNode.children) { child.parent = newRightNode; } // Insert into parent K promotedKey = node.keys.get(promotedKeyIndex); insertIntoParent(node, promotedKey, newRightNode); // Clean up old node node.keys.subList(promotedKeyIndex, node.keys.size()).clear(); node.children.subList(promotedKeyIndex + 1, node.children.size()).clear(); } /** * Helper: Insert promoted key and right child into parent node * Creates new parent if necessary */ private void insertIntoParent(Node leftNode, K promotedKey, Node rightNode) { InternalNode parent = (InternalNode) leftNode.parent; // Ensure parent exists if (parent == null) { parent = new InternalNode(); leftNode.parent = parent; parent.children.add(leftNode); } // Insert promoted key and new right child rightNode.parent = parent; int keyInsertionPoint = -Collections.binarySearch(parent.keys, promotedKey) - 1; parent.keys.add(keyInsertionPoint, promotedKey); parent.children.add(keyInsertionPoint + 1, rightNode); } /** * Print tree structure (for debugging) */ public void printTree() { printNode(root, 0); } private void printNode(Node node, int level) { String indent = \" \".repeat(level); System.out.println(indent + \"Level \" + level + \": \" + node.keys); if (node instanceof InternalNode internal) { for (Node child : internal.children) { printNode(child, level + 1); } } } } Runnable Client Code: public class BPlusTreeClient { public static void main(String[] args) { // Create B+Tree with order 4 (max 4 keys per node) BPlusTree<Integer, String> tree = new BPlusTree<>(4); System.out.println(\"=== B+Tree Demo ===\\n\"); // Test 1: Sequential inserts System.out.println(\"Inserting keys 1-20...\"); for (int i = 1; i <= 20; i++) { tree.insert(i, \"Value\" + i); } System.out.println(\"\\nTree structure:\"); tree.printTree(); // Test 2: Point lookups System.out.println(\"\\n--- Point Lookups ---\"); System.out.println(\"Search(10): \" + tree.search(10)); System.out.println(\"Search(15): \" + tree.search(15)); System.out.println(\"Search(100): \" + tree.search(100)); // Not found // Test 3: Range queries System.out.println(\"\\n--- Range Queries ---\"); List<String> range = tree.rangeQuery(5, 10); System.out.println(\"Range [5, 10]: \" + range); range = tree.rangeQuery(15, 18); System.out.println(\"Range [15, 18]: \" + range); // Test 4: Random inserts System.out.println(\"\\n--- Random Inserts ---\"); BPlusTree<Integer, String> tree2 = new BPlusTree<>(4); int[] randomKeys = {45, 12, 67, 23, 89, 34, 56, 78, 90, 1}; System.out.println(\"Inserting random keys...\"); for (int key : randomKeys) { tree2.insert(key, \"Val\" + key); } tree2.printTree(); // Test 5: Verify sorted order System.out.println(\"\\n--- Verify Sorted Order ---\"); List<String> allValues = tree2.rangeQuery(0, 100); System.out.println(\"All values in order: \" + allValues); } } Part 2: LSM Tree \u00b6 Your task: Implement a simplified LSM Tree with MemTable and SSTables. import java.util.*; import static java.util.Comparator.comparingLong; /** * LSM Tree: Log-Structured Merge Tree optimized for writes * <p> * Architecture: * - Writes go to in-memory MemTable (sorted) * - When MemTable full, flush to SSTable (immutable file) * - Reads check MemTable, then SSTables (newest first) * - Periodically compact SSTables (merge and remove duplicates) */ public class LSMTree<K extends Comparable<K>, V> { private final int memTableSize; // Max entries before flush private TreeMap<K, V> memTable; // In-memory sorted map private List<SSTable<K, V>> sstables; // On-disk sorted tables /** * SSTable: Sorted String Table (immutable) * In production: stored on disk * Here: simplified in-memory representation */ static class SSTable<K extends Comparable<K>, V> { private final SortedMap<K, V> data; private final long timestamp; // When created (for ordering) SSTable(SortedMap<K, V> data) { this.data = Collections.unmodifiableSortedMap(new TreeMap<>(data)); this.timestamp = System.currentTimeMillis(); } V get(K key) { return data.get(key); } boolean containsKey(K key) { return data.containsKey(key); } Set<Map.Entry<K, V>> entrySet() { return data.entrySet(); } int size() { return data.size(); } } public LSMTree(int memTableSize) { this.memTableSize = memTableSize; this.memTable = new TreeMap<>(); this.sstables = new ArrayList<>(); } /** * Insert/Update key-value pair * Time: O(log M) where M = memTable size * <p> * 1. Insert into MemTable * 2. If MemTable full, flush to SSTable */ public void put(K key, V value) { memTable.put(key, value); if (memTable.size() > memTableSize) { flush(); } } /** * Retrieve value for key * Time: O(log M + N*log S) where N = number of SSTables, S = SSTable size * <p> * 1. Check MemTable first (most recent) * 2. Check SSTables in reverse order (newest first) * 3. Return first match */ public V get(K key) { // Check memTable first if (memTable.containsKey(key)) { return memTable.get(key); } // Check SSTables from newest to oldest for (int i = sstables.size() - 1; i >= 0; i--) { SSTable<K, V> table = sstables.get(i); if (table.containsKey(key)) { return table.get(key); } } return null; // Not found } /** * Flush MemTable to SSTable (simulate disk write) */ private void flush() { sstables.add(new SSTable<>(memTable)); memTable.clear(); System.out.println(\"Flushed MemTable to SSTable. Total SSTables: \" + sstables.size()); } /** * Compact SSTables: Merge multiple tables, remove duplicates * Time: O(N * S * log S) where N = tables, S = size * <p> * 1. Merge all SSTables * 2. For duplicate keys, keep newest value * 3. Create new compacted SSTable */ public void compact() { int beforeSize = sstables.size(); if (beforeSize <= 1) { return; // Nothing to compact } // Iterate through SSTables from oldest to newest // Later values overwrite earlier ones (keep newest) TreeMap<K, V> merged = new TreeMap<>(); sstables.stream() .sorted(comparingLong(sst -> sst.timestamp)) .forEach(sst -> merged.putAll(sst.data)); // Replace old SSTables with compacted one sstables.clear(); sstables.add(new SSTable<>(merged)); System.out.println(\"Compacted \" + beforeSize + \" SSTables into 1\"); } /** * Print current state */ public void printState() { System.out.println(\"MemTable size: \" + memTable.size()); System.out.println(\"SSTables: \" + sstables.size()); for (int i = 0; i < sstables.size(); i++) { System.out.println(\" SSTable \" + i + \": \" + sstables.get(i).size() + \" entries\"); } } } Runnable Client Code: public class LSMTreeClient { public static void main(String[] args) { // Create LSM Tree with memTable size = 5 LSMTree<Integer, String> lsm = new LSMTree<>(5); System.out.println(\"=== LSM Tree Demo ===\\n\"); // Test 1: Sequential writes (triggers flush) System.out.println(\"--- Test 1: Sequential Writes ---\"); for (int i = 1; i <= 12; i++) { lsm.put(i, \"Value\" + i); System.out.println(\"Put(\" + i + \", Value\" + i + \")\"); } System.out.println(\"\\nState after 12 inserts:\"); lsm.printState(); // Test 2: Read values System.out.println(\"\\n--- Test 2: Reads ---\"); System.out.println(\"Get(5): \" + lsm.get(5)); // In SSTable System.out.println(\"Get(11): \" + lsm.get(11)); // In MemTable System.out.println(\"Get(100): \" + lsm.get(100)); // Not found // Test 3: Update existing keys System.out.println(\"\\n--- Test 3: Updates ---\"); lsm.put(5, \"UpdatedValue5\"); lsm.put(11, \"UpdatedValue11\"); System.out.println(\"Get(5) after update: \" + lsm.get(5)); System.out.println(\"Get(11) after update: \" + lsm.get(11)); // Test 4: Trigger more flushes System.out.println(\"\\n--- Test 4: More Writes ---\"); for (int i = 20; i <= 35; i++) { lsm.put(i, \"Value\" + i); } lsm.printState(); // Test 5: Compaction System.out.println(\"\\n--- Test 5: Compaction ---\"); lsm.compact(); lsm.printState(); // Test 6: Verify reads after compaction System.out.println(\"\\n--- Test 6: Verify After Compaction ---\"); System.out.println(\"Get(5): \" + lsm.get(5)); System.out.println(\"Get(25): \" + lsm.get(25)); } } Part 3: Benchmark Comparison \u00b6 Your task: Compare B+Tree vs LSM Tree performance. public class StorageBenchmark { public static void main(String[] args) { System.out.println(\"=== Storage Engine Benchmark ===\\n\"); benchmarkWrites(); System.out.println(); benchmarkReads(); System.out.println(); benchmarkMixed(); } static void benchmarkWrites() { System.out.println(\"--- Write Performance ---\"); int numWrites = 10000; // B+Tree writes BPlusTree<Integer, String> btree = new BPlusTree<>(128); long start = System.nanoTime(); for (int i = 0; i < numWrites; i++) { btree.insert(i, \"Value\" + i); } long btreeTime = System.nanoTime() - start; // LSM Tree writes LSMTree<Integer, String> lsm = new LSMTree<>(100); start = System.nanoTime(); for (int i = 0; i < numWrites; i++) { lsm.put(i, \"Value\" + i); } long lsmTime = System.nanoTime() - start; System.out.printf(\"B+Tree: %.2f ms (%.0f writes/sec)%n\", btreeTime / 1e6, numWrites / (btreeTime / 1e9)); System.out.printf(\"LSM Tree: %.2f ms (%.0f writes/sec)%n\", lsmTime / 1e6, numWrites / (lsmTime / 1e9)); System.out.printf(\"LSM is %.2fx faster for writes%n\", (double) btreeTime / lsmTime); } static void benchmarkReads() { System.out.println(\"--- Read Performance ---\"); int numEntries = 10000; int numReads = 1000; // Setup B+Tree BPlusTree<Integer, String> btree = new BPlusTree<>(128); for (int i = 0; i < numEntries; i++) { btree.insert(i, \"Value\" + i); } // Setup LSM Tree LSMTree<Integer, String> lsm = new LSMTree<>(100); for (int i = 0; i < numEntries; i++) { lsm.put(i, \"Value\" + i); } // Benchmark reads Random rand = new Random(42); long start = System.nanoTime(); for (int i = 0; i < numReads; i++) { int key = rand.nextInt(numEntries); btree.search(key); } long btreeTime = System.nanoTime() - start; rand = new Random(42); // Same sequence start = System.nanoTime(); for (int i = 0; i < numReads; i++) { int key = rand.nextInt(numEntries); lsm.get(key); } long lsmTime = System.nanoTime() - start; System.out.printf(\"B+Tree: %.2f ms (%.0f reads/sec)%n\", btreeTime / 1e6, numReads / (btreeTime / 1e9)); System.out.printf(\"LSM Tree: %.2f ms (%.0f reads/sec)%n\", lsmTime / 1e6, numReads / (lsmTime / 1e9)); System.out.printf(\"B+Tree is %.2fx faster for reads%n\", (double) lsmTime / btreeTime); } static void benchmarkMixed() { System.out.println(\"--- Mixed Workload (50% reads, 50% writes) ---\"); int numWrites = 1000; int numReads = 1000; // Benchmark read-writes Random rand = new Random(42); BPlusTree<Integer, String> btree = new BPlusTree<>(128); long start = System.nanoTime(); for (int i = 0; i < numWrites; i++) { btree.insert(i, \"Value\" + i); } for (int i = 0; i < numReads; i++) { int key = rand.nextInt(numWrites); btree.search(key); } long btreeTime = System.nanoTime() - start; rand = new Random(42); // Same sequence LSMTree<Integer, String> lsm = new LSMTree<>(100); start = System.nanoTime(); for (int i = 0; i < numWrites; i++) { lsm.put(i, \"Value\" + i); } for (int i = 0; i < numReads; i++) { int key = rand.nextInt(numWrites); lsm.get(key); } long lsmTime = System.nanoTime() - start; System.out.printf(\"B+Tree: %.2f ms (%.0f reads/sec, %.0f writes/sec)%n\", btreeTime / 1e6, numReads / (btreeTime / 1e9), numWrites / (btreeTime / 1e9)); System.out.printf(\"LSM Tree: %.2f ms (%.0f reads/sec, %.0f writes/sec)%n\", lsmTime / 1e6, numReads / (lsmTime / 1e9), numWrites / (lsmTime / 1e9)); } } Must complete: Implement B+Tree insert, search, rangeQuery Implement LSM Tree put, get, flush, compact Run both client programs successfully Run benchmark and record results Understand WHY each performs better in different scenarios Your benchmark results: Metric B+Tree LSM Tree Winner Write Performance (ms) 4180.07 ms 70.15 ms LSM Tree faster by 59.59x Write Throughput (ops/sec) 2392 writes/sec 142560 writes/sec Read Performance (ms) 391.13 ms 6703.97 ms B+Tree by 17.14x Read Throughput (ops/sec) 2557 reads/sec 149 reads/sec Key insight: LSM Tree writes are more efficient due to batched sequential writes to SSTables, reducing write amplification ~100x compared to B+Trees (which rewrite entire 4KB pages per insert, even if inserting just 10B). B+Tree reads are faster with no read amplification \u2014 single location lookup vs LSM's MemTable + K SSTables check. Debugging Challenges \u00b6 Your task: Find and fix bugs in broken storage engine implementations. This tests your deep understanding. Challenge 1: Broken B+Tree Leaf Split \u00b6 /** * This splitLeaf method is supposed to split a full leaf node. * It has 3 CRITICAL BUGS. Find them! */ private void splitLeaf(LeafNode leaf) { LeafNode newLeaf = new LeafNode(); int midpoint = leaf.keys.size() / 2; // Move half the keys/values to new leaf for (int i = midpoint; i < leaf.keys.size(); i++) { newLeaf.keys.add(leaf.keys.get(i)); newLeaf.values.add(leaf.values.get(i)); } for (int i = midpoint; i < leaf.keys.size(); i++) { leaf.keys.remove(i); leaf.values.remove(i); } // newLeaf.next = ??? InternalNode parent = (InternalNode) leaf.parent; parent.keys.add(newLeaf.keys.get(0)); parent.children.add(newLeaf); } Your debugging: Bug 1: Removing elements from the middle causes index issues (should remove from end) Bug 2: Need to connect next pointers correctly Bug 3: parent may not exist, need to create and also insert in specific position in parent (it's ordered) Click to verify your answers Bug 1 (Lines 13-16): Removing elements while iterating forward breaks indices. Each removal shifts remaining elements left. Fix: // Remove from end to avoid index shifting for (int i = leaf.keys.size() - 1; i >= midpoint; i--) { leaf.keys.remove(i); leaf.values.remove(i); } // OR use subList: leaf.keys.subList(midpoint, leaf.keys.size()).clear(); leaf.values.subList(midpoint, leaf.values.size()).clear(); Bug 2 (After line 11): Must link new leaf into the leaf chain for range queries! Fix: newLeaf.next = leaf.next; // New leaf points to old leaf's next leaf.next = newLeaf; // Old leaf points to new leaf Bug 3 (Lines 22-23): If leaf is root, parent is null - NullPointerException! Fix: if (leaf.parent == null) { // Create new root InternalNode newRoot = new InternalNode(); newRoot.keys.add(newLeaf.keys.get(0)); newRoot.children.add(leaf); newRoot.children.add(newLeaf); leaf.parent = newRoot; newLeaf.parent = newRoot; root = newRoot; } else { InternalNode parent = (InternalNode) leaf.parent; // Insert key in sorted position (not just add!) // Insert newLeaf in corresponding position } Challenge 2: Broken LSM Tree Compaction \u00b6 /** * Compact all SSTables into one. * This has 2 LOGIC BUGS that cause data loss and incorrect ordering. */ public void compact() { if (sstables.size() <= 1) return; TreeMap<K, V> merged = new TreeMap<>(); for (int i = sstables.size() - 1; i >= 0; i--) { SSTable<K, V> table = sstables.get(i); for (Map.Entry<K, V> entry : table.entrySet()) { merged.put(entry.getKey(), entry.getValue()); } } // Create compacted SSTable SSTable<K, V> compacted = new SSTable<>(merged); sstables.add(compacted); System.out.println(\"Compacted into 1 SSTable\"); } Your debugging: Bug 1: Should iterate from oldest to newest (using timestamp) to avoid getting old data Bug 2: Need to clear sstables before adding to it Click to verify your answers Bug 1 (Line 9): Iterating from newest to oldest means older values overwrite newer ones! LSM Trees must keep the NEWEST value for each key. By iterating newest-to-oldest and using put() , when we encounter the key again in an older SSTable, it overwrites the newer value. Fix: // Iterate from OLDEST to NEWEST for (int i = 0; i < sstables.size(); i++) { SSTable<K, V> table = sstables.get(i); for (Map.Entry<K, V> entry : table.entrySet()) { merged.put(entry.getKey(), entry.getValue()); // Later puts overwrite earlier } } Bug 2 (Line 18): We add the compacted SSTable but never remove the old ones! Memory leak! Fix: sstables.clear(); // Remove all old SSTables sstables.add(compacted); // Add compacted one Challenge 3: B+Tree Search with Wrong Child Selection \u00b6 /** * Find the leaf node where key should be located. * This has 1 SUBTLE BUG in binary search logic. */ private LeafNode findLeaf(K key) { Node current = root; while (!current.isLeaf()) { InternalNode internal = (InternalNode) current; int i = 0; while (i < internal.keys.size() && key.compareTo(internal.keys.get(i)) > 0) { i++; } current = internal.children.get(i); } return (LeafNode) current; } Your debugging: Bug: Well firstly it's not binary search, it's just linear scan. But also It should be >= ( greater than or equal to) because an equal key should move on to the next room Trace through manually: Internal node: keys=[20], children=[ChildA, ChildB] ChildA contains: [10, 15] ChildB contains: [20, 25, 30] Search key=20: - Line 12: i=0, key(20) > keys[0](20)? NO - i stays 0 - Visit children[0] = ChildA - BUG: Key 20 is actually in ChildB! Click to verify your answer Bug (Line 12): Using > instead of >= causes keys equal to split points to go left when they should go right. B+Tree invariant: Internal node key K means \"left child < K, right child >= K\" Fix: while (i < internal.keys.size() && key.compareTo(internal.keys.get(i)) >= 0) { i++; } OR be more explicit: int i; for (i = 0; i < internal.keys.size(); i++) { if (key.compareTo(internal.keys.get(i)) < 0) { break; } } current = internal.children.get(i); Challenge 4: LSM Tree Missing Flush \u00b6 /** * This LSM Tree mysteriously loses data after many inserts. * Find the CRITICAL MISSING OPERATION. */ public class LSMTree<K extends Comparable<K>, V> { private TreeMap<K, V> memTable; private List<SSTable<K, V>> sstables; private final int memTableSize = 100; public void put(K key, V value) { memTable.put(key, value); if (memTable.size() >= memTableSize) { flush(); } } private void flush() { SSTable<K, V> newTable = new SSTable<>(memTable); sstables.add(newTable); System.out.println(\"Flushed to SSTable\"); } } Your debugging: Bug: Memtable is never cleared after flushing to the SSTable Click to verify your answer Bug (After line 20): We never clear the MemTable after flushing! Result: MemTable keeps growing forever. After first flush, memTable has 100 items. After second flush, it has 200. Eventually OutOfMemoryError. Also: Data gets duplicated across SSTables because we keep the same data in memory and keep flushing it again. Fix: private void flush() { SSTable<K, V> newTable = new SSTable<>(memTable); sstables.add(newTable); memTable.clear(); // CRITICAL: Clear for new writes! System.out.println(\"Flushed to SSTable\"); } OR initialize a new MemTable: memTable = new TreeMap<>(); Challenge 5: Range Query Doesn't Stop \u00b6 /** * B+Tree range query implementation. * This has 1 CRITICAL BUG causing incorrect results. */ public List<V> rangeQuery(K startKey, K endKey) { List<V> results = new ArrayList<>(); LeafNode leaf = findLeaf(startKey); // Traverse leaves until we exceed endKey while (leaf != null) { for (int i = 0; i < leaf.keys.size(); i++) { K key = leaf.keys.get(i); if (key.compareTo(startKey) >= 0) { results.add(leaf.values.get(i)); } } leaf = leaf.next; } return results; } Your debugging: Bug: We never check the endkey to start excluding results and end the traversal Click to verify your answer Bug (Lines 11-17): We check if key >= startKey but never check if key > endKey! Result: Range query returns ALL keys from startKey to the end of the tree, ignoring endKey. Fix: while (leaf != null) { for (int i = 0; i < leaf.keys.size(); i++) { K key = leaf.keys.get(i); if (key.compareTo(endKey) > 0) { return results; // Stop when we exceed endKey } if (key.compareTo(startKey) >= 0) { results.add(leaf.values.get(i)); } } leaf = leaf.next; } Optimization: Can also break out of outer loop: if (key.compareTo(endKey) > 0) { break; // Exit inner loop } // After inner loop: if (leaf.keys.size() > 0 && leaf.keys.get(leaf.keys.size()-1).compareTo(endKey) > 0) { break; // Exit outer loop } Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 9+ bugs across 5 challenges Understood WHY each bug causes data corruption or incorrect results Could explain the fix to someone else Learned common storage engine mistakes to avoid Common mistakes you discovered: [Index manipulation while iterating] [Missing pointer updates in tree structures] [Wrong iteration order in merge operations] [Forgetting to clear/reset data structures] [Incomplete boundary checks in range queries] Decision Framework \u00b6 Your task: Build decision trees for when to use each storage engine. Question 1: Write-heavy or Read-heavy? \u00b6 Answer after implementing and benchmarking: My answer: Write-heavy needs LSM trees Why does this matter? B-Trees are slow in this use case due to write amplification Performance difference I observed: LSM trees heaps faster Question 2: Need range queries? \u00b6 Answer: Do B+Trees support range queries? Yes, leaf nodes are linked to each other Do LSM Trees support range queries? Yes technically but it's basically needing to go through the memtable and every single SSTable since data is clustered by write time instead of value and there's no linkage between leaf nodes Which is faster for range queries? B+Trees for sure Question 3: Sequential or random writes? \u00b6 Answer: B+Tree with random writes: Writing random data will cause write amplification and needing to seek to different parts of the disk LSM Tree with random writes: Best because random data is first stored in the memtable and then sequentially written to disk Your observation from implementation: LSM way faster Your Decision Tree \u00b6 Build this after understanding trade-offs: flowchart LR Start[\"Storage Engine Selection\"] Start --> Q0{\"Write volume<br/>extreme (>100K/sec)?\"} Q0 -->|\"YES\"| A0([\"LSM Tree \u2713<br/>B+Trees won't survive\"]) Q0 -->|\"NO\"| Q1{\"Write-heavy workload<br/>(>70% writes)?\"} Q1 -->|\"YES\"| Q2{\"Read pattern?\"} Q2 -->|\"Rare reads<br/>(write-dominated)\"| A1([\"LSM Tree \u2713<br/>Write optimization critical\"]) Q2 -->|\"Recent data<br/>(temporal locality)\"| A2([\"LSM Tree \u2713<br/>MemTable serves hot data\"]) Q2 -->|\"Random point<br/>lookups\"| A4([\"B+Tree likely better<br/>LSM needs SSTable search\"]) Q1 -->|\"NO\"| A6([\"B+Tree \u2713<br/>LSM read penalty not worth\"]) Practice \u00b6 Scenario 1: Social Media Posts Table \u00b6 Design storage for this table: CREATE TABLE posts ( id BIGINT PRIMARY KEY, user_id BIGINT, content TEXT, created_at TIMESTAMP, likes_count INT ); Queries: Q1: Get recent posts by user (sorted by created_at) Q2: Get top posts by likes in last 24 hours Q3: Insert new posts (10,000 posts/sec) Your design: Storage engine choice: B-Tree Reasoning: Write volume: 10,000 TPS Read patterns: Read-heavy use case (10:1) and range queries Your choice: B-Tree Index design: What indexes would you create? (user_id, created_at) and (created_at, likes_count) Why these specific indexes? To fulfill the two read patterns What's the column order and why? Filter columns come first, then sort columns after. More selective filters earlier Scenario 2: Time-Series Metrics \u00b6 Design storage for metrics data: CREATE TABLE metrics ( metric_name VARCHAR(100), timestamp TIMESTAMP, value DOUBLE, tags JSONB ); Access patterns: Writes: 1M data points/second Reads: Recent data (last 1 hour) queried frequently Older data rarely accessed Retention: 30 days Your design: Storage engine: LSM Tree Why? GT 70% write Mostly read recent data, more likely to stay in memtable Metrics are an append-only workload (no/few updates) Scenario 3: E-commerce Inventory \u00b6 CREATE TABLE inventory ( product_id BIGINT PRIMARY KEY, quantity INT, reserved INT, last_updated TIMESTAMP ); Access patterns: Reads: Very frequent (1M reads/sec) Writes: Updates when orders placed (10k writes/sec) Consistency: Critical (no overselling) Your design: Storage engine: B-Tree Trade-offs you considered: High read frequency compared to writes Read patterns -> mostly point lookups (LSM Trees are slower due to checking multiple SSTables. LSM trees could do recent data queries or full table scans faster) Update-heavy workload ((LSM Trees' write advantage lessens for updates vs inserts, because updates still need to search existing data first) Review Checklist \u00b6 Before moving to the next topic: Implementation B+Tree insert, search, range query work correctly LSM Tree put, get, flush, compact work correctly All client code runs without errors Benchmarks completed and results recorded Understanding Can explain B+Tree in simple terms (filled ELI5) Can explain LSM Tree in simple terms (filled ELI5) Understand why writes are different speeds Understand why reads are different speeds Decision Making Built complete decision tree Solved all 3 practice scenarios Can justify each design choice Mastery Check Could implement both from memory Could explain trade-offs in an interview Know when to use each without looking at notes Mastery Certification \u00b6 I certify that I can: Implement B+Tree insert, search, and range query from memory Explain B+Tree node splitting and balancing Implement LSM Tree put, get, flush, and compact operations Explain write amplification in B+Trees vs LSM Trees Explain read amplification in LSM Trees Understand the role of compaction in LSM Trees Choose between B+Tree and LSM Tree for a given workload Explain when to use each storage engine Benchmark and analyze performance differences Debug common storage engine implementation issues Explain these concepts in a system design interview Teach these concepts to someone else APPENDIX: The Historical Evolution - From First Principles \u00b6 Why this appendix exists : The main chapter teaches B+Trees and LSM Trees side-by-side. But historically, B+Trees came first and dominated for 30 years. Understanding this evolution provides deeper intuition about why these designs exist. The Historical Truth \u00b6 1970s-2000s : If you said \"database storage engine,\" you meant B+Tree . Oracle, MySQL, PostgreSQL, SQL Server - all B+Trees Learned in every databases class The default, the standard, the only choice 2006 : Google's BigTable paper changes everything Describes LSM-style architecture for web-scale writes Solves write amplification problem in B+Trees 2008-2012 : NoSQL movement adopts LSM Trees Cassandra, HBase, RocksDB, LevelDB Narrative: \"B+Trees are old SQL. LSM Trees are modern NoSQL.\" Reality : Both solve the same problem (organizing data on disk) with different trade-offs. The Evolution: Starting from Absolute Zero \u00b6 Let's trace the path that led to these designs, starting from the simplest possible database. How to use this section: At each level, try to predict what will break before reading ahead. This builds the intuition for why each innovation was necessary. Level 0: Unsorted Append-Only File (Heap File) \u00b6 \u26a0\ufe0f STOP: Before reading the code below, predict the problem: You're building the simplest possible database. You decide to just append every write to a file. Prediction Challenge: What operation will become slow as the database grows? [Reads/Writes/Both?] Why? [Your reasoning] At what scale does it become unbearable? [100 records? 1M records?] Check your prediction Answer : Reads become O(N) - must scan entire file for every lookup. Math : With 1M records at 1 microsecond per comparison: Average search time: 500,000 comparisons = 500ms \ud83d\udc0c This is unacceptable for any interactive application (target: <100ms) // The simplest possible database public class SimpleDB { File dataFile; public void insert(K key, V value) { dataFile.append(key + \",\" + value + \"\\n\"); // O(1) - Fast! \u2713 } public V search(K key) { for (String line : dataFile.readAllLines()) { // O(N) - Slow! \u2717 if (line.startsWith(key)) { return parseValue(line); } } return null; } } Characteristics : \u2705 Writes: O(1) - just append \u2705 Simple to implement \u274c Reads: O(N) - must scan entire file \u274c Updates: O(N) - must scan to find, then append new version Problem : With 1 million records, every search reads 1 million lines. Unbearable. Level 1: Sorted File \u00b6 \u26a0\ufe0f STOP: Before reading the code below, predict the problem: We fixed reads by keeping the file sorted (enabling binary search). But what's the cost? Prediction Challenge: How does insert performance change? [Better/Worse/Same?] Why? [What must happen to maintain sorted order?] For 1M records, what's the average cost of one insert? [How many records moved?] Check your prediction Answer : Inserts become O(N) - must shift data to maintain sorted order. Math : With 1M records: Binary search finds position: log\u2082(1M) = 20 comparisons (fast) Shift half the file on average: 500,000 records moved Each record = 100 bytes \u2192 50MB rewritten per insert \ud83d\udc0c This is worse than the unsorted file! Trade-off discovered : Optimizing reads (sorting) made writes slower. // Keep file sorted by key - enables binary search public class SortedFileDB { File sortedFile; // Maintained in sorted order by key public void insert(K key, V value) { // 1. Binary search to find position: O(log N) int position = binarySearchFile(key); // 2. Shift everything after position right: O(N) \u2717 shiftDataRight(position); // 3. Insert new record writeAt(position, key, value); // Result: Must rewrite ~50% of file on average! } public V search(K key) { // Binary search through file: O(log N) \u2713 return binarySearchFile(key); } } Characteristics : \u2705 Reads: O(log N) - binary search \u274c Writes: O(N) - must shift data to maintain sort order \u274c Every insert rewrites half the file on average Problem : Writes went from O(1) to O(N). Optimization for reads broke writes. Level 2: Binary Search Tree (BST) - In-Memory \u00b6 \u26a0\ufe0f STOP: Before reading the code below, predict the problem: Binary Search Trees (BST) give us O(log N) for both reads and writes. Perfect! But what happens when we put this on disk? Prediction Challenge: How many disk seeks are needed to search a BST with 1M records? [Hint: What's the tree height?] If each disk seek takes 10ms, how long does one search take? [Calculate total time] What's inefficient about storing each BST node on disk? [Think about disk page sizes] Check your prediction Answer : BST creates a TALL tree with TINY nodes - terrible for disk I/O. Math : With 1M records: Tree height: log\u2082(1M) \u2248 20 levels Each level = 1 disk seek Disk seek time: ~10ms Total: 20 \u00d7 10ms = 200ms per query \ud83d\udc0c Inefficiency : BST node size: ~32 bytes (2 pointers + key + value) Disk page size: 4KB Wasting 99% of each disk read! Problem discovered : We need to pack more data per disk read to minimize seeks. class Node { K key; V value; Node left; // Pointer to left child Node right; // Pointer to right child } public class BSTDB { Node root; public void insert(K key, V value) { root = insertRec(root, key, value); // O(log N) average \u2713 } public V search(K key) { return searchRec(root, key); // O(log N) average \u2713 } } Looks great! Both reads and writes are O(log N). But... when you put this on disk: Problem 1: Each node is tiny Node size: 2 pointers (16 bytes) + key (8 bytes) + value (8 bytes) = ~32 bytes Disk page size: 4KB Wasting 99% of each disk read! Problem 2: Tree is TALL For 1M records: height = log\u2082(1M) \u2248 20 levels Each level = 1 disk seek Disk seek time on HDD: ~10ms Total: 20 seeks \u00d7 10ms = 200ms per query \ud83d\udc0c Problem 3: Can become unbalanced If inserts are sorted: tree becomes linked list O(log N) becomes O(N) worst case Level 3: B-Tree (1972) - The Breakthrough \u00b6 Key insight : Disk I/O is expensive. Minimize disk seeks by making nodes match disk page size. class BTreeNode { List<K> keys; // 100-1000 keys per node (depending on key size) List<Node> children; // 101-1001 children // Each node fits in one disk page (4KB) } Why this is revolutionary : Binary Search Tree (1M records): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Node size: 32 bytes \u2502 \u2502 Height: log\u2082(1,000,000) \u2248 20 \u2502 \u2502 Disk seeks per query: 20 \u2502 \u2502 Query time: 20 \u00d7 10ms = 200ms \u274c \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 B-Tree with order 100 (1M records): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Node size: 4KB (one disk page) \u2502 \u2502 Keys per node: ~100 \u2502 \u2502 Height: log\u2081\u2080\u2080(1,000,000) \u2248 3 \u2502 \u2502 Disk seeks per query: 3 \u2502 \u2502 Query time: 3 \u00d7 10ms = 30ms \u2713 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Result: 6-7x faster! The key insight : Each disk read should fetch as much useful data as possible. Wide nodes = short tree = fewer seeks. B-Tree characteristics : \u2705 Reads: O(log N) with minimal disk seeks \u2705 Writes: O(log N) with in-place updates \u2705 Self-balancing \u26a0\ufe0f Data scattered throughout tree (internal + leaf nodes) Level 4: B+Tree - Optimized for Range Queries \u00b6 Problem with B-Tree : Range queries are awkward. // Range query in B-Tree public List<V> rangeQuery(K start, K end) { List<V> results = new ArrayList<>(); // Problem: Data is scattered throughout the tree // Must do tree traversal for EACH key in range // Jumps around different levels - inefficient! for (K key = start; key <= end; key++) { results.add(search(key)); // Each search = tree traversal } return results; } B+Tree solution : All data in leaf nodes only (internal nodes = just keys for navigation) Link leaf nodes together (sequential list) class LeafNode { List<K> keys; List<V> values; LeafNode next; // \u2190 The magic pointer } public List<V> rangeQuery(K start, K end) { LeafNode leaf = findLeaf(start); // O(log N) to find start List<V> results = new ArrayList<>(); while (leaf != null) { for (int i = 0; i < leaf.keys.size(); i++) { K key = leaf.keys.get(i); if (key > end) return results; // Done if (key >= start) results.add(leaf.values.get(i)); } leaf = leaf.next; // \u2190 Sequential! Fast! } return results; } Result : Range queries are sequential after finding the start point. Perfect for databases. B+Trees dominated databases for 30+ years (1972-2000s). Oracle, MySQL, PostgreSQL, SQL Server - all B+Trees. The Problem That Necessitated LSM Trees \u00b6 The Google Problem (mid-2000s) \u00b6 Workload : Indexing the web for Google Search Write volume : Billions of page updates per day Write pattern : Mostly inserts (new pages discovered) Read pattern : Batch processing MapReduce jobs (can tolerate some latency) B+Tree performance breakdown : // Every insert in B+Tree requires: public void insert(K key, V value) { // 1. Read root page from disk (4KB) // 2. Read internal node from disk (4KB) // 3. Read leaf page from disk (4KB) // 4. Modify leaf (change maybe 100 bytes of data) // 5. Write entire leaf page back to disk (4KB) \u2190 WRITE AMPLIFICATION! // 6. If leaf is full, split it: // - Create new leaf page (4KB write) // - Update parent pointer (4KB read + 4KB write) // 7. May cascade up the tree // Result: 1 logical write (100 bytes) // = 5-10 physical I/O operations (20-40KB) // = 200-400x write amplification! } The math that broke B+Trees : 1 billion inserts per day 40KB average I/O per insert (due to write amplification) = 40TB of disk I/O per day Actual new data: ~100GB Write amplification: 400x Additional problem : B+Trees do random I/O (tree traversal jumps around disk) On spinning disks: random I/O = 100x slower than sequential I/O Random seeks kill throughput First Principles \u2192 LSM Tree \u00b6 Question : How can we optimize for massive write volume? Core Insight: Delay Sorting \u00b6 When do you pay the cost of sorting? \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u2502 On Every Write \u2502 \u2502 (B+Tree) \u2502 \u2502 vs \u2502 \u2502 In Batches \u2502 \u2502 (LSM Tree) \u2502 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 B+Tree philosophy : Keep data sorted all the time Insert cost: O(log N) - must maintain sort order immediately Read cost: O(log N) - data is always sorted LSM Tree philosophy : Sort in batches, not per-write Insert cost: O(log M) - just update in-memory buffer (M << N) Read cost: O(K \u00d7 log S) - check multiple sorted files Amortize sorting cost over many writes The LSM Tree Evolution \u00b6 Step 1: Recognize append-only is fastest // Fastest possible write: append to log public void write(K key, V value) { log.append(key + \",\" + value); // O(1), sequential I/O \u2713 } // But reads are O(N) - scan entire log \u2717 Step 2: Buffer writes in memory (sorted) TreeMap<K, V> memTable = new TreeMap<>(); // Sorted in memory public void write(K key, V value) { memTable.put(key, value); // O(log M) where M is small (e.g., 10K) // Fast! In-memory operation, no disk I/O } public V read(K key) { return memTable.get(key); // O(log M) } Benefit : Writes are fast (in-memory), data is sorted. Problem : Limited by RAM size. Need to flush to disk eventually. Step 3: Flush to sorted files periodically public void write(K key, V value) { memTable.put(key, value); if (memTable.size() >= threshold) { // Flush to disk as sorted file (SSTable) SSTable newTable = writeSSTable(memTable); // Sequential write - FAST! sstables.add(newTable); memTable.clear(); } } Benefit : Batched writes: 1000 inserts \u2192 1 sequential flush 100-1000x less I/O than B+Tree Sequential writes (fast on HDDs) Step 4: Read from multiple locations public V read(K key) { // Check memory first (most recent) if (memTable.containsKey(key)) { return memTable.get(key); } // Check on-disk files (newest to oldest) for (int i = sstables.size() - 1; i >= 0; i--) { SSTable table = sstables.get(i); V value = table.get(key); // Binary search in sorted file if (value != null) return value; } return null; // Not found } Trade-off : \u274c Reads are slower (check multiple places) \u2705 But acceptable for write-heavy workloads This is called \"read amplification\" : must check K files instead of 1 tree. Step 5: Compact periodically // Problem: Too many SSTables \u2192 slow reads // Solution: Merge files periodically public void compact() { TreeMap<K, V> merged = new TreeMap<>(); // Merge all SSTables (oldest to newest) for (SSTable table : sstables) { for (Entry<K, V> entry : table.entrySet()) { merged.put(entry.getKey(), entry.getValue()); // Newer values overwrite older ones } } // Replace old SSTables with one compacted file sstables.clear(); sstables.add(new SSTable(merged)); } Result : Fewer files = faster reads. Amortize compaction cost over time. Where WAL (Write-Ahead Log) Fits \u00b6 WAL is orthogonal to your storage engine choice. It's about durability , not structure. Problem : MemTable is in RAM - what if crash before flush? // Without WAL: Data lost on crash \u274c public void put(K key, V value) { memTable.put(key, value); // If crash here, data is LOST! if (memTable.size() >= threshold) { flushToSSTable(memTable); } } Solution: WAL (Write-Ahead Log) // With WAL: Durable \u2713 public void put(K key, V value) { // 1. Write to WAL FIRST (append-only log on disk) wal.append(key, value); // Persist to disk immediately // 2. Update in-memory MemTable memTable.put(key, value); // Fast in-memory update if (memTable.size() >= threshold) { flushToSSTable(memTable); wal.clear(); // Can delete WAL after successful flush } } // On crash recovery: public void recover() { memTable = replayWAL(); // Rebuild MemTable from WAL // Then continue normal operations } WAL characteristics : Append-only (sequential writes - fast) Only stores recent uncommitted data Deleted after flush Used for crash recovery Both B+Trees and LSM Trees use WAL for durability. It's a separate layer from the core storage structure. In-Memory Storage Engines (Redis, Memcached) \u00b6 Completely different trade-off : RAM vs Disk Redis (Pure In-Memory) \u00b6 // Everything in RAM - no disk I/O for reads! Map<K, V> data = new HashMap<>(); public void put(K key, V value) { data.put(key, value); // O(1) - instant! \u26a1 } public V get(K key) { return data.get(key); // O(1) - instant! \u26a1 } // Optional: Persist to disk asynchronously (doesn't block) public void backgroundSave() { fork(); // Copy-on-write childProcess.writeToDisk(data); // Snapshot } Advantages : \u26a1 Extremely fast: O(1) for hash operations No disk I/O latency (microseconds vs milliseconds) Simple architecture Disadvantages : \ud83d\udcb0 RAM is 30-50x more expensive than SSD \ud83d\udccf Limited capacity: can't store more than RAM \u274c Data loss risk: if crash before persistence \u274c No range queries (hash table, not sorted) When to use : Cache (can rebuild from database if lost) Session storage (acceptable to lose some sessions) Real-time counters (like/view counts) Leaderboards (can reconstruct) Pub/sub (transient messages) When NOT to use : \u274c Primary data store (too expensive, data loss risk) \u274c Large datasets (> available RAM) \u274c Requires durability guarantees Complete Comparison Table \u00b6 Storage Engine Write Speed Read Speed Range Queries Capacity Durability Best For Heap File \u26a1\u26a1\u26a1 O(1) \ud83d\udc0c O(N) \u274c No Unlimited \u2713 Append-only logs Sorted File \ud83d\udc0c O(N) \u26a1\u26a1 O(log N) \u2713 Unlimited \u2713 Read-only data B+Tree \u26a1 O(log N) \u26a1\u26a1\u26a1 O(log N) \u2713\u2713\u2713 Excellent Unlimited \u2713 (with WAL) Read-heavy OLTP LSM Tree \u26a1\u26a1\u26a1 O(log M) \u26a1\u26a1 O(K\u00d7log S) \u2713\u2713 Good Unlimited \u2713 (with WAL) Write-heavy OLTP Redis (RAM) \u26a1\u26a1\u26a1 O(1) \u26a1\u26a1\u26a1 O(1) \u274c Limited RAM-bound \u26a0\ufe0f Optional Cache, sessions Where: N = total number of records M = MemTable size (typically 1K-100K) K = number of SSTables S = SSTable size The Complete Historical Sequence \u00b6 1970s: B-Trees invented (Rudolf Bayer, Boeing) \u2514\u2500\u2500 Goal: Minimize disk seeks on spinning disks 1972: B+Trees emerge \u251c\u2500\u2500 Optimize B-Trees for range queries \u2514\u2500\u2500 Become standard in databases 1980s-2000s: B+Trees dominate \u251c\u2500\u2500 Oracle, MySQL, PostgreSQL, SQL Server \u251c\u2500\u2500 Perfect for balanced read/write workloads \u2514\u2500\u2500 Optimized over decades 1996: LSM Trees invented (Patrick O'Neil et al.) \u251c\u2500\u2500 Published in academic paper \u251c\u2500\u2500 Designed for write-heavy workloads \u2514\u2500\u2500 Mostly ignored by industry 2006: Google BigTable paper (THE TURNING POINT) \u251c\u2500\u2500 Describes LSM-style architecture \u251c\u2500\u2500 Proves it works at massive scale (indexing the web) \u2514\u2500\u2500 Makes LSM Trees \"real\" for industry 2008-2012: NoSQL movement \u251c\u2500\u2500 Cassandra (2008): Facebook's LSM database \u251c\u2500\u2500 HBase (2008): Hadoop's BigTable clone \u251c\u2500\u2500 LevelDB (2011): Google's open-source LSM \u251c\u2500\u2500 RocksDB (2012): Facebook's LevelDB fork \u2514\u2500\u2500 Narrative: \"LSM Trees are modern, B+Trees are legacy\" 2010s: SSDs change the game \u251c\u2500\u2500 Random I/O becomes cheaper \u251c\u2500\u2500 Gap between B+Trees and LSM Trees narrows \u2514\u2500\u2500 Both remain viable depending on workload Modern day: Hybrid approaches \u251c\u2500\u2500 MongoDB/WiredTiger supports both engines \u251c\u2500\u2500 Choice depends on workload characteristics \u2514\u2500\u2500 No universal \"best\" - only trade-offs Key Takeaways \u00b6 B+Trees came first (1972) and dominated for 30+ years Optimize for disk seeks (main bottleneck on HDDs) Perfect for balanced read/write workloads LSM Trees emerged (popularized 2006) to solve specific problem Google needed massive write throughput for web indexing B+Tree write amplification became bottleneck LSM Trees trade read performance for write performance WAL is separate from storage engine choice Both B+Trees and LSM Trees use WAL for durability It's about crash recovery, not core structure In-memory engines (Redis) are different trade-off entirely RAM vs disk capacity Speed vs durability Use for caching, not primary storage No universal \"best\" B+Tree: read-heavy, range queries, OLTP LSM Tree: write-heavy, insert-heavy, analytics ingestion Redis: extremely low latency, acceptable data loss The lesson: Understand the workload, then choose the tool. Real-World Examples \u00b6 B+Tree Storage Engines : MySQL InnoDB PostgreSQL SQLite SQL Server Oracle Database LSM Tree Storage Engines : Cassandra HBase RocksDB (used by MyRocks, CockroachDB, TiDB) LevelDB ScyllaDB Hybrid (supports both) : MongoDB (WiredTiger can use either) In-Memory : Redis Memcached VoltDB End of Appendix Return to main content","title":"01. Storage Engines"},{"location":"systems/01-storage-engines/#storage-engines","text":"B+Trees vs LSM Trees - The foundation of all database decisions","title":"Storage Engines"},{"location":"systems/01-storage-engines/#important-notes","text":"B+Trees Leaf nodes have equal keys and values, Internal nodes have one extra value since keys are signposts (or \" bouncers\"). Their split logic diffs slightly as a result. LSM Trees Writes are more efficient due to batched sequential writes to SSTables, reducing write amplification ~100x compared to B+Trees (which rewrite entire 4KB pages per insert, even if inserting just 10B) // Binary search result is tricky // When not found: `result = -insertionPoint - 1` // and extraction is: `insertionPoint = -result - 1` int result = Collections.binarySearch(List.of(\"a\", \"b\"), \"c\"); if (result > 0) { int foundIndex = result; } else { int insertionPoint = -result - 1; }","title":"Important notes"},{"location":"systems/01-storage-engines/#eli5-explain-like-im-5","text":"Your task: After implementing and testing both storage engines, explain them simply. Prompts to guide you: What is a B+Tree in one sentence? Your answer: A read-optimised data structure enabling fast lookups (N-ary tree) and range queries (linked leaves) by trading off slower writes (random disk access) Why do databases use B+Trees? Your answer: It enables fast point and range queries Real-world analogy for B+Tree: Your analogy: A B+Tree is like a library - you organise your books into shelves, bays, and stacks. It's fast to find your book but it takes longer to reshelve since you have to randomly walk around What is an LSM Tree in one sentence? Your answer: A write-optimised data structure enabling fast writes (memtable, sstable merge, sequential disk access) by trading off slow reads (sstable iteration) Why do write-heavy databases use LSM Trees? Your answer: To avoid B+Trees' random write bottleneck at high volume Real-world analogy for LSM Tree: Your analogy: An LSM Tree is like a 2nd hand bookstore - you sort a few donations, box them and put them in the back. You can accept lots of books quickly but finding means searching heaps of boxes","title":"ELI5: Explain Like I'm 5"},{"location":"systems/01-storage-engines/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/01-storage-engines/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized storage approaches to understand the trade-offs.","title":"Before/After: Why This Pattern Matters"},{"location":"systems/01-storage-engines/#case-studies-storage-engines-in-the-wild","text":"","title":"Case Studies: Storage Engines in the Wild"},{"location":"systems/01-storage-engines/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/01-storage-engines/#debugging-challenges","text":"Your task: Find and fix bugs in broken storage engine implementations. This tests your deep understanding.","title":"Debugging Challenges"},{"location":"systems/01-storage-engines/#decision-framework","text":"Your task: Build decision trees for when to use each storage engine.","title":"Decision Framework"},{"location":"systems/01-storage-engines/#practice","text":"","title":"Practice"},{"location":"systems/01-storage-engines/#review-checklist","text":"Before moving to the next topic: Implementation B+Tree insert, search, range query work correctly LSM Tree put, get, flush, compact work correctly All client code runs without errors Benchmarks completed and results recorded Understanding Can explain B+Tree in simple terms (filled ELI5) Can explain LSM Tree in simple terms (filled ELI5) Understand why writes are different speeds Understand why reads are different speeds Decision Making Built complete decision tree Solved all 3 practice scenarios Can justify each design choice Mastery Check Could implement both from memory Could explain trade-offs in an interview Know when to use each without looking at notes","title":"Review Checklist"},{"location":"systems/01-storage-engines/#appendix-the-historical-evolution-from-first-principles","text":"Why this appendix exists : The main chapter teaches B+Trees and LSM Trees side-by-side. But historically, B+Trees came first and dominated for 30 years. Understanding this evolution provides deeper intuition about why these designs exist.","title":"APPENDIX: The Historical Evolution - From First Principles"},{"location":"systems/01-storage-engines/#the-historical-truth","text":"1970s-2000s : If you said \"database storage engine,\" you meant B+Tree . Oracle, MySQL, PostgreSQL, SQL Server - all B+Trees Learned in every databases class The default, the standard, the only choice 2006 : Google's BigTable paper changes everything Describes LSM-style architecture for web-scale writes Solves write amplification problem in B+Trees 2008-2012 : NoSQL movement adopts LSM Trees Cassandra, HBase, RocksDB, LevelDB Narrative: \"B+Trees are old SQL. LSM Trees are modern NoSQL.\" Reality : Both solve the same problem (organizing data on disk) with different trade-offs.","title":"The Historical Truth"},{"location":"systems/01-storage-engines/#the-evolution-starting-from-absolute-zero","text":"Let's trace the path that led to these designs, starting from the simplest possible database. How to use this section: At each level, try to predict what will break before reading ahead. This builds the intuition for why each innovation was necessary.","title":"The Evolution: Starting from Absolute Zero"},{"location":"systems/01-storage-engines/#the-problem-that-necessitated-lsm-trees","text":"","title":"The Problem That Necessitated LSM Trees"},{"location":"systems/01-storage-engines/#first-principles-lsm-tree","text":"Question : How can we optimize for massive write volume?","title":"First Principles \u2192 LSM Tree"},{"location":"systems/01-storage-engines/#where-wal-write-ahead-log-fits","text":"WAL is orthogonal to your storage engine choice. It's about durability , not structure. Problem : MemTable is in RAM - what if crash before flush? // Without WAL: Data lost on crash \u274c public void put(K key, V value) { memTable.put(key, value); // If crash here, data is LOST! if (memTable.size() >= threshold) { flushToSSTable(memTable); } } Solution: WAL (Write-Ahead Log) // With WAL: Durable \u2713 public void put(K key, V value) { // 1. Write to WAL FIRST (append-only log on disk) wal.append(key, value); // Persist to disk immediately // 2. Update in-memory MemTable memTable.put(key, value); // Fast in-memory update if (memTable.size() >= threshold) { flushToSSTable(memTable); wal.clear(); // Can delete WAL after successful flush } } // On crash recovery: public void recover() { memTable = replayWAL(); // Rebuild MemTable from WAL // Then continue normal operations } WAL characteristics : Append-only (sequential writes - fast) Only stores recent uncommitted data Deleted after flush Used for crash recovery Both B+Trees and LSM Trees use WAL for durability. It's a separate layer from the core storage structure.","title":"Where WAL (Write-Ahead Log) Fits"},{"location":"systems/01-storage-engines/#in-memory-storage-engines-redis-memcached","text":"Completely different trade-off : RAM vs Disk","title":"In-Memory Storage Engines (Redis, Memcached)"},{"location":"systems/01-storage-engines/#complete-comparison-table","text":"Storage Engine Write Speed Read Speed Range Queries Capacity Durability Best For Heap File \u26a1\u26a1\u26a1 O(1) \ud83d\udc0c O(N) \u274c No Unlimited \u2713 Append-only logs Sorted File \ud83d\udc0c O(N) \u26a1\u26a1 O(log N) \u2713 Unlimited \u2713 Read-only data B+Tree \u26a1 O(log N) \u26a1\u26a1\u26a1 O(log N) \u2713\u2713\u2713 Excellent Unlimited \u2713 (with WAL) Read-heavy OLTP LSM Tree \u26a1\u26a1\u26a1 O(log M) \u26a1\u26a1 O(K\u00d7log S) \u2713\u2713 Good Unlimited \u2713 (with WAL) Write-heavy OLTP Redis (RAM) \u26a1\u26a1\u26a1 O(1) \u26a1\u26a1\u26a1 O(1) \u274c Limited RAM-bound \u26a0\ufe0f Optional Cache, sessions Where: N = total number of records M = MemTable size (typically 1K-100K) K = number of SSTables S = SSTable size","title":"Complete Comparison Table"},{"location":"systems/01-storage-engines/#the-complete-historical-sequence","text":"1970s: B-Trees invented (Rudolf Bayer, Boeing) \u2514\u2500\u2500 Goal: Minimize disk seeks on spinning disks 1972: B+Trees emerge \u251c\u2500\u2500 Optimize B-Trees for range queries \u2514\u2500\u2500 Become standard in databases 1980s-2000s: B+Trees dominate \u251c\u2500\u2500 Oracle, MySQL, PostgreSQL, SQL Server \u251c\u2500\u2500 Perfect for balanced read/write workloads \u2514\u2500\u2500 Optimized over decades 1996: LSM Trees invented (Patrick O'Neil et al.) \u251c\u2500\u2500 Published in academic paper \u251c\u2500\u2500 Designed for write-heavy workloads \u2514\u2500\u2500 Mostly ignored by industry 2006: Google BigTable paper (THE TURNING POINT) \u251c\u2500\u2500 Describes LSM-style architecture \u251c\u2500\u2500 Proves it works at massive scale (indexing the web) \u2514\u2500\u2500 Makes LSM Trees \"real\" for industry 2008-2012: NoSQL movement \u251c\u2500\u2500 Cassandra (2008): Facebook's LSM database \u251c\u2500\u2500 HBase (2008): Hadoop's BigTable clone \u251c\u2500\u2500 LevelDB (2011): Google's open-source LSM \u251c\u2500\u2500 RocksDB (2012): Facebook's LevelDB fork \u2514\u2500\u2500 Narrative: \"LSM Trees are modern, B+Trees are legacy\" 2010s: SSDs change the game \u251c\u2500\u2500 Random I/O becomes cheaper \u251c\u2500\u2500 Gap between B+Trees and LSM Trees narrows \u2514\u2500\u2500 Both remain viable depending on workload Modern day: Hybrid approaches \u251c\u2500\u2500 MongoDB/WiredTiger supports both engines \u251c\u2500\u2500 Choice depends on workload characteristics \u2514\u2500\u2500 No universal \"best\" - only trade-offs","title":"The Complete Historical Sequence"},{"location":"systems/01-storage-engines/#key-takeaways","text":"B+Trees came first (1972) and dominated for 30+ years Optimize for disk seeks (main bottleneck on HDDs) Perfect for balanced read/write workloads LSM Trees emerged (popularized 2006) to solve specific problem Google needed massive write throughput for web indexing B+Tree write amplification became bottleneck LSM Trees trade read performance for write performance WAL is separate from storage engine choice Both B+Trees and LSM Trees use WAL for durability It's about crash recovery, not core structure In-memory engines (Redis) are different trade-off entirely RAM vs disk capacity Speed vs durability Use for caching, not primary storage No universal \"best\" B+Tree: read-heavy, range queries, OLTP LSM Tree: write-heavy, insert-heavy, analytics ingestion Redis: extremely low latency, acceptable data loss The lesson: Understand the workload, then choose the tool.","title":"Key Takeaways"},{"location":"systems/01-storage-engines/#real-world-examples","text":"B+Tree Storage Engines : MySQL InnoDB PostgreSQL SQLite SQL Server Oracle Database LSM Tree Storage Engines : Cassandra HBase RocksDB (used by MyRocks, CockroachDB, TiDB) LevelDB ScyllaDB Hybrid (supports both) : MongoDB (WiredTiger can use either) In-Memory : Redis Memcached VoltDB End of Appendix Return to main content","title":"Real-World Examples"},{"location":"systems/02-row-vs-column-storage/","text":"Row vs Column Storage \u00b6 OLTP vs OLAP - Why your database layout fundamentally changes the game ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing and testing both storage layouts, explain them simply. Prompts to guide you: What is row-oriented storage in one sentence? Your answer: [Fill in after implementation] What is column-oriented storage in one sentence? Your answer: [Fill in after implementation] Real-world analogy for row storage: Example: \"Row storage is like filing cabinets where each drawer contains one person's complete file...\" Your analogy: [Fill in] Real-world analogy for column storage: Example: \"Column storage is like having separate filing cabinets for each attribute...\" Your analogy: [Fill in] When would you use row storage? Your answer: [Fill in after implementation] When would you use column storage? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Performance Predictions \u00b6 Row storage: Fetch one complete user record Expected I/O operations: [How many disk reads?] Verified after implementation: [Actual] Column storage: Fetch one complete user record Expected I/O operations: [How many disk reads?] Verified: [Actual] Row storage: Calculate average of one column across 1M rows Expected I/O: [How much data read?] Verified: [Actual] Column storage: Calculate average of one column across 1M rows Expected I/O: [How much data read?] Verified: [Actual] Scenario Predictions \u00b6 Scenario 1: E-commerce order processing (insert orders, fetch by order_id) Best storage layout? [Row/Column?] Why? [Explain] Scenario 2: Business intelligence dashboard (revenue by month, top products) Best storage layout? [Row/Column?] Why? [Explain] Scenario 3: Social media user profiles (lookup by user_id, update profile) Best storage layout? [Row/Column?] Why? [Explain] Before/After: Why This Pattern Matters \u00b6 Your task: Understand the fundamental trade-off between row and column layouts. The Core Problem \u00b6 You have a table with 1 million users: CREATE TABLE users ( id INT, name VARCHAR(100), email VARCHAR(100), age INT, city VARCHAR(50), salary INT ); Two different queries with radically different performance: -- Query 1: OLTP - Fetch one user by ID SELECT * FROM users WHERE id = 12345; -- Query 2: OLAP - Analytics across millions SELECT AVG(salary), city FROM users GROUP BY city; The question: How should you physically store this data on disk? Approach 1: Row-Oriented Storage \u00b6 Physical layout: Store entire rows together Disk layout (row-oriented): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 [1, \"Alice\", \"a@x.com\", 30, \"NYC\", 100000] \u2502 \u2190 Row 1 \u2502 [2, \"Bob\", \"b@x.com\", 25, \"SF\", 120000] \u2502 \u2190 Row 2 \u2502 [3, \"Carol\", \"c@x.com\", 35, \"LA\", 90000] \u2502 \u2190 Row 3 \u2502 ... \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Query 1 performance (fetch one user): // Single disk read gets entire row public User getUser(int id) { // 1 disk seek to row location // Read entire row (~200 bytes) return parseRow(diskRead(rowOffset(id))); // O(1) - FAST! \u2713 } Query 2 performance (aggregate salary by city): // Must read ALL rows to get salary + city columns public Map<String, Double> avgSalaryByCity() { for (int i = 0; i < 1_000_000; i++) { byte[] row = diskRead(rowOffset(i)); // Read entire row (~200 bytes) // But only need salary (4 bytes) + city (50 bytes) // Wasting 146 bytes per row! } // Total read: 1M * 200 bytes = 200MB // Actual needed: 1M * 54 bytes = 54MB // Waste: 73% of I/O! \u2717 } Row storage characteristics: \u2705 Point lookups: Excellent (single disk read) \u2705 Insert/Update full row: Excellent (single write) \u274c Column scans: Poor (read unnecessary data) \u274c Compression: Limited (mixed data types per row) Approach 2: Column-Oriented Storage \u00b6 Physical layout: Store each column separately Disk layout (column-oriented): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 id: [1,2,3,...] \u2502 \u2190 All IDs together \u2502 name: [\"Alice\",\"Bob\",...] \u2502 \u2190 All names together \u2502 email: [\"a@x\",\"b@x\",...] \u2502 \u2190 All emails together \u2502 age: [30,25,35,...] \u2502 \u2190 All ages together \u2502 city: [\"NYC\",\"SF\",\"LA\",...] \u2502 \u2190 All cities together \u2502 salary: [100000,120000,...] \u2502 \u2190 All salaries together \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Query 1 performance (fetch one user): // Must read from EACH column file public User getUser(int id) { // 6 disk seeks (one per column) int userId = idColumn.read(id); String name = nameColumn.read(id); String email = emailColumn.read(id); int age = ageColumn.read(id); String city = cityColumn.read(id); int salary = salaryColumn.read(id); return new User(userId, name, email, age, city, salary); // 6 disk seeks - SLOW! \u2717 } Query 2 performance (aggregate salary by city): // Only read the columns we need! public Map<String, Double> avgSalaryByCity() { int[] salaries = salaryColumn.readAll(); // 1M * 4 bytes = 4MB String[] cities = cityColumn.readAll(); // 1M * 50 bytes = 50MB // Total read: 54MB (only what we need!) // vs 200MB with row storage // 73% less I/O! \u2713 // Bonus: Columns compress MUCH better // salary: All integers, similar range // city: Many duplicates (\"NYC\", \"SF\", \"LA\"...) // With compression: 54MB \u2192 ~10MB! \u2713\u2713 } Column storage characteristics: \u274c Point lookups: Poor (must read from N columns) \u274c Insert/Update: Complex (update N separate files) \u2705 Column scans: Excellent (only read needed columns) \u2705 Compression: Excellent (similar data types) \u2705 SIMD/Vectorization: Possible (homogeneous data) The Fundamental Trade-off \u00b6 Feature Row Storage Column Storage Point Lookups \u26a1\u26a1\u26a1 (1 read) \ud83d\udc0c (N reads) Full Row Scans \ud83d\udc0c (wasted I/O) \ud83d\udc0c (N files) Column Scans \ud83d\udc0c (wasted I/O) \u26a1\u26a1\u26a1 (targeted) Compression \u26a1 (limited) \u26a1\u26a1\u26a1 (excellent) Inserts \u26a1\u26a1\u26a1 (single write) \ud83d\udc0c (N writes) Updates \u26a1\u26a1 (single write) \ud83d\udc0c (N writes) Best For OLTP (Transactions) OLAP (Analytics) Key insight: OLTP: \"Give me order #12345\" \u2192 Need entire row \u2192 Use row storage OLAP: \"Show revenue by category\" \u2192 Need specific columns \u2192 Use column storage Case Studies: Row vs. Column Storage in the Wild \u00b6 PostgreSQL & MySQL: The Row-Oriented Champions \u00b6 Pattern: Row-Oriented Storage. How it works: In databases like PostgreSQL and MySQL (with InnoDB), all the values for a single row ( id , name , email , city , salary ) are stored contiguously on disk. Use Case: This is ideal for OLTP (Online Transaction Processing) workloads. When you query SELECT * FROM users WHERE id = 123 , the database performs a single read to fetch the entire user record, which is highly efficient. Creating or updating a user is also a single write operation. Key Takeaway: For applications where you primarily work with entire records at a time (e.g., e-commerce backends, content management systems, user account services), row-oriented storage provides the best performance for read, write, and update operations. Amazon Redshift & Google BigQuery: Columnar for Analytics \u00b6 Pattern: Column-Oriented Storage. How it works: Data warehouses like Redshift, BigQuery, and Snowflake store data in columns. All user_id values are stored together, all city values are stored together, and so on. Use Case: This is built for OLAP (Online Analytical Processing) . When an analyst runs a query like SELECT city, AVG(salary) FROM users GROUP BY city , the database only needs to read the city and salary columns. It completely ignores the id , name , and email columns, drastically reducing the amount of data read from disk. Key Takeaway: Columnar storage provides orders-of-magnitude performance improvements for analytical queries that aggregate over a small subset of columns in a large dataset. The high compression ratios achieved also lead to significant cost savings. ClickHouse: High-Performance Real-Time Analytics \u00b6 Pattern: Column-Oriented Storage. How it works: ClickHouse is an open-source columnar database designed for extreme speed on analytical queries. It not only uses columnar storage but also processes data in vectors using a vectorized query execution engine to maximize CPU efficiency. Key Takeaway: For use cases like real-time dashboards, log analysis, and telemetry monitoring, where you need to slice and dice massive datasets interactively, a performance-focused columnar database like ClickHouse is the optimal choice. It demonstrates that the benefits of columnar storage go beyond just I/O reduction to include computational efficiency. Core Implementation \u00b6 Part 1: Row-Oriented Storage \u00b6 Your task: Implement a simple row-oriented storage engine. import java.util.*; /** * Row-Oriented Storage: All columns for a row stored together * * Use case: OLTP - transactional workloads * Optimized for: Point lookups, full row access */ public class RowStore { // Each row stored as a complete unit private Map<Integer, Row> rows = new HashMap<>(); static class Row { int id; String name; String email; int age; String city; int salary; Row(int id, String name, String email, int age, String city, int salary) { this.id = id; this.name = name; this.email = email; this.age = age; this.city = city; this.salary = salary; } } /** * Insert: O(1) - single write * All columns written together in one operation * * TODO: Implement insert */ public void insert(Row row) { // TODO: Store entire row in map // In reality: Write entire row to one disk location } /** * Point lookup: O(1) - optimal! * Single disk read gets all columns * * TODO: Implement point lookup */ public Row getById(int id) { // TODO: Retrieve row from map // In reality: One disk seek, read entire row return null; } /** * Column scan: O(N) - inefficient! * Must read entire rows even though we only need one column * * TODO: Implement column scan */ public double avgSalary() { // TODO: Calculate average salary // Note: You're reading ALL columns just to get salary // This is the key inefficiency of row storage for analytics! return 0.0; } /** * Multi-column aggregation * Still reads full rows * * TODO: Implement aggregation by city */ public Map<String, Double> avgSalaryByCity() { // TODO: Group salaries by city and calculate averages // Note: Still reading entire rows even though only using 2 columns return new HashMap<>(); } } Part 2: Column-Oriented Storage \u00b6 Your task: Implement a simple column-oriented storage engine. import java.util.*; /** * Column-Oriented Storage: Each column stored separately * * Use case: OLAP - analytical workloads * Optimized for: Column scans, aggregations */ public class ColumnStore { // Each column stored separately private List<Integer> idColumn = new ArrayList<>(); private List<String> nameColumn = new ArrayList<>(); private List<String> emailColumn = new ArrayList<>(); private List<Integer> ageColumn = new ArrayList<>(); private List<String> cityColumn = new ArrayList<>(); private List<Integer> salaryColumn = new ArrayList<>(); static class Row { int id; String name; String email; int age; String city; int salary; Row(int id, String name, String email, int age, String city, int salary) { this.id = id; this.name = name; this.email = email; this.age = age; this.city = city; this.salary = salary; } } /** * Insert: O(C) where C = number of columns - slower! * Must write to each column separately * * TODO: Implement insert */ public void insert(Row row) { // TODO: Add each field to its corresponding column // Must write to 6 separate column lists // In reality: 6 separate disk writes - write amplification! } /** * Point lookup: O(C) - inefficient! * Must read from each column file * * TODO: Implement point lookup */ public Row getById(int id) { // TODO: Find the index for this ID // TODO: Read from each column at that index // In reality: 6 disk seeks (one per column) return null; } /** * Column scan: O(N) - optimal! * Only read the column we need * * TODO: Implement column scan */ public double avgSalary() { // TODO: Calculate average of salary column only // Key advantage: Ignore all other columns! // If 1M rows: Column store reads 4MB, Row store reads 200MB return 0.0; } /** * Multi-column aggregation - still efficient! * Only read the columns we need * * TODO: Implement aggregation by city */ public Map<String, Double> avgSalaryByCity() { // TODO: Read only city and salary columns // TODO: Group by city and calculate averages // Key advantage: Only 2 columns read instead of all 6 return new HashMap<>(); } /** * Column pruning: Read only what's needed * This is the killer feature of column stores * * TODO: Implement selective column query */ public List<Integer> getSalariesInCity(String targetCity) { // TODO: Filter city column and return matching salaries // Only read city and salary columns - ignore the other 4! return new ArrayList<>(); } } Part 3: Benchmark Comparison \u00b6 Your task: Compare row vs column storage for different workloads. import java.util.*; public class StorageLayoutBenchmark { public static void main(String[] args) { System.out.println(\"=== Row vs Column Storage Benchmark ===\\n\"); benchmarkInserts(); System.out.println(); benchmarkPointLookups(); System.out.println(); benchmarkColumnScans(); System.out.println(); benchmarkAggregations(); } static void benchmarkInserts() { System.out.println(\"--- Insert Performance ---\"); int numRows = 100000; // TODO: Benchmark row store inserts RowStore rowStore = new RowStore(); long start = System.nanoTime(); // TODO: Insert numRows rows into rowStore long rowTime = System.nanoTime() - start; // TODO: Benchmark column store inserts ColumnStore colStore = new ColumnStore(); start = System.nanoTime(); // TODO: Insert numRows rows into colStore long colTime = System.nanoTime() - start; System.out.printf(\"Row Store: %.2f ms (%.0f inserts/sec)%n\", rowTime / 1e6, numRows / (rowTime / 1e9)); System.out.printf(\"Column Store: %.2f ms (%.0f inserts/sec)%n\", colTime / 1e6, numRows / (colTime / 1e9)); System.out.printf(\"Row store is %.2fx faster for inserts%n\", (double) colTime / rowTime); } static void benchmarkPointLookups() { System.out.println(\"--- Point Lookup Performance ---\"); int numRows = 100000; int numLookups = 1000; // TODO: Setup - populate both stores RowStore rowStore = new RowStore(); ColumnStore colStore = new ColumnStore(); // TODO: Insert numRows into both stores // TODO: Benchmark row store lookups Random rand = new Random(42); long start = System.nanoTime(); // TODO: Perform numLookups random getById calls on rowStore long rowTime = System.nanoTime() - start; // TODO: Benchmark column store lookups rand = new Random(42); start = System.nanoTime(); // TODO: Perform numLookups random getById calls on colStore long colTime = System.nanoTime() - start; System.out.printf(\"Row Store: %.2f ms (%.0f lookups/sec)%n\", rowTime / 1e6, numLookups / (rowTime / 1e9)); System.out.printf(\"Column Store: %.2f ms (%.0f lookups/sec)%n\", colTime / 1e6, numLookups / (colTime / 1e9)); System.out.printf(\"Row store is %.2fx faster for point lookups%n\", (double) colTime / rowTime); } static void benchmarkColumnScans() { System.out.println(\"--- Column Scan Performance (avg salary) ---\"); int numRows = 100000; // TODO: Setup - populate both stores RowStore rowStore = new RowStore(); ColumnStore colStore = new ColumnStore(); // TODO: Insert numRows into both stores // TODO: Benchmark row store column scan long start = System.nanoTime(); double rowAvg = 0.0; // TODO: Call rowStore.avgSalary() long rowTime = System.nanoTime() - start; // TODO: Benchmark column store column scan start = System.nanoTime(); double colAvg = 0.0; // TODO: Call colStore.avgSalary() long colTime = System.nanoTime() - start; System.out.printf(\"Row Store: %.2f ms (result: %.2f)%n\", rowTime / 1e6, rowAvg); System.out.printf(\"Column Store: %.2f ms (result: %.2f)%n\", colTime / 1e6, colAvg); System.out.printf(\"Column store is %.2fx faster for column scans%n\", (double) rowTime / colTime); } static void benchmarkAggregations() { System.out.println(\"--- Aggregation Performance (avg salary by city) ---\"); int numRows = 100000; // TODO: Setup - populate both stores RowStore rowStore = new RowStore(); ColumnStore colStore = new ColumnStore(); // TODO: Insert numRows into both stores // TODO: Benchmark row store aggregation long start = System.nanoTime(); Map<String, Double> rowResult = null; // TODO: Call rowStore.avgSalaryByCity() long rowTime = System.nanoTime() - start; // TODO: Benchmark column store aggregation start = System.nanoTime(); Map<String, Double> colResult = null; // TODO: Call colStore.avgSalaryByCity() long colTime = System.nanoTime() - start; System.out.printf(\"Row Store: %.2f ms%n\", rowTime / 1e6); System.out.printf(\"Column Store: %.2f ms%n\", colTime / 1e6); System.out.printf(\"Column store is %.2fx faster for aggregations%n\", (double) rowTime / colTime); } } Must complete: Implement RowStore insert, getById, avgSalary, avgSalaryByCity Implement ColumnStore insert, getById, avgSalary, avgSalaryByCity Run benchmarks and record results Understand WHY each performs better for different workloads Your benchmark results: Operation Row Store Column Store Winner Inserts (100k rows) ___ ms ___ ms ___ Point Lookups (1k) ___ ms ___ ms ___ Column Scan (avg salary) ___ ms ___ ms ___ Aggregation (by city) ___ ms ___ ms ___ Key insight: [Why does column storage win for analytics?] Compression: The Hidden Superpower of Column Stores \u00b6 Why column stores compress better: Row-oriented (mixed data types per row): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 [1, \"Alice\", \"a@x.com\", 30, \"NYC\", 100000] \u2502 \u2502 [2, \"Bob\", \"b@x.com\", 25, \"SF\", 120000] \u2502 \u2502 [3, \"Carol\", \"c@x.com\", 35, \"LA\", 90000] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Hard to compress: Different data types, no patterns Column-oriented (homogeneous data): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 id: [1,2,3,4,5,6,7,8,9,10,...] \u2502 \u2190 Sequential integers \u2502 city: [\"NYC\",\"NYC\",\"SF\",\"SF\",\"LA\",...] \u2502 \u2190 Many duplicates \u2502 salary: [100000,120000,90000,95000,...] \u2502 \u2190 Similar ranges \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Easy to compress: Patterns, repetition, similar types Compression Techniques for Columns \u00b6 1. Run-Length Encoding (RLE) - Great for sorted/repeated values Before: [\"NYC\", \"NYC\", \"NYC\", \"SF\", \"SF\", \"LA\", \"LA\", \"LA\", \"LA\"] After: [(NYC, 3), (SF, 2), (LA, 4)] Space saved: 9 strings \u2192 3 tuples = 67% reduction 2. Dictionary Encoding - Great for low-cardinality columns Before: [\"NYC\", \"SF\", \"NYC\", \"LA\", \"NYC\", \"SF\"] Dictionary: {0: \"NYC\", 1: \"SF\", 2: \"LA\"} After: [0, 1, 0, 2, 0, 1] Space saved: 6 strings (18 bytes) \u2192 6 integers (24 bits) = 87% reduction 3. Delta Encoding - Great for sequential/timestamp columns Before: [1000, 1001, 1002, 1003, 1004, 1005] Base: 1000 After: [0, 1, 1, 1, 1, 1] (store differences) Space saved: 6 ints (24 bytes) \u2192 1 int + 5 bytes (9 bytes) = 62% reduction Real-world impact: 1 billion rows, 10 columns: Row store (uncompressed): Row size: 200 bytes Total: 200 GB Column store (compressed): IDs: 4 GB \u2192 500 MB (delta encoding) Names: 100 GB \u2192 10 GB (dictionary encoding) Cities: 50 GB \u2192 500 MB (RLE + dictionary) Salaries: 4 GB \u2192 1 GB (delta encoding) ... Total: ~50 GB (75% reduction!) Decision Framework \u00b6 Your task: Build decision trees for when to use each storage layout. Question 1: OLTP or OLAP Workload? \u00b6 Answer after implementing and benchmarking: My workload type: [Fill in] Why does this matter? [Fill in] Performance difference I observed: [Fill in] Question 2: Query Patterns \u00b6 Answer: Do I need full rows? [Yes/No - when?] Do I need selective columns? [Yes/No - how many?] Which is faster for my queries? [Fill in after testing] Question 3: Data Volume and Compression \u00b6 Answer: Table size: [Small/Medium/Large - how many rows?] Column cardinality: [High/Low - does it matter?] Compression benefits observed: [Fill in after implementation] Your Decision Tree \u00b6 Build this after understanding trade-offs: flowchart LR Start[\"Storage Layout Selection\"] Start --> Q1{\"What's the primary<br/>workload?\"} Q1 -->|\"OLTP<br/>(Transactions)\"| Q2{\"Query pattern?\"} Q1 -->|\"OLAP<br/>(Analytics)\"| Q3{\"Data volume?\"} Q2 -->|\"Point lookups<br/>(by key)\"| A1([\"Use Row Storage \u2713\"]) Q2 -->|\"Full row scans\"| A2([\"Use Row Storage \u2713\"]) Q2 -->|\"Few columns<br/>from many rows\"| Q3 Q3 -->|\"< 1M rows\"| A3[\"Either works<br/>(test both)\"] Q3 -->|\"> 1M rows\"| Q4{\"How many columns<br/>accessed?\"} Q4 -->|\"Most/All columns\"| A4[\"Row Storage<br/>(less overhead)\"] Q4 -->|\"Few columns<br/>(< 20%)\"| A5([\"Use Column Storage \u2713\"]) A3 --> A6[\"Benchmark with<br/>real queries\"] Review Checklist \u00b6 Before moving to the next topic: Implementation RowStore works correctly (insert, point lookup, scans) ColumnStore works correctly (insert, point lookup, scans) Benchmarks completed and results recorded Understanding Can explain why row storage is faster for point lookups Can explain why column storage is faster for aggregations Understand compression advantages of column storage Decision Making Can identify OLTP vs OLAP workloads Know when to use each storage layout Understand the trade-offs Mastery Certification \u00b6 I certify that I can: Implement row-oriented storage (insert, point lookup, column scan) Implement column-oriented storage (insert, point lookup, column scan) Explain why row storage is faster for point lookups Explain why column storage is faster for aggregations Understand compression advantages of column storage Identify OLTP vs OLAP workloads from requirements Choose appropriate storage layout for a given workload Explain the fundamental read/write trade-offs Understand when to use each storage representation Benchmark and analyze performance differences Debug storage layout issues Explain these concepts in a system design interview APPENDIX \u00b6 Real world technologies \u00b6 When to Use Row Storage \u00b6 Use row storage when: \u2705 Point lookups by key (\"Get user #12345\") \u2705 Insert/update full records (OLTP transactions) \u2705 Need full row access (most queries touch all columns) \u2705 Small table scans (< 100k rows) Real-world examples: E-commerce order processing \u2192 MySQL InnoDB, PostgreSQL User authentication/sessions \u2192 PostgreSQL, MongoDB Banking transactions \u2192 Oracle, SQL Server Social media user profiles \u2192 MySQL, MongoDB When to Use Column Storage \u00b6 Use column storage when: \u2705 Aggregate queries (\"AVG salary by department\") \u2705 Selective column access (only need 2-3 out of 50 columns) \u2705 Large table scans (millions+ rows) \u2705 Read-heavy analytics (dashboards, reports) \u2705 Time-series data (metrics, logs, events) Real-world examples: Business intelligence dashboards \u2192 ClickHouse, Redshift Data warehouse analytics \u2192 Snowflake, BigQuery Log analysis \u2192 ClickHouse, Druid Metrics/monitoring \u2192 Prometheus, InfluxDB Machine learning feature stores \u2192 Parquet files Database Examples \u00b6 Row-Oriented: MySQL InnoDB - OLTP transactions PostgreSQL - General-purpose OLTP MongoDB - Document store (row-like) Cassandra - Wide column store (row-oriented within partition) Column-Oriented: Apache Parquet - File format for Hadoop/Spark ClickHouse - Real-time analytics Amazon Redshift - Data warehouse Google BigQuery - Serverless data warehouse Apache Druid - Real-time analytics Snowflake - Cloud data warehouse Hybrid Approaches: Apache Kudu - Supports both row and column scans InfluxDB - Time-series with column-like storage TimescaleDB - PostgreSQL extension with columnar compression","title":"02. Row vs Column Storage"},{"location":"systems/02-row-vs-column-storage/#row-vs-column-storage","text":"OLTP vs OLAP - Why your database layout fundamentally changes the game","title":"Row vs Column Storage"},{"location":"systems/02-row-vs-column-storage/#eli5-explain-like-im-5","text":"Your task: After implementing and testing both storage layouts, explain them simply. Prompts to guide you: What is row-oriented storage in one sentence? Your answer: [Fill in after implementation] What is column-oriented storage in one sentence? Your answer: [Fill in after implementation] Real-world analogy for row storage: Example: \"Row storage is like filing cabinets where each drawer contains one person's complete file...\" Your analogy: [Fill in] Real-world analogy for column storage: Example: \"Column storage is like having separate filing cabinets for each attribute...\" Your analogy: [Fill in] When would you use row storage? Your answer: [Fill in after implementation] When would you use column storage? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/02-row-vs-column-storage/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/02-row-vs-column-storage/#beforeafter-why-this-pattern-matters","text":"Your task: Understand the fundamental trade-off between row and column layouts.","title":"Before/After: Why This Pattern Matters"},{"location":"systems/02-row-vs-column-storage/#the-fundamental-trade-off","text":"Feature Row Storage Column Storage Point Lookups \u26a1\u26a1\u26a1 (1 read) \ud83d\udc0c (N reads) Full Row Scans \ud83d\udc0c (wasted I/O) \ud83d\udc0c (N files) Column Scans \ud83d\udc0c (wasted I/O) \u26a1\u26a1\u26a1 (targeted) Compression \u26a1 (limited) \u26a1\u26a1\u26a1 (excellent) Inserts \u26a1\u26a1\u26a1 (single write) \ud83d\udc0c (N writes) Updates \u26a1\u26a1 (single write) \ud83d\udc0c (N writes) Best For OLTP (Transactions) OLAP (Analytics) Key insight: OLTP: \"Give me order #12345\" \u2192 Need entire row \u2192 Use row storage OLAP: \"Show revenue by category\" \u2192 Need specific columns \u2192 Use column storage","title":"The Fundamental Trade-off"},{"location":"systems/02-row-vs-column-storage/#case-studies-row-vs-column-storage-in-the-wild","text":"","title":"Case Studies: Row vs. Column Storage in the Wild"},{"location":"systems/02-row-vs-column-storage/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/02-row-vs-column-storage/#compression-the-hidden-superpower-of-column-stores","text":"Why column stores compress better: Row-oriented (mixed data types per row): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 [1, \"Alice\", \"a@x.com\", 30, \"NYC\", 100000] \u2502 \u2502 [2, \"Bob\", \"b@x.com\", 25, \"SF\", 120000] \u2502 \u2502 [3, \"Carol\", \"c@x.com\", 35, \"LA\", 90000] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Hard to compress: Different data types, no patterns Column-oriented (homogeneous data): \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 id: [1,2,3,4,5,6,7,8,9,10,...] \u2502 \u2190 Sequential integers \u2502 city: [\"NYC\",\"NYC\",\"SF\",\"SF\",\"LA\",...] \u2502 \u2190 Many duplicates \u2502 salary: [100000,120000,90000,95000,...] \u2502 \u2190 Similar ranges \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Easy to compress: Patterns, repetition, similar types","title":"Compression: The Hidden Superpower of Column Stores"},{"location":"systems/02-row-vs-column-storage/#decision-framework","text":"Your task: Build decision trees for when to use each storage layout.","title":"Decision Framework"},{"location":"systems/02-row-vs-column-storage/#review-checklist","text":"Before moving to the next topic: Implementation RowStore works correctly (insert, point lookup, scans) ColumnStore works correctly (insert, point lookup, scans) Benchmarks completed and results recorded Understanding Can explain why row storage is faster for point lookups Can explain why column storage is faster for aggregations Understand compression advantages of column storage Decision Making Can identify OLTP vs OLAP workloads Know when to use each storage layout Understand the trade-offs","title":"Review Checklist"},{"location":"systems/02-row-vs-column-storage/#appendix","text":"","title":"APPENDIX"},{"location":"systems/02-row-vs-column-storage/#real-world-technologies","text":"","title":"Real world technologies"},{"location":"systems/03-networking-fundamentals/","text":"Networking Fundamentals \u00b6 TCP/IP vs UDP, HTTP versions, WebSockets, DNS, TLS, and load balancing ELI5: Explain Like I'm 5 \u00b6 Your task: After learning networking fundamentals, explain them simply. Prompts to guide you: What is the difference between TCP and UDP in one sentence? Your answer: [Fill in after learning] Why do we need DNS? Your answer: [Fill in after learning] Real-world analogy for TCP vs UDP: Example: \"TCP is like certified mail where...\" Your analogy: [Fill in] What does HTTP/2 multiplexing solve? Your answer: [Fill in after learning] Real-world analogy for WebSockets: Example: \"WebSockets are like a phone call where...\" Your analogy: [Fill in] Why do we need load balancers? Your answer: [Fill in after practice] Quick Quiz (Do BEFORE learning) \u00b6 Your task: Test your intuition about networking without looking at details. Answer these, then verify after learning. Protocol Predictions \u00b6 When would you choose UDP over TCP? Your guess: [What scenarios?] Verified: [Actual use cases] What's the main benefit of HTTP/2 over HTTP/1.1? Your guess: [Speed? Efficiency? Something else?] Verified: [Actual benefit] WebSockets vs HTTP polling - which is better for real-time chat? Your guess: [Which one and why?] Verified: [Actual answer with trade-offs] Scenario Predictions \u00b6 Scenario 1: Video streaming application with 1M concurrent users Protocol choice: [TCP/UDP - Why?] DNS strategy: [How to handle this scale?] Load balancing: [L4 or L7?] Scenario 2: Real-time multiplayer game with low latency requirement Protocol choice: [TCP/UDP - Why?] Packet loss handling: [What happens?] Expected latency: [Milliseconds? Seconds?] Scenario 3: Financial trading platform requiring guaranteed message delivery Protocol choice: [TCP/UDP - Why?] TLS overhead: [Worth it?] Connection pooling: [Helpful?] Before/After: Why Networking Fundamentals Matter \u00b6 Your task: Compare naive networking assumptions vs proper understanding to see the impact. Example: HTTP Connection Management \u00b6 Problem: Mobile app making 100 API requests to load user dashboard Approach 1: Naive HTTP/1.1 (Sequential Requests) \u00b6 Client needs to load dashboard with: - User profile (1 request) - 20 recent posts (20 requests) - 50 friend suggestions (50 requests) - 10 notifications (10 requests) - Analytics data (19 requests) Total: 100 requests HTTP/1.1 behavior: - Opens connection - Request 1 \u2192 Response 1 - Request 2 \u2192 Response 2 - ... - Request 100 \u2192 Response 100 - Closes connection Time analysis: - Each request: ~50ms (network RTT) + processing - Sequential: 100 * 50ms = 5,000ms = 5 seconds! - User sees: Loading spinner for 5 seconds Problems: Head-of-line blocking (each request waits for previous) Connection overhead repeated Poor mobile experience Inefficient bandwidth usage Approach 2: HTTP/2 Multiplexing \u00b6 HTTP/2 behavior: - Opens single connection - Sends all 100 requests simultaneously (multiplexed) - Server streams responses back as ready - Uses single TCP connection efficiently Time analysis: - All requests sent: ~50ms (single RTT) - Server processing: ~200ms (parallel) - Total: ~250ms vs 5,000ms - 20x faster! Additional benefits: - Header compression (HPACK) - Server push (preload resources) - Stream prioritization Real-world impact: HTTP/1.1: 5 second load time \u2192 user abandonment HTTP/2: 250ms load time \u2192 seamless experience Mobile data savings: ~40% from header compression Your calculation: For 50 concurrent requests: HTTP/1.1 time: _____ ms HTTP/2 time: _____ ms Speedup factor: _____ x Case Studies: Networking in the Wild \u00b6 Online Gaming (Fortnite, Call of Duty): UDP for Speed \u00b6 Pattern: UDP for real-time game data. How it works: Player movements, actions, and shots are sent via UDP packets. If a packet is lost (e.g., showing a player's position from 50ms ago), the game doesn't wait. It simply discards the old data and uses the next available packet. Waiting for a TCP retransmission would cause noticeable lag (rubber-banding). Key Takeaway: For applications where the most recent data is more important than guaranteed delivery of every single piece of data, UDP is the superior choice. The trade-off is that the application layer must handle potential packet loss. Google & YouTube: HTTP/2 and HTTP/3 (QUIC) Adoption \u00b6 Pattern: Modern HTTP protocols for web performance. How it works: Google was a pioneer of both SPDY (the precursor to HTTP/2) and QUIC (the transport protocol for HTTP/3). On sites like YouTube, QUIC significantly reduces connection and stream setup time. This is especially noticeable on mobile networks, where it can seamlessly migrate a user's connection from Wi-Fi to cellular data without interrupting the video stream, a major weakness of TCP. Key Takeaway: Adopting modern protocols like HTTP/2 and HTTP/3 is critical for performance at scale. The move from TCP to a UDP-based protocol (QUIC) in HTTP/3 solves fundamental transport-layer problems like Head-of-Line blocking. Slack & Discord: WebSockets for Real-Time Chat \u00b6 Pattern: WebSockets for persistent, bidirectional communication. How it works: When you open Slack or Discord, your client establishes a single, long-lived WebSocket connection to their servers. When a new message is sent in a channel, the server pushes that message immediately to all connected clients in that channel. Key Takeaway: Compared to old-school HTTP polling, WebSockets reduce latency from seconds to milliseconds and drastically decrease unnecessary network traffic and server load, making them essential for any real-time interactive application. Netflix: DNS for Global Load Balancing \u00b6 Pattern: DNS-based Global Server Load Balancing (GSLB). How it works: When you press play on Netflix, your device makes a DNS request for the server hosting the video content. Netflix's DNS servers don't just return a single IP address; they return the IP address of the Open Connect Appliance (OCA) cache server that is geographically and topologically closest to you. Key Takeaway: DNS is not just for finding IPs. It's a powerful tool for global traffic routing. By directing users to the nearest server at the DNS level, Netflix ensures low latency, high-quality streaming and distributes load across its global content delivery network. Core Concepts \u00b6 Topic 1: Transport Layer - TCP vs UDP \u00b6 Concept: Two fundamental protocols for sending data over networks, with different guarantees and trade-offs. Key Differences: TCP (Transmission Control Protocol) Reliable: Guarantees delivery, ordering, and error checking Connection-oriented: Establishes connection before data transfer (3-way handshake) Flow control: Prevents overwhelming receiver Congestion control: Adapts to network conditions Overhead: Higher due to acknowledgments and retransmissions Use cases: HTTP, email, file transfers, databases UDP (User Datagram Protocol) Unreliable: Best-effort delivery, no guarantees Connectionless: Send packets without establishing connection No flow/congestion control: Fast but can lose packets Low overhead: Minimal protocol overhead Use cases: Video streaming, gaming, DNS, VoIP TCP 3-Way Handshake: Client Server | | | 1. SYN (seq=100) | |----------------------------->| | | | 2. SYN-ACK (seq=300, | | ack=101) | |<-----------------------------| | | | 3. ACK (ack=301) | |----------------------------->| | | | Connection established! | | Now can send data | TCP Flow Control (Sliding Window): Sender Receiver Window size = 4 packets Send: [1][2][3][4] \u2193 \u2193 \u2193 \u2193 \u2190------- ACK 1 \u2190------- ACK 2 \u2190------- ACK 3 Slide window \u2192 Send: [5][6][7][8] \u2193 \u2193 \u2193 \u2193 \u2190------- ACK 4 \u2190------- ACK 5 (LOST!) Timeout! Retransmit packet 5 [5] \u2193 \u2190------- ACK 5 (success) Decision Matrix: Requirement Protocol Why Must guarantee delivery TCP Retransmission on loss Need ordered packets TCP Sequence numbers Low latency critical UDP No overhead Can tolerate packet loss UDP No retransmission delay Real-time streaming UDP Prefer fresh data File transfer TCP Data integrity critical Live video UDP Skip lost frames Database connection TCP Reliability required After learning, explain in your own words: When would packet loss be acceptable? [Your answer] Why is TCP slower than UDP? [Your answer] What's the trade-off with reliability? [Your answer] Topic 2: HTTP Protocol Evolution \u00b6 Concept: HTTP has evolved from simple request-response to sophisticated multiplexed, compressed connections. HTTP/1.0 \u2192 HTTP/1.1 \u2192 HTTP/2 \u2192 HTTP/3 HTTP/1.1 (1997) \u00b6 Features: Persistent connections (keep-alive) Pipelining (send multiple requests without waiting) Chunked transfer encoding Host header (virtual hosting) Problems: Head-of-line blocking (HOL blocking) No multiplexing (sequential processing) Redundant headers (repeated with each request) Limited connections per domain (~6) Example of HOL Blocking: Browser needs: index.html, style.css, app.js, image.png Connection 1: GET /index.html [WAIT 200ms] \u2190 Response (large HTML file) GET /style.css [WAIT for index.html to finish!] \u2190 Response GET /app.js [WAIT for style.css!] \u2190 Response Total time: Sequential, each waits for previous HTTP/2 (2015) \u00b6 Major improvements: Binary protocol: More efficient than text-based HTTP/1.1 Multiplexing: Multiple requests/responses on single connection Server push: Proactively send resources Header compression (HPACK): Reduce overhead Stream prioritization: Critical resources first Multiplexing Example: Single TCP Connection \u2193 [Stream 1: index.html ] [Stream 2: style.css ] \u2190 All sent simultaneously [Stream 3: app.js ] [Stream 4: image.png ] \u2193 Responses interleaved on same connection: \u2190 [Stream 3 chunk] [Stream 1 chunk] [Stream 2 chunk] [Stream 4 chunk] Total time: ~RTT + max(processing times) Much faster than sequential! Header Compression (HPACK): HTTP/1.1 (repeated headers): Request 1: GET /api/users Host: api.example.com User-Agent: Mozilla/5.0... Accept: application/json Cookie: session=abc123... (500 bytes total) Request 2: GET /api/posts Host: api.example.com User-Agent: Mozilla/5.0... \u2190 Repeated! Accept: application/json \u2190 Repeated! Cookie: session=abc123... \u2190 Repeated! (500 bytes total) HTTP/2 (with HPACK): Request 1: 500 bytes (full headers) Request 2: 50 bytes (only differences!) Savings: 90% reduction HTTP/3 (2020) \u00b6 Built on QUIC (UDP-based): No HOL blocking at transport layer: HTTP/2 still suffered from TCP HOL blocking Faster connection establishment: 0-RTT handshake Better mobile performance: Connection migration (IP change resilience) Improved congestion control: Per-stream instead of per-connection HTTP/2 vs HTTP/3 (TCP vs QUIC): HTTP/2 (over TCP): Packet 5 lost in stream 2 \u2193 All streams blocked until packet 5 retransmitted! Streams 1, 3, 4 wait even though their packets arrived HTTP/3 (over QUIC): Packet 5 lost in stream 2 \u2193 Only stream 2 blocks, streams 1, 3, 4 continue Independent stream recovery Performance Comparison: Metric HTTP/1.1 HTTP/2 HTTP/3 Connections per domain 6 1 1 Multiplexing No Yes Yes Header compression No Yes (HPACK) Yes (QPACK) HOL blocking Yes Partial (TCP) No Connection setup 1-2 RTT 1-2 RTT 0-1 RTT Mobile resilience Poor Poor Excellent Topic 3: WebSockets & Real-Time Communication \u00b6 Concept: Persistent, bidirectional communication channel for real-time updates. HTTP vs WebSockets: HTTP (Request-Response): Client Server | | | HTTP GET | |----------------------->| | | | Response | |<-----------------------| | | | (Connection closed) | WebSocket (Persistent Connection): Client Server | | | HTTP Upgrade Request | |----------------------->| | | | 101 Switching | |<-----------------------| | | | \u2190------ Bidirectional --\u2192 | | Message | |----------------------->| | Message | |<-----------------------| | (Connection stays open) | WebSocket Handshake: Client \u2192 Server: GET /chat HTTP/1.1 Host: example.com Upgrade: websocket Connection: Upgrade Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ== Sec-WebSocket-Version: 13 Server \u2192 Client: HTTP/1.1 101 Switching Protocols Upgrade: websocket Connection: Upgrade Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo= [Connection now upgraded to WebSocket] Use Cases: Scenario HTTP Polling Server-Sent Events WebSockets Real-time chat \u2717 (wasteful) \u2717 (unidirectional) \u2713 (perfect) Stock ticker \u25b3 (acceptable) \u2713 (efficient) \u2713 (best) Notifications \u25b3 (wastes bandwidth) \u2713 (ideal) \u25b3 (overkill) Multiplayer game \u2717 (too slow) \u2717 (one-way) \u2713 (required) Social feed \u2713 (simple) \u2713 (efficient) \u25b3 (complex) Polling vs WebSockets (Message Frequency): Polling (every 5 seconds): \u250c\u25005s\u2500\u2510\u25005s\u2500\u2510\u25005s\u2500\u2510\u25005s\u2500\u2510 Req Req Req Req ... \u2193 \u2193 \u2193 \u2193 Res Res Res Res (empty)(data)(empty)(data) Overhead: 4 requests for 2 updates Latency: Up to 5s delay WebSockets (event-driven): Connection established \u2193 [Open connection] \u2193 \u2193 Data Data (instant push) Overhead: 0 extra requests Latency: ~RTT (milliseconds) Topic 4: DNS (Domain Name System) \u00b6 Concept: Hierarchical, distributed system that translates domain names to IP addresses. DNS Hierarchy: . (root) | \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 | | | .com .org .net | | example.com | \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510 www api DNS Resolution Process: User types: www.example.com 1. Browser cache check \u2514\u2500 Miss \u2192 Continue 2. OS cache check \u2514\u2500 Miss \u2192 Continue 3. Recursive resolver (ISP) \u2514\u2500 Queries root server 4. Root server \u2514\u2500 Returns .com nameserver address 5. .com nameserver \u2514\u2500 Returns example.com nameserver 6. example.com nameserver \u2514\u2500 Returns IP: 93.184.216.34 7. Resolver caches result (TTL: 3600s) 8. Returns IP to browser 9. Browser connects to 93.184.216.34 DNS Record Types: Record Type Purpose Example A IPv4 address example.com \u2192 93.184.216.34 AAAA IPv6 address example.com \u2192 2606:2800:220:1:... CNAME Alias to another domain www.example.com \u2192 example.com MX Mail server example.com \u2192 mail.example.com TXT Arbitrary text SPF, DKIM verification NS Name server example.com \u2192 ns1.dns.com DNS Caching & TTL: TTL (Time To Live) = 3600 seconds (1 hour) Query at 12:00 PM: example.com \u2192 1.2.3.4 (TTL: 3600) Cached until 1:00 PM Query at 12:30 PM: \u2190 Returns from cache (no network request) Query at 1:01 PM: Cache expired \u2192 New DNS query example.com \u2192 1.2.3.5 (IP changed!) DNS Load Balancing: Round-robin DNS: example.com can return multiple A records: Query 1: 1.2.3.4 Query 2: 1.2.3.5 Query 3: 1.2.3.6 Query 4: 1.2.3.4 (cycles back) Poor man's load balancing: + Simple, no extra infrastructure - No health checks - Client caching interferes - Not smart about server load Topic 5: TLS/SSL (Transport Layer Security) \u00b6 Concept: Cryptographic protocol for secure communication over networks. TLS Handshake (TLS 1.3 - Simplified): Client Server | | | 1. ClientHello | | - Supported cipher suites | | - Random nonce | | - Key share | |----------------------------->| | | | 2. ServerHello | | - Chosen cipher | | - Key share | | - Certificate | |<-----------------------------| | | | 3. [Derive shared secret] | | 4. Finished (encrypted) | |----------------------------->| | | | 5. Finished | | (encrypted) | |<-----------------------------| | | | \u2190------ Encrypted data ----\u2192 | Total: 1-RTT handshake (faster than TLS 1.2's 2-RTT) Certificate Chain Validation: Browser trusts: Root CA (in browser) | Intermediate CA | example.com cert Server sends: 1. example.com certificate 2. Intermediate CA certificate Browser validates: 1. example.com cert signed by Intermediate CA? \u2713 2. Intermediate CA cert signed by Root CA? \u2713 3. Root CA in trusted store? \u2713 4. Certificate not expired? \u2713 5. Domain matches? \u2713 All checks pass \u2192 Connection trusted Performance Impact: HTTPS (TLS) overhead: - Initial handshake: +1 RTT (~50-100ms) - Encryption/decryption CPU: ~5% server CPU - Certificate chain: +2-4 KB per connection Mitigation strategies: - Session resumption (reuse session keys) - TLS 1.3 0-RTT (resume with 0 round trips) - OCSP stapling (reduce cert validation RTT) - Connection pooling (amortize handshake cost) Topic 6: Load Balancing (L4 vs L7) \u00b6 Concept: Distribute traffic across multiple servers for reliability and performance. Layer 4 (Transport Layer) Load Balancing: Client connects to: lb.example.com:443 L4 Load Balancer: \u250c\u2500 Looks at: IP address, Port, Protocol \u2502 Does NOT look at: HTTP headers, cookies, URLs \u2502 \u2514\u2500 Forwards TCP connection to backend: Server 1: 10.0.1.5:443 Server 2: 10.0.1.6:443 Server 3: 10.0.1.7:443 Pros: + Fast (no packet inspection) + Works for any TCP/UDP protocol + Low latency (simple forwarding) Cons: - Can't route based on URL/headers - No application-aware decisions - Sticky sessions require IP hashing Layer 7 (Application Layer) Load Balancing: Client request: https://example.com/api/users L7 Load Balancer: \u250c\u2500 Terminates TLS connection \u2502 Reads full HTTP request \u2502 Inspects: URL path, headers, cookies \u2502 \u2514\u2500 Routes based on rules: /api/* \u2192 API servers (10.0.2.x) /static/* \u2192 CDN servers (10.0.3.x) /admin/* \u2192 Admin servers (10.0.4.x) Pros: + Intelligent routing (URL, headers, etc.) + Session affinity (cookie-based) + Content-based caching + SSL termination Cons: - Slower (packet inspection overhead) - More CPU intensive - Application-specific (HTTP/gRPC/etc.) Load Balancing Algorithms: Algorithm How It Works Use Case Round Robin Cycle through servers 1\u21922\u21923\u21921 Uniform workload Least Connections Send to server with fewest active connections Varied request duration IP Hash Hash client IP to pick server Session persistence Least Response Time Route to fastest server Performance-critical Weighted Round Robin More traffic to powerful servers Heterogeneous servers Health Checks: Load balancer \u2192 Server health check Active checks (periodic): Every 10s: GET /health If 3 consecutive failures \u2192 Mark unhealthy If 2 consecutive successes \u2192 Mark healthy Passive checks (observed): Track response times, error rates If error rate > 5% \u2192 Reduce traffic If response time > 1s \u2192 Reduce traffic Unhealthy server: \u250c\u2500 Stop sending new requests \u2514\u2500 Drain existing connections (graceful) Decision Framework \u00b6 Your task: Build decision trees for when to use each networking approach. Question 1: Which Protocol? \u00b6 Use TCP when: Reliability is critical: [Financial transactions, file transfers] Ordering matters: [Database replication, messaging] Data integrity required: [API calls, downloads] Use UDP when: Low latency critical: [Gaming, VoIP] Packet loss acceptable: [Video streaming, DNS] Real-time more important than reliability: [Live broadcasts] Question 2: HTTP Version? \u00b6 Use HTTP/1.1 when: Legacy client support required Simple deployment (no special infrastructure) Low concurrency use case Use HTTP/2 when: Modern browsers/clients High concurrency (many resources) API with many endpoints Use HTTP/3 when: Mobile-first application Global user base (varying network quality) WebRTC or real-time features Question 3: Real-Time Communication? \u00b6 Use HTTP polling when: Infrequent updates (> 30 seconds) Simple implementation required Client controls update frequency Use Server-Sent Events (SSE) when: Server \u2192 Client updates only Text-based data (JSON) Browser compatibility important Use WebSockets when: Bidirectional communication required Low latency critical (< 100ms) High message frequency (> 1/sec) Question 4: Load Balancer Layer? \u00b6 Use L4 load balancing when: Protocol-agnostic (TCP/UDP) Maximum performance needed Simple traffic distribution Use L7 load balancing when: Content-based routing required SSL termination beneficial Application-aware features needed Practice Scenarios \u00b6 Scenario 1: E-Commerce Platform \u00b6 Requirements: Product catalog browsing Real-time inventory updates Checkout process 10K concurrent users Global user base Your design: Protocol choices: Catalog API: [HTTP/2 or HTTP/3? Why?] Inventory updates: [WebSocket/SSE/Polling?] Checkout: [HTTPS with what considerations?] Load balancing: Layer: [L4 or L7? Why?] Algorithm: [Which algorithm?] Sticky sessions: [Needed?] DNS strategy: TTL: [How long?] Multi-region: [GeoDNS?] Scenario 2: Multiplayer Game \u00b6 Requirements: 60 tick rate (updates per second) < 50ms latency requirement 100 players per game Unreliable networks (mobile) Your design: Protocol: [TCP or UDP? Why?] Packet loss handling: Strategy: [How to handle?] Acceptable loss rate: [X%?] Latency optimization: Connection pooling: [Helpful?] Regional servers: [Required?] Scenario 3: Video Conferencing \u00b6 Requirements: Real-time audio/video Screen sharing Chat messaging Recording capability Your design: Media protocols: Audio/Video: [UDP/TCP/WebRTC?] Chat: [WebSocket/HTTP?] Recording: [How to implement?] Quality vs Latency: Packet loss: [How to handle?] Bandwidth adaptation: [Strategy?] Review Checklist \u00b6 Before moving to the next topic: Understanding Understand TCP 3-way handshake Know difference between TCP and UDP Understand HTTP/2 multiplexing Know how WebSockets work Understand DNS resolution process Know TLS handshake basics Understand L4 vs L7 load balancing Protocol Selection Can choose between TCP and UDP Know when to use HTTP/2 vs HTTP/3 Understand WebSocket use cases Can design DNS strategy Performance Optimization Know connection pooling benefits Understand header compression Can calculate latency impact Know caching strategies Decision Making Completed practice scenarios Can explain trade-offs Understand failure modes Mastery Certification \u00b6 I certify that I can: Explain TCP vs UDP trade-offs Design protocol choice for new system Understand HTTP evolution benefits Choose appropriate real-time protocol Configure DNS for performance Explain TLS handshake process Select load balancing strategy Optimize network performance Debug networking issues Teach networking concepts to others","title":"03. Networking Fundamentals"},{"location":"systems/03-networking-fundamentals/#networking-fundamentals","text":"TCP/IP vs UDP, HTTP versions, WebSockets, DNS, TLS, and load balancing","title":"Networking Fundamentals"},{"location":"systems/03-networking-fundamentals/#eli5-explain-like-im-5","text":"Your task: After learning networking fundamentals, explain them simply. Prompts to guide you: What is the difference between TCP and UDP in one sentence? Your answer: [Fill in after learning] Why do we need DNS? Your answer: [Fill in after learning] Real-world analogy for TCP vs UDP: Example: \"TCP is like certified mail where...\" Your analogy: [Fill in] What does HTTP/2 multiplexing solve? Your answer: [Fill in after learning] Real-world analogy for WebSockets: Example: \"WebSockets are like a phone call where...\" Your analogy: [Fill in] Why do we need load balancers? Your answer: [Fill in after practice]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/03-networking-fundamentals/#quick-quiz-do-before-learning","text":"Your task: Test your intuition about networking without looking at details. Answer these, then verify after learning.","title":"Quick Quiz (Do BEFORE learning)"},{"location":"systems/03-networking-fundamentals/#beforeafter-why-networking-fundamentals-matter","text":"Your task: Compare naive networking assumptions vs proper understanding to see the impact.","title":"Before/After: Why Networking Fundamentals Matter"},{"location":"systems/03-networking-fundamentals/#case-studies-networking-in-the-wild","text":"","title":"Case Studies: Networking in the Wild"},{"location":"systems/03-networking-fundamentals/#core-concepts","text":"","title":"Core Concepts"},{"location":"systems/03-networking-fundamentals/#decision-framework","text":"Your task: Build decision trees for when to use each networking approach.","title":"Decision Framework"},{"location":"systems/03-networking-fundamentals/#practice-scenarios","text":"","title":"Practice Scenarios"},{"location":"systems/03-networking-fundamentals/#review-checklist","text":"Before moving to the next topic: Understanding Understand TCP 3-way handshake Know difference between TCP and UDP Understand HTTP/2 multiplexing Know how WebSockets work Understand DNS resolution process Know TLS handshake basics Understand L4 vs L7 load balancing Protocol Selection Can choose between TCP and UDP Know when to use HTTP/2 vs HTTP/3 Understand WebSocket use cases Can design DNS strategy Performance Optimization Know connection pooling benefits Understand header compression Can calculate latency impact Know caching strategies Decision Making Completed practice scenarios Can explain trade-offs Understand failure modes","title":"Review Checklist"},{"location":"systems/04-search-and-indexing/","text":"Search & Indexing \u00b6 Inverted indexes, full-text search, ranking algorithms, and distributed search systems ELI5: Explain Like I'm 5 \u00b6 Your task: After learning search and indexing, explain them simply. Prompts to guide you: What is an inverted index in one sentence? Your answer: [Fill in after learning] Why can't we just search through all documents linearly? Your answer: [Fill in after learning] Real-world analogy for inverted index: Example: \"An inverted index is like a book's index where...\" Your analogy: [Fill in] What makes search results \"relevant\"? Your answer: [Fill in after learning] Real-world analogy for search ranking: Example: \"TF-IDF is like voting where...\" Your analogy: [Fill in] Why do we need sharding for search? Your answer: [Fill in after practice] Quick Quiz (Do BEFORE learning) \u00b6 Your task: Test your intuition about search without looking at details. Answer these, then verify after learning. Complexity Predictions \u00b6 Linear search through 1M documents: Time complexity: [Your guess: O(?)] Estimated time: [Milliseconds? Seconds?] Verified: [Actual] Inverted index lookup: Time complexity: [Your guess: O(?)] Space overhead: [How much extra storage?] Verified: [Actual] Autocomplete suggestions with trie: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Scenario Predictions \u00b6 Scenario 1: Search query \"machine learning\" in 10M documents Without index: [How long?] With inverted index: [How long?] Speedup factor: [____x faster?] Scenario 2: E-commerce product search with typos Exact match only: [User experience?] Fuzzy search: [How to implement?] Trade-off: [Performance vs accuracy?] Scenario 3: Ranking search results by relevance Algorithm: [TF-IDF? BM25? Something else?] Factors to consider: [What matters?] Personalization: [How to incorporate?] Before/After: Why Search Indexing Matters \u00b6 Your task: Compare naive search vs indexed search to understand the impact. Example: Product Search on E-Commerce Site \u00b6 Problem: Search for \"wireless headphones\" across 10 million products Approach 1: Naive Linear Search \u00b6 SELECT * FROM products WHERE LOWER(name) LIKE '%wireless%' AND LOWER(name) LIKE '%headphones%' OR LOWER(description) LIKE '%wireless%' AND LOWER(description) LIKE '%headphones%'; What happens: Database scans all 10M rows: Row 1: Check name, check description \u2192 No match Row 2: Check name, check description \u2192 No match ... Row 45,234: Check name \u2192 MATCH! (add to results) ... Row 10,000,000: Check name, check description \u2192 No match Time: ~10-30 seconds Database CPU: 100% User experience: Loading spinner... user leaves Problems: O(N \u00d7 M) where N = documents, M = avg document size No ranking (random order) No fuzzy matching (\"wireles\" returns nothing) Kills database under load Approach 2: Inverted Index Search \u00b6 Index structure: Inverted Index: \"wireless\" \u2192 [doc45234, doc89123, doc234556, ...] \"headphones\" \u2192 [doc12345, doc45234, doc67890, ...] Intersection: [doc45234] \u2190 Both terms present Ranking (TF-IDF): doc45234: score = 8.7 (both in title, high frequency) doc89123: score = 6.2 (both in description) doc12345: score = 4.1 (only \"headphones\" in title) Query execution: 1. Look up \"wireless\" \u2192 [45234, 89123, 234556, ...] (1ms) 2. Look up \"headphones\" \u2192 [12345, 45234, 67890, ...] (1ms) 3. Intersect lists \u2192 [45234, ...] (5ms) 4. Rank by score \u2192 sorted results (10ms) 5. Return top 20 results (1ms) Total: ~20ms Database CPU: 5% User experience: Instant results \u2713 Performance comparison: Metric Linear Search Inverted Index Improvement Time 10-30s 20ms 500-1500x faster CPU usage 100% 5% 20x less Scalability O(N) O(log N) Sublinear Ranking No Yes (TF-IDF) Better UX Fuzzy match Hard Easy More results Real-world impact: Without indexing: 80% bounce rate (users leave) With indexing: 5% bounce rate, 10x more conversions Cost: $100K/month in extra servers vs $10K/month with proper indexing Your calculation: For 100M documents: Linear search time: _____ seconds Indexed search time: _____ ms Users served per second: Linear _ vs Indexed ___ Case Studies: Search & Indexing in the Wild \u00b6 Google Search: PageRank and the Inverted Index \u00b6 Pattern: Distributed Inverted Index combined with the PageRank ranking algorithm. How it works: Google's crawlers build a massive inverted index of the web. When you search, your query terms are used to retrieve a list of matching documents. The magic is in the ranking: PageRank analyzes the web's link structure, treating a link from page A to page B as a \"vote\" for page B. It ranks pages higher if they are linked to by many other high-ranking pages. Key Takeaway: A fast inverted index is only half the battle. The relevance of search results is determined by sophisticated ranking algorithms. PageRank revolutionized search by using the collective intelligence of the web itself to determine authority and importance. Elasticsearch: Powering Enterprise Search \u00b6 Pattern: Distributed, Sharded Inverted Index (using Apache Lucene). How it works: Companies like Uber (for searching trips), Stack Overflow (for finding questions), and * Netflix (for catalog search) use Elasticsearch. It automatically builds an inverted index on JSON documents. To scale, it partitions the index into multiple shards *, and replicates them for fault tolerance. A query is sent to all shards in parallel, and the results are aggregated by a coordinating node. Key Takeaway: Elasticsearch democratized high-quality search. It packages the complex concepts of inverted indexes, text analysis, and distributed systems into a scalable, easy-to-use product, making it the de-facto standard for adding search capabilities to applications. Algolia: Search-as-a-Service for Speed \u00b6 Pattern: In-Memory, Prefix-based Trie/Index Hybrid. How it works: Algolia is designed for \"instant search\" and autocomplete experiences. They store their indices entirely in RAM and distribute them across multiple data centers for low latency. Their ranking is often based on a tie-breaking algorithm that can be heavily customized with business metrics (e.g., for an e-commerce site, rank products with more sales higher). Key Takeaway: For user-facing search where speed is paramount, in-memory indices and pre-computed ranking can provide a superior user experience. The trade-off is higher cost and a focus on prefix-matching rather than complex full-text relevance ranking. Core Concepts \u00b6 Topic 1: Inverted Index \u00b6 Concept: Data structure that maps terms to documents, enabling fast full-text search. How It Works: Forward Index (Document \u2192 Terms): doc1: \"the quick brown fox\" doc2: \"the lazy dog\" doc3: \"quick brown dog\" Not efficient for search! To find \"quick\", must scan all documents. Inverted Index (Term \u2192 Documents): \"the\" \u2192 [doc1, doc2] \"quick\" \u2192 [doc1, doc3] \"brown\" \u2192 [doc1, doc3] \"fox\" \u2192 [doc1] \"lazy\" \u2192 [doc2] \"dog\" \u2192 [doc2, doc3] Search \"quick\" \u2192 O(1) lookup \u2192 [doc1, doc3] With Positional Information: \"quick\" \u2192 [ doc1: [positions: 1], doc3: [positions: 0] ] \"brown\" \u2192 [ doc1: [positions: 2], doc3: [positions: 1] ] Phrase search \"quick brown\": - Find docs with both terms - Check if positions are adjacent - doc1: positions 1,2 \u2192 \u2713 Match! - doc3: positions 0,1 \u2192 \u2713 Match! Building an Inverted Index: Input documents: doc1: \"Machine Learning Basics\" doc2: \"Deep Learning with Python\" doc3: \"Machine Learning Algorithms\" Step 1: Tokenization doc1: [\"machine\", \"learning\", \"basics\"] doc2: [\"deep\", \"learning\", \"with\", \"python\"] doc3: [\"machine\", \"learning\", \"algorithms\"] Step 2: Normalization (lowercase, stemming) doc1: [\"machin\", \"learn\", \"basic\"] doc2: [\"deep\", \"learn\", \"with\", \"python\"] doc3: [\"machin\", \"learn\", \"algorithm\"] Step 3: Build index { \"machin\": [doc1, doc3], \"learn\": [doc1, doc2, doc3], \"basic\": [doc1], \"deep\": [doc2], \"with\": [doc2], \"python\": [doc2], \"algorithm\": [doc3] } Step 4: Add term frequencies (TF) { \"learn\": [ {doc: doc1, tf: 1, positions: [1]}, {doc: doc2, tf: 1, positions: [1]}, {doc: doc3, tf: 1, positions: [1]} ] } Inverted Index Data Structure (Simplified): class InvertedIndex { // Term \u2192 Posting List Map<String, PostingList> index; class PostingList { List<Posting> postings; int documentFrequency; // How many docs contain this term } class Posting { int documentId; int termFrequency; // How many times in this doc List<Integer> positions; // Where in doc } // Search for term PostingList search(String term) { return index.get(normalize(term)); } // Boolean AND query: term1 AND term2 List<Integer> intersect(String term1, String term2) { PostingList p1 = search(term1); PostingList p2 = search(term2); // Merge postings lists (efficient with sorted lists) return mergeSortedLists(p1, p2); } } Time Complexity: Operation Complexity Notes Single term lookup O(1) or O(log V) V = vocabulary size Boolean AND (2 terms) O(n\u2081 + n\u2082) n = posting list sizes Boolean OR O(n\u2081 + n\u2082) Merge lists Phrase query O(n \u00d7 k) k = terms in phrase Index construction O(N \u00d7 M) N = docs, M = avg size Topic 2: Text Analysis & Tokenization \u00b6 Concept: Process of converting raw text into searchable terms. Text Analysis Pipeline: Input: \"The Quick Brown Fox's 2024 Adventure!\" Step 1: Character Filtering \u2192 \"The Quick Brown Fox's 2024 Adventure\" (Remove special characters) Step 2: Tokenization \u2192 [\"The\", \"Quick\", \"Brown\", \"Fox's\", \"2024\", \"Adventure\"] (Split into words) Step 3: Lowercase Filter \u2192 [\"the\", \"quick\", \"brown\", \"fox's\", \"2024\", \"adventure\"] Step 4: Stop Word Removal (optional) \u2192 [\"quick\", \"brown\", \"fox's\", \"2024\", \"adventure\"] (Remove \"the\") Step 5: Stemming/Lemmatization \u2192 [\"quick\", \"brown\", \"fox\", \"2024\", \"adventur\"] (fox's \u2192 fox, adventure \u2192 adventur) Step 6: N-gram Generation (optional) \u2192 [\"quick\", \"brown\", \"fox\", \"quick brown\", \"brown fox\"] Tokenization Strategies: Word-based tokenization: Input: \"I'm learning Elasticsearch 8.0!\" Whitespace tokenizer: \u2192 [\"I'm\", \"learning\", \"Elasticsearch\", \"8.0!\"] Standard tokenizer (Unicode-aware): \u2192 [\"I'm\", \"learning\", \"Elasticsearch\", \"8\", \"0\"] Letter tokenizer (only letters): \u2192 [\"I\", \"m\", \"learning\", \"Elasticsearch\"] N-gram tokenization (for autocomplete, fuzzy search): Input: \"search\" Unigrams: [\"s\", \"e\", \"a\", \"r\", \"c\", \"h\"] Bigrams: [\"se\", \"ea\", \"ar\", \"rc\", \"ch\"] Trigrams: [\"sea\", \"ear\", \"arc\", \"rch\"] Use case: Fuzzy search Query \"serch\" (typo): - Trigrams: [\"ser\", \"erc\", \"rch\"] - \"search\" trigrams: [\"sea\", \"ear\", \"arc\", \"rch\"] - Overlap: [\"erc\", \"rch\"] \u2192 Possible match! Stemming vs Lemmatization: Stemming (rule-based, faster, less accurate): running \u2192 run runs \u2192 run runner \u2192 runner (different stem!) better \u2192 better (no change) Lemmatization (dictionary-based, slower, accurate): running \u2192 run runs \u2192 run runner \u2192 runner better \u2192 good (finds lemma!) Trade-off: Stemming: Fast, simple, handles unknown words Lemmatization: Accurate, context-aware, slower Topic 3: Ranking Algorithms \u00b6 Concept: Algorithms to score and rank search results by relevance. TF-IDF (Term Frequency-Inverse Document Frequency): Formula: TF-IDF(term, doc) = TF(term, doc) \u00d7 IDF(term) TF(term, doc) = (count of term in doc) / (total terms in doc) IDF(term) = log(total documents / documents containing term) Example: Documents: doc1: \"cat dog cat\" doc2: \"dog bird\" doc3: \"cat bird fish\" doc4: \"fish\" Query: \"cat dog\" For \"cat\" in doc1: TF = 2/3 = 0.67 (appears 2 times in 3 words) IDF = log(4/2) = log(2) = 0.30 (in 2 out of 4 docs) TF-IDF = 0.67 \u00d7 0.30 = 0.20 For \"dog\" in doc1: TF = 1/3 = 0.33 IDF = log(4/2) = 0.30 TF-IDF = 0.33 \u00d7 0.30 = 0.10 doc1 score = 0.20 + 0.10 = 0.30 doc2 score = [calculate similarly] doc3 score = [calculate similarly] Ranked results: 1. doc1 (0.30) \u2190 Both terms, high frequency 2. doc2 (0.15) \u2190 Has \"dog\" only 3. doc3 (0.10) \u2190 Has \"cat\" only BM25 (Best Match 25) - Modern Improvement: BM25 advantages over TF-IDF: 1. Diminishing returns for term frequency (10 occurrences not 10x better than 1) 2. Document length normalization (Shorter docs don't get unfair advantage) 3. Tunable parameters (k\u2081, b) Formula (simplified): score = IDF \u00d7 (TF \u00d7 (k\u2081 + 1)) / (TF + k\u2081 \u00d7 (1 - b + b \u00d7 docLen/avgDocLen)) Where: k\u2081 = term frequency saturation (default: 1.2) b = length normalization (default: 0.75) Practical Example (Elasticsearch): Query: \"machine learning tutorial\" BM25 scoring: doc1: \"Machine Learning Tutorial for Beginners\" - All 3 terms present \u2713 - Terms in title (boosted) \u2713 - Short document \u2713 Score: 12.5 doc2: \"Introduction to Machine Learning: A Complete Tutorial Guide...\" - All 3 terms present \u2713 - \"machine learning\" together (phrase bonus) \u2713 - Long document \u2717 Score: 10.2 doc3: \"Python Tutorial\" - 1 term present (\"tutorial\") - Common term (lower IDF) \u2717 Score: 2.1 Ranking: doc1 > doc2 > doc3 Field Boosting: { \"query\": { \"multi_match\": { \"query\": \"elasticsearch\", \"fields\": [ \"title^3\", // 3x weight \"category^2\", // 2x weight \"content^1\" // 1x weight (default) ] } } } Effect: Match in title = 3x more important than content Prioritizes documents with query terms in prominent fields Topic 4: Advanced Search Features \u00b6 Concept: Techniques for fuzzy matching, autocomplete, and query optimization. Fuzzy Search (Edit Distance): Levenshtein Distance: minimum edits to transform one string into another Example: \"quick\" \u2192 \"quik\" Operations: 1. Delete 'c' \u2192 \"quik\" Distance = 1 Query: \"elasticsarch\" (typo) Fuzzy search with distance 2: - \"elasticsearch\" (distance 1: insert 'e') - \"elasticcache\" (too far, distance 3) Implementation: query: { \"fuzzy\": { \"name\": { \"value\": \"elasticsarch\", \"fuzziness\": \"AUTO\" // 0,1,2 based on term length } } } Autocomplete with Tries: Trie structure for autocomplete: root / \\ s c / \\ \\ e t a / \\ \\ a o t / \\ r p / c h Words: \"search\", \"stop\", \"cat\" Autocomplete \"se\": 1. Traverse: root \u2192 s \u2192 e 2. Find all descendants: \"search\" 3. Return top 10 by frequency Time: O(k) where k = prefix length Space: O(N \u00d7 M) where N = words, M = avg length Autocomplete with Edge N-grams: Term: \"search\" Edge n-grams (prefix-based): s se sea sear searc search Index these as tokens: \"s\" \u2192 [search, ...] \"se\" \u2192 [search, set, ...] \"sea\" \u2192 [search, season, ...] Query \"sea\": \u2192 Instant lookup \u2192 [search, season, ...] Advantage: O(1) lookup vs Trie traversal Disadvantage: Higher storage cost Highlighting: Query: \"machine learning\" Result: \"Introduction to Machine Learning algorithms...\" Highlighted: \"Introduction to <em>Machine</em> <em>Learning</em> algorithms...\" Implementation: 1. Find term positions from inverted index 2. Extract surrounding context (\u00b150 chars) 3. Insert highlight tags 4. Return fragments Advanced: Multi-field highlighting Show snippets from title, description, and content Rank snippets by relevance Topic 5: Distributed Search (Elasticsearch Architecture) \u00b6 Concept: Scale search across multiple nodes for performance and reliability. Sharding: Index: 10M documents Shard into 5 primary shards: Shard 0: docs 0-2M Shard 1: docs 2M-4M Shard 2: docs 4M-6M Shard 3: docs 6M-8M Shard 4: docs 8M-10M Query execution: 1. Send query to all 5 shards (parallel) 2. Each shard searches its 2M docs 3. Return top 10 from each shard 4. Coordinator merges 50 results 5. Return final top 10 Time: O(N/S) where S = shards Parallelization: 5x faster (ideal) Replication: Cluster: 3 nodes Index: 2 primary shards, 1 replica Distribution: Node 1: Primary 0, Replica 1 Node 2: Primary 1, Replica 0 Node 3: Replica 0, Replica 1 Availability: - Node 1 fails \u2192 Node 2 has Primary 1 and Replica 0 - All data still accessible - Automatic promotion of replicas Performance: - Reads can use replicas (load balancing) - Writes go to primary, then replicated Query Execution Flow: Client \u2192 Load Balancer \u2192 Coordinating Node Coordinating Node: 1. Parse query 2. Determine target shards 3. Broadcast to all shards 4. Collect results 5. Merge and rank 6. Return to client Coordinator / | \\ Shard0 Shard1 Shard2 \u2193 \u2193 \u2193 [10] [10] [10] \u2190 Top 10 from each \\ | / Merge & Sort \u2193 Final Top 10 Routing: Document ID \u2192 Shard assignment Hash-based routing: shard = hash(document_id) % num_shards Custom routing (co-locate related docs): PUT /users/_doc/user123?routing=tenant_A \u2192 All tenant_A docs in same shard \u2192 Faster tenant-scoped queries Trade-off: - Better locality - Risk of unbalanced shards Topic 6: Search Optimization \u00b6 Concept: Techniques to improve search performance and relevance. Index-Time Optimizations: 1. Selective Indexing: { \"mappings\": { \"properties\": { \"id\": { \"type\": \"keyword\", \"index\": false // Don't index, only store }, \"title\": { \"type\": \"text\", \"analyzer\": \"english\" // Full-text search }, \"created_at\": { \"type\": \"date\", \"index\": true // For filtering/sorting } } } } Effect: - Smaller index size - Faster indexing - Reduced memory usage 2. Doc Values (Column Store): Traditional (row-oriented): doc1: {name: \"Alice\", age: 30, city: \"NYC\"} doc2: {name: \"Bob\", age: 25, city: \"LA\"} Doc values (column-oriented): name: [\"Alice\", \"Bob\"] age: [30, 25] city: [\"NYC\", \"LA\"] Use case: Aggregations, sorting Query: \"Average age by city\" \u2192 Scan age and city columns only \u2192 Much faster than loading full docs Query-Time Optimizations: 1. Filter Context vs Query Context: // Query context (scored): { \"query\": { \"match\": { \"content\": \"elasticsearch\" // Score by relevance } } } // Filter context (not scored, cacheable): { \"query\": { \"bool\": { \"must\": { \"match\": { \"content\": \"elasticsearch\" } }, \"filter\": { \"term\": { \"status\": \"published\" } // Binary: yes/no } } } } Filter benefits: + Cached (reused across queries) + Faster (no scoring) + Use for: dates, categories, flags 2. Query Caching: Request cache (entire query result): { \"size\": 0, // Only aggregations, no docs \"aggs\": { ... } } \u2192 Cached for 1 minute \u2192 Subsequent identical queries instant Shard request cache: - Caches query results per shard - Invalidated on shard changes - Shared across all queries hitting shard 3. Pagination: Deep pagination problem: GET /products?from=10000&size=10 Process: 1. Each shard returns top 10,010 results 2. Coordinator merges (5 shards \u00d7 10,010 = 50,050 docs) 3. Sorts all 50,050 4. Returns results 10,000-10,010 Memory intensive! CPU intensive! Solution 1: Search After (cursor-based) GET /products?search_after=[value] \u2192 Continue from last result \u2192 No offset limit Solution 2: Scroll API (for exports) GET /products?scroll=1m \u2192 Snapshot of results \u2192 Iterate through batches Decision Framework \u00b6 Question 1: When to build inverted index? \u00b6 Build inverted index when: Full-text search required: [Search within documents] Fast lookups critical: [< 100ms query time] Dataset size > 10K docs: [Linear search too slow] Skip inverted index when: Exact key lookups only: [Use hash table] Tiny dataset (< 1K docs): [Linear scan acceptable] Write-heavy, rare reads: [Index overhead not worth it] Question 2: TF-IDF vs BM25? \u00b6 Use TF-IDF when: Simple implementation needed Legacy system compatibility Small datasets Use BM25 when: Production search system Varied document lengths Better ranking required (modern default) Question 3: Sharding strategy? \u00b6 Use hash-based sharding when: Uniform distribution desired No co-location requirements Simple setup Use custom routing when: Multi-tenancy (tenant per shard) Co-locate related documents Optimize specific query patterns Practice Scenarios \u00b6 Scenario 1: E-Commerce Product Search \u00b6 Requirements: 10M products Autocomplete (< 50ms) Fuzzy matching for typos Category filters Price range filters Rank by relevance + popularity Your design: Index structure: Shards: [How many?] Replicas: [How many for HA?] Mapping: [Which fields indexed?] Query strategy: Autocomplete: [Trie? Edge n-grams?] Fuzzy: [Levenshtein distance?] Ranking: [BM25 + custom boost?] Scenario 2: Log Search (Observability) \u00b6 Requirements: 1B log entries/day Time-range queries Full-text search on messages Retention: 30 days Real-time ingestion Your design: Index strategy: Time-based indices: [Daily? Hourly?] Rollover policy: [When?] Deletion: [How to handle retention?] Query optimization: Filter by time: [Use filter context?] Aggregations: [Doc values?] Pagination: [Scroll API?] Scenario 3: Knowledge Base Search \u00b6 Requirements: 100K articles Multi-language support Highlight search terms \"Did you mean?\" suggestions Related articles Your design: Text analysis: Language detection: [How?] Stemming: [Language-specific?] Synonyms: [How to implement?] Features: Highlighting: [Unified? Plain?] Suggestions: [Fuzzy + frequency?] Related: [More Like This query?] Review Checklist \u00b6 Before moving to the next topic: Understanding Understand inverted index structure Know tokenization pipeline Understand TF-IDF and BM25 Know sharding and replication Understand query execution flow Implementation Can design index mapping Know when to use analyzers Understand filter vs query context Can optimize queries Decision Making Know when to shard Can choose ranking algorithm Understand trade-offs Completed practice scenarios Mastery Certification \u00b6 I certify that I can: Explain inverted index structure Design search system for requirements Choose appropriate text analysis Implement ranking algorithms Optimize query performance Configure sharding strategy Debug search relevance issues Build autocomplete feature Handle multi-language search Teach search concepts to others","title":"04. Search & Indexing"},{"location":"systems/04-search-and-indexing/#search-indexing","text":"Inverted indexes, full-text search, ranking algorithms, and distributed search systems","title":"Search &amp; Indexing"},{"location":"systems/04-search-and-indexing/#eli5-explain-like-im-5","text":"Your task: After learning search and indexing, explain them simply. Prompts to guide you: What is an inverted index in one sentence? Your answer: [Fill in after learning] Why can't we just search through all documents linearly? Your answer: [Fill in after learning] Real-world analogy for inverted index: Example: \"An inverted index is like a book's index where...\" Your analogy: [Fill in] What makes search results \"relevant\"? Your answer: [Fill in after learning] Real-world analogy for search ranking: Example: \"TF-IDF is like voting where...\" Your analogy: [Fill in] Why do we need sharding for search? Your answer: [Fill in after practice]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/04-search-and-indexing/#quick-quiz-do-before-learning","text":"Your task: Test your intuition about search without looking at details. Answer these, then verify after learning.","title":"Quick Quiz (Do BEFORE learning)"},{"location":"systems/04-search-and-indexing/#beforeafter-why-search-indexing-matters","text":"Your task: Compare naive search vs indexed search to understand the impact.","title":"Before/After: Why Search Indexing Matters"},{"location":"systems/04-search-and-indexing/#case-studies-search-indexing-in-the-wild","text":"","title":"Case Studies: Search &amp; Indexing in the Wild"},{"location":"systems/04-search-and-indexing/#core-concepts","text":"","title":"Core Concepts"},{"location":"systems/04-search-and-indexing/#decision-framework","text":"","title":"Decision Framework"},{"location":"systems/04-search-and-indexing/#practice-scenarios","text":"","title":"Practice Scenarios"},{"location":"systems/04-search-and-indexing/#review-checklist","text":"Before moving to the next topic: Understanding Understand inverted index structure Know tokenization pipeline Understand TF-IDF and BM25 Know sharding and replication Understand query execution flow Implementation Can design index mapping Know when to use analyzers Understand filter vs query context Can optimize queries Decision Making Know when to shard Can choose ranking algorithm Understand trade-offs Completed practice scenarios","title":"Review Checklist"},{"location":"systems/05-caching-patterns/","text":"Caching Patterns \u00b6 Master LRU, LFU, and write policies for high-performance systems ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is caching in one sentence? Your answer: [Fill in after implementation] Why/when do we use caching? Your answer: [Fill in after implementation] Real-world analogy: Example: \"A cache is like keeping your favorite books on your desk instead of walking to the library...\" Your analogy: [Fill in] What's the difference between LRU and LFU? Your answer: [Fill in after solving problems] When should you use Write-Through vs Write-Back? Your answer: [Fill in after practice] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Direct database query for each request: Time per request: [Your guess: O(?)] If DB query takes 50ms, how many requests/sec can you handle? _____ Verified after learning: [Actual] Cache lookup + occasional DB query: Cache hit time: [Your guess: O(?)] Cache miss time: [Your guess: O(?)] If 90% cache hit rate, average latency: _____ ms Verified: [Actual] Hit rate calculation: 1000 requests, 900 cache hits, 100 misses Hit rate: _____ % If cache saves 45ms per hit, total time saved: _____ ms Scenario Predictions \u00b6 Scenario 1: E-commerce product catalog with access pattern: Product A: accessed 5 times Product B: accessed 10 times Product C: accessed 3 times Product D: accessed 8 times Cache capacity: 2 items With LRU, which items remain after all accesses? [Fill in - trace manually] With LFU, which items remain? [Fill in - trace manually] Which is better for this pattern? [LRU/LFU - Why?] Scenario 2: User session cache (last access time matters) Session A: last accessed 10 min ago Session B: last accessed 2 min ago Session C: last accessed 5 min ago Cache full, new session arrives Which eviction policy makes sense? [LRU/LFU - Why?] Which session gets evicted? [Fill in] Scenario 3: Write policies Request: Update user profile Write-Through: Cache + DB both take 5ms each Write-Back: Cache takes 1ms, DB flush happens later Write-Through total latency: _____ms Write-Back perceived latency: _____ms If DB fails during Write-Back flush, what happens? [Fill in] Which is safer? [Fill in - Why?] Trade-off Quiz \u00b6 Question 1: When would direct database queries be BETTER than caching? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question 2: What's the MAIN benefit of caching? Reduces database load Reduces latency Saves money All of the above Verify after implementation: [Which one(s)?] Question 3: Cache hit rate drops from 90% to 50%. How does this affect performance? Your calculation: [Fill in] Verified impact: [Fill in after implementation] Before/After: Why This Pattern Matters \u00b6 Your task: Compare direct database access vs caching to understand the impact. Example: Product Lookup API \u00b6 Problem: Fetch product details for 1000 concurrent users. Approach 1: Direct Database Query (No Cache) \u00b6 // Naive approach - Query database for every request public class DirectDatabaseLookup { private final Database database; public Product getProduct(String productId) { // Direct database query every time return database.query(\"SELECT * FROM products WHERE id = ?\", productId); } } // Performance test public static void benchmarkDirectDB() { DirectDatabaseLookup service = new DirectDatabaseLookup(database); long start = System.currentTimeMillis(); for (int i = 0; i < 1000; i++) { service.getProduct(\"prod-123\"); // Same product queried 1000 times } long end = System.currentTimeMillis(); System.out.println(\"Total time: \" + (end - start) + \"ms\"); } Analysis: Time per DB query: ~50ms For 1000 requests: 1000 \u00d7 50ms = 50,000ms (50 seconds) Database load: 1000 queries/sec Cost: High (database compute, network latency) Throughput: ~20 requests/sec per thread Approach 2: LRU Cache (Optimized) \u00b6 // Optimized approach - Cache frequent lookups public class CachedProductLookup { private final LRUCache<String, Product> cache; private final Database database; public CachedProductLookup(int cacheSize, Database database) { this.cache = new LRUCache<>(cacheSize); this.database = database; } public Product getProduct(String productId) { // Try cache first (O(1), ~1ms) Product product = cache.get(productId); if (product != null) { return product; // Cache hit - fast! } // Cache miss - query database (~50ms) product = database.query(\"SELECT * FROM products WHERE id = ?\", productId); if (product != null) { cache.put(productId, product); // Populate cache } return product; } } // Performance test public static void benchmarkCached() { CachedProductLookup service = new CachedProductLookup(100, database); long start = System.currentTimeMillis(); for (int i = 0; i < 1000; i++) { service.getProduct(\"prod-123\"); // Same product queried 1000 times } long end = System.currentTimeMillis(); System.out.println(\"Total time: \" + (end - start) + \"ms\"); } Analysis: First request (cache miss): ~50ms Subsequent requests (cache hits): ~1ms each For 1000 requests: 50ms + (999 \u00d7 1ms) = 1,049ms (~1 second) Database load: 1 query for 1000 requests Cache hit rate: 99.9% Throughput: ~950 requests/sec per thread Performance Comparison \u00b6 Metric Direct DB With Cache Improvement Total time (1000 requests) 50,000ms 1,049ms 47.6x faster Average latency 50ms 1.05ms 47.6x faster Database queries 1000 1 99.9% reduction Throughput 20 req/sec 950 req/sec 47.5x higher DB cost (estimate) $100/day $2/day $98/day savings Your calculation: For 10,000 requests with 90% cache hit rate: Cache hits: 10,000 \u00d7 0.9 = _ requests \u00d7 1ms = ___ ms Cache misses: 10,000 \u00d7 0.1 = _ requests \u00d7 50ms = ___ ms Total time: _ + = __ ms Speedup vs direct DB: _____ times faster Hit Rate Analysis \u00b6 How hit rate affects performance: Cache Hit Rate Analysis (1000 requests) Hit Rate | Cache Hits | DB Queries | Total Time | Speedup ---------|------------|------------|------------|-------- 0% | 0 | 1000 | 50,000ms | 1x 50% | 500 | 500 | 25,500ms | 2x 75% | 750 | 250 | 13,250ms | 3.8x 90% | 900 | 100 | 5,900ms | 8.5x 95% | 950 | 50 | 3,450ms | 14.5x 99% | 990 | 10 | 1,490ms | 33.6x 99.9% | 999 | 1 | 1,049ms | 47.7x Key insight: Even a modest 75% hit rate gives 3.8x speedup! After implementing, explain in your own words: Why does caching provide such dramatic speedup? [Your answer] What happens when hit rate drops below 50%? [Your answer] When might caching not be worth it? [Your answer] Write Policy Comparison \u00b6 Scenario: Update user profile (name change) // Write-Through Example public void updateUserProfile_WriteThrough(String userId, String newName) { long start = System.currentTimeMillis(); // Write to cache (1ms) cache.put(userId, newName); // Write to database (50ms) - BLOCKS until complete database.update(userId, newName); long end = System.currentTimeMillis(); System.out.println(\"Write-Through latency: \" + (end - start) + \"ms\"); // Output: ~51ms } // Write-Back Example public void updateUserProfile_WriteBack(String userId, String newName) { long start = System.currentTimeMillis(); // Write to cache (1ms) - IMMEDIATE RETURN cache.put(userId, newName); // Mark as dirty for async flush dirtyEntries.put(userId, newName); long end = System.currentTimeMillis(); System.out.println(\"Write-Back latency: \" + (end - start) + \"ms\"); // Output: ~1ms // Database write happens later asynchronously } Write Policy Performance: Policy User Latency DB Write Consistency Data Loss Risk Write-Through 51ms Synchronous Immediate None Write-Back 1ms Asynchronous Eventual If crash before flush Speedup 51x faster - Trade-off Trade-off Your analysis: When would you choose each? Write-Through: [Fill in scenarios] Write-Back: [Fill in scenarios] Case Studies: Caching in the Wild \u00b6 Facebook's Social Graph: TAO and Memcached \u00b6 Pattern: Cache-Aside with a custom distributed caching layer (TAO). How it works: Facebook's social graph (friends, posts, comments) is too large and interconnected to query from a database for every request. They built TAO, a geographically distributed caching system on top of Memcached. When a user requests their feed, the application first queries TAO. If the data is present (cache hit), it's returned instantly. If not (cache miss), TAO fetches the data from the master database (MySQL), populates the cache, and then returns it. Key Takeaway: At massive scale, a simple cache isn't enough. Facebook needed to build a custom caching service that handles eventual consistency, replication across data centers, and the \"thundering herd\" problem. It showcases the cache-aside pattern on a global scale. Twitter's Timeline Cache: Redis for Real-Time Feeds \u00b6 Pattern: Pre-computed timelines with a Cache-Aside strategy. How it works: A user's home timeline is one of the most frequently read pieces of data. Generating it on-the-fly for every request is too slow. Instead, Twitter pre-computes user timelines and stores them in a massive Redis cluster. When you open Twitter, your app fetches this pre-computed list directly from the cache. When a user you follow tweets, a background \"fan-out\" service pushes that tweet into the timeline caches of all their followers. Key Takeaway: For read-heavy workloads with complex data generation, it's often better to do the work ahead of time and cache the results . Redis is a perfect fit for this due to its high performance and versatile data structures (like sorted lists for timelines). Content Delivery Networks (CDNs): Caching at the Edge \u00b6 Pattern: Multi-layered caching with LRU/LFU eviction. How it works: A Content Delivery Network (CDN) like Cloudflare or Akamai acts as a massive, distributed cache for a website's static assets (images, CSS, JS). When a user in London requests an image, it's served from the CDN's London edge server, not from the origin server in California. The first request might be slow, but subsequent requests from that region are served from the local cache. Key Takeaway: Caching isn't just for databases; it's for any data that can be served closer to the user. CDNs demonstrate how layered caching and intelligent eviction policies (like LRU to keep popular assets hot) can dramatically improve website performance and reduce bandwidth costs for the origin server. Core Implementation \u00b6 Pattern 1: LRU Cache (Least Recently Used) \u00b6 Your task: Implement LRU cache with O(1) get and put operations. import java.util.*; /** * LRU Cache - Evicts least recently used items when full * Time: O(1) for get/put * Space: O(capacity) * * Key insight: Combine HashMap for O(1) lookup + Doubly Linked List for O(1) move/remove */ public class LRUCache<K, V> { private final int capacity; private final Map<K, Node<K, V>> cache; private final DoublyLinkedList<K, V> list; static class Node<K, V> { K key; V value; Node<K, V> prev, next; Node(K key, V value) { this.key = key; this.value = value; } } static class DoublyLinkedList<K, V> { Node<K, V> head, tail; DoublyLinkedList() { // TODO: Initialize sentinel nodes for cleaner edge case handling } /** * Add node to front (most recently used position) * * TODO: Implement addToFront */ void addToFront(Node<K, V> node) { // TODO: Insert node right after head } /** * Remove node from list * * TODO: Implement remove */ void remove(Node<K, V> node) { // TODO: Update prev/next pointers to bypass this node } /** * Remove and return least recently used (node before tail) * * TODO: Implement removeLast */ Node<K, V> removeLast() { // TODO: Remove the node closest to tail // Handle empty list case return null; // Replace } /** * Move existing node to front * * TODO: Implement moveToFront */ void moveToFront(Node<K, V> node) { // TODO: Reposition node to mark it as most recently used } } public LRUCache(int capacity) { this.capacity = capacity; this.cache = new HashMap<>(); this.list = new DoublyLinkedList<>(); } /** * Get value for key * Time: O(1) * * TODO: Implement get */ public V get(K key) { // TODO: Lookup and update recency return null; // Replace } /** * Put key-value pair * Time: O(1) * * TODO: Implement put */ public void put(K key, V value) { // TODO: Handle updates to existing keys // Handle eviction when at capacity // Add new entries appropriately } public int size() { return cache.size(); } } Pattern 2: LFU Cache (Least Frequently Used) \u00b6 Your task: Implement LFU cache with O(1) get and put operations. import java.util.*; /** * LFU Cache - Evicts least frequently used items when full * Time: O(1) for get/put * Space: O(capacity) * * Key insight: Track frequency for each node, maintain lists per frequency level */ public class LFUCache<K, V> { private final int capacity; private int minFreq; private final Map<K, Node<K, V>> cache; private final Map<Integer, DoublyLinkedList<K, V>> freqMap; // freq -> list of nodes static class Node<K, V> { K key; V value; int freq; Node<K, V> prev, next; Node(K key, V value) { this.key = key; this.value = value; this.freq = 1; } } static class DoublyLinkedList<K, V> { Node<K, V> head, tail; DoublyLinkedList() { // TODO: Initialize sentinel nodes } void addToFront(Node<K, V> node) { // TODO: Add to front of list } void remove(Node<K, V> node) { // TODO: Remove from list } Node<K, V> removeLast() { // TODO: Remove least frequently used return null; } boolean isEmpty() { // TODO: Check if list has any nodes return true; } } public LFUCache(int capacity) { this.capacity = capacity; this.minFreq = 0; this.cache = new HashMap<>(); this.freqMap = new HashMap<>(); } /** * Get value for key * Time: O(1) * * TODO: Implement get */ public V get(K key) { // TODO: Lookup and update frequency tracking return null; // Replace } /** * Put key-value pair * Time: O(1) * * TODO: Implement put */ public void put(K key, V value) { if (capacity <= 0) return; // TODO: Handle updates and new insertions // Evict least frequently used when at capacity // Manage frequency tracking structures } /** * Update frequency of node * * TODO: Implement updateFrequency */ private void updateFrequency(Node<K, V> node) { // TODO: Move node from current frequency list to next } } Pattern 3: Write-Through Cache \u00b6 Your task: Implement write-through cache pattern. /** * Write-Through Cache - Writes go to cache AND database synchronously * * Pros: Data consistency, simple * Cons: Higher write latency */ public class WriteThroughCache<K, V> { private final LRUCache<K, V> cache; private final Database<K, V> database; interface Database<K, V> { V read(K key); void write(K key, V value); } public WriteThroughCache(int capacity, Database<K, V> database) { this.cache = new LRUCache<>(capacity); this.database = database; } /** * Get value * * TODO: Implement cache-aside pattern */ public V get(K key) { // TODO: Check cache first, then fallback to database return null; // Replace } /** * Put value * * TODO: Implement write-through */ public void put(K key, V value) { // TODO: Update both cache and database synchronously } } Pattern 4: Write-Back (Write-Behind) Cache \u00b6 Your task: Implement write-back cache with async flush. import java.util.*; import java.util.concurrent.*; /** * Write-Back Cache - Writes go to cache immediately, database asynchronously * * Pros: Lower write latency * Cons: Risk of data loss, more complex */ public class WriteBackCache<K, V> { private final LRUCache<K, V> cache; private final Database<K, V> database; private final Map<K, V> dirtyEntries; private final ScheduledExecutorService flusher; interface Database<K, V> { V read(K key); void write(K key, V value); } public WriteBackCache(int capacity, Database<K, V> database, long flushIntervalMs) { this.cache = new LRUCache<>(capacity); this.database = database; this.dirtyEntries = new ConcurrentHashMap<>(); this.flusher = Executors.newSingleThreadScheduledExecutor(); // TODO: Schedule background flush task } /** * Get value * * TODO: Implement get */ public V get(K key) { // TODO: Check cache, dirty entries, then database return null; // Replace } /** * Put value * * TODO: Implement write-back */ public void put(K key, V value) { // TODO: Update cache immediately // Mark for later database flush } /** * Flush dirty entries to database * * TODO: Implement flush */ private void flushDirtyEntries() { // TODO: Write all dirty entries to database // Handle failures appropriately } public void shutdown() { // TODO: Ensure all data is flushed before shutdown } } Client Code \u00b6 import java.util.*; public class CachingPatternsClient { // Mock database for testing static class MockDatabase<K, V> implements WriteThroughCache.Database<K, V> { private final Map<K, V> storage = new HashMap<>(); @Override public V read(K key) { System.out.println(\" [DB READ] \" + key); return storage.get(key); } @Override public void write(K key, V value) { System.out.println(\" [DB WRITE] \" + key + \" = \" + value); storage.put(key, value); } } public static void main(String[] args) { testLRUCache(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testLFUCache(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testWriteThroughCache(); } static void testLRUCache() { System.out.println(\"=== LRU Cache Test ===\\n\"); LRUCache<String, String> cache = new LRUCache<>(3); cache.put(\"user:1\", \"Alice\"); cache.put(\"user:2\", \"Bob\"); cache.put(\"user:3\", \"Charlie\"); System.out.println(\"Cache size: \" + cache.size()); cache.get(\"user:1\"); // Access Alice (makes it most recent) cache.put(\"user:4\", \"David\"); // Should evict Bob (LRU) System.out.println(\"Get user:1: \" + cache.get(\"user:1\")); // Should be Alice System.out.println(\"Get user:2: \" + cache.get(\"user:2\")); // Should be null (evicted) System.out.println(\"Get user:3: \" + cache.get(\"user:3\")); // Should be Charlie System.out.println(\"Get user:4: \" + cache.get(\"user:4\")); // Should be David } static void testLFUCache() { System.out.println(\"=== LFU Cache Test ===\\n\"); LFUCache<String, String> cache = new LFUCache<>(2); cache.put(\"key1\", \"value1\"); cache.put(\"key2\", \"value2\"); System.out.println(\"Get key1: \" + cache.get(\"key1\")); // freq: key1=2, key2=1 cache.put(\"key3\", \"value3\"); // Should evict key2 (LFU) System.out.println(\"Get key2: \" + cache.get(\"key2\")); // Should be null (evicted) System.out.println(\"Get key3: \" + cache.get(\"key3\")); // Should be value3 System.out.println(\"Get key1: \" + cache.get(\"key1\")); // Should be value1 } static void testWriteThroughCache() { System.out.println(\"=== Write-Through Cache Test ===\\n\"); MockDatabase<String, String> db = new MockDatabase<>(); WriteThroughCache<String, String> cache = new WriteThroughCache<>(2, db); System.out.println(\"Put user:1\"); cache.put(\"user:1\", \"Alice\"); // Writes to both cache and DB System.out.println(\"\\nGet user:1\"); System.out.println(\"Value: \" + cache.get(\"user:1\")); // Cache hit (no DB read) System.out.println(\"\\nGet user:2\"); db.storage.put(\"user:2\", \"Bob\"); // Add directly to DB System.out.println(\"Value: \" + cache.get(\"user:2\")); // Cache miss, DB hit } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken caching implementations. This tests your understanding. Challenge 1: Cache Stampede Bug \u00b6 /** * This cache has a CRITICAL BUG during cache misses. * Multiple threads can cause \"cache stampede\" - all hit DB simultaneously! */ public class StampedeLRUCache<K, V> { private final LRUCache<K, V> cache; private final Database<K, V> database; public V get(K key) { V value = cache.get(key); if (value == null) { // If 1000 threads miss cache at same time, // all 1000 will query database for same key! value = database.read(key); if (value != null) { cache.put(key, value); } } return value; } } Your debugging: Bug location: [Which lines?] Bug explanation: [What happens with concurrent requests?] Test case to expose the bug: // Simulate 1000 concurrent requests for same key (cache miss) ExecutorService executor = Executors.newFixedThreadPool(1000); for (int i = 0; i < 1000; i++) { executor.submit(() -> cache.get(\"popular-item\")); } // Expected: 1 DB query // Actual with bug: <span class=\"fill-in\">_____</span> DB queries Click to verify your answer Bug: No synchronization during cache miss. Multiple threads can simultaneously detect cache miss and all query the database. Fix 1 - Simple locking (but blocks all reads): public synchronized V get(K key) { // ... same logic } Fix 2 - Better: Per-key locking to avoid thundering herd: private final ConcurrentHashMap<K, CompletableFuture<V>> inFlightRequests = new ConcurrentHashMap<>(); public V get(K key) { V value = cache.get(key); if (value != null) return value; // Only one thread per key will query DB CompletableFuture<V> future = inFlightRequests.computeIfAbsent(key, k -> { return CompletableFuture.supplyAsync(() -> { V dbValue = database.read(k); if (dbValue != null) cache.put(k, dbValue); return dbValue; }); }); try { value = future.get(); inFlightRequests.remove(key); return value; } catch (Exception e) { inFlightRequests.remove(key); throw new RuntimeException(e); } } Key insight: Cache stampede can overwhelm your database. Always protect cache misses with per-key synchronization. Challenge 2: Stale Data Bug \u00b6 /** * This write-back cache has a STALE DATA bug. * Readers can get old data even after a write! */ public class StaleWriteBackCache<K, V> { private final LRUCache<K, V> cache; private final Map<K, V> dirtyEntries; private final Database<K, V> database; public V get(K key) { V value = cache.get(key); if (value != null) return value; // Check dirty entries value = dirtyEntries.get(key); if (value != null) return value; // Read from database return database.read(key); } public void put(K key, V value) { cache.put(key, value); dirtyEntries.put(key, value); } public void flush() { // Async flush to database for (Map.Entry<K, V> entry : dirtyEntries.entrySet()) { database.write(entry.getKey(), entry.getValue()); } dirtyEntries.clear(); } } Your debugging: Bug: [What's wrong with the get() logic?] Scenario that breaks: 1. put(\"key1\", \"value1\") - goes to cache and dirty 2. cache evicts key1 (capacity full) 3. get(\"key1\") - what do you get? Expected: \"value1\" (from dirty entries) Actual: [What happens?] Fix: [Correct the order of checks] Click to verify your answer Bug: Cache lookup happens before dirty entries check. If cache evicts an item that's in dirtyEntries, we'll miss the latest value. Correct order: public V get(K key) { // Check dirty entries FIRST (most recent writes) V value = dirtyEntries.get(key); if (value != null) return value; // Then check cache value = cache.get(key); if (value != null) return value; // Finally, check database return database.read(key); } Key insight: With write-back caching, dirty entries hold the \"source of truth\" until flushed. Always check them first! Challenge 3: Thundering Herd on Expiration \u00b6 /** * This cache has TTL support but causes \"thundering herd\" * when many items expire at the same time. */ public class TTLCache<K, V> { static class CacheEntry<V> { V value; long expiryTime; CacheEntry(V value, long ttlMs) { this.value = value; this.expiryTime = System.currentTimeMillis() + ttlMs; } boolean isExpired() { return System.currentTimeMillis() > expiryTime; } } private final Map<K, CacheEntry<V>> cache = new ConcurrentHashMap<>(); private final Database<K, V> database; private final long ttlMs; public TTLCache(Database<K, V> database, long ttlMs) { this.database = database; this.ttlMs = ttlMs; } public V get(K key) { CacheEntry<V> entry = cache.get(key); // Check expiration if (entry == null || entry.isExpired()) { // All requests hit database simultaneously! V value = database.read(key); if (value != null) { cache.put(key, new CacheEntry<>(value, ttlMs)); } return value; } return entry.value; } } Your debugging: Bug: [What causes thundering herd?] Scenario: 10:00:00 - Cache is populated with 1000 items, all expire at 10:05:00 10:05:00 - First request arrives What happens? Expected: Smooth database load Actual: [What happens to database?] Click to verify your answer Bug: All items created at the same time will expire at the same time, causing synchronized cache misses and database overload. Fix 1 - Add jitter to TTL: private final Random random = new Random(); public V get(K key) { // ... existing logic ... if (value != null) { // Add \u00b120% jitter to TTL long jitter = (long) (ttlMs * (0.8 + random.nextDouble() * 0.4)); cache.put(key, new CacheEntry<>(value, jitter)); } return value; } Fix 2 - Probabilistic early expiration (XFetch algorithm): public V get(K key) { CacheEntry<V> entry = cache.get(key); if (entry == null) { return refreshFromDB(key); } // Probabilistic early expiration // As item gets older, higher chance of refresh long timeLeft = entry.expiryTime - System.currentTimeMillis(); double refreshProbability = 1.0 - ((double) timeLeft / ttlMs); if (entry.isExpired() || random.nextDouble() < refreshProbability * 0.1) { // Refresh asynchronously CompletableFuture.runAsync(() -> refreshFromDB(key)); } return entry.value; } private V refreshFromDB(K key) { V value = database.read(key); if (value != null) { long jitter = (long) (ttlMs * (0.8 + random.nextDouble() * 0.4)); cache.put(key, new CacheEntry<>(value, jitter)); } return value; } Key insight: Synchronized expiration creates thundering herd. Add jitter and probabilistic early expiration to spread load. Challenge 4: LFU Frequency Update Bug \u00b6 /** * This LFU cache has a subtle frequency update bug. * Can you spot it? */ public class BuggyLFUCache<K, V> { static class Node<K, V> { K key; V value; int freq; Node(K key, V value) { this.key = key; this.value = value; this.freq = 1; } } private final Map<K, Node<K, V>> cache; private final Map<Integer, LinkedHashSet<K>> freqMap; private int minFreq; private final int capacity; public V get(K key) { Node<K, V> node = cache.get(key); if (node == null) return null; updateFrequency(node); return node.value; } private void updateFrequency(Node<K, V> node) { int freq = node.freq; // Remove from current frequency list freqMap.get(freq).remove(node.key); // And what if freq == minFreq? // Increment frequency node.freq++; // Add to new frequency list freqMap.computeIfAbsent(node.freq, k -> new LinkedHashSet<>()) .add(node.key); } public void put(K key, V value) { // ... implementation } } Your debugging: Bug 1: [What happens to minFreq?] Bug 2: [What about empty frequency lists?] Cache capacity = 2 put(\"A\", 1) - freq=1, minFreq=1 get(\"A\") - freq=2 put(\"B\", 2) - freq=1, minFreq should be? // At this point, what is minFreq? What should it be? Fix: [Complete the updateFrequency method] Click to verify your answer Bug 1: When the last node at minFreq is moved to a higher frequency, we must update minFreq. Bug 2: Empty frequency lists should be removed from freqMap to save memory. Correct implementation: private void updateFrequency(Node<K, V> node) { int freq = node.freq; // Remove from current frequency list LinkedHashSet<K> freqList = freqMap.get(freq); freqList.remove(node.key); // If this was the last node at minFreq, increment minFreq if (freq == minFreq && freqList.isEmpty()) { minFreq++; } // Remove empty frequency list if (freqList.isEmpty()) { freqMap.remove(freq); } // Increment frequency node.freq++; // Add to new frequency list freqMap.computeIfAbsent(node.freq, k -> new LinkedHashSet<>()) .add(node.key); } Key insight: LFU requires careful maintenance of minFreq and frequency lists. Missing updates cause incorrect evictions. Challenge 5: Cache Invalidation Race Condition \u00b6 /** * This cache has invalidation logic but contains a race condition. * Data can become inconsistent between cache and database. */ public class InvalidationCache<K, V> { private final LRUCache<K, V> cache; private final Database<K, V> database; public V get(K key) { V value = cache.get(key); if (value == null) { value = database.read(key); if (value != null) { cache.put(key, value); } } return value; } public void update(K key, V newValue) { // Thread 1: Writes to database database.write(key, newValue); // Thread 2: Between these two lines, another thread could: // 1. Read stale value from cache // 2. Not see the invalidation yet // Thread 1: Invalidates cache cache.remove(key); } } Your debugging: Bug: [What race condition exists?] Scenario that fails: T0: cache contains key=\"user:1\" value=\"Alice\" T1: Thread 1 calls update(\"user:1\", \"Bob\") - Writes \"Bob\" to database T2: Thread 2 calls get(\"user:1\") - Reads \"Alice\" from cache (stale!) T3: Thread 1 invalidates cache - Too late! Thread 2 already returned stale data Click to verify your answer Bug: Cache invalidation happens AFTER database write, creating a window where stale data is served. Fix 1 - Invalidate before write: public void update(K key, V newValue) { // Invalidate FIRST cache.remove(key); // Then write to database database.write(key, newValue); // Small window where cache misses hit DB, but at least no stale data } Fix 2 - Use versioning: static class VersionedValue<V> { V value; long version; } public void update(K key, V newValue) { // Increment version long newVersion = getNextVersion(key); // Write to DB with version database.write(key, newValue, newVersion); // Update cache with version cache.put(key, new VersionedValue<>(newValue, newVersion)); } public V get(K key) { VersionedValue<V> cached = cache.get(key); VersionedValue<V> dbValue = database.read(key); // Compare versions, use latest if (cached != null && dbValue != null) { return cached.version >= dbValue.version ? cached.value : dbValue.value; } // ... handle nulls } Fix 3 - Cache-aside with write-through (best): public void update(K key, V newValue) { // Write to both atomically (within transaction if possible) cache.put(key, newValue); database.write(key, newValue); // No invalidation needed - cache is always up to date } Key insight: Cache invalidation is notoriously hard. Order matters: invalidate before write, or use write-through to avoid invalidation entirely. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 5+ critical caching bugs Understood WHY each bug causes problems Could explain the fix to someone else Learned common caching mistakes to avoid Common caching bugs you discovered: Cache stampede (no synchronization on miss) Stale data (wrong read order in write-back) Thundering herd (synchronized expiration) Frequency tracking errors (LFU minFreq bug) Invalidation race conditions Defensive caching patterns to remember: [Fill in patterns you learned] [Fill in] [Fill in] Decision Framework \u00b6 Questions to answer after implementation: 1. When to use LRU vs LFU? \u00b6 LRU Cache: When to use: [Fill in] Pros: [Fill in] Cons: [Fill in] Example scenarios: [Fill in] LFU Cache: When to use: [Fill in] Pros: [Fill in] Cons: [Fill in] Example scenarios: [Fill in] 2. When to use Write-Through vs Write-Back? \u00b6 Write-Through: When to use: [Fill in] Pros: [Fill in] Cons: [Fill in] Example scenarios: [Fill in] Write-Back: When to use: [Fill in] Pros: [Fill in] Cons: [Fill in] Example scenarios: [Fill in] 3. Your Decision Tree \u00b6 Build this after solving practice scenarios: flowchart LR Start[\"Should I use caching?\"] Q1{\"What's the access pattern?\"} Start --> Q1 N2[\"?\"] Q1 -->|\"Recent items accessed again\"| N2 N3[\"?\"] Q1 -->|\"Popular items accessed frequently\"| N3 N4[\"?\"] Q1 -->|\"Mixed/unknown\"| N4 Q5{\"What's the write pattern?\"} Start --> Q5 N6[\"?\"] Q5 -->|\"Consistency critical\"| N6 N7[\"?\"] Q5 -->|\"Performance critical\"| N7 N8[\"?\"] Q5 -->|\"Mixed\"| N8 Q9{\"Other considerations?\"} Start --> Q9 N10[\"?\"] Q9 -->|\"Memory constraints\"| N10 N11[\"?\"] Q9 -->|\"Data freshness requirements\"| N11 N12[\"?\"] Q9 -->|\"Failure tolerance\"| N12 Practice \u00b6 Scenario 1: E-commerce Product Catalog \u00b6 Requirements: 1M products, 100K frequently viewed Reads: 10K/sec, Writes: 100/sec Users browse recent and popular products Your cache design: Eviction policy: [LRU or LFU? Why?] Write policy: [Write-Through or Write-Back? Why?] Capacity: [How much?] TTL: [Time-to-live strategy?] Invalidation: [When to invalidate?] Scenario 2: Social Media Feed \u00b6 Requirements: Each user has 500 followers Feed shows recent posts (last 24h) High read:write ratio (100:1) Eventually consistent is OK Your cache design: Eviction policy: [Fill in] Write policy: [Fill in] Cache key structure: [What to cache?] Invalidation strategy: [Fill in] Capacity planning: [Fill in] Scenario 3: Session Store \u00b6 Requirements: Store user sessions (auth tokens, preferences) Sessions expire after 30 minutes of inactivity High read frequency Consistency required (can't lose session data) Your cache design: Eviction policy: [Fill in] Write policy: [Fill in] TTL strategy: [Fill in] Persistence: [How to ensure durability?] LeetCode Problem \u00b6 Problem: 146. LRU Cache Design and implement a data structure for Least Recently Used (LRU) cache. Your approach: [Data structures needed?] [How to achieve O(1) for both get and put?] [Edge cases to handle?] Review Checklist \u00b6 Before moving to the next topic: Implementation LRU Cache works with O(1) operations LFU Cache works with frequency tracking Write-Through pattern implemented correctly Write-Back pattern with async flush works All client code runs successfully Understanding Filled in all ELI5 explanations Understand LRU vs LFU trade-offs Understand Write-Through vs Write-Back trade-offs Built decision tree Decision Making Know when to use LRU vs LFU Know when to use Write-Through vs Write-Back Completed practice scenarios Can explain trade-offs to someone else Mastery Check Could implement LRU from memory Could implement LFU from memory Could design cache for new scenario Understand when NOT to use caching Mastery Certification \u00b6 I certify that I can: Implement LRU cache from memory in under 15 minutes Implement LFU cache with correct frequency tracking Explain when to use LRU vs LFU with real examples Explain when to use Write-Through vs Write-Back Calculate cache hit rates and performance impact Identify and fix common caching bugs (stampede, stale data, etc.) Design caching strategy for a new system Explain trade-offs to both technical and non-technical audiences","title":"05. Caching Patterns"},{"location":"systems/05-caching-patterns/#caching-patterns","text":"Master LRU, LFU, and write policies for high-performance systems","title":"Caching Patterns"},{"location":"systems/05-caching-patterns/#eli5-explain-like-im-5","text":"Your task: After implementing all patterns, explain them simply. Prompts to guide you: What is caching in one sentence? Your answer: [Fill in after implementation] Why/when do we use caching? Your answer: [Fill in after implementation] Real-world analogy: Example: \"A cache is like keeping your favorite books on your desk instead of walking to the library...\" Your analogy: [Fill in] What's the difference between LRU and LFU? Your answer: [Fill in after solving problems] When should you use Write-Through vs Write-Back? Your answer: [Fill in after practice]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/05-caching-patterns/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/05-caching-patterns/#beforeafter-why-this-pattern-matters","text":"Your task: Compare direct database access vs caching to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"systems/05-caching-patterns/#case-studies-caching-in-the-wild","text":"","title":"Case Studies: Caching in the Wild"},{"location":"systems/05-caching-patterns/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/05-caching-patterns/#client-code","text":"import java.util.*; public class CachingPatternsClient { // Mock database for testing static class MockDatabase<K, V> implements WriteThroughCache.Database<K, V> { private final Map<K, V> storage = new HashMap<>(); @Override public V read(K key) { System.out.println(\" [DB READ] \" + key); return storage.get(key); } @Override public void write(K key, V value) { System.out.println(\" [DB WRITE] \" + key + \" = \" + value); storage.put(key, value); } } public static void main(String[] args) { testLRUCache(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testLFUCache(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testWriteThroughCache(); } static void testLRUCache() { System.out.println(\"=== LRU Cache Test ===\\n\"); LRUCache<String, String> cache = new LRUCache<>(3); cache.put(\"user:1\", \"Alice\"); cache.put(\"user:2\", \"Bob\"); cache.put(\"user:3\", \"Charlie\"); System.out.println(\"Cache size: \" + cache.size()); cache.get(\"user:1\"); // Access Alice (makes it most recent) cache.put(\"user:4\", \"David\"); // Should evict Bob (LRU) System.out.println(\"Get user:1: \" + cache.get(\"user:1\")); // Should be Alice System.out.println(\"Get user:2: \" + cache.get(\"user:2\")); // Should be null (evicted) System.out.println(\"Get user:3: \" + cache.get(\"user:3\")); // Should be Charlie System.out.println(\"Get user:4: \" + cache.get(\"user:4\")); // Should be David } static void testLFUCache() { System.out.println(\"=== LFU Cache Test ===\\n\"); LFUCache<String, String> cache = new LFUCache<>(2); cache.put(\"key1\", \"value1\"); cache.put(\"key2\", \"value2\"); System.out.println(\"Get key1: \" + cache.get(\"key1\")); // freq: key1=2, key2=1 cache.put(\"key3\", \"value3\"); // Should evict key2 (LFU) System.out.println(\"Get key2: \" + cache.get(\"key2\")); // Should be null (evicted) System.out.println(\"Get key3: \" + cache.get(\"key3\")); // Should be value3 System.out.println(\"Get key1: \" + cache.get(\"key1\")); // Should be value1 } static void testWriteThroughCache() { System.out.println(\"=== Write-Through Cache Test ===\\n\"); MockDatabase<String, String> db = new MockDatabase<>(); WriteThroughCache<String, String> cache = new WriteThroughCache<>(2, db); System.out.println(\"Put user:1\"); cache.put(\"user:1\", \"Alice\"); // Writes to both cache and DB System.out.println(\"\\nGet user:1\"); System.out.println(\"Value: \" + cache.get(\"user:1\")); // Cache hit (no DB read) System.out.println(\"\\nGet user:2\"); db.storage.put(\"user:2\", \"Bob\"); // Add directly to DB System.out.println(\"Value: \" + cache.get(\"user:2\")); // Cache miss, DB hit } }","title":"Client Code"},{"location":"systems/05-caching-patterns/#debugging-challenges","text":"Your task: Find and fix bugs in broken caching implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"systems/05-caching-patterns/#decision-framework","text":"Questions to answer after implementation:","title":"Decision Framework"},{"location":"systems/05-caching-patterns/#practice","text":"","title":"Practice"},{"location":"systems/05-caching-patterns/#review-checklist","text":"Before moving to the next topic: Implementation LRU Cache works with O(1) operations LFU Cache works with frequency tracking Write-Through pattern implemented correctly Write-Back pattern with async flush works All client code runs successfully Understanding Filled in all ELI5 explanations Understand LRU vs LFU trade-offs Understand Write-Through vs Write-Back trade-offs Built decision tree Decision Making Know when to use LRU vs LFU Know when to use Write-Through vs Write-Back Completed practice scenarios Can explain trade-offs to someone else Mastery Check Could implement LRU from memory Could implement LFU from memory Could design cache for new scenario Understand when NOT to use caching","title":"Review Checklist"},{"location":"systems/06-api-design/","text":"API Design \u00b6 REST, GraphQL, and RPC - Choosing the right API paradigm ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing different API patterns, explain them simply. Prompts to guide you: What is REST in one sentence? Your answer: [Fill in after implementation] Why do we use REST for web APIs? Your answer: [Fill in after implementation] Real-world analogy for REST: Example: \"REST is like a restaurant menu where...\" Your analogy: [Fill in] What is GraphQL in one sentence? Your answer: [Fill in after implementation] When would you choose GraphQL over REST? Your answer: [Fill in after implementation] Real-world analogy for GraphQL: Example: \"GraphQL is like a buffet where...\" Your analogy: [Fill in] What is RPC (gRPC) in one sentence? Your answer: [Fill in after implementation] When would you use RPC instead of REST? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 REST API endpoint for listing resources: What HTTP method should be used? [Your guess: GET/POST/PUT/DELETE] What status code for success? [Your guess: 200/201/204] Verified after learning: [Actual: ?] GraphQL query that fetches nested data: How many HTTP requests needed? [Your guess: 1/multiple] Compared to REST for same data: [More/Less/Same requests?] Verified: [Actual] API pagination parameters: Common parameter names: [Your guesses] Default limit should be: [Your guess: 10/50/100/unlimited?] Verified: [Actual best practices] Scenario Predictions \u00b6 Scenario 1: Design endpoint to get user with their posts and comments REST approach: How many endpoints? [Your guess] GraphQL approach: How many endpoints? [Your guess] Which has over-fetching risk? [REST/GraphQL - Why?] Which has N+1 query risk? [REST/GraphQL - Why?] Scenario 2: Client needs to update user's email address Which HTTP method? [GET/POST/PUT/PATCH/DELETE] PUT vs PATCH - what's the difference? [Fill in] Success status code: [200/201/204/304] If email already taken, status code: [400/404/409/500] Scenario 3: API versioning strategy Version in URL (/v1/users) or header? [Which is better? Why?] When to create new version? [Fill in your reasoning] How to deprecate old version? [Your approach] Trade-off Quiz \u00b6 Question: When would REST be BETTER than GraphQL? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN benefit of API pagination? Makes API look professional Prevents database from crashing Reduces response size and improves performance Required by HTTP specification Verify after implementation: [Which one(s)?] Question: What makes an API idempotent? Your answer: [Fill in] Example of idempotent operation: [Fill in] Example of non-idempotent operation: [Fill in] Before/After: Why Good API Design Matters \u00b6 Your task: Compare poorly designed vs well-designed APIs to understand the impact. Example 1: API Versioning \u00b6 Problem: API needs to change user field from name to firstName and lastName . Approach 1: Breaking Change (Bad) \u00b6 // Version 1 (deployed to production) class User { String id; String name; // Single name field String email; } // Suddenly changed to Version 2 - BREAKS ALL CLIENTS! class User { String id; String firstName; // Broken change String lastName; // Broken change String email; } Impact: All existing clients break immediately Mobile apps that can't update crash Third-party integrations fail Customer complaints flood in Emergency rollback required Client code that breaks: // This worked yesterday, crashes today! User user = api.getUser(\"123\"); System.out.println(user.name); // ERROR: field not found! Approach 2: Versioned API (Good) \u00b6 // Version 1 - Still supported // GET /v1/users/123 class UserV1 { String id; String name; // Still works for old clients String email; } // Version 2 - New clients can opt in // GET /v2/users/123 class UserV2 { String id; String firstName; // New field String lastName; // New field String email; } // API routes both versions @GetMapping(\"/v1/users/{id}\") public UserV1 getUserV1(String id) { User user = database.getUser(id); // Convert to V1 format return new UserV1(user.id, user.firstName + \" \" + user.lastName, user.email); } @GetMapping(\"/v2/users/{id}\") public UserV2 getUserV2(String id) { User user = database.getUser(id); return new UserV2(user.id, user.firstName, user.lastName, user.email); } Impact: Old clients continue working New clients get better data structure Gradual migration over 6-12 months Deprecation warnings guide users: \"v1 deprecated, migrate to v2 by Dec 2024\" No emergency fixes needed After implementing, answer: Why is versioning critical for APIs? [Your answer] When would you create a new API version? [Your answer] How long should you support old versions? [Your answer] Example 2: Pagination \u00b6 Problem: API endpoint returns user's posts. Approach 1: Return All Data (Bad) \u00b6 // GET /users/123/posts // Returns ALL posts - no pagination @GetMapping(\"/users/{id}/posts\") public List<Post> getUserPosts(String userId) { // Returns everything - could be 10,000 posts! return database.getAllPostsByUser(userId); } Impact: Response size: 10,000 posts = ~50MB JSON Query time: 5+ seconds for large users Database overwhelmed with full table scans Mobile clients crash from memory issues Users see blank screen (timeout) Performance: User's Posts Response Size Load Time Mobile Result 100 posts 500 KB 0.5s Works 1,000 posts 5 MB 2s Slow 10,000 posts 50 MB 10s+ Crashes Approach 2: Paginated API (Good) \u00b6 // GET /users/123/posts?page=1&limit=20 @GetMapping(\"/users/{id}/posts\") public PaginatedResponse<Post> getUserPosts( String userId, @RequestParam(defaultValue = \"1\") int page, @RequestParam(defaultValue = \"20\") int limit ) { // Validate limit if (limit > 100) limit = 100; // Cap at 100 int offset = (page - 1) * limit; List<Post> posts = database.getPostsByUser(userId, offset, limit); int total = database.countPostsByUser(userId); return new PaginatedResponse<>( posts, page, limit, total, \"/users/\" + userId + \"/posts\" // Base URL for navigation ); } class PaginatedResponse<T> { List<T> data; PaginationMeta meta; class PaginationMeta { int currentPage; int pageSize; int totalItems; int totalPages; String nextPage; // \"/users/123/posts?page=2&limit=20\" String prevPage; // \"/users/123/posts?page=1&limit=20\" } } Impact: Response size: 20 posts = ~100 KB (50x smaller!) Query time: <100ms (10x faster) Database uses indexed queries Mobile clients load quickly Smooth infinite scroll UX Performance Comparison: Approach Response Size Query Time Database Load No pagination 50 MB 10s Full scan Paginated (limit=20) 100 KB 100ms Indexed query Speedup 500x smaller 100x faster Minimal load After implementing, answer: Why is pagination essential for production APIs? [Your answer] What's a good default page size? [Your answer] Offset-based vs cursor-based pagination - when to use each? [Your answer] Example 3: Error Handling \u00b6 Problem: User tries to create account with email that already exists. Approach 1: Generic Error (Bad) \u00b6 @PostMapping(\"/users\") public Response createUser(@RequestBody CreateUserRequest req) { try { User user = database.createUser(req.name, req.email); return Response.status(201).body(user); } catch (Exception e) { // Generic error - useless for client! return Response.status(500).body(\"Error occurred\"); } } Response: { \"message\": \"Error occurred\" } Impact: Client has NO IDEA what went wrong Was it duplicate email? Invalid format? Server down? User sees generic \"Something went wrong\" message Developers waste hours debugging Customer support flooded with tickets Approach 2: Structured Errors (Good) \u00b6 @PostMapping(\"/users\") public Response createUser(@RequestBody CreateUserRequest req) { // Validate input if (req.email == null || !req.email.contains(\"@\")) { return Response.status(400).body(new ApiError( \"INVALID_EMAIL\", \"Email address is invalid\", \"email\", \"Must be a valid email format\" )); } try { User user = database.createUser(req.name, req.email); return Response.status(201).body(user); } catch (DuplicateEmailException e) { return Response.status(409).body(new ApiError( \"DUPLICATE_EMAIL\", \"Email address already registered\", \"email\", \"Please use a different email or try logging in\" )); } catch (Exception e) { log.error(\"Unexpected error creating user\", e); return Response.status(500).body(new ApiError( \"INTERNAL_ERROR\", \"An unexpected error occurred\", null, \"Please try again later or contact support\" )); } } class ApiError { String code; // Machine-readable error code String message; // Human-readable message String field; // Which field caused error (for forms) String suggestion; // How to fix it } Response (duplicate email): { \"code\": \"DUPLICATE_EMAIL\", \"message\": \"Email address already registered\", \"field\": \"email\", \"suggestion\": \"Please use a different email or try logging in\" } Impact: Client knows exactly what went wrong Form highlights the specific field User gets actionable error message Proper HTTP status codes (400, 409, 500) Developers debug in minutes, not hours HTTP Status Codes Table: Status Meaning When to Use 200 OK Success GET, PUT successful 201 Created Resource created POST successful 204 No Content Success, no data DELETE successful 400 Bad Request Invalid input Validation failed 401 Unauthorized Auth required Missing/invalid token 403 Forbidden Not allowed Authenticated but not authorized 404 Not Found Resource doesn't exist GET /users/999 (doesn't exist) 409 Conflict Duplicate/conflict Email already exists 429 Too Many Requests Rate limited Exceeded rate limit 500 Internal Error Server error Unexpected exception After implementing, answer: Why are specific HTTP status codes important? [Your answer] What makes a good error message? [Your answer] When should you use 400 vs 409 vs 422? [Your answer] Example 4: Idempotency \u00b6 Problem: Process payment - what if client retries due to network timeout? Approach 1: Non-Idempotent (Bad) \u00b6 // POST /payments @PostMapping(\"/payments\") public Response processPayment(@RequestBody PaymentRequest req) { // No idempotency check! Payment payment = new Payment(req.userId, req.amount); database.save(payment); creditCard.charge(req.cardToken, req.amount); // Charges card return Response.status(201).body(payment); } Scenario: Client: POST /payments (amount=$100) Server: Processes payment, charges card $100 Server: Response lost due to network timeout Client: \"Did it work? Let me retry...\" Client: POST /payments (amount=$100) again Server: Processes payment AGAIN, charges card ANOTHER $100 Result: User charged $200 instead of $100! Impact: Double charging customers Customer disputes and refunds Loss of trust Legal liability Approach 2: Idempotent with Request ID (Good) \u00b6 // POST /payments @PostMapping(\"/payments\") public Response processPayment( @RequestBody PaymentRequest req, @RequestHeader(\"Idempotency-Key\") String idempotencyKey ) { // Check if already processed Payment existing = database.findPaymentByIdempotencyKey(idempotencyKey); if (existing != null) { // Already processed - return same result return Response.status(200).body(existing); } // Process payment Payment payment = new Payment(req.userId, req.amount, idempotencyKey); database.save(payment); // Unique constraint on idempotencyKey creditCard.charge(req.cardToken, req.amount); return Response.status(201).body(payment); } Scenario: Client: POST /payments (Idempotency-Key: \"uuid-12345\", amount=$100) Server: Processes payment, charges card $100, saves with key \"uuid-12345\" Server: Response lost due to network timeout Client: \"Did it work? Let me retry...\" Client: POST /payments (Idempotency-Key: \"uuid-12345\", amount=$100) again Server: Finds existing payment with key \"uuid-12345\" Server: Returns existing payment (status 200, not 201) Result: User charged $100 only once - correct! Impact: Safe retries No double charging Customer trust maintained Works with mobile apps (common retries) Idempotent HTTP Methods: Method Idempotent? Why? GET Yes Reading data multiple times = same result PUT Yes Replacing resource multiple times = same final state DELETE Yes Deleting multiple times = still deleted POST No* Creating multiple times = multiple resources PATCH Maybe** Depends on implementation POST can be made idempotent with Idempotency-Key *PATCH is idempotent if operations are idempotent (set vs increment) After implementing, answer: What does idempotent mean? [Your answer] Why is POST not naturally idempotent? [Your answer] How do you make POST idempotent? [Your answer] Why is PUT idempotent but POST is not? [Your answer] Case Studies: API Design in the Wild \u00b6 Stripe API: The Gold Standard for REST \u00b6 Paradigm: REST. How it works: Stripe's API is a model for resource-oriented design. Resources are represented as nouns (e.g., /v1/customers , /v1/charges , /v1/subscriptions ). It uses HTTP verbs correctly (e.g., POST /v1/charges to create a new charge, GET /v1/charges/{id} to retrieve it). It also excels at developer experience with clear error messages, idempotent request handling, and versioning in the URL. Key Takeaway: For a public-facing API where predictability, scalability, and a wide range of client support are essential, a well-documented REST architecture is a powerful and reliable choice. It sets clear boundaries and is easily explorable. GitHub API: GraphQL for Flexibility \u00b6 Paradigm: GraphQL. How it works: GitHub's v4 API uses GraphQL to allow developers to craft precise queries for the exact data they need. Instead of making multiple REST calls to get a repository, its pull requests, and their review comments, a developer can write a single GraphQL query that specifies this nested structure. Key Takeaway: GraphQL is ideal for applications with complex data models and varied client needs (like mobile vs. web). It solves the over-fetching and under-fetching problems common in REST, but adds complexity to the server-side with query parsing and execution. Google & Netflix: gRPC for Internal Microservices \u00b6 Paradigm: RPC (specifically, gRPC). How it works: In a microservices architecture, services need to communicate with each other at very high speed. Google developed gRPC for this purpose. Services define their interfaces using Protocol Buffers ( .proto files), which act as a strict contract. gRPC then generates client and server code, enabling efficient, low-latency, binary communication over HTTP/2. Key Takeaway: For internal service-to-service communication where performance is critical and contracts need to be strictly enforced, gRPC is often superior to REST. The focus is not on human-readable resources but on high-performance procedure calls. Core Implementation \u00b6 Part 1: REST API Design \u00b6 Your task: Implement a simple REST API with proper resource design. import java.util.*; /** * REST API: Resource-oriented design with HTTP verbs * * Key principles: * - Resources are nouns (users, posts, comments) * - HTTP verbs define actions (GET, POST, PUT, DELETE) * - Stateless communication * - HATEOAS (links to related resources) */ // Resource representation class User { String id; String name; String email; List<String> postIds; // Links to posts public User(String id, String name, String email) { this.id = id; this.name = name; this.email = email; this.postIds = new ArrayList<>(); } } class Post { String id; String authorId; // Link to user String title; String content; long timestamp; public Post(String id, String authorId, String title, String content) { this.id = id; this.authorId = authorId; this.title = title; this.content = content; this.timestamp = System.currentTimeMillis(); } } public class RESTAPIServer { private Map<String, User> users = new HashMap<>(); private Map<String, Post> posts = new HashMap<>(); /** * GET /users/{id} * Retrieve a user by ID * * TODO: Implement user retrieval * - Return user if exists * - Return 404 if not found * - Include links to user's posts (HATEOAS) */ public Response getUser(String userId) { // TODO: Lookup user in users map // TODO: Implement iteration/conditional logic // Include links: { \"posts\": \"/users/{id}/posts\" } // TODO: Implement iteration/conditional logic return null; // Replace } /** * POST /users * Create a new user * * TODO: Implement user creation * - Validate input (name, email required) * - Generate unique ID * - Store user * - Return 201 Created with Location header */ public Response createUser(String name, String email) { // TODO: Validate input // TODO: Generate ID (UUID) // TODO: Create and store user // TODO: Return 201 with Location: /users/{id} return null; // Replace } /** * PUT /users/{id} * Update an existing user * * TODO: Implement user update * - Full replacement of resource * - Return 200 if updated, 404 if not found */ public Response updateUser(String userId, String name, String email) { // TODO: Check if user exists // TODO: Replace user data completely // TODO: Return appropriate status return null; // Replace } /** * DELETE /users/{id} * Delete a user * * TODO: Implement user deletion * - Remove user and cascade delete posts * - Return 204 No Content if successful */ public Response deleteUser(String userId) { // TODO: Remove user // TODO: Remove all user's posts // TODO: Return 204 return null; // Replace } /** * GET /users/{userId}/posts * Get all posts by a user (nested resource) * * TODO: Implement nested resource retrieval * - Support pagination (page, limit) * - Return list of posts with links */ public Response getUserPosts(String userId, int page, int limit) { // TODO: Get user's post IDs // TODO: Paginate results // TODO: Return posts with pagination metadata return null; // Replace } // Response wrapper static class Response { int statusCode; Object body; Map<String, String> headers; Response(int statusCode, Object body) { this.statusCode = statusCode; this.body = body; this.headers = new HashMap<>(); } } } Part 2: GraphQL Query Engine \u00b6 Your task: Implement a simple GraphQL query resolver. /** * GraphQL: Client specifies exactly what data they need * * Key principles: * - Single endpoint * - Client defines query shape * - No over-fetching or under-fetching * - Strong typing with schema */ class GraphQLSchema { // Schema definition String schema = \"\"\" type User { id: ID! name: String! email: String! posts: [Post] } type Post { id: ID! title: String! content: String! author: User! } type Query { user(id: ID!): User users: [User] post(id: ID!): Post } \"\"\"; } public class GraphQLResolver { private Map<String, User> users = new HashMap<>(); private Map<String, Post> posts = new HashMap<>(); /** * Resolve a GraphQL query * * Example query: * { * user(id: \"123\") { * name * posts { * title * } * } * } * * TODO: Implement query resolver * 1. Parse query (simplified - assume already parsed) * 2. Resolve requested fields only * 3. Handle nested relationships */ public Map<String, Object> executeQuery(String query) { // TODO: Parse query to understand requested fields // TODO: Resolve root field (user, users, post) // TODO: Implement iteration/conditional logic // TODO: Return only requested data return null; // Replace } /** * Field resolver for User.posts * Only called if \"posts\" is in the query * * TODO: Implement field resolver * - Get user's post IDs * - Return list of Post objects */ public List<Post> resolveUserPosts(User user) { // TODO: Look up posts by user.postIds return null; // Replace } /** * Field resolver for Post.author * Only called if \"author\" is in the query * * TODO: Implement field resolver * - Get post's author * - Return User object */ public User resolvePostAuthor(Post post) { // TODO: Look up user by post.authorId return null; // Replace } } Part 3: RPC (Remote Procedure Call) \u00b6 Your task: Implement a simple RPC server with method invocation. /** * RPC: Call remote methods as if they were local * * Key principles: * - Action-oriented (not resource-oriented) * - Direct method invocation * - Binary protocols (e.g., Protocol Buffers) * - Efficient for service-to-service calls */ interface UserService { User getUser(String userId); String createUser(String name, String email); boolean updateUser(String userId, String name, String email); boolean deleteUser(String userId); List<Post> getUserPosts(String userId); } public class RPCServer implements UserService { private Map<String, User> users = new HashMap<>(); private Map<String, Post> posts = new HashMap<>(); /** * RPC Method: getUser * Direct method call, no HTTP concepts * * TODO: Implement user retrieval * - Return user object or null * - Throw exception if error */ @Override public User getUser(String userId) { // TODO: Lookup and return user return null; // Replace } /** * RPC Method: createUser * * TODO: Implement user creation * - Return new user ID * - Throw exception if validation fails */ @Override public String createUser(String name, String email) { // TODO: Validate and create user // TODO: Return generated ID return null; // Replace } /** * RPC Method: updateUser * * TODO: Implement user update * - Return true if success, false if not found * - Throw exception if validation fails */ @Override public boolean updateUser(String userId, String name, String email) { // TODO: Update user return false; // Replace } /** * RPC Method: deleteUser * * TODO: Implement user deletion * - Return true if deleted, false if not found */ @Override public boolean deleteUser(String userId) { // TODO: Delete user and posts return false; // Replace } /** * RPC Method: getUserPosts * * TODO: Implement posts retrieval * - Return list of posts * - Return empty list if user not found */ @Override public List<Post> getUserPosts(String userId) { // TODO: Get user's posts return null; // Replace } } Client Code \u00b6 public class APIDesignClient { public static void main(String[] args) { testRESTAPI(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testGraphQL(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testRPC(); } static void testRESTAPI() { System.out.println(\"=== REST API Test ===\\n\"); RESTAPIServer server = new RESTAPIServer(); // Test: Create user System.out.println(\"POST /users\"); RESTAPIServer.Response createResp = server.createUser(\"Alice\", \"alice@example.com\"); System.out.println(\"Status: \" + createResp.statusCode); System.out.println(\"Body: \" + createResp.body); // Test: Get user System.out.println(\"\\nGET /users/123\"); RESTAPIServer.Response getResp = server.getUser(\"123\"); System.out.println(\"Status: \" + getResp.statusCode); System.out.println(\"Body: \" + getResp.body); // Test: Get user's posts System.out.println(\"\\nGET /users/123/posts?page=1&limit=10\"); RESTAPIServer.Response postsResp = server.getUserPosts(\"123\", 1, 10); System.out.println(\"Status: \" + postsResp.statusCode); System.out.println(\"Body: \" + postsResp.body); } static void testGraphQL() { System.out.println(\"=== GraphQL Test ===\\n\"); GraphQLResolver resolver = new GraphQLResolver(); // Test: Query user with specific fields String query = \"\"\" { user(id: \"123\") { name email posts { title } } } \"\"\"; System.out.println(\"Query:\"); System.out.println(query); Map<String, Object> result = resolver.executeQuery(query); System.out.println(\"\\nResult: \" + result); } static void testRPC() { System.out.println(\"=== RPC Test ===\\n\"); UserService service = new RPCServer(); // Test: Create user (direct method call) System.out.println(\"createUser(\\\"Bob\\\", \\\"bob@example.com\\\")\"); String userId = service.createUser(\"Bob\", \"bob@example.com\"); System.out.println(\"Returned ID: \" + userId); // Test: Get user System.out.println(\"\\ngetUser(\\\"\" + userId + \"\\\")\"); User user = service.getUser(userId); System.out.println(\"Returned: \" + (user != null ? user.name : \"null\")); // Test: Get user posts System.out.println(\"\\ngetUserPosts(\\\"\" + userId + \"\\\")\"); List<Post> posts = service.getUserPosts(userId); System.out.println(\"Returned: \" + posts.size() + \" posts\"); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken API implementations. This tests your understanding. Challenge 1: Broken API Versioning \u00b6 /** * API versioning attempt - but has 2 CRITICAL BUGS. * Find them! */ @RestController public class UserController { // Version 1 endpoint @GetMapping(\"/v1/users/{id}\") public UserV1 getUserV1(String id) { User user = database.getUser(id); return convertToV1(user); } // Version 2 endpoint - BUG 1: What's wrong with this route? @GetMapping(\"/users/{id}\") public UserV2 getUserV2(String id) { User user = database.getUser(id); return new UserV2(user.id, user.firstName, user.lastName, user.email); } // Update endpoint - BUG 2: Version handling issue @PutMapping(\"/users/{id}\") public Response updateUser(String id, @RequestBody UpdateUserRequest req) { // Updates internal model User user = database.getUser(id); user.firstName = req.firstName; // What if v1 client calls this? user.lastName = req.lastName; database.save(user); return Response.ok(user); } } Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Click to verify your answers Bug 1: Version 2 endpoint uses /users/{id} instead of /v2/users/{id} . This creates ambiguity - which version is the \"default\"? Clients expect explicit versioning. Fix: @GetMapping(\"/v2/users/{id}\") public UserV2 getUserV2(String id) { // Now explicit } Bug 2: Single update endpoint doesn't handle version differences. V1 clients send name field, V2 clients send firstName / lastName . Need separate endpoints or request versioning. Fix: @PutMapping(\"/v1/users/{id}\") public Response updateUserV1(String id, @RequestBody UpdateUserV1Request req) { User user = database.getUser(id); // Parse single name into firstName/lastName String[] parts = req.name.split(\" \", 2); user.firstName = parts[0]; user.lastName = parts.length > 1 ? parts[1] : \"\"; database.save(user); return Response.ok(convertToV1(user)); } @PutMapping(\"/v2/users/{id}\") public Response updateUserV2(String id, @RequestBody UpdateUserV2Request req) { User user = database.getUser(id); user.firstName = req.firstName; user.lastName = req.lastName; database.save(user); return Response.ok(new UserV2(user)); } Challenge 2: Broken Pagination \u00b6 /** * Pagination implementation with 3 BUGS. * Two are logic errors, one is a performance issue. */ @GetMapping(\"/users/{userId}/posts\") public PaginatedResponse getPosts( @PathParam(\"userId\") String userId, @RequestParam(\"page\") int page, @RequestParam(\"limit\") int limit) { // Calculate offset int offset = page * limit; List<Post> posts = database.query( \"SELECT * FROM posts WHERE user_id = ? LIMIT ? OFFSET ?\", userId, limit, offset ); int total = database.query( \"SELECT COUNT(*) FROM posts WHERE user_id = ?\", userId ); return new PaginatedResponse(posts, page, limit, total); } Your debugging: Bug 1: [What happens if client doesn't send page/limit parameters?] Impact: [What error occurs?] Fix: [What should be added?] Bug 2: [What if client sends limit=999999?] Impact: [What problem does this cause?] Fix: [How to prevent abuse?] Bug 3: [Is the offset calculation correct?] Test: page=1, limit=10 \u2192 offset = ___ Expected: offset should be ___ Fix: [Correct formula?] Click to verify your answers Bug 1: Missing default values. If client doesn't send parameters, request fails with 400 Bad Request. Fix: @RequestParam(defaultValue = \"1\") int page, @RequestParam(defaultValue = \"20\") int limit Bug 2: No limit validation. Malicious client could send limit=999999 and DOS the database. Fix: if (limit > 100) limit = 100; // Cap at max if (limit < 1) limit = 20; // Minimum 1 Bug 3: Offset calculation is off by one. Page 1 should start at offset 0, not 10. Correct calculation: Page 1, limit 10: offset = (1-1) * 10 = 0 (items 0-9) Page 2, limit 10: offset = (2-1) * 10 = 10 (items 10-19) Fix: int offset = (page - 1) * limit; Challenge 3: Wrong HTTP Methods \u00b6 /** * API with incorrect HTTP method usage. * Find which methods are WRONG and explain why. */ // Operation 1: Get user profile @PostMapping(\"/users/{id}/profile\")public User getProfile(@PathParam(\"id\") String id) { return database.getUser(id); } // Operation 2: Update user's email @GetMapping(\"/users/{id}/updateEmail\")public Response updateEmail( @PathParam(\"id\") String id, @RequestParam(\"email\") String newEmail ) { User user = database.getUser(id); user.email = newEmail; database.save(user); return Response.ok(\"Email updated\"); } // Operation 3: Delete user @PostMapping(\"/users/{id}/delete\")public Response deleteUser(@PathParam(\"id\") String id) { database.deleteUser(id); return Response.ok(\"User deleted\"); } // Operation 4: Partial update of user @PutMapping(\"/users/{id}\")public Response partialUpdate( @PathParam(\"id\") String id, @RequestBody Map<String, Object> updates ) { User user = database.getUser(id); // Apply only provided fields if (updates.containsKey(\"email\")) { user.email = (String) updates.get(\"email\"); } database.save(user); return Response.ok(user); } Your debugging: Operation Current Method Correct Method Why? Get profile POST [Fill in] [Explain] Update email GET [Fill in] [Explain] Delete user POST [Fill in] [Explain] Partial update PUT [Fill in] [Explain] Click to verify your answers Operation Current Method Correct Method Why? Get profile POST GET Reading data, no side effects, should be cacheable Update email GET PATCH or PUT Modifying data, GET should never modify state Delete user POST DELETE Deleting resource, use proper semantic method Partial update PUT PATCH PUT replaces entire resource, PATCH for partial updates Correct implementations: // 1. Get profile - use GET @GetMapping(\"/users/{id}/profile\") public User getProfile(@PathParam(\"id\") String id) { return database.getUser(id); } // 2. Update email - use PATCH @PatchMapping(\"/users/{id}\") public Response updateEmail( @PathParam(\"id\") String id, @RequestBody UpdateEmailRequest req ) { User user = database.getUser(id); user.email = req.email; database.save(user); return Response.ok(user); } // 3. Delete user - use DELETE @DeleteMapping(\"/users/{id}\") public Response deleteUser(@PathParam(\"id\") String id) { database.deleteUser(id); return Response.status(204).build(); // 204 No Content } // 4. Partial update - use PATCH @PatchMapping(\"/users/{id}\") public Response partialUpdate( @PathParam(\"id\") String id, @RequestBody Map<String, Object> updates ) { // Implementation stays the same } Key principle: HTTP methods have semantic meaning - use them correctly for proper REST API design. Challenge 4: Poor Error Handling \u00b6 /** * Error handling that makes debugging IMPOSSIBLE. * Find all the problems! */ @PostMapping(\"/users\") public Response createUser(@RequestBody CreateUserRequest req) { try { // Validation if (req.email == null) { return Response.status(500).body(\"Error\"); } if (!req.email.contains(\"@\")) { return Response.status(200).body(null); } // Check duplicate User existing = database.findByEmail(req.email); if (existing != null) { return Response.status(400).body(\"Error\"); } // Create user User user = database.createUser(req); return Response.status(200).body(user); } catch (Exception e) { return Response.status(500).body(\"Something went wrong\"); } } Your debugging: Bug 1: Missing email should return status ___ with message ___ Bug 2: Invalid email format should return status ___ with message ___ Bug 3: Duplicate email should return status ___ with message ___ Current: 400 Bad Request Correct: [Fill in] Why: [Explain the difference] Bug 4: Successful creation should return status ___ Current: 200 OK Correct: [Fill in] Why: [Explain] Bug 5: Generic catch block problems: [What's lost?] [How to fix?] [What should be logged?] Click to verify your answers Correct implementation: @PostMapping(\"/users\") public Response createUser(@RequestBody CreateUserRequest req) { // Validate required fields if (req.email == null || req.email.isEmpty()) { return Response.status(400).body(new ApiError( \"MISSING_EMAIL\", \"Email is required\", \"email\", \"Please provide a valid email address\" )); } // Validate format if (!req.email.contains(\"@\")) { return Response.status(400).body(new ApiError( \"INVALID_EMAIL\", \"Email format is invalid\", \"email\", \"Email must contain @ symbol\" )); } try { // Check duplicate User existing = database.findByEmail(req.email); if (existing != null) { return Response.status(409).body(new ApiError( \"DUPLICATE_EMAIL\", \"Email already registered\", \"email\", \"Please use a different email or try logging in\" )); } // Create user User user = database.createUser(req); return Response.status(201) .header(\"Location\", \"/users/\" + user.id) .body(user); } catch (DatabaseException e) { log.error(\"Database error creating user: {}\", req.email, e); return Response.status(500).body(new ApiError( \"DATABASE_ERROR\", \"Unable to create user\", null, \"Please try again later\" )); } catch (Exception e) { log.error(\"Unexpected error creating user: {}\", req.email, e); return Response.status(500).body(new ApiError( \"INTERNAL_ERROR\", \"An unexpected error occurred\", null, \"Please try again later or contact support\" )); } } Status code fixes: Bug 1: 400 Bad Request (not 500) - client error, not server error Bug 2: 400 Bad Request with proper message Bug 3: 409 Conflict (not 400) - resource conflict, specific error type Bug 4: 201 Created (not 200) - resource was created Bug 5: Always log exceptions, provide structured errors, differentiate error types Challenge 5: Missing Idempotency \u00b6 /** * Payment processing without idempotency. * What could go wrong? */ @PostMapping(\"/payments\") public Response processPayment(@RequestBody PaymentRequest req) { // Create payment record Payment payment = new Payment(); payment.userId = req.userId; payment.amount = req.amount; payment.status = \"PENDING\"; database.save(payment); // Charge credit card try { creditCardService.charge(req.cardToken, req.amount); payment.status = \"COMPLETED\"; database.save(payment); return Response.status(201).body(payment); } catch (PaymentFailedException e) { payment.status = \"FAILED\"; database.save(payment); return Response.status(400).body(\"Payment failed\"); } } Your debugging: Scenario 1: Client submits payment, network timeout before response Client doesn't know if payment succeeded Client retries request What happens? [Fill in] Impact: [Fill in] Scenario 2: Request processed, card charged, but database save fails Card was charged: [Yes/No] Payment record exists: [Yes/No] User's money: [What happened?] Fixes needed: Add [what field?] to ensure idempotency Check [what?] before processing Handle [what scenario?] to prevent double charging Click to verify your answer Problems: No idempotency = double charging on retry No transaction = money charged but no record No way to safely retry Correct implementation: @PostMapping(\"/payments\") public Response processPayment( @RequestBody PaymentRequest req, @RequestHeader(\"Idempotency-Key\") String idempotencyKey ) { // Check if already processed Payment existing = database.findByIdempotencyKey(idempotencyKey); if (existing != null) { // Already processed - return existing result return Response.status(200).body(existing); } // Start transaction Transaction tx = database.beginTransaction(); try { // Create payment record with idempotency key Payment payment = new Payment(); payment.userId = req.userId; payment.amount = req.amount; payment.idempotencyKey = idempotencyKey; // Unique constraint payment.status = \"PENDING\"; database.save(payment); // Charge credit card String chargeId = creditCardService.charge(req.cardToken, req.amount); payment.chargeId = chargeId; payment.status = \"COMPLETED\"; database.save(payment); tx.commit(); return Response.status(201).body(payment); } catch (PaymentFailedException e) { tx.rollback(); payment.status = \"FAILED\"; database.save(payment); return Response.status(400).body(new ApiError( \"PAYMENT_FAILED\", \"Payment could not be processed\", null, e.getMessage() )); } catch (Exception e) { tx.rollback(); log.error(\"Payment processing failed\", e); return Response.status(500).body(new ApiError( \"PAYMENT_ERROR\", \"Payment processing error\", null, \"Please try again\" )); } } Key fixes: Idempotency-Key header required Check for existing payment before processing Transaction ensures atomicity Safe to retry with same key Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all versioning issues Fixed pagination bugs and understood why they matter Corrected HTTP method usage Improved error handling with proper status codes Implemented idempotency for critical operations Common API mistakes you discovered: [List the patterns you noticed] [Fill in] [Fill in] Production-critical issues: Which bug could cause financial loss? [Fill in] Which bug would break clients immediately? [Fill in] Which bug would cause performance issues at scale? [Fill in] Decision Framework \u00b6 Questions to answer after implementation: 1. REST vs GraphQL vs RPC \u00b6 When to use REST? Your scenario: [Fill in] Key factors: [Fill in] When to use GraphQL? Your scenario: [Fill in] Key factors: [Fill in] When to use RPC? Your scenario: [Fill in] Key factors: [Fill in] 2. Trade-offs \u00b6 REST: Pros: [Fill in after understanding] Cons: [Fill in after understanding] GraphQL: Pros: [Fill in after understanding] Cons: [Fill in after understanding] RPC: Pros: [Fill in after understanding] Cons: [Fill in after understanding] 3. Your Decision Tree \u00b6 Build your decision tree after practicing: flowchart LR Start[\"What kind of API are you building?\"] N1[\"?\"] Start -->|\"Public web API for third parties\"| N1 N2[\"?\"] Start -->|\"Mobile app backend\"| N2 N3[\"?\"] Start -->|\"Service-to-service communication\"| N3 N4[\"?\"] Start -->|\"Complex data fetching with relationships\"| N4 Practice \u00b6 Scenario 1: Design API for a blogging platform \u00b6 Requirements: Users can create, edit, delete posts Users can comment on posts Users can follow other users Feed shows posts from followed users Your API design: Which paradigm would you choose? [Fill in] Why? [Fill in] Key endpoints/queries: [Fill in] How to handle feed generation? [Fill in] Scenario 2: Design API for microservices \u00b6 Requirements: Order service needs to call Payment service Payment service needs to call Notification service Low latency required Services are within same data center Your API design: Which paradigm would you choose? [Fill in] Why? [Fill in] How to handle errors? [Fill in] How to handle retries? [Fill in] Scenario 3: Design API for mobile app with poor network \u00b6 Requirements: Mobile app on slow 3G network Needs user profile, posts, and comments Different screens need different data Want to minimize requests Your API design: Which paradigm would you choose? [Fill in] Why? [Fill in] How to optimize for mobile? [Fill in] Caching strategy? [Fill in] Review Checklist \u00b6 REST API implemented with proper resource design GraphQL resolver implemented with field resolution RPC service implemented with method calls Understand when to use each paradigm Can explain trade-offs between approaches Built decision tree for API selection Completed practice scenarios Mastery Certification \u00b6 I certify that I can: Design REST APIs with proper resource modeling Choose appropriate HTTP methods and status codes Implement pagination for list endpoints Handle errors with structured responses Version APIs without breaking clients Make critical operations idempotent Compare REST, GraphQL, and RPC trade-offs Identify breaking vs compatible API changes Debug common API design mistakes Teach API design concepts to others Your API Design Principles (Write Your Own) \u00b6 Based on everything you've learned, write your personal API design checklist: My Top 5 API Design Principles: [Fill in] [Fill in] [Fill in] [Fill in] [Fill in] My Top 3 API Anti-Patterns to Avoid: [Fill in] [Fill in] [Fill in] When I review an API, I check: [Your checklist item] [Your checklist item] [Your checklist item] [Your checklist item] [Your checklist item]","title":"06. API Design"},{"location":"systems/06-api-design/#api-design","text":"REST, GraphQL, and RPC - Choosing the right API paradigm","title":"API Design"},{"location":"systems/06-api-design/#eli5-explain-like-im-5","text":"Your task: After implementing different API patterns, explain them simply. Prompts to guide you: What is REST in one sentence? Your answer: [Fill in after implementation] Why do we use REST for web APIs? Your answer: [Fill in after implementation] Real-world analogy for REST: Example: \"REST is like a restaurant menu where...\" Your analogy: [Fill in] What is GraphQL in one sentence? Your answer: [Fill in after implementation] When would you choose GraphQL over REST? Your answer: [Fill in after implementation] Real-world analogy for GraphQL: Example: \"GraphQL is like a buffet where...\" Your analogy: [Fill in] What is RPC (gRPC) in one sentence? Your answer: [Fill in after implementation] When would you use RPC instead of REST? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/06-api-design/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/06-api-design/#beforeafter-why-good-api-design-matters","text":"Your task: Compare poorly designed vs well-designed APIs to understand the impact.","title":"Before/After: Why Good API Design Matters"},{"location":"systems/06-api-design/#case-studies-api-design-in-the-wild","text":"","title":"Case Studies: API Design in the Wild"},{"location":"systems/06-api-design/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/06-api-design/#client-code","text":"public class APIDesignClient { public static void main(String[] args) { testRESTAPI(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testGraphQL(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testRPC(); } static void testRESTAPI() { System.out.println(\"=== REST API Test ===\\n\"); RESTAPIServer server = new RESTAPIServer(); // Test: Create user System.out.println(\"POST /users\"); RESTAPIServer.Response createResp = server.createUser(\"Alice\", \"alice@example.com\"); System.out.println(\"Status: \" + createResp.statusCode); System.out.println(\"Body: \" + createResp.body); // Test: Get user System.out.println(\"\\nGET /users/123\"); RESTAPIServer.Response getResp = server.getUser(\"123\"); System.out.println(\"Status: \" + getResp.statusCode); System.out.println(\"Body: \" + getResp.body); // Test: Get user's posts System.out.println(\"\\nGET /users/123/posts?page=1&limit=10\"); RESTAPIServer.Response postsResp = server.getUserPosts(\"123\", 1, 10); System.out.println(\"Status: \" + postsResp.statusCode); System.out.println(\"Body: \" + postsResp.body); } static void testGraphQL() { System.out.println(\"=== GraphQL Test ===\\n\"); GraphQLResolver resolver = new GraphQLResolver(); // Test: Query user with specific fields String query = \"\"\" { user(id: \"123\") { name email posts { title } } } \"\"\"; System.out.println(\"Query:\"); System.out.println(query); Map<String, Object> result = resolver.executeQuery(query); System.out.println(\"\\nResult: \" + result); } static void testRPC() { System.out.println(\"=== RPC Test ===\\n\"); UserService service = new RPCServer(); // Test: Create user (direct method call) System.out.println(\"createUser(\\\"Bob\\\", \\\"bob@example.com\\\")\"); String userId = service.createUser(\"Bob\", \"bob@example.com\"); System.out.println(\"Returned ID: \" + userId); // Test: Get user System.out.println(\"\\ngetUser(\\\"\" + userId + \"\\\")\"); User user = service.getUser(userId); System.out.println(\"Returned: \" + (user != null ? user.name : \"null\")); // Test: Get user posts System.out.println(\"\\ngetUserPosts(\\\"\" + userId + \"\\\")\"); List<Post> posts = service.getUserPosts(userId); System.out.println(\"Returned: \" + posts.size() + \" posts\"); } }","title":"Client Code"},{"location":"systems/06-api-design/#debugging-challenges","text":"Your task: Find and fix bugs in broken API implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"systems/06-api-design/#decision-framework","text":"Questions to answer after implementation:","title":"Decision Framework"},{"location":"systems/06-api-design/#practice","text":"","title":"Practice"},{"location":"systems/06-api-design/#review-checklist","text":"REST API implemented with proper resource design GraphQL resolver implemented with field resolution RPC service implemented with method calls Understand when to use each paradigm Can explain trade-offs between approaches Built decision tree for API selection Completed practice scenarios","title":"Review Checklist"},{"location":"systems/06-api-design/#your-api-design-principles-write-your-own","text":"Based on everything you've learned, write your personal API design checklist: My Top 5 API Design Principles: [Fill in] [Fill in] [Fill in] [Fill in] [Fill in] My Top 3 API Anti-Patterns to Avoid: [Fill in] [Fill in] [Fill in] When I review an API, I check: [Your checklist item] [Your checklist item] [Your checklist item] [Your checklist item] [Your checklist item]","title":"Your API Design Principles (Write Your Own)"},{"location":"systems/07-security-patterns/","text":"Security Patterns \u00b6 Authentication, authorization, and securing distributed systems ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing security patterns, explain them simply. Prompts to guide you: What is authentication in one sentence? Your answer: [Fill in after implementation] What is authorization in one sentence? Your answer: [Fill in after implementation] Real-world analogy for authentication: Example: \"Authentication is like showing your ID at the door...\" Your analogy: [Fill in] Real-world analogy for authorization: Example: \"Authorization is like having a key to certain rooms...\" Your analogy: [Fill in] What is JWT in one sentence? Your answer: [Fill in after implementation] When should you use JWT vs sessions? Your answer: [Fill in after practice] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your security intuition without looking at code. Answer these, then verify after implementation. Security Concept Predictions \u00b6 Authentication vs Authorization: Authentication is: [Your definition] Authorization is: [Your definition] Example scenario: [Think of real-world example] JWT token structure: Three parts: [What are they?] Why is signature needed?: [Your guess] Can client modify payload?: [Yes/No - Why?] RBAC complexity: Checking permission for user with 3 roles: O(?) Better than checking each permission individually?: [Yes/No - Why?] Scenario Predictions \u00b6 Scenario 1: A user tries to access a protected resource with JWT token What gets validated first? [Signature? Expiration? Claims?] If signature invalid, what does it mean? [Token tampered? Expired? Wrong secret?] What happens if token expired but signature valid? [Allow? Deny? Refresh?] Scenario 2: Implementing RBAC for a blog platform Roles needed: [List them] VIEWER can: [What permissions?] EDITOR can: [What permissions beyond VIEWER?] ADMIN can: [Everything or specific permissions?] Scenario 3: API key gets leaked on GitHub Immediate action: [What to do first?] Why rotation matters: [Explain] How to prevent: [Fill in] Security Trade-off Quiz \u00b6 Question: When would Session-based auth be BETTER than JWT? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN advantage of JWT over sessions? More secure Stateless (no server-side storage) Easier to implement Better performance Verify after implementation: [Which one(s)?] Question: Why use HMAC for JWT signature instead of just Base64 encoding? Your answer: [Fill in] Verified: [Fill in after implementation] Before/After: Why Security Patterns Matter \u00b6 Your task: Compare insecure vs secure approaches to understand the security impact. Example 1: Authentication - Insecure vs Secure \u00b6 Problem: Implement user authentication for an API. Approach 1: Insecure - Username/Password in Every Request \u00b6 // INSECURE: Sending credentials with every request public class InsecureAuth { public boolean authenticateRequest(String username, String password) { // Problem: Credentials sent with EVERY API call // - Exposed in logs, network traffic // - No expiration mechanism // - Can't revoke access without changing password return checkDatabase(username, password); } // Client code public void makeAPICall() { // INSECURE: Username and password in every request String response = httpClient.get(\"/api/data\", \"username=john\", \"password=secret123\"); // Exposed! } } Security Issues: Credentials exposed in every request (logs, network, proxy) No way to revoke access without password change Password transmitted repeatedly (more attack surface) Can't implement session timeout Difficult to audit (which requests from which session?) Approach 2: Secure - JWT Token-Based Auth \u00b6 // SECURE: Token-based authentication public class SecureJWTAuth { private final String secret = System.getenv(\"JWT_SECRET\"); // From environment private final long expirationMs = 3600000; // 1 hour // Step 1: Login once to get token public String login(String username, String password) { if (!validateCredentials(username, password)) { return null; } // Generate short-lived token return generateJWT(username, expirationMs); } // Step 2: Use token for subsequent requests public boolean authenticateRequest(String token) { try { String userId = validateJWT(token); // Check expiration if (isExpired(token)) { return false; // Must re-login } return userId != null; } catch (SecurityException e) { return false; // Invalid signature } } // Client code public void makeAPICall() { // SECURE: Token in Authorization header String token = loginOnce(\"john\", \"secret123\"); String response = httpClient.get(\"/api/data\", headers: {\"Authorization\": \"Bearer \" + token}); // Password only sent once during login! } } Security Improvements: Credentials only sent once during login Token expires automatically (limited exposure window) Can revoke tokens without password change Signature prevents tampering Stateless (scales horizontally) Audit-friendly (track token usage) Performance & Security Comparison \u00b6 Aspect Insecure (Creds Every Request) Secure (JWT) Improvement Credential exposure Every request Login only 100x less Revocation Change password Revoke token Immediate Expiration None Built-in Auto-logout Tampering protection None HMAC signature Detectable Scalability Database hit every request No database lookup 10-100x faster Audit trail Difficult Token ID trackable Complete Your calculation: If a user makes 1,000 API calls per day: Insecure approach exposes credentials _____ times Secure approach exposes credentials _____ time(s) Security improvement: _____ x safer Example 2: Authorization - No Checks vs RBAC \u00b6 Problem: Control who can delete blog posts. Approach 1: No Authorization Checks \u00b6 // INSECURE: No authorization, anyone can delete public class NoAuthzBlog { public boolean deletePost(String postId, String userId) { // Any authenticated user can delete any post database.delete(\"posts\", postId); return true; } // Security hole: Attacker can delete all posts public void attackExample() { // Even a VIEWER role can do this! deletePost(\"important-post\", \"attacker-user-id\"); } } Security Issues: Any authenticated user can perform any action No distinction between roles (viewer, editor, admin) Privilege escalation: viewer acts as admin No audit trail of who did what Cannot implement least privilege principle Approach 2: RBAC Authorization \u00b6 // SECURE: Role-Based Access Control public class RBACBlog { private final RBACAuthorizer rbac; public boolean deletePost(String postId, String userId) { // Step 1: Check authorization BEFORE action if (!rbac.hasPermission(userId, Permission.DELETE)) { auditLog.warn(\"Unauthorized delete attempt\", userId, postId); throw new SecurityException(\"Insufficient permissions\"); } // Step 2: Additional check - can only delete own posts (unless admin) Post post = database.getPost(postId); if (!post.authorId.equals(userId) && !rbac.hasRole(userId, Role.ADMIN)) { throw new SecurityException(\"Can only delete own posts\"); } // Step 3: Perform action with audit logging database.delete(\"posts\", postId); auditLog.info(\"Post deleted\", userId, postId); return true; } // Secure example: Permission properly checked public void secureExample() { try { deletePost(\"important-post\", \"viewer-user-id\"); } catch (SecurityException e) { // Blocked! Viewer doesn't have DELETE permission System.out.println(\"Access denied: \" + e.getMessage()); } } } Security Improvements: Explicit permission check before every sensitive action Role-based hierarchy (viewer < editor < admin) Audit logging for security events Fail-secure (deny by default) Least privilege principle enforced Security Comparison \u00b6 Attack Vector No Authorization With RBAC Prevented? Viewer deletes posts Succeeds Blocked \u2713 Editor deletes other's posts Succeeds Blocked \u2713 User promotes self to admin Succeeds Blocked \u2713 Audit trail of actions None Complete \u2713 Privilege escalation Easy Impossible \u2713 After implementing, explain in your own words: Why is \"deny by default\" important? [Your answer] What happens if you forget one authorization check? [Your answer] How does RBAC prevent privilege escalation? [Your answer] Example 3: Secrets Management - Hardcoded vs Encrypted \u00b6 Problem: Store database password for application use. Approach 1: Hardcoded Credentials \u00b6 // INSECURE: Hardcoded credentials in source code public class HardcodedSecrets { private static final String DB_PASSWORD = \"super-secret-pwd-123\"; public Connection connectToDatabase() { // Problems: // 1. Password in version control (git history) // 2. Visible to anyone with code access // 3. Can't rotate without redeploying // 4. Same password in dev, staging, prod return DriverManager.getConnection( \"jdbc:postgresql://db.example.com/mydb\", \"dbuser\", DB_PASSWORD // EXPOSED! ); } } Security Issues: Secret in git history (can't remove) Visible in source code reviews Leaked in compiled binaries/JAR files Can't rotate without code changes + redeployment Same secret across all environments Exposed in logs, stack traces, debugging Approach 2: Encrypted Secrets Management \u00b6 // SECURE: Encrypted secrets with rotation public class SecureSecretsManagement { private final SecretsManager secretsManager; public Connection connectToDatabase() { // Step 1: Retrieve secret from encrypted store // - Master key stored in environment/HSM // - Secrets encrypted at rest // - Access controlled per service String dbPassword = secretsManager.getSecret( \"db_password\", getCurrentServiceId() ); // Step 2: Use secret (never log it!) return DriverManager.getConnection( \"jdbc:postgresql://db.example.com/mydb\", \"dbuser\", dbPassword // Retrieved securely ); // Step 3: dbPassword cleared from memory after use } // Rotation without downtime public void rotatePassword() { // 1. Generate new password String newPassword = generateSecurePassword(); // 2. Update database with new password database.updateUserPassword(\"dbuser\", newPassword); // 3. Store new version in secrets manager secretsManager.rotateSecret(\"db_password\", newPassword); // 4. Old version still valid for grace period // 5. After grace period, old version deleted } } Security Improvements: Secrets never in source code or version control Encrypted at rest with master key Access control (only authorized services can read) Audit logging (who accessed what secret when) Rotation without code changes or redeployment Different secrets per environment (dev/staging/prod) Automatic expiration and rotation Security Impact Analysis \u00b6 Risk Hardcoded Secrets Manager Mitigation Git leak Exposed forever Not in git \u2713 Code review leak Visible Not visible \u2713 Rotation cost Redeploy API call \u2713 Audit capability None Full logging \u2713 Blast radius All environments Isolated \u2713 Post-breach response Manual everywhere Rotate instantly \u2713 Real-world impact: In 2019, 50,000+ hardcoded secrets leaked on GitHub led to major breaches. Your reflection after implementation: How would you rotate a leaked hardcoded password? [Your answer] What's the blast radius if secrets manager is breached vs. hardcoded? [Your answer] Why is \"secrets in environment variables\" better than hardcoded but still not ideal? [Your answer] Case Studies: Security Patterns in the Wild \u00b6 Google & Facebook Login: OAuth 2.0 and OpenID Connect \u00b6 Pattern: OAuth 2.0 for delegated authorization and OIDC for authentication. How it works: When you click \"Login with Google\" on a third-party site (like Stack Overflow), the site (the Client) redirects you to Google (the Authorization Server). You grant permission for Stack Overflow to access your basic profile. Google then redirects you back with an authorization code. Stack Overflow's server exchanges this code for an Access Token (a JWT). It can then use this token to fetch your profile information from Google's API. OIDC provides the identity layer on top of OAuth 2.0, standardizing how profile information is shared. Key Takeaway: OAuth 2.0 is the standard for delegated authorization, allowing users to grant limited access to their data without sharing their passwords. It separates the roles of the user, the client application, and the authorization server. AWS & Google Cloud APIs: API Keys and IAM \u00b6 Pattern: API Keys for programmatic access and IAM for fine-grained authorization. How it works: When a developer wants to use an AWS S3 or Google Maps API from their server, they generate an API Key. This key is a long, unique string that is passed in an HTTP header with each request. The API gateway validates the key to authenticate the calling service. The key is linked to an IAM (Identity and Access Management) role, which defines exactly what actions that key is authorized to perform (e.g., s3:GetObject but not s3:DeleteObject ). Key Takeaway: API keys are a simple and effective way to authenticate server-to-server communication. However, authentication alone is not enough; it must be paired with a robust authorization system like IAM to enforce the principle of least privilege. Netflix Microservices: JWT Propagation for Internal Authorization \u00b6 Pattern: Passing JWTs between internal services for user context. How it works: When a user streams a movie, their initial request to the Netflix API Gateway includes a JWT that identifies them. As that request fans out to internal microservices (e.g., the Bookmarks service, the Recommendations service, the Playback service), that JWT is passed along with each internal call. Each microservice can independently validate the JWT's signature (using a shared public key) and check its claims (like userId and scopes ) to authorize the action without needing to call an external authentication service. Key Takeaway: JWTs are stateless and portable, making them ideal for microservice architectures. Propagating the user's identity allows for decentralized authorization decisions and helps maintain user context for logging and auditing throughout a distributed system. Core Implementation \u00b6 Pattern 1: JWT-Based Authentication \u00b6 Concept: Stateless authentication using JSON Web Tokens. Use case: Microservices, API authentication, mobile apps. import java.util.*; import java.nio.charset.StandardCharsets; import java.util.Base64; import javax.crypto.Mac; import javax.crypto.spec.SecretKeySpec; /** * JWT Authentication: Stateless token-based auth * * Token structure: header.payload.signature * - Header: algorithm and token type * - Payload: claims (user data, expiration) * - Signature: HMAC of header+payload with secret */ public class JWTAuthenticator { private final String secret; private final long expirationMs; public JWTAuthenticator(String secret, long expirationMs) { this.secret = secret; this.expirationMs = expirationMs; } /** * Generate JWT token for user * Time: O(1), Space: O(1) * * TODO: Implement JWT generation * 1. Create header: {\"alg\": \"HS256\", \"typ\": \"JWT\"} * 2. Create payload: {\"sub\": userId, \"exp\": expiration, \"iat\": issuedAt} * 3. Base64 encode header and payload * 4. Sign with HMAC-SHA256 * 5. Return header.payload.signature */ public String generateToken(String userId) { // TODO: Create header // TODO: Create payload with expiration // TODO: Create signature // TODO: Return JWT return null; // Replace } /** * Validate and extract user from JWT * Time: O(1), Space: O(1) * * TODO: Implement JWT validation * 1. Split token into parts * 2. Verify signature * 3. Check expiration * 4. Extract and return user ID */ public String validateToken(String token) { // TODO: Split token // TODO: Verify signature // TODO: Decode and check expiration // TODO: Return userId from payload return null; // Replace } /** * Helper: Base64 URL-safe encoding * * TODO: Implement base64 URL encoding */ private String base64UrlEncode(String input) { // TODO: Encode and make URL-safe return null; // Replace } /** * Helper: HMAC-SHA256 signature * * TODO: Implement HMAC signing */ private String hmacSha256(String data, String key) { // TODO: Use Mac with HmacSHA256 return null; // Replace } } Runnable Client Code: public class JWTClient { public static void main(String[] args) { System.out.println(\"=== JWT Authentication ===\\n\"); String secret = \"your-256-bit-secret\"; long expirationMs = 3600000; // 1 hour JWTAuthenticator auth = new JWTAuthenticator(secret, expirationMs); // Test 1: Generate token System.out.println(\"--- Test 1: Generate Token ---\"); String token = auth.generateToken(\"user123\"); System.out.println(\"Generated token: \" + token); // Test 2: Validate token System.out.println(\"\\n--- Test 2: Validate Token ---\"); String userId = auth.validateToken(token); System.out.println(\"Extracted user: \" + userId); // Test 3: Invalid token System.out.println(\"\\n--- Test 3: Invalid Token ---\"); String invalidToken = \"invalid.token.here\"; String result = auth.validateToken(invalidToken); System.out.println(\"Validation result: \" + result); } } Pattern 2: Role-Based Access Control (RBAC) \u00b6 Concept: Authorization based on user roles and permissions. Use case: Multi-tenant systems, enterprise applications, admin panels. import java.util.*; /** * RBAC: Role-Based Access Control * * Concepts: * - Users have roles (admin, editor, viewer) * - Roles have permissions (read, write, delete) * - Check permission before allowing action */ public class RBACAuthorizer { // Role definitions enum Role { ADMIN, EDITOR, VIEWER } enum Permission { READ, WRITE, DELETE, MANAGE_USERS } // Role -> Permissions mapping private final Map<Role, Set<Permission>> rolePermissions; // User -> Roles mapping private final Map<String, Set<Role>> userRoles; public RBACAuthorizer() { this.rolePermissions = new HashMap<>(); this.userRoles = new HashMap<>(); initializeRolePermissions(); } /** * Initialize default role permissions * * TODO: Set up role hierarchies * - ADMIN: all permissions * - EDITOR: read, write * - VIEWER: read only */ private void initializeRolePermissions() { // TODO: Define ADMIN permissions // TODO: Define EDITOR permissions // TODO: Define VIEWER permissions } /** * Assign role to user * Time: O(1), Space: O(1) * * TODO: Implement role assignment */ public void assignRole(String userId, Role role) { // TODO: Add role to user's role set } /** * Check if user has permission * Time: O(R) where R = number of roles, Space: O(1) * * TODO: Implement permission check * 1. Get user's roles * 2. For each role, check if it has the permission * 3. Return true if any role grants permission */ public boolean hasPermission(String userId, Permission permission) { // TODO: Get user roles // TODO: Check each role's permissions return false; // Replace } /** * Get all permissions for user * Time: O(R*P), Space: O(P) * * TODO: Implement permission aggregation */ public Set<Permission> getUserPermissions(String userId) { Set<Permission> allPermissions = new HashSet<>(); // TODO: Aggregate permissions from all roles return allPermissions; // Replace } /** * Remove role from user * Time: O(1), Space: O(1) * * TODO: Implement role revocation */ public void revokeRole(String userId, Role role) { // TODO: Remove role from user } } Runnable Client Code: import static RBACAuthorizer.*; public class RBACClient { public static void main(String[] args) { System.out.println(\"=== RBAC Authorization ===\\n\"); RBACAuthorizer rbac = new RBACAuthorizer(); // Test 1: Assign roles System.out.println(\"--- Test 1: Role Assignment ---\"); rbac.assignRole(\"alice\", Role.ADMIN); rbac.assignRole(\"bob\", Role.EDITOR); rbac.assignRole(\"charlie\", Role.VIEWER); System.out.println(\"Roles assigned\"); // Test 2: Check permissions System.out.println(\"\\n--- Test 2: Permission Checks ---\"); System.out.println(\"Alice (ADMIN) can DELETE: \" + rbac.hasPermission(\"alice\", Permission.DELETE)); System.out.println(\"Bob (EDITOR) can WRITE: \" + rbac.hasPermission(\"bob\", Permission.WRITE)); System.out.println(\"Charlie (VIEWER) can DELETE: \" + rbac.hasPermission(\"charlie\", Permission.DELETE)); // Test 3: Get all permissions System.out.println(\"\\n--- Test 3: All Permissions ---\"); System.out.println(\"Alice permissions: \" + rbac.getUserPermissions(\"alice\")); System.out.println(\"Bob permissions: \" + rbac.getUserPermissions(\"bob\")); System.out.println(\"Charlie permissions: \" + rbac.getUserPermissions(\"charlie\")); } } Pattern 3: API Key Authentication \u00b6 Concept: Long-lived tokens for service-to-service authentication. Use case: REST APIs, webhooks, third-party integrations. import java.util.*; import java.security.SecureRandom; /** * API Key Authentication * * Key properties: * - Long-lived credentials * - Scoped to specific resources * - Can be rate-limited per key * - Easy to rotate and revoke */ public class APIKeyAuth { static class APIKey { String key; String userId; Set<String> scopes; long createdAt; long lastUsedAt; int usageCount; APIKey(String key, String userId, Set<String> scopes) { this.key = key; this.userId = userId; this.scopes = scopes; this.createdAt = System.currentTimeMillis(); this.lastUsedAt = createdAt; this.usageCount = 0; } } private final Map<String, APIKey> keys; private final SecureRandom random; public APIKeyAuth() { this.keys = new HashMap<>(); this.random = new SecureRandom(); } /** * Generate new API key * Time: O(1), Space: O(1) * * TODO: Implement key generation * 1. Generate random key (32-byte hex) * 2. Store with user ID and scopes * 3. Return key */ public String generateKey(String userId, Set<String> scopes) { // TODO: Generate secure random key // TODO: Store key return null; // Replace } /** * Validate API key and check scope * Time: O(1), Space: O(1) * * TODO: Implement key validation * 1. Lookup key * 2. Check if scope is allowed * 3. Update usage metrics * 4. Return user ID or null */ public String validateKey(String key, String requiredScope) { // TODO: Lookup key // TODO: Check scope // TODO: Update usage return null; // Replace } /** * Revoke API key * Time: O(1), Space: O(1) * * TODO: Implement key revocation */ public boolean revokeKey(String key) { // TODO: Remove key from storage return false; // Replace } /** * Get usage statistics for key * Time: O(1), Space: O(1) * * TODO: Implement usage tracking */ public Map<String, Object> getKeyStats(String key) { Map<String, Object> stats = new HashMap<>(); // TODO: Return key statistics return stats; // Replace } private String bytesToHex(byte[] bytes) { StringBuilder sb = new StringBuilder(); for (byte b : bytes) { sb.append(String.format(\"%02x\", b)); } return sb.toString(); } } Runnable Client Code: import java.util.*; public class APIKeyClient { public static void main(String[] args) { System.out.println(\"=== API Key Authentication ===\\n\"); APIKeyAuth apiKeyAuth = new APIKeyAuth(); // Test 1: Generate keys System.out.println(\"--- Test 1: Generate API Keys ---\"); Set<String> scopes1 = new HashSet<>(Arrays.asList(\"read\", \"write\")); String key1 = apiKeyAuth.generateKey(\"service1\", scopes1); System.out.println(\"Generated key for service1: \" + key1); Set<String> scopes2 = new HashSet<>(Arrays.asList(\"read\")); String key2 = apiKeyAuth.generateKey(\"service2\", scopes2); System.out.println(\"Generated key for service2: \" + key2); // Test 2: Validate keys System.out.println(\"\\n--- Test 2: Validate Keys ---\"); String userId1 = apiKeyAuth.validateKey(key1, \"write\"); System.out.println(\"Key1 with 'write' scope: \" + userId1); String userId2 = apiKeyAuth.validateKey(key2, \"write\"); System.out.println(\"Key2 with 'write' scope: \" + userId2); // Test 3: Revoke key System.out.println(\"\\n--- Test 3: Revoke Key ---\"); boolean revoked = apiKeyAuth.revokeKey(key1); System.out.println(\"Key1 revoked: \" + revoked); String userId3 = apiKeyAuth.validateKey(key1, \"read\"); System.out.println(\"Key1 after revocation: \" + userId3); } } Pattern 4: Secrets Management \u00b6 Concept: Secure storage and rotation of sensitive credentials. Use case: Database passwords, API keys, encryption keys. import javax.crypto.Cipher; import javax.crypto.KeyGenerator; import javax.crypto.SecretKey; import javax.crypto.spec.SecretKeySpec; import java.util.*; /** * Secrets Manager * * Features: * - Encrypted storage * - Versioning for rotation * - Access control per secret * - Audit logging */ public class SecretsManager { static class Secret { String name; byte[] encryptedValue; int version; long createdAt; Set<String> authorizedUsers; Secret(String name, byte[] encryptedValue, int version, Set<String> authorizedUsers) { this.name = name; this.encryptedValue = encryptedValue; this.version = version; this.createdAt = System.currentTimeMillis(); this.authorizedUsers = authorizedUsers; } } private final Map<String, List<Secret>> secrets; // name -> versions private final SecretKey masterKey; /** * Initialize secrets manager with master encryption key * * TODO: Set up encryption */ public SecretsManager(SecretKey masterKey) { this.secrets = new HashMap<>(); this.masterKey = masterKey; } /** * Store secret with encryption * Time: O(1), Space: O(1) * * TODO: Implement secret storage * 1. Encrypt value with master key * 2. Store with version number * 3. Set authorized users */ public void storeSecret(String name, String value, Set<String> authorizedUsers) { // TODO: Encrypt secret value // TODO: Create new version } /** * Retrieve secret with authorization check * Time: O(V) where V = versions, Space: O(1) * * TODO: Implement secret retrieval * 1. Check authorization * 2. Get latest version * 3. Decrypt and return */ public String getSecret(String name, String userId) { // TODO: Get latest version // TODO: Check authorization // TODO: Decrypt and return return null; // Replace } /** * Rotate secret (create new version) * Time: O(1), Space: O(1) * * TODO: Implement secret rotation */ public void rotateSecret(String name, String newValue, String userId) { // TODO: Verify authorization to rotate // TODO: Create new version with new value // TODO: Keep old versions for grace period } /** * Helper: Encrypt data * * TODO: Implement AES encryption */ private byte[] encrypt(byte[] data, SecretKey key) { // TODO: Use AES/GCM for authenticated encryption return null; // Replace } /** * Helper: Decrypt data * * TODO: Implement AES decryption */ private byte[] decrypt(byte[] encryptedData, SecretKey key) { // TODO: Use AES/GCM for decryption return null; // Replace } } Runnable Client Code: import javax.crypto.KeyGenerator; import javax.crypto.SecretKey; import java.util.*; public class SecretsManagerClient { public static void main(String[] args) throws Exception { System.out.println(\"=== Secrets Management ===\\n\"); // Generate master key KeyGenerator keyGen = KeyGenerator.getInstance(\"AES\"); keyGen.init(256); SecretKey masterKey = keyGen.generateKey(); SecretsManager sm = new SecretsManager(masterKey); // Test 1: Store secrets System.out.println(\"--- Test 1: Store Secrets ---\"); Set<String> users1 = new HashSet<>(Arrays.asList(\"admin\", \"service1\")); sm.storeSecret(\"db_password\", \"super-secret-pwd\", users1); System.out.println(\"Stored db_password\"); Set<String> users2 = new HashSet<>(Arrays.asList(\"service2\")); sm.storeSecret(\"api_key\", \"sk_live_123456\", users2); System.out.println(\"Stored api_key\"); // Test 2: Retrieve secrets System.out.println(\"\\n--- Test 2: Retrieve Secrets ---\"); String pwd = sm.getSecret(\"db_password\", \"admin\"); System.out.println(\"Retrieved db_password: \" + pwd); // Test 3: Unauthorized access System.out.println(\"\\n--- Test 3: Unauthorized Access ---\"); try { String key = sm.getSecret(\"api_key\", \"admin\"); System.out.println(\"Retrieved api_key: \" + key); } catch (SecurityException e) { System.out.println(\"Access denied: \" + e.getMessage()); } } } Debugging Challenges \u00b6 Your task: Find and fix security bugs in broken implementations. This tests your understanding. Challenge 1: Broken JWT Validation \u00b6 /** * This JWT validator has 3 CRITICAL SECURITY BUGS. Find them! */ public class BrokenJWTValidator { private final String secret = \"my-secret-key\"; public String validateToken_Buggy(String token) { String[] parts = token.split(\"\\\\.\"); // Extract payload String payload = parts[1]; String decodedPayload = base64Decode(payload); // Parse JSON to get user ID String userId = extractUserId(decodedPayload); return userId; } } Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Bug 3: [What\\'s the bug?] Security impact: What can an attacker do with these bugs? Bug 1 impact: [Fill in] Bug 2 impact: [Fill in - this is the worst one!] Bug 3 impact: [Fill in] Click to verify your answers Bug 1 (Line 9): No null/length check on token or parts. Attacker can send malformed token causing ArrayIndexOutOfBoundsException or NullPointerException. Fix: if (token == null || token.isEmpty()) return null; String[] parts = token.split(\"\\\\.\"); if (parts.length != 3) return null; // JWT must have 3 parts Bug 2 (Lines 12-16): NEVER VALIDATES SIGNATURE! This is critical - attacker can forge any token! Fix: // Before extracting payload, verify signature String toVerify = parts[0] + \".\" + parts[1]; String expectedSig = hmacSha256(toVerify, secret); if (!expectedSig.equals(parts[2])) { throw new SecurityException(\"Invalid signature\"); } Bug 3 (Line 18): No expiration check. Token valid forever even after user logout or password change. Fix: long exp = extractExpiration(decodedPayload); if (System.currentTimeMillis() / 1000 > exp) { throw new SecurityException(\"Token expired\"); } Security impact: Bug 1: Denial of service, crashes Bug 2: Complete authentication bypass - attacker can impersonate any user! Bug 3: Stolen tokens work forever, can't revoke access Challenge 2: RBAC Permission Bypass \u00b6 /** * This RBAC implementation has 2 AUTHORIZATION BUGS. */ public class BrokenRBAC { private Map<String, Set<Role>> userRoles = new HashMap<>(); private Map<Role, Set<Permission>> rolePermissions = new HashMap<>(); public boolean hasPermission_Buggy(String userId, Permission permission) { Set<Role> roles = userRoles.get(userId); for (Role role : roles) { Set<Permission> perms = rolePermissions.get(role); if (perms.contains(permission)) { return true; } } return false; } public void deleteResource_Buggy(String resourceId, String userId) { database.delete(resourceId); System.out.println(\"Deleted: \" + resourceId); } } Your debugging: Bug 1: [What exception occurs?] Bug 1 exploit: [Can attacker use this?] Bug 1 fix: [Add what check?] Bug 2: [What's the security flaw?] Bug 2 exploit: [How can attacker abuse this?] Bug 2 fix: [What MUST happen before delete?] Trace through attack scenario: Attacker with no roles calls deleteResource(\"admin-data\", \"attacker\") What happens at Bug 1? [Fill in] What happens at Bug 2? [Fill in] Final result: [Is resource deleted? Should it be?] Click to verify your answers Bug 1 (Line 13): NullPointerException if user has no roles. userRoles.get(userId) returns null. Fix: Set<Role> roles = userRoles.get(userId); if (roles == null || roles.isEmpty()) { return false; // No roles = no permissions } Bug 2 (Lines 23-26): NO PERMISSION CHECK BEFORE DELETION! Classic authorization bypass. Fix: public void deleteResource_Buggy(String resourceId, String userId) { // MUST check permission first if (!hasPermission(userId, Permission.DELETE)) { throw new SecurityException(\"Insufficient permissions\"); } database.delete(resourceId); auditLog.log(\"Deleted: \" + resourceId + \" by \" + userId); } Attack scenario: Attacker calls deleteResource(\"admin-data\", \"attacker\") Bug 2: No permission check, deletion proceeds Result: Resource deleted despite no authorization! Key lesson: EVERY sensitive operation MUST have explicit authorization check. One missing check = security hole. Challenge 3: Timing Attack on Token Comparison \u00b6 /** * This token validator has a SUBTLE TIMING ATTACK vulnerability. */ public class TimingAttackVulnerable { private static final String VALID_API_KEY = \"sk_live_a1b2c3d4e5f6\"; public boolean validateAPIKey_Buggy(String providedKey) { if (providedKey.equals(VALID_API_KEY)) { return true; } return false; } // Alternative buggy version using manual comparison public boolean validateAPIKey_Buggy2(String providedKey) { if (providedKey.length() != VALID_API_KEY.length()) { return false; } for (int i = 0; i < VALID_API_KEY.length(); i++) { if (providedKey.charAt(i) != VALID_API_KEY.charAt(i)) { return false; // Returns immediately on first mismatch } } return true; } } Your debugging: Bug location: [Both versions have the same class of bug] Bug type: [What kind of attack is possible?] Bug explanation: [How does attacker exploit timing differences?] Why is this dangerous? [Can attacker guess the secret?] Attack simulation: Try key: \"sk_live_XXXXXXXX\" (wrong prefix) \u2192 Takes ___ time Try key: \"sk_live_a1XXXXXX\" (first 2 chars match) \u2192 Takes ___ time Pattern: [What does attacker learn from timing?] Your fix: [How to compare in constant time?] Click to verify your answers Bug: Both use non-constant-time comparison. String comparison returns early on first mismatch, leaking information about which characters are correct. Attack: Attacker measures response times: \"sk_live_XXXXXXXX\" \u2192 Fast (fails at 8th char) \"sk_live_a1XXXXXX\" \u2192 Slightly slower (fails at 10th char) Attacker learns: first 2 chars after underscore are \"a1\" Repeat for each character \u2192 Bruteforce key character-by-character! Fix - Constant-time comparison: public boolean validateAPIKey_Secure(String providedKey) { if (providedKey == null || providedKey.length() != VALID_API_KEY.length()) { return false; } // Constant-time comparison: always checks all characters int result = 0; for (int i = 0; i < VALID_API_KEY.length(); i++) { result |= providedKey.charAt(i) ^ VALID_API_KEY.charAt(i); } return result == 0; // 0 means all characters matched } Or use Java's built-in: import java.security.MessageDigest; public boolean validateAPIKey_Secure(String providedKey) { return MessageDigest.isEqual( providedKey.getBytes(), VALID_API_KEY.getBytes() ); } Key lesson: String/token comparisons MUST be constant-time to prevent timing attacks. This applies to passwords, API keys, HMAC signatures, etc. Challenge 4: Secret Exposure in Logs \u00b6 /** * This code accidentally leaks secrets. Find 3 leak points! */ public class SecretLeakage { private final String dbPassword = System.getenv(\"DB_PASSWORD\"); public void connectToDatabase() { String connectionUrl = \"jdbc:postgresql://db.example.com/mydb\" + \"?user=dbuser&password=\" + dbPassword; System.out.println(\"Connecting to: \" + connectionUrl); try { Connection conn = DriverManager.getConnection(connectionUrl); } catch (SQLException e) { e.printStackTrace(); logger.error(\"Database connection failed: \" + e.getMessage()); } } public String generateJWT(String userId) { String secret = System.getenv(\"JWT_SECRET\"); String token = createToken(userId, secret); logger.info(\"Generated token for user \" + userId + \": \" + token); - **Bug 1:** <span class=\"fill-in\">[What's exposed in the URL?]</span> - **Bug 2:** <span class=\"fill-in\">[What gets printed to console?]</span> - **Bug 3:** <span class=\"fill-in\">[What's in the SQLException details?]</span> - **Bug 4:** <span class=\"fill-in\">[Is logging the token a security issue? Why?]</span> **Real-world impact:** - Logs stored in: <span class=\"fill-in\">[Where can these secrets end up?]</span> - Who can access logs: <span class=\"fill-in\">[List potential exposure points]</span> - Lifetime: <span class=\"fill-in\">[How long do logs persist?]</span> **Your fixes:** 1. <span class=\"fill-in\">[How to connect without password in URL?]</span> 2. <span class=\"fill-in\">[How to log without secrets?]</span> 3. <span class=\"fill-in\">[How to handle exceptions securely?]</span> 4. <span class=\"fill-in\">[What to log instead of full token?]</span> <details markdown> <summary>Click to verify your answers</summary> **Bug 1 (Line 10):** Password in URL! If URL is logged, password exposed. **Bug 2 (Line 12):** Prints connection URL with password to console/logs! **Bug 3 (Lines 17-18):** SQLException stack trace may contain connection URL with password. `e.printStackTrace()` goes to stderr (often logged). **Bug 4 (Line 24):** Logging full JWT token. If logs compromised, attacker can impersonate user. **Fixes:** ```java // Fix 1 & 2: Use Properties, don't put password in URL public void connectToDatabase_Secure() { String url = \"jdbc:postgresql://db.example.com/mydb\"; Properties props = new Properties(); props.setProperty(\"user\", \"dbuser\"); props.setProperty(\"password\", dbPassword); // Not in URL // Secure logging - no secrets System.out.println(\"Connecting to: \" + url); // URL only, no password try { Connection conn = DriverManager.getConnection(url, props); } catch (SQLException e) { // Secure error handling - don't expose details logger.error(\"Database connection failed\", e.getErrorCode()); // Don't log e.getMessage() - may contain connection details } } // Fix 4: Log token ID only, not full token public String generateJWT_Secure(String userId) { String secret = System.getenv(\"JWT_SECRET\"); String token = createToken(userId, secret); // Log token ID/fingerprint, not full token String tokenId = extractTokenId(token); // Or hash first 16 chars logger.info(\"Generated token \" + tokenId + \" for user \" + userId); return token; } Real-world impact: Logs go to: files, centralized logging (Splunk/ELK), monitoring, backups, cloud storage Access by: developers, ops, security team, log aggregation services Lifetime: Days to years (compliance may require long retention) One leaked secret in logs = permanent exposure! Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found JWT signature validation bypass (Challenge 1) Found RBAC authorization bypass (Challenge 2) Understood timing attack vulnerability (Challenge 3) Found all secret leakage points (Challenge 4) Could explain WHY each bug is dangerous Learned common security mistakes to avoid Common security bugs you discovered: [List the patterns - e.g., \"Missing signature validation\"] [Fill in] [Fill in] [Fill in] Your security checklist for code review: All JWT tokens validated (signature + expiration) All sensitive operations have authorization checks No secrets in logs, URLs, or error messages Constant-time comparison for secrets/tokens Null checks before accessing collections Fail-secure (deny by default) Decision Framework \u00b6 Your task: Build decision trees for when to use each security pattern. Question 1: JWT vs Session-Based Auth? \u00b6 Answer after implementation: Use JWT when: Stateless architecture: [No session storage needed] Microservices: [Token contains all necessary data] Mobile/SPA apps: [Easy to store and send] Cross-domain: [Can share across services] Use Session-based when: Traditional web apps: [Server-side sessions] Need to revoke immediately: [Can invalidate server-side] Large user data: [Don't want to send in every request] Simpler security model: [Server controls everything] Question 2: When to use API Keys vs JWT? \u00b6 API Keys when: Service-to-service: [Long-lived credentials] Simple auth: [Just need to identify caller] Third-party integrations: [Easy to rotate] JWT when: User authentication: [Short-lived, contains user claims] Need user context: [Embedded in token] Stateless: [No lookup needed] Question 3: RBAC vs ABAC (Attribute-Based)? \u00b6 RBAC when: Clear role hierarchy: [Admin, Editor, Viewer] Simple permissions: [Read, Write, Delete] Most users: [70% of access control needs] ABAC when: Complex rules: [Based on time, location, resource attributes] Fine-grained control: [User can edit own posts only] Dynamic policies: [Rules change frequently] Your Decision Tree \u00b6 Build this after solving practice scenarios: flowchart LR Start[\"Security Pattern Selection\"] Q1{\"What are you securing?\"} Start --> Q1 N2[\"JWT or Session-based\"] Q1 -->|\"User sessions\"| N2 N3[\"API Keys or JWT\"] Q1 -->|\"API endpoints\"| N3 N4[\"RBAC or ABAC\"] Q1 -->|\"Resources\"| N4 Q5{\"What's the architecture?\"} Start --> Q5 N6[\"Session-based + RBAC\"] Q5 -->|\"Monolith\"| N6 N7[\"JWT + RBAC\"] Q5 -->|\"Microservices\"| N7 N8[\"JWT + API Keys\"] Q5 -->|\"Serverless\"| N8 Q9{\"What's the threat model?\"} Start --> Q9 N10[\"Strong encryption, rotation\"] Q9 -->|\"External attackers\"| N10 N11[\"Audit logging, least privilege\"] Q9 -->|\"Internal threats\"| N11 N12[\"Secrets management, encryption at rest\"] Q9 -->|\"Compliance (PCI/HIPAA)\"| N12 Q13{\"Performance requirements?\"} Start --> Q13 N14[\"Stateless<br/>(JWT, API keys)\"] Q13 -->|\"High throughput\"| N14 N15[\"Stateful<br/>(Sessions, central auth)\"] Q13 -->|\"Strong consistency\"| N15 N16[\"JWT with refresh tokens\"] Q13 -->|\"Offline support\"| N16 Practice \u00b6 Scenario 1: E-commerce API Security \u00b6 Requirements: REST API for orders, payments, user data Mobile app and web frontend Third-party integrations (shipping, payments) Must handle 10K requests/sec Your security design: Auth mechanism: [JWT or API keys? Why?] Authorization: [RBAC setup for customer, admin, partner roles] Secrets: [How to manage payment gateway keys?] Token expiry: [Short-lived or long-lived? Refresh strategy?] Rate limiting: [Per user? Per API key?] Scenario 2: Multi-Tenant SaaS Platform \u00b6 Requirements: Tenants: organizations with multiple users Data isolation between tenants Admin panel for tenant admins SSO support for enterprise customers Your security design: Tenant isolation: [How to ensure data separation?] User roles: [Super admin, tenant admin, user] SSO integration: [SAML, OAuth2, or both?] Token claims: [What to include in JWT?] Cross-tenant attacks: [How to prevent?] Scenario 3: Microservices Internal Auth \u00b6 Requirements: 20 microservices Services call each other Need to track which service made request Some services more privileged than others Your security design: Service-to-service auth: [Mutual TLS? JWT? API keys?] Service identity: [How to identify calling service?] Permission model: [Service-level RBAC?] Secret distribution: [How do services get credentials?] Rotation: [How to rotate without downtime?] Review Checklist \u00b6 Before moving to the next topic: Implementation JWT generation and validation work RBAC role assignment and permission checks work API key generation and validation work Secrets manager encrypt/decrypt work All client code runs successfully Understanding Filled in all ELI5 explanations Understand JWT structure and claims Know difference between authentication and authorization Understand RBAC role hierarchies Know when to use each auth mechanism Security Principles Never store passwords in plain text Always use HTTPS for token transmission Implement token expiration and refresh Use strong random for key generation Validate and sanitize all inputs Decision Making Know when to use JWT vs sessions Know when to use API keys vs JWT Completed practice scenarios Can explain security trade-offs Mastery Check Could implement JWT from memory Could design auth for new system Understand security threat models Know common vulnerabilities (OWASP Top 10) Mastery Certification \u00b6 I certify that I can: Implement JWT generation and validation from memory Implement RBAC with role hierarchy Explain authentication vs authorization clearly Identify common security vulnerabilities Design auth systems for different architectures Compare trade-offs between auth mechanisms Debug security issues systematically Teach these concepts to someone else Security mindset check: I think \"how can this be attacked?\" when reviewing code I validate and sanitize ALL inputs I check authorization before sensitive operations I use constant-time comparisons for secrets I never log sensitive data I design with \"fail secure\" principle I understand the security/usability trade-off","title":"07. Security Patterns"},{"location":"systems/07-security-patterns/#security-patterns","text":"Authentication, authorization, and securing distributed systems","title":"Security Patterns"},{"location":"systems/07-security-patterns/#eli5-explain-like-im-5","text":"Your task: After implementing security patterns, explain them simply. Prompts to guide you: What is authentication in one sentence? Your answer: [Fill in after implementation] What is authorization in one sentence? Your answer: [Fill in after implementation] Real-world analogy for authentication: Example: \"Authentication is like showing your ID at the door...\" Your analogy: [Fill in] Real-world analogy for authorization: Example: \"Authorization is like having a key to certain rooms...\" Your analogy: [Fill in] What is JWT in one sentence? Your answer: [Fill in after implementation] When should you use JWT vs sessions? Your answer: [Fill in after practice]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/07-security-patterns/#quick-quiz-do-before-implementing","text":"Your task: Test your security intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/07-security-patterns/#beforeafter-why-security-patterns-matter","text":"Your task: Compare insecure vs secure approaches to understand the security impact.","title":"Before/After: Why Security Patterns Matter"},{"location":"systems/07-security-patterns/#case-studies-security-patterns-in-the-wild","text":"","title":"Case Studies: Security Patterns in the Wild"},{"location":"systems/07-security-patterns/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/07-security-patterns/#debugging-challenges","text":"Your task: Find and fix security bugs in broken implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"systems/07-security-patterns/#decision-framework","text":"Your task: Build decision trees for when to use each security pattern.","title":"Decision Framework"},{"location":"systems/07-security-patterns/#practice","text":"","title":"Practice"},{"location":"systems/07-security-patterns/#review-checklist","text":"Before moving to the next topic: Implementation JWT generation and validation work RBAC role assignment and permission checks work API key generation and validation work Secrets manager encrypt/decrypt work All client code runs successfully Understanding Filled in all ELI5 explanations Understand JWT structure and claims Know difference between authentication and authorization Understand RBAC role hierarchies Know when to use each auth mechanism Security Principles Never store passwords in plain text Always use HTTPS for token transmission Implement token expiration and refresh Use strong random for key generation Validate and sanitize all inputs Decision Making Know when to use JWT vs sessions Know when to use API keys vs JWT Completed practice scenarios Can explain security trade-offs Mastery Check Could implement JWT from memory Could design auth for new system Understand security threat models Know common vulnerabilities (OWASP Top 10)","title":"Review Checklist"},{"location":"systems/08-rate-limiting/","text":"Rate Limiting \u00b6 Protecting APIs from abuse and ensuring fair resource usage ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing different rate limiting algorithms, explain them simply. Prompts to guide you: What is rate limiting in one sentence? Your answer: [Fill in after implementation] Why do we need rate limiting? Your answer: [Fill in after implementation] Real-world analogy for token bucket: Example: \"Token bucket is like a piggy bank where...\" Your analogy: [Fill in] What is the token bucket algorithm in one sentence? Your answer: [Fill in after implementation] How is leaky bucket different from token bucket? Your answer: [Fill in after implementation] Real-world analogy for leaky bucket: Example: \"Leaky bucket is like a water tower where...\" Your analogy: [Fill in] What is sliding window algorithm in one sentence? Your answer: [Fill in after implementation] When would you use fixed window vs sliding window? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition about rate limiting algorithms. Answer these, then verify after implementation. Algorithm Understanding Predictions \u00b6 Token bucket with 10 tokens, 2 tokens/sec refill: How many requests can burst immediately? [Your guess] After waiting 5 seconds, how many tokens? [Your guess] Verified after implementation: [Actual] Leaky bucket vs token bucket for 100 req/sec: Which allows bursts? [Token/Leaky] Which smooths traffic? [Token/Leaky] Verified: [Fill in] Fixed window: 10 req/min starting at 12:00:00: 9 requests at 12:00:58 9 requests at 12:01:01 Total allowed in 3 seconds: [Your guess: 9? 10? 18?] Why is this a problem? [Fill in] Verified: [Actual behavior] Scenario Predictions \u00b6 Scenario 1: API needs to handle traffic spikes but prevent abuse Best algorithm? [Token bucket/Leaky bucket/Fixed window/Sliding window] Why? [Your reasoning] Verified after implementation: [Fill in] Scenario 2: Login system needs steady rate limiting (no bursts) Best algorithm? [Token bucket/Leaky bucket/Fixed window/Sliding window] Why? [Your reasoning] Verified after implementation: [Fill in] Scenario 3: Rate limit 100 requests per minute with 1 million users Fixed window memory: [Estimate: bytes per user] Sliding window log memory: [Estimate: bytes per user] Which is more memory efficient? [Your guess] Verified: [Fill in calculations] Refill Logic Quiz \u00b6 Token bucket refills 5 tokens/second, last refill at T=0: At T=2 seconds, how many tokens added? [Your calculation] At T=2.5 seconds, how many tokens added? [Your calculation] If capacity is 10 and current tokens = 8: After 1 second: [tokens available?] After 5 seconds: [tokens available? can exceed capacity?] Verify these calculations after implementation! Trade-off Quiz \u00b6 Question: Why would you choose fixed window over sliding window? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: Token bucket allows 100 burst requests. Is this ALWAYS good? Yes, bursts are always beneficial No, depends on backend capacity No, depends on abuse prevention needs It depends on the use case Verify after implementation: [Which one(s) and why?] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized rate limiting approaches. Example: Protecting an API Endpoint \u00b6 Problem: Prevent API abuse while allowing legitimate traffic. Approach 1: No Rate Limiting \u00b6 // Naive approach - No protection public class NoRateLimiting { public Response handleRequest(Request req) { return processRequest(req); // Process every request } } Analysis: Time: O(1) per request Space: O(1) Protection: NONE - vulnerable to abuse Result: Server overload, DDoS attacks succeed For 1 million malicious requests: Server crashes Approach 2: Fixed Window Rate Limiter \u00b6 // Simple fixed window - Memory efficient public class FixedWindowRateLimiter { private int counter = 0; private long windowStart = System.currentTimeMillis(); private final int maxRequests = 100; private final long windowMs = 60000; // 1 minute public boolean tryAcquire() { long now = System.currentTimeMillis(); // Reset window if expired if (now - windowStart >= windowMs) { counter = 0; windowStart = now; } // Check limit if (counter < maxRequests) { counter++; return true; } return false; } } Analysis: Time: O(1) per request Space: O(1) - only 3 variables per user Protection: Good - limits to 100 req/min Issue: Allows 200 requests in 2 seconds at window boundary For 1 million users: ~12 bytes \u00d7 1M = ~12 MB Approach 3: Token Bucket (Better for Bursts) \u00b6 // Token bucket - Allows controlled bursts public class TokenBucketRateLimiter { private double tokens; private long lastRefillTime; private final int capacity = 100; private final double refillRate = 10.0; // tokens/sec public boolean tryAcquire() { refill(); if (tokens >= 1) { tokens -= 1; return true; } return false; } private void refill() { long now = System.currentTimeMillis(); double elapsed = (now - lastRefillTime) / 1000.0; tokens = Math.min(capacity, tokens + elapsed * refillRate); lastRefillTime = now; } } Analysis: Time: O(1) per request Space: O(1) - 3 variables per user Protection: Excellent - smooth rate limiting + burst capacity Flexibility: Can handle legitimate traffic spikes For 1 million users: ~16 bytes \u00d7 1M = ~16 MB Performance Comparison \u00b6 Scenario No Limiting Fixed Window Token Bucket Sliding Window Log Memory per user 0 bytes 12 bytes 16 bytes ~800 bytes (100 timestamps) 1M users memory 0 MB 12 MB 16 MB 800 MB Prevents abuse No Yes Yes Yes Allows bursts N/A No Yes No Accurate rate N/A Boundary issue Accurate Most accurate CPU per request Low Low Low Medium (cleanup) Your calculation: For 10 million users with sliding window log tracking 1000 requests each: Memory needed: [Calculate after implementation] Why this might be impractical: [Fill in] Boundary Attack Demonstration \u00b6 Fixed Window Problem: Window 1: 12:00:00 - 12:01:00 (limit: 100 req) Window 2: 12:01:00 - 12:02:00 (limit: 100 req) Attack pattern: - 12:00:59 \u2192 100 requests (allowed, fills window 1) - 12:01:01 \u2192 100 requests (allowed, new window 2) - Total: 200 requests in 2 seconds! Token Bucket Solution: Capacity: 100 tokens, Refill: 10 tokens/sec Attack pattern: - 12:00:59 \u2192 100 requests (bucket empties) - 12:01:01 \u2192 20 requests (only ~20 tokens refilled) - Total: 120 requests max (controlled burst) After implementing, explain in your own words: Why does fixed window allow double rate at boundaries? [Fill in] How does token bucket prevent this? [Fill in] When is fixed window \"good enough\"? [Fill in] Case Studies: Rate Limiting in the Wild \u00b6 Stripe API: The Leaky Bucket for Smooth Traffic \u00b6 Pattern: Leaky Bucket algorithm. How it works: Stripe's API processes requests at a fixed, steady rate, smoothing out bursts. Imagine a bucket with a small hole in the bottom. Incoming requests fill the bucket, and they are processed at the rate water \"leaks\" out. If requests arrive too quickly, the bucket overflows, and requests are rejected with a 429 Too Many Requests status code. Key Takeaway: The Leaky Bucket algorithm is excellent for services that require a predictable, stable load and want to prevent being overwhelmed by sudden bursts of traffic. It enforces a very smooth processing rate. GitHub API: The Token Bucket for Flexibility \u00b6 Pattern: Token Bucket algorithm. How it works: GitHub provides each API client with a \"bucket\" of tokens (e.g., 5,000) that refills over time ( e.g., per hour). Each API request consumes one token. This allows clients to make short, intense bursts of requests as long as they have tokens remaining. The API's HTTP response headers ( X-RateLimit-Limit , X-RateLimit-Remaining , X-RateLimit-Reset ) clearly communicate the client's current status, allowing applications to gracefully back off. Key Takeaway: The Token Bucket algorithm provides more flexibility than the Leaky Bucket, as it permits bursty traffic. This is user-friendly for clients who may have legitimate reasons to make many requests in a short period, as long as their average rate remains within the limit. Cloudflare: Fixed Windows for DDoS Protection \u00b6 Pattern: Fixed Window Counters for security. How it works: As a security company, Cloudflare's priority is blocking malicious traffic. They use simple, high-performance fixed window counters at their edge locations. They might have a rule like: \"Block any IP that makes more than 100 requests in any 10-second window.\" While this can be cheated by a sophisticated attacker who distributes requests across windows, it is extremely effective at stopping basic brute-force attacks and application-layer DDoS attempts with very little overhead. Key Takeaway: For security and DDoS mitigation, the raw performance and simplicity of a fixed window counter can be the most effective choice. The goal isn't perfect fairness but rather the rapid identification and blocking of abusive behavior. Core Implementation \u00b6 Part 1: Token Bucket Algorithm \u00b6 Your task: Implement token bucket rate limiter with refill mechanism. import java.util.*; import java.util.concurrent.*; /** * Token Bucket: Tokens refill at constant rate, burst traffic allowed * * Key principles: * - Bucket holds tokens (capacity) * - Tokens refill at fixed rate * - Request consumes token(s) * - Allows burst traffic up to capacity */ public class TokenBucketRateLimiter { private final int capacity; // Max tokens in bucket private final double refillRate; // Tokens per second private double tokens; // Current tokens private long lastRefillTime; // Last refill timestamp /** * Initialize token bucket * * @param capacity Maximum tokens (burst size) * @param refillRate Tokens added per second * * TODO: Initialize bucket * - Set capacity and refill rate * - Start with full bucket * - Record current time */ public TokenBucketRateLimiter(int capacity, double refillRate) { // TODO: Track state // TODO: Initialize tokens to capacity (bucket starts full) // TODO: Record current time in milliseconds this.capacity = 0; // Replace this.refillRate = 0; // Replace } /** * Attempt to acquire a token * * @return true if token acquired, false if rate limited * * TODO: Implement token acquisition * 1. Refill tokens based on time elapsed * 2. Check if token available * 3. Consume token if available * * Hint: tokens_to_add = time_elapsed * refill_rate */ public synchronized boolean tryAcquire() { // TODO: Calculate time elapsed since last refill // TODO: Calculate new tokens to add // tokens_to_add = elapsed_seconds * refillRate // TODO: Add tokens but cap at capacity // tokens = Math.min(tokens + tokens_to_add, capacity) // TODO: Update lastRefillTime to now // TODO: Implement iteration/conditional logic // TODO: Otherwise return false (rate limited) return false; // Replace } /** * Try to acquire multiple tokens (for weighted rate limiting) * * @param tokensNeeded Number of tokens to acquire * @return true if acquired, false if insufficient tokens * * TODO: Implement multi-token acquisition * - Refill tokens first * - Check if enough tokens available * - Consume requested tokens */ public synchronized boolean tryAcquire(int tokensNeeded) { // TODO: Refill tokens (same as tryAcquire()) // TODO: Check if tokens >= tokensNeeded // TODO: Implement iteration/conditional logic // TODO: Otherwise return false return false; // Replace } /** * Get current token count (for monitoring) */ public synchronized double getTokens() { refill(); return tokens; } /** * Refill tokens based on elapsed time * * TODO: Extract refill logic * - Calculate elapsed time * - Add tokens * - Cap at capacity */ private void refill() { // TODO: Implement refill logic } } Part 2: Leaky Bucket Algorithm \u00b6 Your task: Implement leaky bucket rate limiter with constant outflow. /** * Leaky Bucket: Requests leak out at constant rate * * Key principles: * - Bucket holds pending requests (queue) * - Requests leak out at fixed rate * - Smooths burst traffic * - Rejects requests if bucket full */ public class LeakyBucketRateLimiter { private final int capacity; // Max queue size private final double leakRate; // Requests per second private final Queue<Long> bucket; // Request timestamps private long lastLeakTime; /** * Initialize leaky bucket * * @param capacity Maximum pending requests * @param leakRate Requests processed per second * * TODO: Initialize bucket * - Create queue with capacity * - Set leak rate * - Record current time */ public LeakyBucketRateLimiter(int capacity, double leakRate) { // TODO: Track state // TODO: Initialize queue (LinkedList) // TODO: Record current time this.capacity = 0; // Replace this.leakRate = 0; // Replace this.bucket = null; // Replace } /** * Try to add request to bucket * * @return true if accepted, false if bucket full * * TODO: Implement request acceptance * 1. Leak out old requests * 2. Check if space available * 3. Add current request * * Hint: Remove requests older than (current_time - 1/leak_rate) */ public synchronized boolean tryAcquire() { // TODO: Leak out processed requests // requests_to_leak = elapsed_seconds * leakRate // TODO: Remove that many requests from queue // TODO: Implement iteration/conditional logic // TODO: Otherwise return false (bucket full) return false; // Replace } /** * Leak out processed requests * * TODO: Remove requests based on elapsed time * - Calculate requests that should leak * - Remove from queue * - Update lastLeakTime */ private void leak() { // TODO: Calculate elapsed time // TODO: Calculate requests to leak // requestsToLeak = elapsed_seconds * leakRate // TODO: Poll requestsToLeak items from queue // TODO: Update lastLeakTime } /** * Get current bucket size (for monitoring) */ public synchronized int getQueueSize() { leak(); return bucket.size(); } } Part 3: Fixed Window Algorithm \u00b6 Your task: Implement simple fixed window counter. /** * Fixed Window: Count requests in fixed time windows * * Key principles: * - Reset counter at window boundaries * - Simple and memory efficient * - Can allow 2x rate at window boundaries */ public class FixedWindowRateLimiter { private final int maxRequests; // Max requests per window private final long windowSizeMs; // Window size in milliseconds private int counter; // Requests in current window private long windowStart; // Current window start time /** * Initialize fixed window rate limiter * * @param maxRequests Maximum requests per window * @param windowSizeMs Window size in milliseconds * * TODO: Initialize window * - Set max requests and window size * - Start counter at 0 * - Record window start time */ public FixedWindowRateLimiter(int maxRequests, long windowSizeMs) { // TODO: Track state // TODO: Initialize counter to 0 // TODO: Track state this.maxRequests = 0; // Replace this.windowSizeMs = 0; // Replace } /** * Try to acquire permission * * @return true if allowed, false if rate limited * * TODO: Implement fixed window logic * 1. Check if window expired (reset if needed) * 2. Check if under limit * 3. Increment counter */ public synchronized boolean tryAcquire() { long now = System.currentTimeMillis(); // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic // TODO: Otherwise return false return false; // Replace } /** * Get current window stats (for monitoring) */ public synchronized WindowStats getStats() { return new WindowStats(counter, maxRequests, windowStart); } static class WindowStats { int current; int max; long windowStart; WindowStats(int current, int max, long windowStart) { this.current = current; this.max = max; this.windowStart = windowStart; } } } Part 4: Sliding Window Log Algorithm \u00b6 Your task: Implement sliding window with request log. /** * Sliding Window Log: Track individual request timestamps * * Key principles: * - Store timestamp of each request * - Count requests in sliding window * - More accurate than fixed window * - Higher memory usage */ public class SlidingWindowLogRateLimiter { private final int maxRequests; // Max requests per window private final long windowSizeMs; // Window size in milliseconds private final Queue<Long> requestLog; // Request timestamps /** * Initialize sliding window log * * @param maxRequests Maximum requests per window * @param windowSizeMs Window size in milliseconds * * TODO: Initialize log * - Set max requests and window size * - Create queue for timestamps */ public SlidingWindowLogRateLimiter(int maxRequests, long windowSizeMs) { // TODO: Track state // TODO: Initialize LinkedList for request log this.maxRequests = 0; // Replace this.windowSizeMs = 0; // Replace this.requestLog = null; // Replace } /** * Try to acquire permission * * @return true if allowed, false if rate limited * * TODO: Implement sliding window logic * 1. Remove old requests outside window * 2. Check if under limit * 3. Add current timestamp * * Hint: Remove requests older than (now - windowSizeMs) */ public synchronized boolean tryAcquire() { long now = System.currentTimeMillis(); // TODO: Remove timestamps older than (now - windowSizeMs) // TODO: Implement iteration/conditional logic // TODO: Otherwise return false return false; // Replace } /** * Get current request count (for monitoring) */ public synchronized int getCurrentCount() { long now = System.currentTimeMillis(); while (!requestLog.isEmpty() && requestLog.peek() <= now - windowSizeMs) { requestLog.poll(); } return requestLog.size(); } } Part 5: Sliding Window Counter (Hybrid) \u00b6 Your task: Implement memory-efficient sliding window counter. /** * Sliding Window Counter: Hybrid of fixed window and sliding window * * Key principles: * - Two counters: current and previous window * - Weighted average based on time in window * - More accurate than fixed window * - Less memory than sliding log */ public class SlidingWindowCounterRateLimiter { private final int maxRequests; private final long windowSizeMs; private int currentWindowCount; private int previousWindowCount; private long currentWindowStart; /** * Initialize sliding window counter * * @param maxRequests Maximum requests per window * @param windowSizeMs Window size in milliseconds * * TODO: Initialize counters * - Set max requests and window size * - Initialize both counters * - Record window start */ public SlidingWindowCounterRateLimiter(int maxRequests, long windowSizeMs) { // TODO: Track state // TODO: Initialize currentWindowCount to 0 // TODO: Initialize previousWindowCount to 0 // TODO: Track state this.maxRequests = 0; // Replace this.windowSizeMs = 0; // Replace } /** * Try to acquire permission * * @return true if allowed, false if rate limited * * TODO: Implement sliding window counter * 1. Rotate windows if needed * 2. Calculate weighted count * 3. Check against limit * * Formula: * weighted_count = previous_count * (1 - elapsed_ratio) + current_count * where elapsed_ratio = time_in_window / window_size */ public synchronized boolean tryAcquire() { long now = System.currentTimeMillis(); // TODO: Check if window expired // TODO: Calculate time elapsed in current window // elapsedRatio = (now - currentWindowStart) / windowSizeMs // TODO: Calculate weighted count // weightedCount = previousWindowCount * (1 - elapsedRatio) + currentWindowCount // TODO: Implement iteration/conditional logic // TODO: Otherwise return false return false; // Replace } /** * Get estimated current count (for monitoring) */ public synchronized double getEstimatedCount() { long now = System.currentTimeMillis(); double elapsedRatio = (double)(now - currentWindowStart) / windowSizeMs; return previousWindowCount * (1 - elapsedRatio) + currentWindowCount; } } Client Code \u00b6 import java.util.concurrent.*; public class RateLimitingClient { public static void main(String[] args) throws Exception { testTokenBucket(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testLeakyBucket(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testFixedWindow(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testSlidingWindowLog(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testSlidingWindowCounter(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); compareBurstTraffic(); } static void testTokenBucket() { System.out.println(\"=== Token Bucket Test ===\\n\"); // 10 tokens capacity, 2 tokens/second refill TokenBucketRateLimiter limiter = new TokenBucketRateLimiter(10, 2.0); // Test: Burst traffic System.out.println(\"Burst: 15 rapid requests\"); int allowed = 0; for (int i = 0; i < 15; i++) { if (limiter.tryAcquire()) { allowed++; } } System.out.println(\"Allowed: \" + allowed + \"/15\"); System.out.println(\"Remaining tokens: \" + limiter.getTokens()); // Test: Wait and retry System.out.println(\"\\nWait 2 seconds for refill...\"); sleep(2000); System.out.println(\"Tokens after refill: \" + limiter.getTokens()); // Test: Weighted request (costs 3 tokens) System.out.println(\"\\nWeighted request (3 tokens):\"); boolean acquired = limiter.tryAcquire(3); System.out.println(\"Acquired: \" + acquired); System.out.println(\"Remaining tokens: \" + limiter.getTokens()); } static void testLeakyBucket() { System.out.println(\"=== Leaky Bucket Test ===\\n\"); // 5 capacity, 1 request/second leak rate LeakyBucketRateLimiter limiter = new LeakyBucketRateLimiter(5, 1.0); // Test: Fill bucket System.out.println(\"Fill bucket with 5 requests\"); int allowed = 0; for (int i = 0; i < 5; i++) { if (limiter.tryAcquire()) { allowed++; } } System.out.println(\"Allowed: \" + allowed + \"/5\"); System.out.println(\"Queue size: \" + limiter.getQueueSize()); // Test: Overflow System.out.println(\"\\nTry 3 more requests (should overflow)\"); int overflow = 0; for (int i = 0; i < 3; i++) { if (limiter.tryAcquire()) { overflow++; } } System.out.println(\"Allowed: \" + overflow + \"/3\"); // Test: Wait and retry System.out.println(\"\\nWait 2 seconds for leak...\"); sleep(2000); System.out.println(\"Queue size after leak: \" + limiter.getQueueSize()); } static void testFixedWindow() { System.out.println(\"=== Fixed Window Test ===\\n\"); // 5 requests per 2 second window FixedWindowRateLimiter limiter = new FixedWindowRateLimiter(5, 2000); // Test: Fill window System.out.println(\"Make 5 requests (should all succeed)\"); testRequests(limiter, 5); // Test: Overflow System.out.println(\"\\nMake 3 more requests (should fail)\"); testRequests(limiter, 3); // Test: Window boundary System.out.println(\"\\nWait for window reset...\"); sleep(2100); System.out.println(\"Make 5 requests in new window\"); testRequests(limiter, 5); } static void testSlidingWindowLog() { System.out.println(\"=== Sliding Window Log Test ===\\n\"); // 5 requests per 2 second window SlidingWindowLogRateLimiter limiter = new SlidingWindowLogRateLimiter(5, 2000); // Test: Fill window System.out.println(\"Make 5 requests\"); testRequests(limiter, 5); System.out.println(\"Current count: \" + limiter.getCurrentCount()); // Test: Wait partial window System.out.println(\"\\nWait 1 second (half window)...\"); sleep(1000); System.out.println(\"Current count: \" + limiter.getCurrentCount()); // Test: Make more requests System.out.println(\"\\nMake 3 more requests\"); testRequests(limiter, 3); } static void testSlidingWindowCounter() { System.out.println(\"=== Sliding Window Counter Test ===\\n\"); // 5 requests per 2 second window SlidingWindowCounterRateLimiter limiter = new SlidingWindowCounterRateLimiter(5, 2000); // Test: Fill window System.out.println(\"Make 5 requests\"); testRequests(limiter, 5); System.out.println(\"Estimated count: \" + limiter.getEstimatedCount()); // Test: Wait partial window System.out.println(\"\\nWait 1 second...\"); sleep(1000); System.out.println(\"Estimated count: \" + limiter.getEstimatedCount()); // Test: Make more requests System.out.println(\"\\nMake 3 more requests\"); testRequests(limiter, 3); System.out.println(\"Estimated count: \" + limiter.getEstimatedCount()); } static void compareBurstTraffic() { System.out.println(\"=== Burst Traffic Comparison ===\\n\"); TokenBucketRateLimiter tokenBucket = new TokenBucketRateLimiter(10, 2.0); LeakyBucketRateLimiter leakyBucket = new LeakyBucketRateLimiter(10, 2.0); FixedWindowRateLimiter fixedWindow = new FixedWindowRateLimiter(10, 5000); // Simulate burst of 20 requests System.out.println(\"Sending 20 rapid requests...\"); int tokenAllowed = 0, leakyAllowed = 0, fixedAllowed = 0; for (int i = 0; i < 20; i++) { if (tokenBucket.tryAcquire()) tokenAllowed++; if (leakyBucket.tryAcquire()) leakyAllowed++; if (fixedWindow.tryAcquire()) fixedAllowed++; } System.out.println(\"Token Bucket allowed: \" + tokenAllowed + \"/20\"); System.out.println(\"Leaky Bucket allowed: \" + leakyAllowed + \"/20\"); System.out.println(\"Fixed Window allowed: \" + fixedAllowed + \"/20\"); } static void testRequests(FixedWindowRateLimiter limiter, int count) { int allowed = 0; for (int i = 0; i < count; i++) { if (limiter.tryAcquire()) allowed++; } System.out.println(\"Allowed: \" + allowed + \"/\" + count); } static void testRequests(SlidingWindowLogRateLimiter limiter, int count) { int allowed = 0; for (int i = 0; i < count; i++) { if (limiter.tryAcquire()) allowed++; } System.out.println(\"Allowed: \" + allowed + \"/\" + count); } static void testRequests(SlidingWindowCounterRateLimiter limiter, int count) { int allowed = 0; for (int i = 0; i < count; i++) { if (limiter.tryAcquire()) allowed++; } System.out.println(\"Allowed: \" + allowed + \"/\" + count); } static void sleep(long ms) { try { Thread.sleep(ms); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken rate limiter implementations. This tests your understanding. Challenge 1: Broken Token Bucket Refill \u00b6 /** * This token bucket has a CRITICAL bug in the refill logic. * It works initially but breaks after a few minutes. */ public class BrokenTokenBucket { private int tokens; private long lastRefillTime; private final int capacity = 10; private final int refillRate = 2; // tokens per second public BrokenTokenBucket() { this.tokens = capacity; this.lastRefillTime = System.currentTimeMillis(); } public synchronized boolean tryAcquire() { // Refill tokens long now = System.currentTimeMillis(); long elapsed = now - lastRefillTime; int tokensToAdd = (int)(elapsed * refillRate); tokens = Math.min(capacity, tokens + tokensToAdd); lastRefillTime = now; // Consume token if (tokens >= 1) { tokens--; return true; } return false; } } Your debugging: Bug location: [Which line?] Bug explanation: [What's wrong with the calculation?] Bug manifestation: [What happens? Too many/few tokens?] Current calculation: 2000 * 2 = 4000 tokens! [Why is this wrong?] Expected: [How many tokens should be added?] Fix: [Correct the formula] Click to verify your answer Bug: The formula elapsed * refillRate multiplies milliseconds by tokens/second, giving a massive number. Correct formula: double elapsedSeconds = elapsed / 1000.0; int tokensToAdd = (int)(elapsedSeconds * refillRate); OR: int tokensToAdd = (int)((elapsed / 1000.0) * refillRate); Test: 2 seconds \u00d7 2 tokens/sec = 4 tokens (not 4000!) Challenge 2: Race Condition in Token Bucket \u00b6 /** * This token bucket has a RACE CONDITION bug. * Works fine single-threaded, breaks under concurrent load. */ public class RacyTokenBucket { private double tokens; private long lastRefillTime; private final int capacity = 100; private final double refillRate = 10.0; public boolean tryAcquire() { refill(); if (tokens >= 1) { tokens--; return true; } return false; } private void refill() { long now = System.currentTimeMillis(); double elapsed = (now - lastRefillTime) / 1000.0; tokens = Math.min(capacity, tokens + elapsed * refillRate); lastRefillTime = now; } } Your debugging: Bug 1: [What's missing from method signature?] Bug 2: [What happens when 2 threads check tokens >= 1 simultaneously?] Scenario to expose bug: Thread 1: checks tokens (1.0) >= 1 \u2713 Thread 2: checks tokens (1.0) >= 1 \u2713 [race!] Thread 1: tokens-- (now 0.0) Thread 2: tokens-- (now -1.0) [negative tokens!] Fix: [How to prevent race condition?] Click to verify your answer Bug: Missing synchronized keyword. Multiple threads can pass the tokens >= 1 check before any thread decrements. Fix: public synchronized boolean tryAcquire() { refill(); if (tokens >= 1) { tokens--; return true; } return false; } private synchronized void refill() { // ... refill logic } Alternative: Use AtomicReference or Lock for finer-grained control. Challenge 3: Fixed Window Off-by-One Error \u00b6 /** * Fixed window counter with subtle off-by-one bug. * Sometimes allows one extra request. */ public class BrokenFixedWindow { private int counter = 0; private long windowStart; private final int maxRequests = 10; private final long windowMs = 60000; public BrokenFixedWindow() { this.windowStart = System.currentTimeMillis(); } public synchronized boolean tryAcquire() { long now = System.currentTimeMillis(); // Reset window if needed if (now - windowStart > windowMs) { counter = 0; windowStart = now; } // Check limit if (counter <= maxRequests) { counter++; return true; } return false; } } Your debugging: Bug 1: Line with now - windowStart > windowMs Why > might be wrong: [Fill in] Should it be >=? [Yes/No and why] Bug 2: Line with counter <= maxRequests Expected: Allow 10 requests (counter 0-9) Actual: [How many requests allowed?] Fix: [Should be < or <= ?] Counter values: 0, 1, 2, 3 \u2192 [How many increments? Is 4 requests allowed?] Click to verify your answer Bug 1: Using > instead of >= means windows that expire exactly at the boundary don't reset. Using >= is standard and clearer. Bug 2: The condition counter <= maxRequests allows 11 requests: counter = 0 \u2192 increment to 1 (1st request) counter = 1 \u2192 increment to 2 (2nd request) ... counter = 9 \u2192 increment to 10 (10th request) counter = 10 \u2192 increment to 11 (11th request) \u2190 BUG! Fix: if (counter < maxRequests) { // Use < not <= counter++; return true; } OR increment first, then check: counter++; if (counter <= maxRequests) { return true; } counter--; // rollback return false; Challenge 4: Sliding Window Log Memory Leak \u00b6 /** * Sliding window log with memory leak bug. * Memory grows indefinitely under load. */ public class LeakyWindowLog { private final int maxRequests = 100; private final long windowMs = 60000; private final Queue<Long> requestLog = new LinkedList<>(); public synchronized boolean tryAcquire() { long now = System.currentTimeMillis(); while (!requestLog.isEmpty() && requestLog.peek() < now - windowMs) { requestLog.poll(); } if (requestLog.size() < maxRequests) { requestLog.add(now); return true; } return false; } } Your debugging: Bug scenario: Low traffic for 5 minutes, then burst of 10,000 rejected requests How many timestamps in queue? [Fill in] Are rejected requests added to log? [Yes/No] Wait, re-read the code carefully... [What actually happens?] Memory analysis: Each timestamp: 8 bytes (long) After 1 hour of 100 req/min: [Calculate queue size] After 24 hours: [Calculate queue size] The ACTUAL bug: [Is there actually a memory leak, or is this a trick question?] Discussion: [Is the cleanup sufficient? When would memory grow?] Click to verify your answer Trick question! The code is actually correct for the most part. The cleanup loop runs on every request and removes old timestamps. However, there ARE subtle issues: Peak memory usage: During high traffic, the queue holds up to maxRequests timestamps (100 \u00d7 8 bytes = 800 bytes per user). For 1 million users: ~800 MB. Stale data if no requests: If a user makes 100 requests then stops, those timestamps stay in memory for the full window duration (60 seconds). Not a traditional memory leak: Memory is bounded by maxRequests \u00d7 number_of_active_users . Better approach: Use a background cleanup thread or TTL-based cache eviction for inactive users. Actual memory leak scenario: If implementing per-user rate limiting, you might store a Map<UserId, RateLimiter> that grows indefinitely without eviction of inactive users. Challenge 5: Incorrect Sliding Window Counter Weight \u00b6 /** * Sliding window counter with incorrect weighted calculation. * Rate limiting is too strict or too lenient. */ public class BrokenSlidingCounter { private int currentCount = 0; private int previousCount = 0; private long windowStart; private final int maxRequests = 100; private final long windowMs = 60000; public BrokenSlidingCounter() { this.windowStart = System.currentTimeMillis(); } public synchronized boolean tryAcquire() { long now = System.currentTimeMillis(); // Rotate window if needed if (now - windowStart >= windowMs) { previousCount = currentCount; currentCount = 0; windowStart = now; } // Calculate weighted count double elapsed = now - windowStart; double ratio = elapsed / windowMs; double weightedCount = previousCount * ratio + currentCount; if (weightedCount < maxRequests) { currentCount++; return true; } return false; } } Your debugging: Formula analysis: previousCount * ratio + currentCount If elapsed = 0ms (start of window): ratio = 0, weighted = [calculate] If elapsed = 30s (halfway): ratio = 0.5, weighted = [calculate] If elapsed = 60s (end of window): ratio = 1.0, weighted = [calculate] Is this correct? [Should previous count INCREASE or DECREASE as time passes?] Expected: As time passes, previous window matters less Start of window: previous = 100%, current = 0% Middle of window: previous = 50%, current = 50% End of window: previous = 0%, current = 100% Bug: [What's wrong with the formula?] Fix: [Correct formula] Click to verify your answer Bug: The formula previousCount * ratio makes the previous count INCREASE as time passes. It should DECREASE! Correct formula: double weightedCount = previousCount * (1 - ratio) + currentCount; Why: ratio = 0.0 (start of window) \u2192 previous weight = 1.0 (100%) ratio = 0.5 (halfway) \u2192 previous weight = 0.5 (50%) ratio = 1.0 (end of window) \u2192 previous weight = 0.0 (0%) Example: previousCount = 80, currentCount = 40 Elapsed = 30s out of 60s \u2192 ratio = 0.5 Correct: 80 \u00d7 (1 - 0.5) + 40 = 80 \u00d7 0.5 + 40 = 40 + 40 = 80 Buggy: 80 \u00d7 0.5 + 40 = 40 + 40 = 80 (happens to match, but wrong at other ratios!) At ratio = 0.75: Correct: 80 \u00d7 0.25 + 40 = 20 + 40 = 60 Buggy: 80 \u00d7 0.75 + 40 = 60 + 40 = 100 (too lenient!) Challenge 6: Leaky Bucket Leak Rate Bug \u00b6 /** * Leaky bucket that leaks at wrong rate. * Sometimes too fast, sometimes too slow. */ public class BrokenLeakyBucket { private final Queue<Long> bucket = new LinkedList<>(); private long lastLeakTime; private final int capacity = 10; private final double leakRate = 2.0; // requests per second public BrokenLeakyBucket() { this.lastLeakTime = System.currentTimeMillis(); } public synchronized boolean tryAcquire() { leak(); if (bucket.size() < capacity) { bucket.add(System.currentTimeMillis()); return true; } return false; } private void leak() { long now = System.currentTimeMillis(); long elapsed = now - lastLeakTime; int requestsToLeak = (int)(elapsed * leakRate); for (int i = 0; i < requestsToLeak && !bucket.isEmpty(); i++) { bucket.poll(); } lastLeakTime = now; } } Your debugging: Bug 1: Calculation elapsed * leakRate elapsed = 2000ms, leakRate = 2 req/sec Current: 2000 \u00d7 2 = [What?] Expected: [How many requests should leak in 2 seconds?] Fix: [Correct formula] Bug 2: lastLeakTime = now happens every call Scenario: elapsed = 500ms, leakRate = 2 req/sec requests to leak = 1 request We update lastLeakTime, \"losing\" the remaining 0.5 requests worth of time Fix: [Should we account for fractional leak time?] Click to verify your answer Bug 1: Same as token bucket - multiplying milliseconds by rate per second. Fix: double elapsedSeconds = elapsed / 1000.0; int requestsToLeak = (int)(elapsedSeconds * leakRate); Bug 2: Updating lastLeakTime every call loses fractional seconds. Better approach: private void leak() { long now = System.currentTimeMillis(); double elapsedSeconds = (now - lastLeakTime) / 1000.0; int requestsToLeak = (int)(elapsedSeconds * leakRate); for (int i = 0; i < requestsToLeak && !bucket.isEmpty(); i++) { bucket.poll(); } // Only update time for the requests we actually leaked if (requestsToLeak > 0) { lastLeakTime += (long)((requestsToLeak / leakRate) * 1000); } } This preserves fractional time for more accurate leaking. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found time unit conversion errors (ms vs seconds) Identified race conditions in concurrent access Caught off-by-one errors in counters Understood memory implications of different approaches Fixed incorrect weight formulas in sliding window Recognized fractional time loss in leak calculations Common rate limiting bugs you discovered: [List the patterns you noticed] [Fill in] [Fill in] Testing strategies you learned: How would you test for race conditions? [Your answer] How would you test boundary conditions? [Your answer] How would you test time-based calculations? [Your answer] Decision Framework \u00b6 Questions to answer after implementation: 1. Algorithm Selection \u00b6 When to use Token Bucket? Your scenario: [Fill in] Key factors: [Fill in] When to use Leaky Bucket? Your scenario: [Fill in] Key factors: [Fill in] When to use Fixed Window? Your scenario: [Fill in] Key factors: [Fill in] When to use Sliding Window? Your scenario: [Fill in] Key factors: [Fill in] 2. Trade-offs \u00b6 Token Bucket: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Leaky Bucket: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Fixed Window: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Sliding Window: Pros: [Fill in after understanding] Cons: [Fill in after understanding] 3. Your Decision Tree \u00b6 Build your decision tree after practicing: flowchart LR Start[\"What is your priority?\"] N1[\"?\"] Start -->|\"Allow burst traffic\"| N1 N2[\"?\"] Start -->|\"Smooth traffic flow\"| N2 N3[\"?\"] Start -->|\"Simple and memory efficient\"| N3 N4[\"?\"] Start -->|\"Accurate rate limiting\"| N4 Practice \u00b6 Scenario 1: Rate limit public API \u00b6 Requirements: Public REST API Need to allow burst traffic 100 requests per minute per user Premium users get 1000 requests per minute Your design: Which algorithm would you choose? [Fill in] Why? [Fill in] How to handle different user tiers? [Fill in] How to handle distributed servers? [Fill in] Scenario 2: Rate limit login attempts \u00b6 Requirements: Prevent brute force attacks 5 login attempts per minute Smooth out retry attempts Block for 15 minutes after limit Your design: Which algorithm would you choose? [Fill in] Why? [Fill in] How to implement blocking? [Fill in] How to handle false positives? [Fill in] Scenario 3: Rate limit microservice calls \u00b6 Requirements: Service A calls Service B Protect Service B from overload Service B can handle 1000 req/sec Need graceful degradation Your design: Which algorithm would you choose? [Fill in] Why? [Fill in] How to handle backpressure? [Fill in] Circuit breaker integration? [Fill in] Review Checklist \u00b6 Token bucket implemented with refill mechanism Leaky bucket implemented with constant outflow Fixed window implemented with reset logic Sliding window log implemented with timestamp tracking Sliding window counter implemented with weighted average Understand when to use each algorithm Can explain trade-offs between algorithms Built decision tree for algorithm selection Completed practice scenarios Mastery Certification \u00b6 I certify that I can: Implement all rate limiting algorithms from memory Explain when and why to use each algorithm Calculate memory and time complexity for each Identify the correct algorithm for new scenarios Debug common rate limiting bugs (time units, race conditions, boundaries) Design distributed rate limiting solutions Compare trade-offs with alternative approaches Teach these concepts to someone else","title":"08. Rate Limiting"},{"location":"systems/08-rate-limiting/#rate-limiting","text":"Protecting APIs from abuse and ensuring fair resource usage","title":"Rate Limiting"},{"location":"systems/08-rate-limiting/#eli5-explain-like-im-5","text":"Your task: After implementing different rate limiting algorithms, explain them simply. Prompts to guide you: What is rate limiting in one sentence? Your answer: [Fill in after implementation] Why do we need rate limiting? Your answer: [Fill in after implementation] Real-world analogy for token bucket: Example: \"Token bucket is like a piggy bank where...\" Your analogy: [Fill in] What is the token bucket algorithm in one sentence? Your answer: [Fill in after implementation] How is leaky bucket different from token bucket? Your answer: [Fill in after implementation] Real-world analogy for leaky bucket: Example: \"Leaky bucket is like a water tower where...\" Your analogy: [Fill in] What is sliding window algorithm in one sentence? Your answer: [Fill in after implementation] When would you use fixed window vs sliding window? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/08-rate-limiting/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition about rate limiting algorithms. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/08-rate-limiting/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized rate limiting approaches.","title":"Before/After: Why This Pattern Matters"},{"location":"systems/08-rate-limiting/#case-studies-rate-limiting-in-the-wild","text":"","title":"Case Studies: Rate Limiting in the Wild"},{"location":"systems/08-rate-limiting/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/08-rate-limiting/#client-code","text":"import java.util.concurrent.*; public class RateLimitingClient { public static void main(String[] args) throws Exception { testTokenBucket(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testLeakyBucket(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testFixedWindow(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testSlidingWindowLog(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testSlidingWindowCounter(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); compareBurstTraffic(); } static void testTokenBucket() { System.out.println(\"=== Token Bucket Test ===\\n\"); // 10 tokens capacity, 2 tokens/second refill TokenBucketRateLimiter limiter = new TokenBucketRateLimiter(10, 2.0); // Test: Burst traffic System.out.println(\"Burst: 15 rapid requests\"); int allowed = 0; for (int i = 0; i < 15; i++) { if (limiter.tryAcquire()) { allowed++; } } System.out.println(\"Allowed: \" + allowed + \"/15\"); System.out.println(\"Remaining tokens: \" + limiter.getTokens()); // Test: Wait and retry System.out.println(\"\\nWait 2 seconds for refill...\"); sleep(2000); System.out.println(\"Tokens after refill: \" + limiter.getTokens()); // Test: Weighted request (costs 3 tokens) System.out.println(\"\\nWeighted request (3 tokens):\"); boolean acquired = limiter.tryAcquire(3); System.out.println(\"Acquired: \" + acquired); System.out.println(\"Remaining tokens: \" + limiter.getTokens()); } static void testLeakyBucket() { System.out.println(\"=== Leaky Bucket Test ===\\n\"); // 5 capacity, 1 request/second leak rate LeakyBucketRateLimiter limiter = new LeakyBucketRateLimiter(5, 1.0); // Test: Fill bucket System.out.println(\"Fill bucket with 5 requests\"); int allowed = 0; for (int i = 0; i < 5; i++) { if (limiter.tryAcquire()) { allowed++; } } System.out.println(\"Allowed: \" + allowed + \"/5\"); System.out.println(\"Queue size: \" + limiter.getQueueSize()); // Test: Overflow System.out.println(\"\\nTry 3 more requests (should overflow)\"); int overflow = 0; for (int i = 0; i < 3; i++) { if (limiter.tryAcquire()) { overflow++; } } System.out.println(\"Allowed: \" + overflow + \"/3\"); // Test: Wait and retry System.out.println(\"\\nWait 2 seconds for leak...\"); sleep(2000); System.out.println(\"Queue size after leak: \" + limiter.getQueueSize()); } static void testFixedWindow() { System.out.println(\"=== Fixed Window Test ===\\n\"); // 5 requests per 2 second window FixedWindowRateLimiter limiter = new FixedWindowRateLimiter(5, 2000); // Test: Fill window System.out.println(\"Make 5 requests (should all succeed)\"); testRequests(limiter, 5); // Test: Overflow System.out.println(\"\\nMake 3 more requests (should fail)\"); testRequests(limiter, 3); // Test: Window boundary System.out.println(\"\\nWait for window reset...\"); sleep(2100); System.out.println(\"Make 5 requests in new window\"); testRequests(limiter, 5); } static void testSlidingWindowLog() { System.out.println(\"=== Sliding Window Log Test ===\\n\"); // 5 requests per 2 second window SlidingWindowLogRateLimiter limiter = new SlidingWindowLogRateLimiter(5, 2000); // Test: Fill window System.out.println(\"Make 5 requests\"); testRequests(limiter, 5); System.out.println(\"Current count: \" + limiter.getCurrentCount()); // Test: Wait partial window System.out.println(\"\\nWait 1 second (half window)...\"); sleep(1000); System.out.println(\"Current count: \" + limiter.getCurrentCount()); // Test: Make more requests System.out.println(\"\\nMake 3 more requests\"); testRequests(limiter, 3); } static void testSlidingWindowCounter() { System.out.println(\"=== Sliding Window Counter Test ===\\n\"); // 5 requests per 2 second window SlidingWindowCounterRateLimiter limiter = new SlidingWindowCounterRateLimiter(5, 2000); // Test: Fill window System.out.println(\"Make 5 requests\"); testRequests(limiter, 5); System.out.println(\"Estimated count: \" + limiter.getEstimatedCount()); // Test: Wait partial window System.out.println(\"\\nWait 1 second...\"); sleep(1000); System.out.println(\"Estimated count: \" + limiter.getEstimatedCount()); // Test: Make more requests System.out.println(\"\\nMake 3 more requests\"); testRequests(limiter, 3); System.out.println(\"Estimated count: \" + limiter.getEstimatedCount()); } static void compareBurstTraffic() { System.out.println(\"=== Burst Traffic Comparison ===\\n\"); TokenBucketRateLimiter tokenBucket = new TokenBucketRateLimiter(10, 2.0); LeakyBucketRateLimiter leakyBucket = new LeakyBucketRateLimiter(10, 2.0); FixedWindowRateLimiter fixedWindow = new FixedWindowRateLimiter(10, 5000); // Simulate burst of 20 requests System.out.println(\"Sending 20 rapid requests...\"); int tokenAllowed = 0, leakyAllowed = 0, fixedAllowed = 0; for (int i = 0; i < 20; i++) { if (tokenBucket.tryAcquire()) tokenAllowed++; if (leakyBucket.tryAcquire()) leakyAllowed++; if (fixedWindow.tryAcquire()) fixedAllowed++; } System.out.println(\"Token Bucket allowed: \" + tokenAllowed + \"/20\"); System.out.println(\"Leaky Bucket allowed: \" + leakyAllowed + \"/20\"); System.out.println(\"Fixed Window allowed: \" + fixedAllowed + \"/20\"); } static void testRequests(FixedWindowRateLimiter limiter, int count) { int allowed = 0; for (int i = 0; i < count; i++) { if (limiter.tryAcquire()) allowed++; } System.out.println(\"Allowed: \" + allowed + \"/\" + count); } static void testRequests(SlidingWindowLogRateLimiter limiter, int count) { int allowed = 0; for (int i = 0; i < count; i++) { if (limiter.tryAcquire()) allowed++; } System.out.println(\"Allowed: \" + allowed + \"/\" + count); } static void testRequests(SlidingWindowCounterRateLimiter limiter, int count) { int allowed = 0; for (int i = 0; i < count; i++) { if (limiter.tryAcquire()) allowed++; } System.out.println(\"Allowed: \" + allowed + \"/\" + count); } static void sleep(long ms) { try { Thread.sleep(ms); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } }","title":"Client Code"},{"location":"systems/08-rate-limiting/#debugging-challenges","text":"Your task: Find and fix bugs in broken rate limiter implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"systems/08-rate-limiting/#decision-framework","text":"Questions to answer after implementation:","title":"Decision Framework"},{"location":"systems/08-rate-limiting/#practice","text":"","title":"Practice"},{"location":"systems/08-rate-limiting/#review-checklist","text":"Token bucket implemented with refill mechanism Leaky bucket implemented with constant outflow Fixed window implemented with reset logic Sliding window log implemented with timestamp tracking Sliding window counter implemented with weighted average Understand when to use each algorithm Can explain trade-offs between algorithms Built decision tree for algorithm selection Completed practice scenarios","title":"Review Checklist"},{"location":"systems/09-load-balancing/","text":"Load Balancing \u00b6 Distributing requests across multiple servers for scalability and reliability ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing different load balancing algorithms, explain them simply. Prompts to guide you: What is load balancing in one sentence? Your answer: [Fill in after implementation] Why do we need load balancers? Your answer: [Fill in after implementation] Real-world analogy for round robin: Example: \"Round robin is like a carousel where...\" Your analogy: [Fill in] What is round robin in one sentence? Your answer: [Fill in after implementation] How is least connections different from round robin? Your answer: [Fill in after implementation] Real-world analogy for consistent hashing: Example: \"Consistent hashing is like a clock where...\" Your analogy: [Fill in] What is consistent hashing in one sentence? Your answer: [Fill in after implementation] When would you use weighted load balancing? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Round robin server selection: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Least connections server selection: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Consistent hashing lookup: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Scenario Predictions \u00b6 Scenario 1: 3 identical servers, 12 requests using round robin How many requests does each server get? [Fill in] If we add a 4th server after 8 requests, what happens? [Explain] Is this distribution optimal for all workloads? [Yes/No - Why?] Scenario 2: 3 servers, server 1 has 5 connections, server 2 has 2, server 3 has 3 Which server does least connections choose? [Fill in] Why not round robin in this case? [Explain] What if requests have varying durations? [Which algorithm is better?] Scenario 3: Consistent hashing with 3 servers, 100 cache keys If you add a 4th server, approximately what % of keys are remapped? [Fill in] If you remove 1 server, what % of keys are remapped? [Fill in] Why is this better than simple hash % server_count? [Explain] Trade-off Quiz \u00b6 Question: When would IP hash be WORSE than round robin? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN problem with simple hash-based load balancing? It's too slow It doesn't distribute evenly Adding/removing servers causes massive redistribution It requires too much memory Verify after implementation: [Which one(s)?] Question: What problem do virtual nodes solve in consistent hashing? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Before/After: Why This Pattern Matters \u00b6 Your task: Compare different load balancing approaches to understand the impact. Example: Distributing 10 Requests \u00b6 Problem: Route 10 requests across 3 servers with different algorithms. Approach 1: Round Robin \u00b6 // Round robin - Simple circular rotation List<Server> servers = Arrays.asList(server1, server2, server3); int currentIndex = 0; for (int i = 1; i <= 10; i++) { Server selected = servers.get(currentIndex); System.out.println(\"Request \" + i + \" -> \" + selected.id); currentIndex = (currentIndex + 1) % servers.size(); } Output: Request 1 -> S1, Request 2 -> S2, Request 3 -> S3 Request 4 -> S1, Request 5 -> S2, Request 6 -> S3 Request 7 -> S1, Request 8 -> S2, Request 9 -> S3 Request 10 -> S1 Analysis: Distribution: S1=4, S2=3, S3=3 (even) Time: O(1) per request Space: O(1) Problem: Doesn't consider if S1 is slower or overloaded Approach 2: Least Connections \u00b6 // Least connections - Route to server with fewest active connections // Assume current state: S1=5 connections, S2=2 connections, S3=3 connections ServerWithStats min = servers.get(0); for (ServerWithStats s : servers) { if (s.activeConnections < min.activeConnections) { min = s; } } System.out.println(\"Route to: \" + min.server.id); min.activeConnections++; Analysis: Routes to S2 (only 2 connections vs 5 and 3) Time: O(n) per request (scan all servers) Space: O(n) (track connection counts) Benefit: Adapts to varying request durations Problem: More overhead than round robin Approach 3: Consistent Hashing \u00b6 // Consistent hashing - Map user to consistent server TreeMap<Integer, Server> ring = new TreeMap<>(); // Place servers on ring with virtual nodes for (Server s : servers) { for (int i = 0; i < 3; i++) { int hash = hash(s.id + \"-\" + i); ring.put(hash, s); } } // Route request by user ID String userId = \"user123\"; int userHash = hash(userId); Map.Entry<Integer, Server> entry = ring.ceilingEntry(userHash); if (entry == null) entry = ring.firstEntry(); System.out.println(userId + \" -> \" + entry.getValue().id); Analysis: Time: O(log n) per request (TreeMap lookup) Space: O(n * virtual_nodes) Benefit: Adding server only remaps ~1/n keys Problem: More complex, uneven distribution without enough virtual nodes Performance Comparison: Adding a Server \u00b6 Algorithm Before After Keys Remapped Simple Hash (key % n) 3 servers 4 servers ~75% (3 out of 4 keys) Consistent Hashing 3 servers 4 servers ~25% (1 out of 4 keys) Why does this matter for caching? Simple hash: 75% cache miss rate after adding server Consistent hash: 25% cache miss rate after adding server 3x improvement in cache hit rate during scaling Your calculation: With 100 servers and 1,000,000 cached keys: Simple hash remapping: _____ keys need to move Consistent hash remapping: _____ keys need to move Improvement factor: _____ times better Comparison: Session Persistence \u00b6 Scenario: User with shopping cart stored in server memory Round Robin: Request 1 (user123) -> S1 (cart created) Request 2 (user123) -> S2 (cart lost! No session data) Request 3 (user123) -> S3 (cart lost! No session data) Result: Cart lost, poor user experience IP Hash: hash(\"192.168.1.100\") % 3 = 1 -> S2 Request 1 (user123 from 192.168.1.100) -> S2 (cart created) Request 2 (user123 from 192.168.1.100) -> S2 (cart found!) Request 3 (user123 from 192.168.1.100) -> S2 (cart persists) Result: Same server every time, session maintained After implementing, explain in your own words: When would you choose round robin over least connections? [Your answer] Why use consistent hashing for a cache cluster? [Your answer] What are the trade-offs of IP hash for session persistence? [Your answer] Case Studies: Load Balancing in the Wild \u00b6 Netflix: Zuul API Gateway \u00b6 Pattern: L7 Load Balancing. How it works: Netflix's Zuul gateway acts as a smart L7 router. It inspects incoming HTTP requests and routes them to the appropriate microservice (e.g., api/v1/movies -> Movie Service, api/v1/billing -> Billing Service). Key Takeaway: They use it for more than just traffic distribution; it handles dynamic routing, security, and monitoring, acting as the \"front door\" to their entire microservices architecture. Facebook: Katran Load Balancer \u00b6 Pattern: L4 Load Balancing. How it works: Katran is a high-performance L4 load balancer that operates at the network packet level (TCP/UDP). It uses a technique called Direct Server Return (DSR) where the backend server responds directly to the client, bypassing the load balancer on the return path to reduce latency. Key Takeaway: For massive scale, Facebook optimized at a lower level to achieve extreme performance, sacrificing application-level routing logic at the edge for raw speed. Cloudflare: Anycast and Geo-Routing \u00b6 Pattern: Global Load Balancing (GSLB). How it works: Cloudflare uses Anycast DNS to route users to the nearest data center based on network topology. An L4/L7 load balancer within that data center then distributes traffic locally. Key Takeaway: Load balancing isn't just for a single data center. It's a critical tool for global traffic management, improving latency and providing DDoS protection by distributing traffic geographically. Core Implementation \u00b6 Part 1: Round Robin Load Balancer \u00b6 Your task: Implement simple round robin algorithm. import java.util.*; /** * Round Robin: Distribute requests evenly in rotation * * Key principles: * - Circular iteration through servers * - Simple and fair distribution * - Doesn't consider server load * - Works well for similar servers */ public class RoundRobinLoadBalancer { private final List<Server> servers; private int currentIndex; /** * Initialize round robin load balancer * * @param servers List of backend servers * * TODO: Initialize balancer * - Store server list * - Start index at 0 */ public RoundRobinLoadBalancer(List<Server> servers) { // TODO: Store servers (defensive copy) // TODO: Initialize currentIndex to 0 this.servers = null; // Replace this.currentIndex = 0; } /** * Select next server using round robin * * @return Next server in rotation * * TODO: Implement round robin selection * 1. Get server at current index * 2. Increment index (with wraparound) * 3. Return server * * Hint: Use modulo for wraparound */ public synchronized Server getNextServer() { // TODO: Check if servers list is empty // TODO: Get server at currentIndex // TODO: Increment currentIndex with wraparound // currentIndex = (currentIndex + 1) % servers.size() // TODO: Return selected server return null; // Replace } /** * Add server to pool */ public synchronized void addServer(Server server) { // TODO: Add server to list } /** * Remove server from pool */ public synchronized void removeServer(Server server) { // TODO: Remove server from list // TODO: Adjust currentIndex if needed } static class Server { String id; String host; int port; public Server(String id, String host, int port) { this.id = id; this.host = host; this.port = port; } @Override public String toString() { return id + \" (\" + host + \":\" + port + \")\"; } } } Part 2: Least Connections Load Balancer \u00b6 Your task: Implement least connections algorithm. /** * Least Connections: Route to server with fewest active connections * * Key principles: * - Track active connections per server * - Select server with minimum load * - Better for varying request durations * - More overhead than round robin */ public class LeastConnectionsLoadBalancer { private final List<ServerWithStats> servers; /** * Initialize least connections load balancer * * @param servers List of backend servers * * TODO: Initialize balancer * - Create ServerWithStats for each server * - Initialize connection counters to 0 */ public LeastConnectionsLoadBalancer(List<RoundRobinLoadBalancer.Server> servers) { // TODO: Wrap each server with connection counter this.servers = null; // Replace } /** * Select server with fewest connections * * @return Server with minimum active connections * * TODO: Implement least connections selection * 1. Find server with minimum connections * 2. Increment its connection count * 3. Return server * * Hint: Break ties by choosing first found */ public synchronized RoundRobinLoadBalancer.Server getNextServer() { // TODO: Check if servers list is empty // TODO: Find server with minimum connections // TODO: Increment connection count for selected server // TODO: Return selected server return null; // Replace } /** * Release connection when request completes * * @param server Server to release connection from * * TODO: Decrement connection count * - Find server in list * - Decrement activeConnections * - Ensure doesn't go below 0 */ public synchronized void releaseConnection(RoundRobinLoadBalancer.Server server) { // TODO: Find ServerWithStats for given server // TODO: Decrement activeConnections (min 0) } /** * Get server statistics */ public synchronized Map<String, Integer> getStats() { Map<String, Integer> stats = new HashMap<>(); for (ServerWithStats s : servers) { stats.put(s.server.id, s.activeConnections); } return stats; } static class ServerWithStats { RoundRobinLoadBalancer.Server server; int activeConnections; public ServerWithStats(RoundRobinLoadBalancer.Server server) { this.server = server; this.activeConnections = 0; } } } Part 3: Weighted Round Robin \u00b6 Your task: Implement weighted round robin for heterogeneous servers. /** * Weighted Round Robin: Distribute based on server capacity * * Key principles: * - Servers have different weights (capacity) * - Higher weight = more requests * - Smooth distribution using GCD algorithm * - Good for heterogeneous servers */ public class WeightedRoundRobinLoadBalancer { private final List<WeightedServer> servers; private int currentIndex; private int currentWeight; private int maxWeight; private int gcd; // Greatest common divisor of weights /** * Initialize weighted round robin * * @param servers List of servers with weights * * TODO: Initialize balancer * - Store servers * - Calculate max weight and GCD * - Initialize currentWeight to max */ public WeightedRoundRobinLoadBalancer(List<WeightedServer> servers) { // TODO: Store servers // TODO: Calculate maxWeight (max of all weights) // TODO: Calculate GCD of all weights // TODO: Initialize currentIndex to -1 // TODO: Initialize currentWeight to 0 this.servers = null; // Replace } /** * Select next server using weighted round robin * * @return Next server based on weight * * TODO: Implement weighted selection * 1. Loop until finding server with weight >= currentWeight * 2. When loop completes, decrease currentWeight by GCD * 3. Return selected server * * Hint: This ensures smooth distribution */ public synchronized RoundRobinLoadBalancer.Server getNextServer() { // TODO: Loop through servers while (true) { // TODO: Move to next index (wraparound) // currentIndex = (currentIndex + 1) % servers.size() // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic } } /** * Calculate GCD of all weights */ private int calculateGCD() { // TODO: Calculate GCD of all server weights // Hint: Use Euclidean algorithm return 1; // Replace } /** * Calculate GCD of two numbers */ private int gcd(int a, int b) { // TODO: Implement Euclidean algorithm return 0; // Replace } static class WeightedServer { RoundRobinLoadBalancer.Server server; int weight; // Capacity/power of server public WeightedServer(RoundRobinLoadBalancer.Server server, int weight) { this.server = server; this.weight = weight; } } } Part 4: Consistent Hashing \u00b6 Your task: Implement consistent hashing for distributed caching. /** * Consistent Hashing: Map requests to servers using hash ring * * Key principles: * - Servers placed on virtual ring * - Request hashed to position on ring * - Clockwise walk to find server * - Minimal redistribution when servers change */ public class ConsistentHashingLoadBalancer { private final TreeMap<Integer, RoundRobinLoadBalancer.Server> ring; private final int virtualNodesPerServer; /** * Initialize consistent hashing * * @param servers List of servers * @param virtualNodesPerServer Number of virtual nodes per physical server * * TODO: Initialize hash ring * - Create TreeMap for ring * - Add each server with virtual nodes */ public ConsistentHashingLoadBalancer(List<RoundRobinLoadBalancer.Server> servers, int virtualNodesPerServer) { // TODO: Initialize TreeMap // TODO: Store virtualNodesPerServer // TODO: Add all servers to ring this.ring = null; // Replace this.virtualNodesPerServer = 0; } /** * Get server for a given key * * @param key Request key (e.g., user ID, session ID) * @return Server to handle this key * * TODO: Implement consistent hashing lookup * 1. Hash the key to integer * 2. Find next server clockwise on ring * 3. If no server found, wrap to first server */ public RoundRobinLoadBalancer.Server getServer(String key) { // TODO: Check if ring is empty // TODO: Hash key to integer position // TODO: Find next entry on ring (ceilingEntry) // TODO: Return server return null; // Replace } /** * Add server to hash ring * * TODO: Add server with virtual nodes * - For each virtual node: * - Hash \"serverId-virtualNodeIndex\" * - Place on ring */ public void addServer(RoundRobinLoadBalancer.Server server) { // TODO: Add virtualNodesPerServer copies of this server } /** * Remove server from hash ring * * TODO: Remove all virtual nodes for this server */ public void removeServer(RoundRobinLoadBalancer.Server server) { // TODO: Remove all virtual nodes } /** * Hash function * * TODO: Implement simple hash function * - Use string hashCode() * - Ensure positive value */ private int hash(String key) { // TODO: Hash key to integer // Hint: key.hashCode() & 0x7FFFFFFF (remove sign bit) return 0; // Replace } /** * Get ring statistics */ public int getRingSize() { return ring.size(); } } Part 5: IP Hash Load Balancer \u00b6 Your task: Implement IP hash for session persistence. /** * IP Hash: Route client to same server based on IP * * Key principles: * - Hash client IP to select server * - Ensures session persistence * - Client always hits same server * - Issues when server pool changes */ public class IPHashLoadBalancer { private final List<RoundRobinLoadBalancer.Server> servers; /** * Initialize IP hash load balancer * * @param servers List of backend servers */ public IPHashLoadBalancer(List<RoundRobinLoadBalancer.Server> servers) { // TODO: Store servers this.servers = null; // Replace } /** * Select server based on client IP * * @param clientIP Client IP address * @return Server for this client * * TODO: Implement IP hash selection * 1. Hash the client IP * 2. Modulo by server count * 3. Return server at that index */ public synchronized RoundRobinLoadBalancer.Server getServer(String clientIP) { // TODO: Check if servers list is empty // TODO: Hash clientIP to integer // TODO: Get index using modulo // index = abs(hash) % servers.size() // TODO: Return server at index return null; // Replace } /** * Hash IP address */ private int hash(String ip) { // TODO: Hash IP string // Hint: ip.hashCode() return 0; // Replace } /** * Add server (warning: disrupts session persistence) */ public synchronized void addServer(RoundRobinLoadBalancer.Server server) { // TODO: Add server // Note: This will change hash distribution } /** * Remove server (warning: disrupts session persistence) */ public synchronized void removeServer(RoundRobinLoadBalancer.Server server) { // TODO: Remove server // Note: This will change hash distribution } } Client Code \u00b6 import java.util.*; public class LoadBalancingClient { public static void main(String[] args) { testRoundRobin(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testLeastConnections(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testWeightedRoundRobin(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testConsistentHashing(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testIPHash(); } static void testRoundRobin() { System.out.println(\"=== Round Robin Test ===\\n\"); // Create servers List<RoundRobinLoadBalancer.Server> servers = Arrays.asList( new RoundRobinLoadBalancer.Server(\"S1\", \"10.0.0.1\", 8080), new RoundRobinLoadBalancer.Server(\"S2\", \"10.0.0.2\", 8080), new RoundRobinLoadBalancer.Server(\"S3\", \"10.0.0.3\", 8080) ); RoundRobinLoadBalancer lb = new RoundRobinLoadBalancer(servers); // Test: Send 10 requests System.out.println(\"Sending 10 requests:\"); for (int i = 1; i <= 10; i++) { RoundRobinLoadBalancer.Server server = lb.getNextServer(); System.out.println(\"Request \" + i + \" -> \" + server); } } static void testLeastConnections() { System.out.println(\"=== Least Connections Test ===\\n\"); List<RoundRobinLoadBalancer.Server> servers = Arrays.asList( new RoundRobinLoadBalancer.Server(\"S1\", \"10.0.0.1\", 8080), new RoundRobinLoadBalancer.Server(\"S2\", \"10.0.0.2\", 8080), new RoundRobinLoadBalancer.Server(\"S3\", \"10.0.0.3\", 8080) ); LeastConnectionsLoadBalancer lb = new LeastConnectionsLoadBalancer(servers); // Test: Send requests and simulate completion System.out.println(\"Request 1:\"); RoundRobinLoadBalancer.Server s1 = lb.getNextServer(); System.out.println(\"Routed to: \" + s1); System.out.println(\"Stats: \" + lb.getStats()); System.out.println(\"\\nRequest 2:\"); RoundRobinLoadBalancer.Server s2 = lb.getNextServer(); System.out.println(\"Routed to: \" + s2); System.out.println(\"Stats: \" + lb.getStats()); System.out.println(\"\\nRequest 1 completes:\"); lb.releaseConnection(s1); System.out.println(\"Stats: \" + lb.getStats()); System.out.println(\"\\nRequest 3:\"); RoundRobinLoadBalancer.Server s3 = lb.getNextServer(); System.out.println(\"Routed to: \" + s3); System.out.println(\"Stats: \" + lb.getStats()); } static void testWeightedRoundRobin() { System.out.println(\"=== Weighted Round Robin Test ===\\n\"); // Create servers with different capacities List<WeightedRoundRobinLoadBalancer.WeightedServer> servers = Arrays.asList( new WeightedRoundRobinLoadBalancer.WeightedServer( new RoundRobinLoadBalancer.Server(\"S1-Small\", \"10.0.0.1\", 8080), 1), new WeightedRoundRobinLoadBalancer.WeightedServer( new RoundRobinLoadBalancer.Server(\"S2-Medium\", \"10.0.0.2\", 8080), 2), new WeightedRoundRobinLoadBalancer.WeightedServer( new RoundRobinLoadBalancer.Server(\"S3-Large\", \"10.0.0.3\", 8080), 3) ); WeightedRoundRobinLoadBalancer lb = new WeightedRoundRobinLoadBalancer(servers); // Test: Send 12 requests (should distribute 2:4:6) System.out.println(\"Sending 12 requests (expected: 2:4:6 distribution):\"); Map<String, Integer> distribution = new HashMap<>(); for (int i = 1; i <= 12; i++) { RoundRobinLoadBalancer.Server server = lb.getNextServer(); distribution.merge(server.id, 1, Integer::sum); System.out.println(\"Request \" + i + \" -> \" + server.id); } System.out.println(\"\\nDistribution: \" + distribution); } static void testConsistentHashing() { System.out.println(\"=== Consistent Hashing Test ===\\n\"); List<RoundRobinLoadBalancer.Server> servers = Arrays.asList( new RoundRobinLoadBalancer.Server(\"S1\", \"10.0.0.1\", 8080), new RoundRobinLoadBalancer.Server(\"S2\", \"10.0.0.2\", 8080), new RoundRobinLoadBalancer.Server(\"S3\", \"10.0.0.3\", 8080) ); ConsistentHashingLoadBalancer lb = new ConsistentHashingLoadBalancer(servers, 3); // Test: Same key always goes to same server String[] keys = {\"user123\", \"user456\", \"user789\", \"user123\", \"user456\"}; System.out.println(\"Routing requests by user ID:\"); for (String key : keys) { RoundRobinLoadBalancer.Server server = lb.getServer(key); System.out.println(key + \" -> \" + server.id); } // Test: Add server and see minimal redistribution System.out.println(\"\\nAdding new server S4:\"); lb.addServer(new RoundRobinLoadBalancer.Server(\"S4\", \"10.0.0.4\", 8080)); for (String key : keys) { RoundRobinLoadBalancer.Server server = lb.getServer(key); System.out.println(key + \" -> \" + server.id); } } static void testIPHash() { System.out.println(\"=== IP Hash Test ===\\n\"); List<RoundRobinLoadBalancer.Server> servers = Arrays.asList( new RoundRobinLoadBalancer.Server(\"S1\", \"10.0.0.1\", 8080), new RoundRobinLoadBalancer.Server(\"S2\", \"10.0.0.2\", 8080), new RoundRobinLoadBalancer.Server(\"S3\", \"10.0.0.3\", 8080) ); IPHashLoadBalancer lb = new IPHashLoadBalancer(servers); // Test: Same IP always goes to same server String[] clientIPs = {\"192.168.1.100\", \"192.168.1.101\", \"192.168.1.102\", \"192.168.1.100\", \"192.168.1.101\"}; System.out.println(\"Routing by client IP (session persistence):\"); for (String ip : clientIPs) { RoundRobinLoadBalancer.Server server = lb.getServer(ip); System.out.println(ip + \" -> \" + server.id); } } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken load balancing implementations. This tests your understanding. Challenge 1: Broken Round Robin \u00b6 /** * Round robin with wraparound bug * This has 1 CRITICAL BUG that causes crashes. */ public synchronized Server getNextServer_Buggy() { if (servers.isEmpty()) { throw new IllegalStateException(\"No servers available\"); } Server server = servers.get(currentIndex); currentIndex = currentIndex + 1; return server; } Your debugging: Bug: [What\\'s the bug?] Click to verify your answer Bug: Missing modulo operation for wraparound. After 3 requests, currentIndex = 3 which is out of bounds for a list of size 3. Correct: currentIndex = (currentIndex + 1) % servers.size(); Why: The modulo ensures the index wraps to 0 after reaching the end, creating circular iteration. Challenge 2: Broken Least Connections \u00b6 /** * Least connections with race condition * This has 1 SUBTLE BUG in concurrent scenarios. */ public Server getNextServer_Buggy() { ServerWithStats minServer = null; int minConnections = Integer.MAX_VALUE; for (ServerWithStats s : servers) { if (s.activeConnections < minConnections) { minConnections = s.activeConnections; minServer = s; } } minServer.activeConnections++; return minServer.server; } public void releaseConnection_Buggy(Server server) { for (ServerWithStats s : servers) { if (s.server.equals(server)) { s.activeConnections--; break; } } } Your debugging: Bug 1: [What's the concurrency issue?] Bug 1 fix: [How to make it thread-safe?] Bug 2: [What if connections goes negative?] Bug 2 fix: [Add safety check] Trace through scenario: Thread 1: Reads S1.connections = 5 Thread 2: Reads S1.connections = 5 (same value!) Thread 1: Increments to 6 Thread 2: Increments to 6 (should be 7!) Result: Connection count is wrong Click to verify your answers Bug 1: Missing synchronization on activeConnections increment/decrement. Multiple threads can read the same value and create inconsistent state. Fix 1 - Add synchronized: public synchronized Server getNextServer() { // ... selection logic ... minServer.activeConnections++; return minServer.server; } public synchronized void releaseConnection(Server server) { for (ServerWithStats s : servers) { if (s.server.equals(server)) { s.activeConnections = Math.max(0, s.activeConnections - 1); break; } } } Fix 2 - Use AtomicInteger: static class ServerWithStats { Server server; AtomicInteger activeConnections; public ServerWithStats(Server server) { this.server = server; this.activeConnections = new AtomicInteger(0); } } // Then use: minServer.activeConnections.incrementAndGet(); Bug 2: Connection count could go negative if releaseConnection called without matching getNextServer . Fix with Math.max(0, ...) . Challenge 3: Broken Consistent Hashing \u00b6 /** * Consistent hashing with distribution bug * This has 1 LOGIC BUG causing poor distribution. */ public ConsistentHashingLoadBalancer_Buggy(List<Server> servers, int virtualNodesPerServer) { this.ring = new TreeMap<>(); this.virtualNodesPerServer = virtualNodesPerServer; for (Server server : servers) { int hash = hash(server.id); ring.put(hash, server); } } Your debugging: Bug: [What's wrong with this initialization?] Result: [How does this affect distribution?] Fix: [What should the code be?] Test case: 3 servers, virtualNodesPerServer = 3 Expected: 9 entries in ring Actual: 3 entries in ring Problem: Poor key distribution, some servers get no keys Click to verify your answer Bug: Missing loop to create virtual nodes. Only creating 1 node per server instead of virtualNodesPerServer . Correct: for (Server server : servers) { for (int i = 0; i < virtualNodesPerServer; i++) { String virtualKey = server.id + \"-\" + i; int hash = hash(virtualKey); ring.put(hash, server); } } Why virtual nodes matter: Without them: Each server gets 1 point on ring, uneven distribution With them: Each server gets N points, smoother distribution More virtual nodes = better balance, but more memory Challenge 4: Consistent Hashing Wraparound Bug \u00b6 /** * Missing wraparound in hash ring lookup * This has 1 EDGE CASE BUG. */ public Server getServer_Buggy(String key) { if (ring.isEmpty()) { throw new IllegalStateException(\"No servers in ring\"); } int hash = hash(key); Map.Entry<Integer, Server> entry = ring.ceilingEntry(hash); return entry.getValue(); // NullPointerException! } Your debugging: Bug: [When does ceilingEntry return null?] Scenario: [Give an example where this fails] Fix: [How to handle wraparound?] Example: Ring has entries at: [100, 200, 300] Key hashes to: 350 (larger than all ring entries) ceilingEntry(350) returns: null Result: NullPointerException Click to verify your answer Bug: ceilingEntry returns null when the hash is greater than all keys in the ring. Need to wrap around to first entry. Correct: public Server getServer(String key) { if (ring.isEmpty()) { throw new IllegalStateException(\"No servers in ring\"); } int hash = hash(key); Map.Entry<Integer, Server> entry = ring.ceilingEntry(hash); // Handle wraparound: if no entry found, use first entry if (entry == null) { entry = ring.firstEntry(); } return entry.getValue(); } Why: The hash ring is conceptually circular. When you go past the largest position, you wrap to the beginning. Challenge 5: Weighted Round Robin Distribution Bug \u00b6 /** * Weighted round robin with incorrect distribution * This has 1 ALGORITHM BUG. */ public Server getNextServer_Buggy() { while (true) { currentIndex = (currentIndex + 1) % servers.size(); WeightedServer ws = servers.get(currentIndex); if (ws.currentWeight < ws.weight) { ws.currentWeight++; return ws.server; } else { ws.currentWeight = 0; } } } Your debugging: Bug: [What's wrong with this distribution logic?] Result: Servers with weight=3 get 3 requests in a row, then none Expected: Smooth distribution mixed with other servers Fix: [How to implement smooth weighted round robin?] Example with weights [1, 2, 3]: Buggy output: S1, S2, S2, S3, S3, S3, S1, S2, S2, S3, S3, S3 (Clumpy distribution) Expected output: S3, S2, S3, S1, S2, S3 (Smooth distribution using GCD algorithm) Click to verify your answer Bug: Using per-server counters doesn't create smooth distribution. Need to use the GCD-based algorithm with global currentWeight. Correct approach: public Server getNextServer() { while (true) { currentIndex = (currentIndex + 1) % servers.size(); if (currentIndex == 0) { currentWeight = currentWeight - gcd; if (currentWeight <= 0) { currentWeight = maxWeight; } } WeightedServer ws = servers.get(currentIndex); if (ws.weight >= currentWeight) { return ws.server; } } } Why: The GCD algorithm ensures servers are selected proportionally but interleaved smoothly, not in blocks. Challenge 6: Health Check Race Condition \u00b6 /** * Load balancer with health checks * This has 2 RACE CONDITION BUGS. */ public class HealthAwareLoadBalancer { private List<Server> servers; private List<Server> healthyServers; public Server getNextServer() { if (healthyServers.isEmpty()) { throw new IllegalStateException(\"No healthy servers\"); } int index = currentIndex++ % healthyServers.size(); return healthyServers.get(index); } // Health check thread updates this public void updateHealthStatus(Server server, boolean healthy) { if (healthy && !healthyServers.contains(server)) { healthyServers.add(server); } else if (!healthy) { healthyServers.remove(server); } } } Your debugging: Bug 1: [What happens if health check runs during getNextServer?] Bug 2: [What if list size changes during modulo operation?] Fix: [How to make this thread-safe?] Click to verify your answer Bugs: Multiple race conditions with concurrent modification of healthyServers list. Fix: public class HealthAwareLoadBalancer { private final List<Server> servers; private volatile List<Server> healthyServers; // volatile for visibility private final AtomicInteger currentIndex = new AtomicInteger(0); public Server getNextServer() { List<Server> snapshot = healthyServers; // Local copy if (snapshot.isEmpty()) { throw new IllegalStateException(\"No healthy servers\"); } int index = currentIndex.getAndIncrement() % snapshot.size(); return snapshot.get(index); } public synchronized void updateHealthStatus(Server server, boolean healthy) { List<Server> newList = new ArrayList<>(healthyServers); if (healthy && !newList.contains(server)) { newList.add(server); } else if (!healthy) { newList.remove(server); } healthyServers = newList; // Atomic replacement } } Key insights: Use immutable snapshots for reads Copy-on-write for updates Volatile for visibility across threads Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 8+ bugs across 6 challenges Understood WHY each bug causes incorrect behavior Could explain the fix to someone else Learned common load balancing mistakes to avoid Common mistakes you discovered: [List the patterns you noticed] [Fill in] [Fill in] Decision Framework \u00b6 Questions to answer after implementation: 1. Algorithm Selection \u00b6 When to use Round Robin? Your scenario: [Fill in] Key factors: [Fill in] When to use Least Connections? Your scenario: [Fill in] Key factors: [Fill in] When to use Weighted Round Robin? Your scenario: [Fill in] Key factors: [Fill in] When to use Consistent Hashing? Your scenario: [Fill in] Key factors: [Fill in] When to use IP Hash? Your scenario: [Fill in] Key factors: [Fill in] 2. Trade-offs \u00b6 Round Robin: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Least Connections: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Consistent Hashing: Pros: [Fill in after understanding] Cons: [Fill in after understanding] IP Hash: Pros: [Fill in after understanding] Cons: [Fill in after understanding] 3. Your Decision Tree \u00b6 Build your decision tree after practicing: flowchart LR Start[\"What is your priority?\"] N1[\"?\"] Start -->|\"Simple and fair distribution\"| N1 N2[\"?\"] Start -->|\"Consider server load\"| N2 N3[\"?\"] Start -->|\"Heterogeneous servers\"| N3 N4[\"?\"] Start -->|\"Session persistence\"| N4 N5[\"?\"] Start -->|\"Minimal redistribution on changes\"| N5 Practice \u00b6 Scenario 1: Load balance web application \u00b6 Requirements: 5 web servers of equal capacity Stateless application Want even distribution Simple to understand Your design: Which algorithm would you choose? [Fill in] Why? [Fill in] How to handle server failures? [Fill in] Health check strategy? [Fill in] Scenario 2: Load balance with session state \u00b6 Requirements: Users have shopping carts Cart stored in server memory 10 application servers Need session persistence Your design: Which algorithm would you choose? [Fill in] Why? [Fill in] How to handle server additions? [Fill in] Alternative to sticky sessions? [Fill in] Scenario 3: Distributed cache cluster \u00b6 Requirements: Cache cluster with 20 nodes Frequently add/remove nodes Want to minimize cache misses Keys should stay on same node Your design: Which algorithm would you choose? [Fill in] Why? [Fill in] How many virtual nodes? [Fill in] Replication strategy? [Fill in] Review Checklist \u00b6 Round robin implemented with circular iteration Least connections implemented with connection tracking Weighted round robin implemented with GCD algorithm Consistent hashing implemented with virtual nodes IP hash implemented for session persistence Understand when to use each algorithm Can explain trade-offs between algorithms Built decision tree for algorithm selection Completed practice scenarios Mastery Certification \u00b6 I certify that I can: Implement all five load balancing algorithms from memory Explain when and why to use each algorithm Identify the correct algorithm for new scenarios Analyze time and space complexity of each approach Compare trade-offs between algorithms Debug common load balancing bugs Design load balancing for production systems Teach these concepts to someone else","title":"09. Load Balancing"},{"location":"systems/09-load-balancing/#load-balancing","text":"Distributing requests across multiple servers for scalability and reliability","title":"Load Balancing"},{"location":"systems/09-load-balancing/#eli5-explain-like-im-5","text":"Your task: After implementing different load balancing algorithms, explain them simply. Prompts to guide you: What is load balancing in one sentence? Your answer: [Fill in after implementation] Why do we need load balancers? Your answer: [Fill in after implementation] Real-world analogy for round robin: Example: \"Round robin is like a carousel where...\" Your analogy: [Fill in] What is round robin in one sentence? Your answer: [Fill in after implementation] How is least connections different from round robin? Your answer: [Fill in after implementation] Real-world analogy for consistent hashing: Example: \"Consistent hashing is like a clock where...\" Your analogy: [Fill in] What is consistent hashing in one sentence? Your answer: [Fill in after implementation] When would you use weighted load balancing? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/09-load-balancing/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/09-load-balancing/#beforeafter-why-this-pattern-matters","text":"Your task: Compare different load balancing approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"systems/09-load-balancing/#case-studies-load-balancing-in-the-wild","text":"","title":"Case Studies: Load Balancing in the Wild"},{"location":"systems/09-load-balancing/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/09-load-balancing/#client-code","text":"import java.util.*; public class LoadBalancingClient { public static void main(String[] args) { testRoundRobin(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testLeastConnections(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testWeightedRoundRobin(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testConsistentHashing(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testIPHash(); } static void testRoundRobin() { System.out.println(\"=== Round Robin Test ===\\n\"); // Create servers List<RoundRobinLoadBalancer.Server> servers = Arrays.asList( new RoundRobinLoadBalancer.Server(\"S1\", \"10.0.0.1\", 8080), new RoundRobinLoadBalancer.Server(\"S2\", \"10.0.0.2\", 8080), new RoundRobinLoadBalancer.Server(\"S3\", \"10.0.0.3\", 8080) ); RoundRobinLoadBalancer lb = new RoundRobinLoadBalancer(servers); // Test: Send 10 requests System.out.println(\"Sending 10 requests:\"); for (int i = 1; i <= 10; i++) { RoundRobinLoadBalancer.Server server = lb.getNextServer(); System.out.println(\"Request \" + i + \" -> \" + server); } } static void testLeastConnections() { System.out.println(\"=== Least Connections Test ===\\n\"); List<RoundRobinLoadBalancer.Server> servers = Arrays.asList( new RoundRobinLoadBalancer.Server(\"S1\", \"10.0.0.1\", 8080), new RoundRobinLoadBalancer.Server(\"S2\", \"10.0.0.2\", 8080), new RoundRobinLoadBalancer.Server(\"S3\", \"10.0.0.3\", 8080) ); LeastConnectionsLoadBalancer lb = new LeastConnectionsLoadBalancer(servers); // Test: Send requests and simulate completion System.out.println(\"Request 1:\"); RoundRobinLoadBalancer.Server s1 = lb.getNextServer(); System.out.println(\"Routed to: \" + s1); System.out.println(\"Stats: \" + lb.getStats()); System.out.println(\"\\nRequest 2:\"); RoundRobinLoadBalancer.Server s2 = lb.getNextServer(); System.out.println(\"Routed to: \" + s2); System.out.println(\"Stats: \" + lb.getStats()); System.out.println(\"\\nRequest 1 completes:\"); lb.releaseConnection(s1); System.out.println(\"Stats: \" + lb.getStats()); System.out.println(\"\\nRequest 3:\"); RoundRobinLoadBalancer.Server s3 = lb.getNextServer(); System.out.println(\"Routed to: \" + s3); System.out.println(\"Stats: \" + lb.getStats()); } static void testWeightedRoundRobin() { System.out.println(\"=== Weighted Round Robin Test ===\\n\"); // Create servers with different capacities List<WeightedRoundRobinLoadBalancer.WeightedServer> servers = Arrays.asList( new WeightedRoundRobinLoadBalancer.WeightedServer( new RoundRobinLoadBalancer.Server(\"S1-Small\", \"10.0.0.1\", 8080), 1), new WeightedRoundRobinLoadBalancer.WeightedServer( new RoundRobinLoadBalancer.Server(\"S2-Medium\", \"10.0.0.2\", 8080), 2), new WeightedRoundRobinLoadBalancer.WeightedServer( new RoundRobinLoadBalancer.Server(\"S3-Large\", \"10.0.0.3\", 8080), 3) ); WeightedRoundRobinLoadBalancer lb = new WeightedRoundRobinLoadBalancer(servers); // Test: Send 12 requests (should distribute 2:4:6) System.out.println(\"Sending 12 requests (expected: 2:4:6 distribution):\"); Map<String, Integer> distribution = new HashMap<>(); for (int i = 1; i <= 12; i++) { RoundRobinLoadBalancer.Server server = lb.getNextServer(); distribution.merge(server.id, 1, Integer::sum); System.out.println(\"Request \" + i + \" -> \" + server.id); } System.out.println(\"\\nDistribution: \" + distribution); } static void testConsistentHashing() { System.out.println(\"=== Consistent Hashing Test ===\\n\"); List<RoundRobinLoadBalancer.Server> servers = Arrays.asList( new RoundRobinLoadBalancer.Server(\"S1\", \"10.0.0.1\", 8080), new RoundRobinLoadBalancer.Server(\"S2\", \"10.0.0.2\", 8080), new RoundRobinLoadBalancer.Server(\"S3\", \"10.0.0.3\", 8080) ); ConsistentHashingLoadBalancer lb = new ConsistentHashingLoadBalancer(servers, 3); // Test: Same key always goes to same server String[] keys = {\"user123\", \"user456\", \"user789\", \"user123\", \"user456\"}; System.out.println(\"Routing requests by user ID:\"); for (String key : keys) { RoundRobinLoadBalancer.Server server = lb.getServer(key); System.out.println(key + \" -> \" + server.id); } // Test: Add server and see minimal redistribution System.out.println(\"\\nAdding new server S4:\"); lb.addServer(new RoundRobinLoadBalancer.Server(\"S4\", \"10.0.0.4\", 8080)); for (String key : keys) { RoundRobinLoadBalancer.Server server = lb.getServer(key); System.out.println(key + \" -> \" + server.id); } } static void testIPHash() { System.out.println(\"=== IP Hash Test ===\\n\"); List<RoundRobinLoadBalancer.Server> servers = Arrays.asList( new RoundRobinLoadBalancer.Server(\"S1\", \"10.0.0.1\", 8080), new RoundRobinLoadBalancer.Server(\"S2\", \"10.0.0.2\", 8080), new RoundRobinLoadBalancer.Server(\"S3\", \"10.0.0.3\", 8080) ); IPHashLoadBalancer lb = new IPHashLoadBalancer(servers); // Test: Same IP always goes to same server String[] clientIPs = {\"192.168.1.100\", \"192.168.1.101\", \"192.168.1.102\", \"192.168.1.100\", \"192.168.1.101\"}; System.out.println(\"Routing by client IP (session persistence):\"); for (String ip : clientIPs) { RoundRobinLoadBalancer.Server server = lb.getServer(ip); System.out.println(ip + \" -> \" + server.id); } } }","title":"Client Code"},{"location":"systems/09-load-balancing/#debugging-challenges","text":"Your task: Find and fix bugs in broken load balancing implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"systems/09-load-balancing/#decision-framework","text":"Questions to answer after implementation:","title":"Decision Framework"},{"location":"systems/09-load-balancing/#practice","text":"","title":"Practice"},{"location":"systems/09-load-balancing/#review-checklist","text":"Round robin implemented with circular iteration Least connections implemented with connection tracking Weighted round robin implemented with GCD algorithm Consistent hashing implemented with virtual nodes IP hash implemented for session persistence Understand when to use each algorithm Can explain trade-offs between algorithms Built decision tree for algorithm selection Completed practice scenarios","title":"Review Checklist"},{"location":"systems/10-concurrency-patterns/","text":"Concurrency Patterns \u00b6 Locks, thread pools, synchronization, and async patterns - From threads to coroutines to reactive streams ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing concurrency patterns, explain them simply. Prompts to guide you: What is a lock in one sentence? Your answer: [Fill in after implementation] Why do we need locks in concurrent programs? Your answer: [Fill in after implementation] Real-world analogy for ReentrantLock: Example: \"A ReentrantLock is like a bathroom key that you can use multiple times...\" Your analogy: [Fill in] What is a thread pool in one sentence? Your answer: [Fill in after implementation] Why use a thread pool instead of creating threads directly? Your answer: [Fill in after implementation] Real-world analogy for BlockingQueue: Example: \"A BlockingQueue is like a conveyor belt in a factory...\" Your analogy: [Fill in] What is an event loop in one sentence? Your answer: [Fill in after learning] How are virtual threads different from platform threads? Your answer: [Fill in after learning] Real-world analogy for virtual threads: Example: \"Virtual threads are like lightweight workers that...\" Your analogy: [Fill in] What is backpressure in reactive streams? Your answer: [Fill in after learning] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition about concurrency without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Synchronized vs Lock-Free counter with 10 threads, 1M increments each: Synchronized time: [Your guess: faster/slower than lock-free?] Lock-free time: [Your guess] Verified after learning: [Actual results] Producer-Consumer with bounded queue (size 10), 1 producer, 1 consumer: What happens when queue is full? [Your guess] What happens when queue is empty? [Your guess] Verified: [Actual behavior] Thread pool sizing calculation: For 8 CPU cores, 100ms CPU work, 0ms I/O per task: Optimal pool size: [Your guess: O(?)] For 8 CPU cores, 50ms CPU work, 200ms I/O per task: Optimal pool size: [Your guess: O(?)] Verified: [Actual formula and results] Scenario Predictions \u00b6 Scenario 1: Two threads incrementing shared counter without synchronization int counter = 0; Thread t1 = new Thread(() -> { for(int i=0; i<1000; i++) counter++; }); Thread t2 = new Thread(() -> { for(int i=0; i<1000; i++) counter++; }); Expected counter value: [2000, right?] Actual typical value: [Will it be 2000? Why or why not?] This bug is called: [Fill in after learning] Scenario 2: Bank transfer with incorrect locking // Thread 1: transfer(account1, account2, $100) // Thread 2: transfer(account2, account1, $50) // Each thread locks: lock(from), lock(to), then transfers Can deadlock occur? [Yes/No - Why?] How to prevent it? [Fill in your guess] Verified solution: [Fill in after implementation] Scenario 3: ConcurrentHashMap vs Hashtable (synchronized) Which is faster for 100 threads reading? [Your guess] Why? [Reason] What's the key difference? [Fill in after learning about lock striping] Trade-off Quiz \u00b6 Question: When would synchronized be BETTER than ReentrantLock? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: When would lock-free (Atomic) be WORSE than locks? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning about contention and CAS retries] Question: What's the MAIN risk of unbounded thread pools? Slow performance Deadlocks OutOfMemoryError Race conditions Verify after implementation: [Which one(s)?] Before/After: Why These Patterns Matter \u00b6 Your task: Compare unsafe vs synchronized vs lock-free approaches to understand the impact. Example: Thread-Safe Counter \u00b6 Problem: Multiple threads incrementing a shared counter. Approach 1: Unsafe (No Synchronization) \u00b6 // BROKEN - Race condition! public class UnsafeCounter { private int count = 0; public void increment() { count++; // NOT atomic! Actually: read, add, write } public int getCount() { return count; } } Analysis: Time: O(1) per operation Space: O(1) BUG: Race condition - lost updates For 10 threads \u00d7 100,000 increments = 1,000,000 expected Actual result: ~650,000 (varies each run!) Lost updates: ~350,000 increments lost Why it fails: Thread 1: read count=5 Thread 2: read count=5 Thread 1: add 1, write count=6 Thread 2: add 1, write count=6 \u2190 Should be 7! Approach 2: Synchronized (Lock-Based) \u00b6 // CORRECT - Using synchronized public class SynchronizedCounter { private int count = 0; public synchronized void increment() { count++; // Protected by lock } public synchronized int getCount() { return count; } } Analysis: Time: O(1) per operation (with lock overhead ~50-100ns) Space: O(1) CORRECT: Mutual exclusion guarantees atomicity For 10 threads \u00d7 100,000 increments = 1,000,000 expected Actual result: 1,000,000 \u2713 Lock overhead: Low contention: ~50ns per lock High contention: ~500-1000ns (threads wait in queue) Approach 3: Lock-Free (Atomic with CAS) \u00b6 // CORRECT - Using Compare-And-Swap import java.util.concurrent.atomic.AtomicInteger; public class LockFreeCounter { private final AtomicInteger count = new AtomicInteger(0); public void increment() { count.getAndIncrement(); // Uses CAS internally } public int getCount() { return count.get(); } } Analysis: Time: O(1) expected per operation (CAS ~5-10ns when successful) Space: O(1) CORRECT: CAS ensures atomicity without locks For 10 threads \u00d7 100,000 increments = 1,000,000 expected Actual result: 1,000,000 \u2713 CAS behavior: do { current = read count next = current + 1 } while (!compareAndSet(current, next)) // Retry if changed Performance Comparison \u00b6 Threads Unsafe (BROKEN) Synchronized Lock-Free (Atomic) Winner 1 100% (5ms) 100% (5ms) 100% (5ms) Tie 2 75% lost 100% (12ms) 100% (8ms) Lock-free 1.5x 10 35% lost 100% (150ms) 100% (50ms) Lock-free 3x 100 5% lost 100% (2000ms) 100% (500ms) Lock-free 4x Your calculation: For 50 threads, synchronized takes 800ms. Lock-free should take approximately _____ ms. Why Does Lock-Free Win Under Contention? \u00b6 Synchronized (Lock): Thread 1 acquires lock \u2192 others BLOCK and wait in queue Context switches, scheduler overhead Serialized execution under high contention Lock-Free (CAS): All threads attempt CAS simultaneously Failed CAS retries immediately (no blocking) Better CPU utilization, no context switches Scales better with more threads When locks win: Complex operations (need to hold lock for multiple steps) Low contention (lock overhead minimal) Need fairness guarantees (lock provides FIFO) After implementing, explain in your own words: Why does unsynchronized code lose updates? [Your answer] When would you choose synchronized over lock-free? [Your answer] What is the \"ABA problem\" in lock-free algorithms? [Your answer] Case Studies: Concurrency in the Wild \u00b6 Relational Databases: Pessimistic Locking for Consistency \u00b6 Pattern: Pessimistic Locking ( SELECT ... FOR UPDATE ). How it works: In a traditional banking application, when a user initiates a money transfer, the database transaction can place an exclusive lock on the user's account balance row. The transaction reads the balance, updates it, and then releases the lock upon commit. If another transaction tries to access the same account balance row in the meantime, it is blocked and forced to wait. Key Takeaway: For high-contention, mission-critical operations where data integrity is paramount (e.g., financial transactions, inventory management), pessimistic locking is a safe and robust strategy. It prevents conflicts from happening in the first place, at the cost of reduced concurrency. Web Frameworks (Ruby on Rails, Django): Optimistic Locking for High Concurrency \u00b6 Pattern: Optimistic Concurrency Control (OCC) with a version column. How it works: Imagine two admins editing the same product in an e-commerce backend. Admin A loads the product page (version 42). Admin B loads the same page. Admin A saves her changes; the application updates the product and increments its version to 43. When Admin B tries to save his changes, the application sees that he is trying to update version 42, but the current version is 43. The update is rejected, and Admin B is shown an error: \"This record was modified by someone else. Please reload and try again.\" Key Takeaway: Optimistic locking is ideal for web applications where conflicts are rare and high concurrency is desirable. It \"hopes\" for the best and only deals with conflicts when they actually occur, avoiding the overhead of database locks for the majority of non-conflicting operations. PostgreSQL & Oracle: MVCC for Read/Write Isolation \u00b6 Pattern: Multi-Version Concurrency Control (MVCC). How it works: When you run a long analytical query ( SELECT AVG(price) FROM products ) in PostgreSQL, you get a consistent \"snapshot\" of the database at the time your query began. If another user updates a product price while your query is running, PostgreSQL creates a new version of that product row instead of overwriting it. Your long-running query continues to see the old version, while new transactions will see the updated version. Key Takeaway: MVCC is a powerful mechanism that allows readers and writers not to block each other. This is a massive performance benefit for mixed workloads, where long-running reports or analytics can execute alongside fast OLTP transactions without contention. Core Implementation \u00b6 Pattern 1: Lock-Based Synchronization \u00b6 Concept: Using explicit locks to control access to shared resources and prevent race conditions. Use case: Thread-safe counters, banking transactions, resource pooling. import java.util.*; import java.util.concurrent.locks.*; /** * Lock-Based Synchronization * * Key concepts: * - ReentrantLock: Lock that same thread can acquire multiple times * - Read-Write locks: Multiple readers OR single writer * - Lock ordering: Prevents deadlocks by consistent acquisition order * - Try-lock: Non-blocking lock attempts */ public class LockBasedSync { /** * Thread-safe counter with ReentrantLock * Better control than synchronized keyword */ static class ThreadSafeCounter { private int count; private final ReentrantLock lock; public ThreadSafeCounter() { this.count = 0; this.lock = new ReentrantLock(); } /** * Increment counter safely * Time: O(1), Space: O(1) * * TODO: Implement thread-safe increment * 1. Acquire lock * 2. Increment count * 3. Release lock in finally block */ public void increment() { // TODO: Acquire lock } /** * Get current count * Time: O(1), Space: O(1) * * TODO: Implement thread-safe read */ public int getCount() { // TODO: Acquire lock for reading return 0; // Replace } /** * Try to increment with timeout * Time: O(1), Space: O(1) * * TODO: Implement try-lock with timeout * 1. Try to acquire lock with timeout * 2. If acquired, increment and return true * 3. If timeout, return false */ public boolean tryIncrement(long timeoutMs) { // TODO: Try lock with timeout return false; // Replace } /** * Check if lock is held by current thread */ public boolean isLocked() { return lock.isHeldByCurrentThread(); } } /** * Read-Write Lock: Multiple readers, single writer * Optimizes for read-heavy workloads */ static class ReadWriteCache<K, V> { private final Map<K, V> cache; private final ReadWriteLock rwLock; private final Lock readLock; private final Lock writeLock; public ReadWriteCache() { this.cache = new HashMap<>(); this.rwLock = new ReentrantReadWriteLock(); this.readLock = rwLock.readLock(); this.writeLock = rwLock.writeLock(); } /** * Get value (multiple readers allowed) * Time: O(1), Space: O(1) * * TODO: Implement read with read lock * 1. Acquire read lock (doesn't block other readers) * 2. Read from cache * 3. Release read lock */ public V get(K key) { // TODO: Use read lock return null; // Replace } /** * Put value (exclusive write access) * Time: O(1), Space: O(1) * * TODO: Implement write with write lock * 1. Acquire write lock (blocks all readers and writers) * 2. Write to cache * 3. Release write lock */ public void put(K key, V value) { // TODO: Use write lock } /** * Clear cache (write operation) * Time: O(1), Space: O(1) * * TODO: Implement clear with write lock */ public void clear() { // TODO: Use write lock to clear } public int size() { readLock.lock(); try { return cache.size(); } finally { readLock.unlock(); } } } /** * Bank Transfer: Demonstrates lock ordering to prevent deadlock */ static class BankAccount { private final int id; private int balance; private final ReentrantLock lock; public BankAccount(int id, int initialBalance) { this.id = id; this.balance = initialBalance; this.lock = new ReentrantLock(); } /** * Transfer money between accounts * Time: O(1), Space: O(1) * * TODO: Implement transfer with lock ordering * 1. Acquire locks in consistent order (by account ID) * 2. Check sufficient balance * 3. Perform transfer * 4. Release locks in reverse order * * CRITICAL: Always lock in same order to prevent deadlock! * If thread A locks account1 then account2, and thread B locks * account2 then account1, DEADLOCK can occur. */ public static boolean transfer(BankAccount from, BankAccount to, int amount) { // TODO: Lock ordering - always lock lower ID first // TODO: Acquire locks in order return false; // Replace } public int getBalance() { lock.lock(); try { return balance; } finally { lock.unlock(); } } public int getId() { return id; } } } Runnable Client Code: import java.util.concurrent.*; public class LockBasedSyncClient { public static void main(String[] args) throws InterruptedException { System.out.println(\"=== Lock-Based Synchronization ===\\n\"); // Test 1: Thread-safe counter System.out.println(\"--- Test 1: Thread-Safe Counter ---\"); testThreadSafeCounter(); // Test 2: Read-write cache System.out.println(\"\\n--- Test 2: Read-Write Cache ---\"); testReadWriteCache(); // Test 3: Bank transfers (lock ordering) System.out.println(\"\\n--- Test 3: Bank Transfers ---\"); testBankTransfers(); } static void testThreadSafeCounter() throws InterruptedException { LockBasedSync.ThreadSafeCounter counter = new LockBasedSync.ThreadSafeCounter(); int numThreads = 10; int incrementsPerThread = 1000; ExecutorService executor = Executors.newFixedThreadPool(numThreads); // Launch threads to increment for (int i = 0; i < numThreads; i++) { executor.submit(() -> { for (int j = 0; j < incrementsPerThread; j++) { counter.increment(); } }); } executor.shutdown(); executor.awaitTermination(10, TimeUnit.SECONDS); int expected = numThreads * incrementsPerThread; int actual = counter.getCount(); System.out.println(\"Expected: \" + expected); System.out.println(\"Actual: \" + actual); System.out.println(\"Correct: \" + (expected == actual)); } static void testReadWriteCache() throws InterruptedException { LockBasedSync.ReadWriteCache<String, Integer> cache = new LockBasedSync.ReadWriteCache<>(); // Writer thread Thread writer = new Thread(() -> { for (int i = 0; i < 100; i++) { cache.put(\"key\" + i, i); try { Thread.sleep(10); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } }); // Reader threads Thread[] readers = new Thread[5]; for (int i = 0; i < readers.length; i++) { readers[i] = new Thread(() -> { for (int j = 0; j < 100; j++) { Integer val = cache.get(\"key\" + j); if (val != null) { System.out.println(Thread.currentThread().getName() + \" read: \" + val); } try { Thread.sleep(5); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } }); } writer.start(); for (Thread reader : readers) { reader.start(); } writer.join(); for (Thread reader : readers) { reader.join(); } System.out.println(\"Cache size: \" + cache.size()); } static void testBankTransfers() throws InterruptedException { LockBasedSync.BankAccount acc1 = new LockBasedSync.BankAccount(1, 1000); LockBasedSync.BankAccount acc2 = new LockBasedSync.BankAccount(2, 1000); System.out.println(\"Initial balances:\"); System.out.println(\"Account 1: \" + acc1.getBalance()); System.out.println(\"Account 2: \" + acc2.getBalance()); // Concurrent transfers Thread t1 = new Thread(() -> { for (int i = 0; i < 100; i++) { LockBasedSync.BankAccount.transfer(acc1, acc2, 10); } }); Thread t2 = new Thread(() -> { for (int i = 0; i < 100; i++) { LockBasedSync.BankAccount.transfer(acc2, acc1, 5); } }); t1.start(); t2.start(); t1.join(); t2.join(); System.out.println(\"\\nFinal balances:\"); System.out.println(\"Account 1: \" + acc1.getBalance()); System.out.println(\"Account 2: \" + acc2.getBalance()); System.out.println(\"Total: \" + (acc1.getBalance() + acc2.getBalance())); System.out.println(\"Correct: \" + ((acc1.getBalance() + acc2.getBalance()) == 2000)); } } Pattern 2: Producer-Consumer with BlockingQueue \u00b6 Concept: Decoupling producers and consumers using a thread-safe bounded queue. Use case: Task queues, message processing, batch processing pipelines. import java.util.concurrent.*; import java.util.*; /** * Producer-Consumer Pattern * * Classic concurrency problem: * - Producers generate work items * - Consumers process work items * - Bounded buffer prevents memory overflow * - BlockingQueue handles all synchronization */ public class ProducerConsumer { /** * Work item representation */ static class Task { private final int id; private final String data; private final long createdAt; public Task(int id, String data) { this.id = id; this.data = data; this.createdAt = System.currentTimeMillis(); } public int getId() { return id; } public String getData() { return data; } public long getAge() { return System.currentTimeMillis() - createdAt; } @Override public String toString() { return \"Task{id=\" + id + \", data='\" + data + \"'}\"; } } /** * Producer: Generates tasks and puts them in queue */ static class Producer implements Runnable { private final BlockingQueue<Task> queue; private final int numTasks; private final String name; public Producer(BlockingQueue<Task> queue, int numTasks, String name) { this.queue = queue; this.numTasks = numTasks; this.name = name; } /** * Produce tasks * * TODO: Implement producer logic * 1. Generate tasks * 2. Put into queue (blocks if queue is full) * 3. Handle interruption */ @Override public void run() { // TODO: Produce numTasks // // // Simulate work // Thread.sleep(10); // } catch (InterruptedException e) { // Thread.currentThread().interrupt(); // break; // } // } // System.out.println(name + \" finished producing\"); } } /** * Consumer: Takes tasks from queue and processes them */ static class Consumer implements Runnable { private final BlockingQueue<Task> queue; private final String name; private volatile boolean running = true; public Consumer(BlockingQueue<Task> queue, String name) { this.queue = queue; this.name = name; } /** * Consume tasks * * TODO: Implement consumer logic * 1. Take from queue (blocks if empty) * 2. Process task * 3. Check for poison pill (shutdown signal) */ @Override public void run() { // TODO: Consume tasks until stopped // // if (task != null) { // // Process task // processTask(task); // } // } catch (InterruptedException e) { // Thread.currentThread().interrupt(); // break; // } // } // System.out.println(name + \" stopped\"); } private void processTask(Task task) { System.out.println(name + \" processing: \" + task + \" (age: \" + task.getAge() + \"ms)\"); // Simulate processing try { Thread.sleep(50); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } public void stop() { running = false; } } /** * Pipeline: Multiple stages of processing * Each stage is a producer-consumer pair */ static class ProcessingPipeline { private final BlockingQueue<Task> stage1Queue; private final BlockingQueue<Task> stage2Queue; private final BlockingQueue<Task> stage3Queue; public ProcessingPipeline(int queueCapacity) { this.stage1Queue = new ArrayBlockingQueue<>(queueCapacity); this.stage2Queue = new ArrayBlockingQueue<>(queueCapacity); this.stage3Queue = new ArrayBlockingQueue<>(queueCapacity); } /** * Start pipeline processing * Time: O(N) where N = tasks, Space: O(Q) where Q = queue capacity * * TODO: Implement multi-stage pipeline * Stage 1: Validate tasks * Stage 2: Transform tasks * Stage 3: Save results */ public void start(int numTasks) { // TODO: Create stage 1 workers (producers to stage1Queue) // TODO: Create stage 2 workers (consume stage1, produce stage2) // TODO: Create stage 3 workers (consume stage2, produce stage3) // TODO: Create final consumers (consume stage3) System.out.println(\"Pipeline started with \" + numTasks + \" tasks\"); } public BlockingQueue<Task> getStage1Queue() { return stage1Queue; } } } Runnable Client Code: import java.util.concurrent.*; public class ProducerConsumerClient { public static void main(String[] args) throws InterruptedException { System.out.println(\"=== Producer-Consumer Pattern ===\\n\"); // Test 1: Single producer, single consumer System.out.println(\"--- Test 1: Single Producer-Consumer ---\"); testSingleProducerConsumer(); Thread.sleep(2000); // Test 2: Multiple producers, multiple consumers System.out.println(\"\\n--- Test 2: Multiple Producers-Consumers ---\"); testMultipleProducersConsumers(); Thread.sleep(2000); // Test 3: Different queue types System.out.println(\"\\n--- Test 3: Queue Type Comparison ---\"); testQueueTypes(); } static void testSingleProducerConsumer() throws InterruptedException { BlockingQueue<ProducerConsumer.Task> queue = new ArrayBlockingQueue<>(10); ProducerConsumer.Producer producer = new ProducerConsumer.Producer(queue, 20, \"P1\"); ProducerConsumer.Consumer consumer = new ProducerConsumer.Consumer(queue, \"C1\"); Thread producerThread = new Thread(producer); Thread consumerThread = new Thread(consumer); producerThread.start(); consumerThread.start(); producerThread.join(); Thread.sleep(1000); // Let consumer finish consumer.stop(); consumerThread.join(); System.out.println(\"Queue remaining: \" + queue.size()); } static void testMultipleProducersConsumers() throws InterruptedException { BlockingQueue<ProducerConsumer.Task> queue = new ArrayBlockingQueue<>(50); // Create 3 producers Thread[] producers = new Thread[3]; for (int i = 0; i < producers.length; i++) { ProducerConsumer.Producer producer = new ProducerConsumer.Producer(queue, 10, \"P\" + (i+1)); producers[i] = new Thread(producer); producers[i].start(); } // Create 2 consumers ProducerConsumer.Consumer[] consumers = new ProducerConsumer.Consumer[2]; Thread[] consumerThreads = new Thread[2]; for (int i = 0; i < consumers.length; i++) { consumers[i] = new ProducerConsumer.Consumer(queue, \"C\" + (i+1)); consumerThreads[i] = new Thread(consumers[i]); consumerThreads[i].start(); } // Wait for all producers for (Thread producer : producers) { producer.join(); } // Wait for queue to drain Thread.sleep(2000); // Stop consumers for (ProducerConsumer.Consumer consumer : consumers) { consumer.stop(); } for (Thread thread : consumerThreads) { thread.join(); } System.out.println(\"Final queue size: \" + queue.size()); } static void testQueueTypes() throws InterruptedException { System.out.println(\"ArrayBlockingQueue: Bounded, array-backed\"); System.out.println(\"LinkedBlockingQueue: Optionally bounded, linked-list\"); System.out.println(\"PriorityBlockingQueue: Unbounded, ordered by priority\"); System.out.println(\"SynchronousQueue: No capacity, direct handoff\"); // Example: PriorityBlockingQueue BlockingQueue<Integer> priorityQueue = new PriorityBlockingQueue<>(); priorityQueue.put(5); priorityQueue.put(1); priorityQueue.put(10); priorityQueue.put(3); System.out.println(\"\\nPriorityBlockingQueue order:\"); while (!priorityQueue.isEmpty()) { System.out.println(\" \" + priorityQueue.take()); } } } Pattern 3: Thread-Safe Data Structures \u00b6 Concept: Using concurrent collections and lock-free algorithms for high-performance concurrent access. Use case: Caches, shared state, counters in high-throughput systems. import java.util.concurrent.*; import java.util.concurrent.atomic.*; import java.util.*; /** * Thread-Safe Data Structures * * Three approaches: * 1. ConcurrentHashMap: Lock striping for scalability * 2. CopyOnWriteArrayList: Snapshot iteration, write-heavy penalty * 3. Atomic variables: Lock-free using CAS (Compare-And-Swap) */ public class ThreadSafeDataStructures { /** * ConcurrentHashMap: Scalable concurrent hash table * Uses lock striping - different segments can be locked independently */ static class ConcurrentCache<K, V> { private final ConcurrentHashMap<K, V> cache; private final AtomicLong hits; private final AtomicLong misses; public ConcurrentCache() { this.cache = new ConcurrentHashMap<>(); this.hits = new AtomicLong(0); this.misses = new AtomicLong(0); } /** * Get with statistics tracking * Time: O(1) average, Space: O(1) * * TODO: Implement thread-safe get with metrics * 1. Try to get from cache * 2. Track hit/miss * 3. Return value */ public V get(K key) { // TODO: Implement with hit/miss tracking return null; // Replace } /** * Put if absent (atomic operation) * Time: O(1) average, Space: O(1) * * TODO: Implement atomic put-if-absent * 1. Use putIfAbsent (atomic operation) * 2. Return previous value (null if inserted) */ public V putIfAbsent(K key, V value) { // TODO: Use ConcurrentHashMap's putIfAbsent return null; // Replace } /** * Compute value atomically * Time: O(1) average, Space: O(1) * * TODO: Implement atomic compute * Use computeIfAbsent to atomically compute value if missing */ public V computeIfAbsent(K key, java.util.function.Function<K, V> mappingFunction) { // TODO: Use computeIfAbsent return null; // Replace } /** * Atomic update * Time: O(1) average, Space: O(1) * * TODO: Implement atomic value update * Use compute to atomically update existing value */ public V update(K key, java.util.function.BiFunction<K, V, V> remappingFunction) { // TODO: Use compute for atomic update return null; // Replace } public double getHitRate() { long totalHits = hits.get(); long totalRequests = totalHits + misses.get(); return totalRequests == 0 ? 0.0 : (double) totalHits / totalRequests; } public int size() { return cache.size(); } } /** * Copy-On-Write List: Thread-safe list with snapshot iteration * Every write creates a new copy of underlying array * Great for read-heavy workloads with infrequent updates */ static class EventListeners<T> { private final CopyOnWriteArrayList<T> listeners; public EventListeners() { this.listeners = new CopyOnWriteArrayList<>(); } /** * Add listener (creates copy) * Time: O(N), Space: O(N) * * TODO: Implement add listener * CopyOnWriteArrayList handles synchronization */ public void addListener(T listener) { // TODO: Add to list } /** * Remove listener (creates copy) * Time: O(N), Space: O(N) * * TODO: Implement remove listener */ public void removeListener(T listener) { // TODO: Remove from list } /** * Notify all listeners * Time: O(N), Space: O(1) * * Iteration uses snapshot - immune to concurrent modifications * * TODO: Implement notification */ public void notifyListeners(java.util.function.Consumer<T> action) { // TODO: Iterate and apply action } public int size() { return listeners.size(); } } /** * Lock-Free Counter: Using Compare-And-Swap (CAS) * No locks - uses CPU atomic instructions */ static class LockFreeCounter { private final AtomicLong count; public LockFreeCounter() { this.count = new AtomicLong(0); } /** * Increment using CAS * Time: O(1), Space: O(1) * * TODO: Implement CAS-based increment * 1. Read current value * 2. Compute new value * 3. CAS: if current unchanged, update to new value * 4. If CAS fails (value changed), retry */ public long increment() { // TODO: Use getAndIncrement (implements CAS internally) return 0; // Replace } /** * Add value using CAS * Time: O(1) expected, Space: O(1) * * TODO: Implement CAS-based add */ public long addAndGet(long delta) { // TODO: Use addAndGet return 0; // Replace } /** * Custom CAS operation * Time: O(1) expected, Space: O(1) * * TODO: Implement custom CAS logic * Only update if current value is even */ public boolean incrementIfEven() { // TODO: Implement CAS loop return false; // Replace } public long get() { return count.get(); } } /** * Lock-Free Stack: Using CAS for push/pop */ static class LockFreeStack<T> { private static class Node<T> { final T value; Node<T> next; Node(T value) { this.value = value; } } private final AtomicReference<Node<T>> head; public LockFreeStack() { this.head = new AtomicReference<>(null); } /** * Push element using CAS * Time: O(1) expected, Space: O(1) * * TODO: Implement lock-free push * 1. Create new node * 2. Set its next to current head * 3. CAS head to new node * 4. If CAS fails, retry */ public void push(T value) { // TODO: Implement CAS-based push } /** * Pop element using CAS * Time: O(1) expected, Space: O(1) * * TODO: Implement lock-free pop * 1. Read current head * 2. Get next node * 3. CAS head to next * 4. If CAS fails, retry */ public T pop() { // TODO: Implement CAS-based pop return null; // Replace } public boolean isEmpty() { return head.get() == null; } } } Runnable Client Code: import java.util.concurrent.*; public class ThreadSafeDataStructuresClient { public static void main(String[] args) throws InterruptedException { System.out.println(\"=== Thread-Safe Data Structures ===\\n\"); // Test 1: ConcurrentHashMap cache System.out.println(\"--- Test 1: ConcurrentHashMap Cache ---\"); testConcurrentCache(); // Test 2: CopyOnWriteArrayList System.out.println(\"\\n--- Test 2: Copy-On-Write List ---\"); testCopyOnWriteList(); // Test 3: Lock-free counter System.out.println(\"\\n--- Test 3: Lock-Free Counter ---\"); testLockFreeCounter(); // Test 4: Lock-free stack System.out.println(\"\\n--- Test 4: Lock-Free Stack ---\"); testLockFreeStack(); } static void testConcurrentCache() throws InterruptedException { ThreadSafeDataStructures.ConcurrentCache<String, Integer> cache = new ThreadSafeDataStructures.ConcurrentCache<>(); // Populate cache for (int i = 0; i < 100; i++) { cache.putIfAbsent(\"key\" + i, i); } // Concurrent readers ExecutorService executor = Executors.newFixedThreadPool(10); for (int i = 0; i < 10; i++) { executor.submit(() -> { for (int j = 0; j < 1000; j++) { int key = ThreadLocalRandom.current().nextInt(150); cache.get(\"key\" + key); } }); } executor.shutdown(); executor.awaitTermination(10, TimeUnit.SECONDS); System.out.println(\"Cache size: \" + cache.size()); System.out.println(\"Hit rate: \" + String.format(\"%.2f%%\", cache.getHitRate() * 100)); } static void testCopyOnWriteList() throws InterruptedException { ThreadSafeDataStructures.EventListeners<String> listeners = new ThreadSafeDataStructures.EventListeners<>(); // Add initial listeners listeners.addListener(\"Listener1\"); listeners.addListener(\"Listener2\"); listeners.addListener(\"Listener3\"); // Concurrent notifications and modifications Thread notifier = new Thread(() -> { for (int i = 0; i < 100; i++) { listeners.notifyListeners(listener -> { System.out.println(\"Notifying: \" + listener); }); try { Thread.sleep(10); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } }); Thread modifier = new Thread(() -> { for (int i = 0; i < 50; i++) { listeners.addListener(\"NewListener\" + i); try { Thread.sleep(20); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } }); notifier.start(); modifier.start(); notifier.join(); modifier.join(); System.out.println(\"Final listener count: \" + listeners.size()); } static void testLockFreeCounter() throws InterruptedException { ThreadSafeDataStructures.LockFreeCounter counter = new ThreadSafeDataStructures.LockFreeCounter(); int numThreads = 10; int incrementsPerThread = 10000; ExecutorService executor = Executors.newFixedThreadPool(numThreads); for (int i = 0; i < numThreads; i++) { executor.submit(() -> { for (int j = 0; j < incrementsPerThread; j++) { counter.increment(); } }); } executor.shutdown(); executor.awaitTermination(10, TimeUnit.SECONDS); long expected = (long) numThreads * incrementsPerThread; long actual = counter.get(); System.out.println(\"Expected: \" + expected); System.out.println(\"Actual: \" + actual); System.out.println(\"Correct: \" + (expected == actual)); // Test incrementIfEven ThreadSafeDataStructures.LockFreeCounter counter2 = new ThreadSafeDataStructures.LockFreeCounter(); System.out.println(\"\\nTesting incrementIfEven:\"); System.out.println(\"Value: \" + counter2.get() + \", incrementIfEven: \" + counter2.incrementIfEven()); System.out.println(\"Value: \" + counter2.get() + \", incrementIfEven: \" + counter2.incrementIfEven()); System.out.println(\"Value: \" + counter2.get() + \", incrementIfEven: \" + counter2.incrementIfEven()); } static void testLockFreeStack() throws InterruptedException { ThreadSafeDataStructures.LockFreeStack<Integer> stack = new ThreadSafeDataStructures.LockFreeStack<>(); // Concurrent pushes ExecutorService executor = Executors.newFixedThreadPool(5); for (int i = 0; i < 5; i++) { final int threadId = i; executor.submit(() -> { for (int j = 0; j < 100; j++) { stack.push(threadId * 1000 + j); } }); } executor.shutdown(); executor.awaitTermination(10, TimeUnit.SECONDS); // Pop all elements System.out.println(\"Popping first 10 elements:\"); for (int i = 0; i < 10; i++) { System.out.println(\" \" + stack.pop()); } int count = 10; while (!stack.isEmpty()) { stack.pop(); count++; } System.out.println(\"Total elements popped: \" + count); } } Pattern 4: Thread Pools \u00b6 Concept: Reusing threads from a pool instead of creating new threads for each task. Use case: Web servers, background job processing, parallel computations. import java.util.concurrent.*; import java.util.*; /** * Thread Pool Patterns * * Benefits: * - Thread reuse (avoid creation/destruction overhead) * - Bounded resources (limit concurrent threads) * - Task queue management * - Lifecycle management (shutdown, termination) */ public class ThreadPoolPatterns { /** * Basic thread pool usage */ static class BasicThreadPool { /** * Create fixed thread pool * Time: O(1), Space: O(N) where N = pool size * * TODO: Demonstrate different pool types */ public static void demonstratePoolTypes() { // TODO: Create different pool types // Fixed thread pool: N worker threads, unbounded queue // ExecutorService fixed = Executors.newFixedThreadPool(4); // Cached thread pool: Creates threads as needed, reuses idle ones // ExecutorService cached = Executors.newCachedThreadPool(); // Single thread executor: Only one worker thread // ExecutorService single = Executors.newSingleThreadExecutor(); // Scheduled thread pool: For delayed/periodic tasks // ScheduledExecutorService scheduled = Executors.newScheduledThreadPool(2); System.out.println(\"Pool types demonstrated\"); } /** * Submit tasks and handle results * Time: O(1) per task, Space: O(1) * * TODO: Implement task submission */ public static void submitTasks() throws InterruptedException, ExecutionException { ExecutorService executor = Executors.newFixedThreadPool(4); // TODO: Submit Callable (returns result) // TODO: Get result (blocks until complete) // TODO: Submit Runnable (no result) // TODO: Shutdown } } /** * Custom ThreadPoolExecutor: Full control over pool behavior */ static class CustomThreadPool { private final ThreadPoolExecutor executor; /** * Create custom thread pool * * TODO: Configure ThreadPoolExecutor * Parameters: * - corePoolSize: Min threads to keep alive * - maximumPoolSize: Max threads allowed * - keepAliveTime: How long excess idle threads wait * - workQueue: Queue for tasks before execution * - rejectedExecutionHandler: What to do when queue full */ public CustomThreadPool(int corePoolSize, int maxPoolSize, int queueSize) { // TODO: Create ThreadPoolExecutor this.executor = null; // Replace } /** * Submit task * Time: O(1), Space: O(1) * * TODO: Implement task submission */ public Future<?> submit(Runnable task) { // TODO: Submit task return null; // Replace } /** * Get pool statistics * * TODO: Implement statistics gathering */ public void printStats() { // TODO: Print executor statistics } /** * Shutdown pool * * TODO: Implement graceful shutdown * 1. Shutdown (no new tasks) * 2. Wait for completion * 3. Force shutdown if timeout */ public void shutdown() throws InterruptedException { // TODO: Graceful shutdown // // if (!executor.awaitTermination(60, TimeUnit.SECONDS)) { // // Timeout - force shutdown // executor.shutdownNow(); // // if (!executor.awaitTermination(60, TimeUnit.SECONDS)) { // System.err.println(\"Pool did not terminate\"); // } // } } } /** * Work stealing pool: For recursive parallel tasks * Uses fork-join framework */ static class WorkStealingPool { /** * Parallel sum using fork-join * Time: O(N) work, O(log N) span, Space: O(log N) * * TODO: Implement fork-join task */ static class SumTask extends RecursiveTask<Long> { private final int[] array; private final int start; private final int end; private static final int THRESHOLD = 1000; // Sequential threshold public SumTask(int[] array, int start, int end) { this.array = array; this.start = start; this.end = end; } @Override protected Long compute() { // TODO: Implement fork-join logic // // // Split task // int mid = start + (end - start) / 2; // SumTask leftTask = new SumTask(array, start, mid); // SumTask rightTask = new SumTask(array, mid, end); // // leftTask.fork(); // Async execute // long rightResult = rightTask.compute(); // Sync compute // long leftResult = leftTask.join(); // Wait for result // // return leftResult + rightResult; return 0L; // Replace } } public static long parallelSum(int[] array) { // TODO: Execute fork-join task return 0L; // Replace } } /** * Scheduled tasks: Delayed and periodic execution */ static class ScheduledTasks { /** * Schedule tasks * * TODO: Demonstrate scheduled execution */ public static void demonstrateScheduled() { ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(2); // TODO: Schedule one-time delayed task // TODO: Schedule periodic task (fixed rate) // TODO: Schedule periodic task (fixed delay) System.out.println(\"Scheduled tasks started\"); } } } Runnable Client Code: import java.util.concurrent.*; import java.util.*; public class ThreadPoolPatternsClient { public static void main(String[] args) throws Exception { System.out.println(\"=== Thread Pool Patterns ===\\n\"); // Test 1: Basic pool types System.out.println(\"--- Test 1: Pool Types ---\"); testPoolTypes(); Thread.sleep(1000); // Test 2: Custom thread pool System.out.println(\"\\n--- Test 2: Custom Thread Pool ---\"); testCustomThreadPool(); Thread.sleep(1000); // Test 3: Work stealing (fork-join) System.out.println(\"\\n--- Test 3: Work Stealing Pool ---\"); testWorkStealingPool(); Thread.sleep(1000); // Test 4: Scheduled tasks System.out.println(\"\\n--- Test 4: Scheduled Tasks ---\"); testScheduledTasks(); } static void testPoolTypes() throws InterruptedException { // Fixed pool ExecutorService fixed = Executors.newFixedThreadPool(4); System.out.println(\"Fixed pool created with 4 threads\"); for (int i = 0; i < 10; i++) { final int taskId = i; fixed.submit(() -> { System.out.println(\"Fixed pool task \" + taskId + \" on \" + Thread.currentThread().getName()); try { Thread.sleep(100); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } }); } fixed.shutdown(); fixed.awaitTermination(10, TimeUnit.SECONDS); System.out.println(\"Fixed pool completed\"); // Cached pool ExecutorService cached = Executors.newCachedThreadPool(); System.out.println(\"\\nCached pool created\"); for (int i = 0; i < 10; i++) { final int taskId = i; cached.submit(() -> { System.out.println(\"Cached pool task \" + taskId + \" on \" + Thread.currentThread().getName()); }); } cached.shutdown(); cached.awaitTermination(10, TimeUnit.SECONDS); System.out.println(\"Cached pool completed\"); } static void testCustomThreadPool() throws InterruptedException { ThreadPoolPatterns.CustomThreadPool pool = new ThreadPoolPatterns.CustomThreadPool(2, 4, 10); // Submit many tasks for (int i = 0; i < 20; i++) { final int taskId = i; pool.submit(() -> { System.out.println(\"Task \" + taskId + \" executing\"); try { Thread.sleep(500); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } }); } Thread.sleep(1000); pool.printStats(); pool.shutdown(); System.out.println(\"Custom pool shutdown\"); } static void testWorkStealingPool() { int[] array = new int[100000]; for (int i = 0; i < array.length; i++) { array[i] = i + 1; } // Sequential sum long start = System.nanoTime(); long sequentialSum = 0; for (int val : array) { sequentialSum += val; } long sequentialTime = System.nanoTime() - start; // Parallel sum start = System.nanoTime(); long parallelSum = ThreadPoolPatterns.WorkStealingPool.parallelSum(array); long parallelTime = System.nanoTime() - start; System.out.println(\"Sequential sum: \" + sequentialSum + \" (\" + sequentialTime/1000 + \" \u03bcs)\"); System.out.println(\"Parallel sum: \" + parallelSum + \" (\" + parallelTime/1000 + \" \u03bcs)\"); System.out.println(\"Speedup: \" + String.format(\"%.2fx\", (double)sequentialTime/parallelTime)); } static void testScheduledTasks() throws InterruptedException { ScheduledExecutorService scheduler = Executors.newScheduledThreadPool(2); System.out.println(\"Scheduling tasks...\"); // Delayed task scheduler.schedule(() -> { System.out.println(\"[\" + System.currentTimeMillis() + \"] Delayed task executed\"); }, 1, TimeUnit.SECONDS); // Periodic task (fixed rate) ScheduledFuture<?> periodicTask = scheduler.scheduleAtFixedRate(() -> { System.out.println(\"[\" + System.currentTimeMillis() + \"] Periodic task (every 500ms)\"); }, 0, 500, TimeUnit.MILLISECONDS); // Let it run for 3 seconds Thread.sleep(3000); // Cancel periodic task periodicTask.cancel(false); scheduler.shutdown(); scheduler.awaitTermination(5, TimeUnit.SECONDS); System.out.println(\"Scheduled tasks completed\"); } } Pattern 5: Non-Blocking I/O (Event Loop) \u00b6 Concept: Handle many concurrent I/O operations with a single thread using event loops and callbacks. Use case: High-concurrency network servers (Node.js, Netty, Nginx), chat servers, API gateways. import java.nio.*; import java.nio.channels.*; import java.net.*; import java.util.*; /** * Non-Blocking I/O Pattern using Java NIO * * Key Concepts: * - Selector: Multiplexes multiple channels * - SelectionKey: Represents channel registration * - Event-driven: React to I/O events (accept, read, write) * - Single thread handles thousands of connections * * Architecture: * - Reactor Pattern: Single thread event loop * - Proactor Pattern: OS handles I/O, notifies completion */ public class NonBlockingIOPattern { /** * Simple non-blocking echo server * Time: O(1) per event, Space: O(N) where N = connections * * TODO: Implement event loop with Selector */ static class EchoServer { private Selector selector; private ServerSocketChannel serverSocket; private final int port; public EchoServer(int port) throws IOException { this.port = port; // TODO: Initialize selector and server socket this.selector = Selector.open(); this.serverSocket = ServerSocketChannel.open(); // TODO: Configure non-blocking mode serverSocket.configureBlocking(false); serverSocket.bind(new InetSocketAddress(port)); // TODO: Register for ACCEPT events serverSocket.register(selector, SelectionKey.OP_ACCEPT); System.out.println(\"Non-blocking server started on port \" + port); } /** * Event loop: process I/O events */ public void start() throws IOException { while (true) { // TODO: Wait for events (blocking call) selector.select(); Set<SelectionKey> selectedKeys = selector.selectedKeys(); Iterator<SelectionKey> iterator = selectedKeys.iterator(); while (iterator.hasNext()) { SelectionKey key = iterator.next(); iterator.remove(); // TODO: Handle different event types if (key.isAcceptable()) { handleAccept(key); } else if (key.isReadable()) { handleRead(key); } else if (key.isWritable()) { handleWrite(key); } } } } /** * Accept new connection */ private void handleAccept(SelectionKey key) throws IOException { ServerSocketChannel server = (ServerSocketChannel) key.channel(); SocketChannel client = server.accept(); if (client != null) { // TODO: Configure client socket as non-blocking client.configureBlocking(false); // TODO: Register for READ events client.register(selector, SelectionKey.OP_READ); System.out.println(\"Accepted connection from \" + client.getRemoteAddress()); } } /** * Read data from client */ private void handleRead(SelectionKey key) throws IOException { SocketChannel client = (SocketChannel) key.channel(); ByteBuffer buffer = ByteBuffer.allocate(1024); int bytesRead = client.read(buffer); if (bytesRead == -1) { // Connection closed by client client.close(); key.cancel(); return; } if (bytesRead > 0) { // TODO: Echo back (attach buffer to key for writing) buffer.flip(); key.attach(buffer); key.interestOps(SelectionKey.OP_WRITE); } } /** * Write data to client */ private void handleWrite(SelectionKey key) throws IOException { SocketChannel client = (SocketChannel) key.channel(); ByteBuffer buffer = (ByteBuffer) key.attachment(); client.write(buffer); if (!buffer.hasRemaining()) { // TODO: All data written, switch back to READ key.attach(null); key.interestOps(SelectionKey.OP_READ); } } } /** * Comparison: Blocking vs Non-Blocking */ static void compareApproaches() { System.out.println(\"Blocking I/O:\"); System.out.println(\" - One thread per connection\"); System.out.println(\" - 10,000 connections = 10,000 threads\"); System.out.println(\" - Thread overhead: ~1MB per thread = 10GB!\"); System.out.println(\" - Context switching overhead\"); System.out.println(); System.out.println(\"Non-Blocking I/O:\"); System.out.println(\" - Single thread with event loop\"); System.out.println(\" - 10,000 connections = 1 thread\"); System.out.println(\" - Memory: ~few MB total\"); System.out.println(\" - No context switching between I/O operations\"); System.out.println(\" - Perfect for I/O-bound workloads\"); System.out.println(); System.out.println(\"Trade-offs:\"); System.out.println(\" - Non-blocking: Complex callback-based code\"); System.out.println(\" - Non-blocking: Single thread = no CPU parallelism\"); System.out.println(\" - Non-blocking: Best for network I/O, not CPU work\"); } public static void main(String[] args) throws IOException { compareApproaches(); // Start echo server EchoServer server = new EchoServer(8080); server.start(); } } Key Insights: Event Loop: Single thread waits for I/O events using select() / epoll / kqueue Scalability: Handle 10K+ connections with 1 thread (C10K problem solution) No Context Switching: Eliminates thread scheduling overhead for I/O Limitation: Blocking operations (CPU work, DB calls) stall entire event loop When to Use: \u2705 High-concurrency network servers (chat, proxies, API gateways) \u2705 I/O-bound workloads (network, file I/O) \u2705 Need to minimize thread overhead \u274c CPU-intensive tasks (blocks event loop) \u274c Simple request-response patterns (thread-per-request easier) Pattern 6: Virtual Threads (Lightweight Concurrency) \u00b6 Concept: Millions of lightweight threads managed by JVM, not OS. Write blocking-style code with async performance. Use case: Modern Java services (Java 21+), high-concurrency applications, microservices. import java.util.concurrent.*; import java.time.Duration; /** * Virtual Threads Pattern (Java 21+ Project Loom) * * Key Concepts: * - Virtual threads: Lightweight, managed by JVM * - Platform threads: Traditional OS threads * - Carrier threads: OS threads that run virtual threads * - Continuation: Save/restore execution state * * Benefits: * - Write synchronous code, get async performance * - Million+ concurrent tasks without million threads * - No callback hell, readable code */ public class VirtualThreadsPattern { /** * Create and run virtual threads */ static class BasicVirtualThreads { /** * Traditional platform thread (heavyweight) */ public static void platformThreadExample() throws InterruptedException { Thread platformThread = Thread.ofPlatform() .name(\"platform-thread\") .start(() -> { System.out.println(\"Running on platform thread: \" + Thread.currentThread()); }); platformThread.join(); } /** * Virtual thread (lightweight) */ public static void virtualThreadExample() throws InterruptedException { Thread virtualThread = Thread.ofVirtual() .name(\"virtual-thread\") .start(() -> { System.out.println(\"Running on virtual thread: \" + Thread.currentThread()); System.out.println(\"Is virtual? \" + Thread.currentThread().isVirtual()); }); virtualThread.join(); } /** * Executor with virtual threads */ public static void virtualThreadExecutor() throws InterruptedException { // TODO: Create virtual thread executor try (ExecutorService executor = Executors.newVirtualThreadPerTaskExecutor()) { // Submit 10,000 tasks (would be expensive with platform threads!) for (int i = 0; i < 10_000; i++) { int taskId = i; executor.submit(() -> { try { Thread.sleep(Duration.ofMillis(100)); if (taskId % 1000 == 0) { System.out.println(\"Task \" + taskId + \" completed\"); } } catch (InterruptedException e) { Thread.currentThread().interrupt(); } }); } // Wait for completion (executor implements AutoCloseable) System.out.println(\"All 10,000 tasks submitted\"); } System.out.println(\"All tasks completed\"); } } /** * Blocking operations become non-blocking with virtual threads */ static class BlockingWithVirtualThreads { /** * Simulate HTTP call (blocking I/O) */ static String fetchData(String url) { try { // Simulated network call (blocking) Thread.sleep(Duration.ofMillis(100)); return \"Data from \" + url; } catch (InterruptedException e) { Thread.currentThread().interrupt(); return \"Error\"; } } /** * Make many concurrent HTTP calls */ public static void concurrentHttpCalls() throws InterruptedException { long start = System.currentTimeMillis(); try (ExecutorService executor = Executors.newVirtualThreadPerTaskExecutor()) { // Launch 1000 concurrent HTTP calls var futures = new ArrayList<Future<String>>(); for (int i = 0; i < 1000; i++) { Future<String> future = executor.submit(() -> fetchData(\"https://api.example.com/data\") ); futures.add(future); } // Wait for all results for (Future<String> future : futures) { try { future.get(); } catch (ExecutionException e) { e.printStackTrace(); } } } long elapsed = System.currentTimeMillis() - start; System.out.println(\"1000 HTTP calls completed in \" + elapsed + \"ms\"); System.out.println(\"With platform threads: would need 1000 threads (~1GB memory)\"); System.out.println(\"With virtual threads: ~few MB memory\"); } } /** * Structured Concurrency (Java 21+) */ static class StructuredConcurrencyExample { record User(String id, String name) {} record Order(String id, double amount) {} record Response(User user, Order order) {} /** * Fetch user and order concurrently */ public static Response fetchUserData(String userId) throws InterruptedException, ExecutionException { try (var scope = new StructuredTaskScope.ShutdownOnFailure()) { // Launch concurrent tasks Future<User> userFuture = scope.fork(() -> fetchUser(userId)); Future<Order> orderFuture = scope.fork(() -> fetchOrder(userId)); // Wait for both to complete (or first failure) scope.join(); scope.throwIfFailed(); // Both succeeded, get results return new Response(userFuture.resultNow(), orderFuture.resultNow()); } } static User fetchUser(String userId) throws InterruptedException { Thread.sleep(Duration.ofMillis(100)); return new User(userId, \"John Doe\"); } static Order fetchOrder(String userId) throws InterruptedException { Thread.sleep(Duration.ofMillis(150)); return new Order(\"order-123\", 99.99); } } /** * Performance comparison */ static void compareVirtualVsPlatform() { System.out.println(\"Platform Threads:\"); System.out.println(\" - OS-managed, 1:1 mapping to OS threads\"); System.out.println(\" - Heavy: ~1MB stack per thread\"); System.out.println(\" - Limited: Typically 1000-10000 threads max\"); System.out.println(\" - Context switch: expensive (OS scheduler)\"); System.out.println(); System.out.println(\"Virtual Threads:\"); System.out.println(\" - JVM-managed, M:N mapping to carrier threads\"); System.out.println(\" - Lightweight: Few KB per virtual thread\"); System.out.println(\" - Scalable: Million+ virtual threads possible\"); System.out.println(\" - Context switch: cheap (JVM scheduler)\"); System.out.println(\" - Blocking I/O: Automatically unmounts from carrier\"); System.out.println(); System.out.println(\"When Virtual Thread Blocks:\"); System.out.println(\" 1. Virtual thread calls blocking I/O\"); System.out.println(\" 2. JVM detects block, unmounts virtual thread\"); System.out.println(\" 3. Carrier thread freed to run other virtual threads\"); System.out.println(\" 4. When I/O completes, virtual thread remounts\"); } public static void main(String[] args) throws Exception { compareVirtualVsPlatform(); BasicVirtualThreads.virtualThreadExample(); BasicVirtualThreads.virtualThreadExecutor(); BlockingWithVirtualThreads.concurrentHttpCalls(); } } Key Insights: Write Blocking, Get Async: No callback hell, readable synchronous code Scalability: Handle million+ concurrent operations Automatic Yielding: JVM unmounts virtual threads during I/O Structured Concurrency: Cancel child tasks when parent fails When to Use: \u2705 Modern Java applications (21+) \u2705 High concurrency (thousands+ concurrent tasks) \u2705 Blocking I/O operations (HTTP, DB, file I/O) \u2705 Want simple, readable code without callbacks \u274c Java < 21 (use thread pools or reactive) \u274c CPU-bound tasks (use platform threads + parallelism) Pattern 7: Coroutines (Cooperative Multitasking) \u00b6 Concept: Suspendable functions that can pause and resume execution. Lightweight concurrency without threads. Use case: Kotlin backends, Python async/await, Go services. import kotlinx.coroutines.* import kotlin.system.measureTimeMillis /** * Coroutines Pattern (Kotlin) * * Key Concepts: * - suspend function: Can pause and resume * - CoroutineScope: Manages coroutine lifecycle * - Dispatcher: Determines which thread runs coroutine * - Structured concurrency: Parent waits for children * * Coroutine vs Thread: * - Coroutine: Lightweight (bytes), million+ possible * - Thread: Heavyweight (MB), thousands max */ /** * Basic coroutine examples */ object BasicCoroutines { /** * Simple suspend function */ suspend fun fetchData(): String { delay(1000) // Suspends coroutine, doesn't block thread return \"Data fetched\" } /** * Launch coroutine */ fun launchExample() = runBlocking { println(\"Main program starts: ${Thread.currentThread().name}\") // Launch coroutine (fire and forget) launch { println(\"Coroutine starts: ${Thread.currentThread().name}\") delay(500) println(\"Coroutine ends\") } println(\"Main program continues\") delay(1000) // Wait for coroutine println(\"Main program ends\") } /** * Async/await for results */ fun asyncExample() = runBlocking { val deferred = async { delay(1000) \"Result from async\" } println(\"Doing other work...\") val result = deferred.await() // Suspends until result ready println(\"Got result: $result\") } /** * Structured concurrency */ fun structuredExample() = runBlocking { println(\"Parent starts\") coroutineScope { launch { delay(500) println(\"Child 1 completes\") } launch { delay(1000) println(\"Child 2 completes\") } println(\"Waiting for children...\") } // Suspends until all children complete println(\"All children completed, parent continues\") } } /** * Concurrent HTTP calls with coroutines */ object ConcurrentRequests { suspend fun fetchUser(id: Int): String { delay(100) // Simulate HTTP call return \"User $id\" } /** * Sequential vs concurrent fetching */ fun compareSequentialVsConcurrent() = runBlocking { // Sequential: 10 * 100ms = 1000ms val sequentialTime = measureTimeMillis { repeat(10) { fetchUser(it) } } println(\"Sequential: ${sequentialTime}ms\") // Concurrent: max(100ms) = 100ms val concurrentTime = measureTimeMillis { val deferreds = (0 until 10).map { id -> async { fetchUser(id) } } deferreds.awaitAll() // Wait for all } println(\"Concurrent: ${concurrentTime}ms\") } /** * Concurrent with limit (semaphore) */ fun concurrentWithLimit() = runBlocking { val semaphore = Semaphore(3) // Max 3 concurrent requests val results = (0 until 10).map { id -> async { semaphore.withPermit { println(\"Fetching user $id\") fetchUser(id) } } } results.awaitAll() println(\"All fetched with max 3 concurrent\") } } /** * Dispatchers: Control execution context */ object DispatcherExamples { fun demonstrateDispatchers() = runBlocking { // Default: Shared thread pool for CPU-intensive work launch(Dispatchers.Default) { println(\"CPU work: ${Thread.currentThread().name}\") } // IO: Optimized for blocking I/O operations launch(Dispatchers.IO) { println(\"I/O work: ${Thread.currentThread().name}\") // Good for: File I/O, network calls, database } // Main: UI thread (Android) // launch(Dispatchers.Main) { } // Unconfined: Starts in caller thread, resumes in any thread launch(Dispatchers.Unconfined) { println(\"Unconfined: ${Thread.currentThread().name}\") delay(100) println(\"After delay: ${Thread.currentThread().name}\") // May change } delay(200) } } /** * Error handling and cancellation */ object ErrorHandling { suspend fun fetchWithRetry(id: Int, maxRetries: Int = 3): String { repeat(maxRetries) { attempt -> try { delay(100) if (attempt < 2) throw Exception(\"Network error\") return \"User $id fetched on attempt ${attempt + 1}\" } catch (e: Exception) { if (attempt == maxRetries - 1) throw e println(\"Retry $attempt for user $id\") } } error(\"Unreachable\") } fun cancellationExample() = runBlocking { val job = launch { repeat(1000) { i -> if (!isActive) return@launch // Check cancellation println(\"Working $i...\") delay(100) } } delay(500) println(\"Cancelling...\") job.cancel() // Cancel coroutine job.join() // Wait for cancellation println(\"Cancelled\") } fun timeoutExample() = runBlocking { try { withTimeout(1000) { // Timeout after 1 second repeat(10) { println(\"Working...\") delay(200) } } } catch (e: TimeoutCancellationException) { println(\"Timed out!\") } } } /** * Comparison: Coroutines vs Threads */ fun compareCoroutinesVsThreads() { println(\"Threads:\") println(\" - Preemptive: OS scheduler decides when to switch\") println(\" - Heavy: ~1MB per thread\") println(\" - Blocking: Thread blocked during I/O\") println(\" - Limited: Thousands max\") println() println(\"Coroutines:\") println(\" - Cooperative: Coroutine voluntarily suspends (delay, await)\") println(\" - Lightweight: Few bytes per coroutine\") println(\" - Non-blocking: Thread freed during suspend\") println(\" - Scalable: Million+ coroutines possible\") println() println(\"Python async/await:\") println(\" - Similar to Kotlin coroutines\") println(\" - Single-threaded event loop\") println(\" - await keyword suspends coroutine\") println() println(\"Go goroutines:\") println(\" - Lighter than threads, heavier than coroutines\") println(\" - Preemptive (Go 1.14+)\") println(\" - Million+ goroutines possible\") } fun main() { compareCoroutinesVsThreads() BasicCoroutines.launchExample() BasicCoroutines.asyncExample() ConcurrentRequests.compareSequentialVsConcurrent() } Python async/await equivalent: import asyncio import aiohttp from typing import List async def fetch_user(session, user_id: int) -> dict: \"\"\"Suspendable function (coroutine)\"\"\" async with session.get(f'https://api.example.com/users/{user_id}') as response: return await response.json() # await suspends coroutine async def fetch_all_users(user_ids: List[int]): \"\"\"Concurrent HTTP requests\"\"\" async with aiohttp.ClientSession() as session: # Create tasks (coroutines scheduled on event loop) tasks = [fetch_user(session, uid) for uid in user_ids] # Wait for all to complete results = await asyncio.gather(*tasks) return results # Run event loop users = asyncio.run(fetch_all_users([1, 2, 3, 4, 5])) Key Insights: Suspend/Resume: Functions can pause without blocking threads Event Loop: Single thread manages many coroutines (like Node.js) Structured Concurrency: Parent scope waits for children Cancellation: Cancel entire hierarchy with one call When to Use: \u2705 Kotlin/Python/Go applications \u2705 High concurrency with I/O operations \u2705 Want readable async code (no callback hell) \u2705 Mobile apps (Android Kotlin) \u274c Java (use virtual threads instead) \u274c Need true parallelism for CPU work (use threads) Pattern 8: Reactive Streams (Backpressure-Aware) \u00b6 Concept: Asynchronous stream processing with backpressure control. Publisher produces data, subscriber consumes with flow control. Use case: Real-time data pipelines, streaming APIs, event-driven systems. import reactor.core.publisher.*; import reactor.core.scheduler.Schedulers; import java.time.Duration; import java.util.concurrent.atomic.AtomicInteger; /** * Reactive Streams Pattern (Project Reactor) * * Key Concepts: * - Publisher: Produces data (Flux = 0..N, Mono = 0..1) * - Subscriber: Consumes data * - Backpressure: Subscriber controls production rate * - Operators: Transform, filter, combine streams * * Benefits: * - Non-blocking, event-driven * - Backpressure prevents overwhelming consumers * - Composable async operations */ public class ReactiveStreamsPattern { /** * Basic Flux and Mono */ static class BasicReactive { /** * Flux: 0 to N elements */ public static void fluxExample() { Flux<Integer> flux = Flux.range(1, 5); flux.subscribe( item -> System.out.println(\"Received: \" + item), error -> System.err.println(\"Error: \" + error), () -> System.out.println(\"Complete\") ); } /** * Mono: 0 or 1 element */ public static void monoExample() { Mono<String> mono = Mono.just(\"Hello Reactive\"); mono.subscribe(System.out::println); } /** * Create Flux from various sources */ public static void createFlux() { // From collection Flux<String> fromList = Flux.fromIterable( java.util.List.of(\"A\", \"B\", \"C\") ); // From range Flux<Integer> range = Flux.range(1, 10); // From interval (infinite stream) Flux<Long> interval = Flux.interval(Duration.ofMillis(100)); // Custom generator Flux<Integer> generated = Flux.generate( () -> 0, // Initial state (state, sink) -> { sink.next(state); if (state == 10) sink.complete(); return state + 1; } ); } } /** * Stream operators: transform, filter, combine */ static class Operators { /** * Transform operators */ public static void transformOperators() { Flux.range(1, 5) .map(x -> x * 2) // Transform each element .filter(x -> x > 5) // Filter elements .take(2) // Take first 2 .subscribe(System.out::println); // Output: 6, 8 } /** * FlatMap: Async transformation (1 \u2192 many) */ public static void flatMapExample() { Flux.range(1, 3) .flatMap(id -> fetchUser(id)) // Concurrent calls .subscribe(user -> System.out.println(\"User: \" + user)); } static Mono<String> fetchUser(int id) { return Mono.delay(Duration.ofMillis(100)) .map(__ -> \"User\" + id); } /** * Combining streams */ public static void combineStreams() { Flux<Integer> flux1 = Flux.range(1, 3); Flux<Integer> flux2 = Flux.range(10, 3); // Merge: Interleave both streams Flux.merge(flux1, flux2) .subscribe(System.out::println); // Zip: Combine corresponding elements Flux.zip(flux1, flux2, (a, b) -> a + b) .subscribe(sum -> System.out.println(\"Sum: \" + sum)); // Output: 11, 13, 15 } /** * Error handling */ public static void errorHandling() { Flux.range(1, 5) .map(i -> { if (i == 3) throw new RuntimeException(\"Error at 3\"); return i; }) .onErrorReturn(-1) // Fallback value .subscribe(System.out::println); // Output: 1, 2, -1 // Retry Flux.range(1, 3) .map(i -> { if (i < 3) throw new RuntimeException(\"Retry me\"); return i; }) .retry(2) // Retry up to 2 times .subscribe( System.out::println, err -> System.err.println(\"Failed: \" + err) ); } } /** * Backpressure: Subscriber controls production rate */ static class BackpressureExample { /** * Without backpressure: Overflow */ public static void noBackpressure() { Flux.range(1, 1000) .doOnNext(i -> System.out.println(\"Producing: \" + i)) .subscribeOn(Schedulers.parallel()) .publishOn(Schedulers.single()) .subscribe( i -> { // Slow consumer try { Thread.sleep(10); } catch (InterruptedException e) {} System.out.println(\"Consuming: \" + i); }, Throwable::printStackTrace ); // Problem: Producer overwhelms consumer! } /** * With backpressure: Controlled flow */ public static void withBackpressure() { Flux.range(1, 1000) .doOnNext(i -> System.out.println(\"Producing: \" + i)) .onBackpressureBuffer(10) // Buffer up to 10 items .subscribeOn(Schedulers.parallel()) .publishOn(Schedulers.single()) .subscribe( i -> { try { Thread.sleep(10); } catch (InterruptedException e) {} System.out.println(\"Consuming: \" + i); } ); // Producer waits when buffer full } /** * Backpressure strategies */ public static void backpressureStrategies() { Flux<Integer> fast = Flux.range(1, 100); // Buffer: Store items in queue fast.onBackpressureBuffer(10); // Drop: Drop new items when overwhelmed fast.onBackpressureDrop(); // Latest: Keep only latest item fast.onBackpressureLatest(); // Error: Fail when overwhelmed fast.onBackpressureError(); } } /** * Schedulers: Control execution context */ static class SchedulerExamples { public static void demonstrateSchedulers() { // Immediate: Current thread Flux.range(1, 3) .subscribeOn(Schedulers.immediate()) .subscribe(i -> System.out.println(\"Immediate: \" + Thread.currentThread().getName())); // Single: Single reusable thread Flux.range(1, 3) .subscribeOn(Schedulers.single()) .subscribe(i -> System.out.println(\"Single: \" + Thread.currentThread().getName())); // Parallel: Fixed pool for CPU-bound work Flux.range(1, 3) .subscribeOn(Schedulers.parallel()) .subscribe(i -> System.out.println(\"Parallel: \" + Thread.currentThread().getName())); // Bounded Elastic: Bounded pool for blocking I/O Flux.range(1, 3) .subscribeOn(Schedulers.boundedElastic()) .subscribe(i -> System.out.println(\"BoundedElastic: \" + Thread.currentThread().getName())); } } /** * Real-world example: HTTP API with reactive streams */ static class ReactiveAPI { record User(int id, String name) {} record Order(int id, int userId, double amount) {} record UserWithOrders(User user, java.util.List<Order> orders) {} /** * Fetch user with orders (reactive) */ public static Mono<UserWithOrders> fetchUserWithOrders(int userId) { Mono<User> userMono = fetchUser(userId); Flux<Order> ordersFlux = fetchOrders(userId); return Mono.zip( userMono, ordersFlux.collectList(), UserWithOrders::new ); } static Mono<User> fetchUser(int userId) { return Mono.delay(Duration.ofMillis(100)) .map(__ -> new User(userId, \"User\" + userId)); } static Flux<Order> fetchOrders(int userId) { return Flux.range(1, 3) .delayElements(Duration.ofMillis(50)) .map(i -> new Order(i, userId, i * 10.0)); } /** * Process stream of users */ public static Flux<UserWithOrders> processUserStream(Flux<Integer> userIds) { return userIds .flatMap(ReactiveAPI::fetchUserWithOrders, 5) // Max 5 concurrent .onErrorContinue((err, val) -> System.err.println(\"Error processing \" + val + \": \" + err) ); } } /** * Comparison: Reactive vs Imperative */ static void compareApproaches() { System.out.println(\"Imperative (Blocking):\"); System.out.println(\" - Sequential execution\"); System.out.println(\" - Thread blocked during I/O\"); System.out.println(\" - Simple to understand\"); System.out.println(\" - Limited concurrency\"); System.out.println(); System.out.println(\"Reactive (Non-blocking):\"); System.out.println(\" - Asynchronous execution\"); System.out.println(\" - Threads not blocked\"); System.out.println(\" - Backpressure control\"); System.out.println(\" - High concurrency\"); System.out.println(\" - Steeper learning curve\"); System.out.println(); System.out.println(\"When to use Reactive:\"); System.out.println(\" \u2713 High-concurrency services\"); System.out.println(\" \u2713 Streaming data pipelines\"); System.out.println(\" \u2713 Event-driven architectures\"); System.out.println(\" \u2713 Need backpressure control\"); System.out.println(\" \u2717 Simple CRUD APIs (overkill)\"); System.out.println(\" \u2717 Team unfamiliar with reactive\"); } public static void main(String[] args) throws InterruptedException { compareApproaches(); BasicReactive.fluxExample(); Operators.transformOperators(); BackpressureExample.withBackpressure(); Thread.sleep(2000); // Wait for async operations } } Key Insights: Backpressure: Consumer tells producer to slow down Non-blocking: Threads freed during I/O operations Composable: Chain operators to build complex pipelines Error Handling: Resilient with retry, fallback strategies When to Use: \u2705 Streaming data pipelines (Kafka, websockets) \u2705 High-concurrency microservices (Spring WebFlux) \u2705 Event-driven architectures \u2705 Need backpressure control \u274c Simple REST APIs (virtual threads simpler) \u274c Team unfamiliar with reactive (steep learning curve) \u274c Blocking libraries (defeats the purpose) Debugging Challenges \u00b6 Your task: Find and fix concurrency bugs in broken implementations. This tests your understanding of thread safety. Challenge 1: Lost Updates (Race Condition) \u00b6 /** * This bank account has a race condition. * Two threads transferring money simultaneously can cause lost updates. */ public class BrokenBankAccount { private int balance = 1000; public boolean withdraw(int amount) { if (balance >= amount) { // What if another thread withdraws here? balance -= amount; return true; } return false; } public int getBalance() { return balance; } } Your debugging: Bug location: [Which lines?] Bug explanation: [What can go wrong?] Scenario: Thread A and B both call withdraw(600) when balance=1000 Expected: One succeeds, one fails, final balance = 400 Actual: [What happens?] Final balance could be: [Fill in] Bug fix: [How to make it thread-safe?] Click to verify your answer Bug: Check-then-act race condition. The check balance >= amount and the action balance -= amount are not atomic. Scenario trace: Thread A: check balance (1000) >= 600 \u2713 Thread B: check balance (1000) >= 600 \u2713 Thread A: balance = 1000 - 600 = 400 Thread B: balance = 400 - 600 = -200 \u2190 OVERDRAFT! Fix Option 1 - Synchronized: public synchronized boolean withdraw(int amount) { if (balance >= amount) { balance -= amount; return true; } return false; } Fix Option 2 - ReentrantLock: private final ReentrantLock lock = new ReentrantLock(); public boolean withdraw(int amount) { lock.lock(); try { if (balance >= amount) { balance -= amount; return true; } return false; } finally { lock.unlock(); } } Challenge 2: Deadlock (Lock Ordering) \u00b6 /** * This code can deadlock! * Thread 1: transfer(acc1, acc2, 100) * Thread 2: transfer(acc2, acc1, 50) */ public class DeadlockTransfer { static class Account { int balance; final ReentrantLock lock = new ReentrantLock(); } public static boolean transfer(Account from, Account to, int amount) { from.lock.lock(); // Thread 1 locks acc1 try { to.lock.lock(); // Thread 1 tries to lock acc2 // BUT Thread 2 already locked acc2! try { if (from.balance >= amount) { from.balance -= amount; to.balance += amount; return true; } return false; } finally { to.lock.unlock(); } } finally { from.lock.unlock(); } } } Your debugging: Bug explanation: [Why does deadlock occur?] Deadlock scenario: Thread 1: Holds lock on ___, waiting for lock on ___ Thread 2: Holds lock on ___, waiting for lock on ___ Result: [Both threads stuck forever] Bug fix: [What's the solution?] Click to verify your answer Bug: Circular lock dependency causes deadlock. Deadlock scenario: Thread 1: transfer(acc1, acc2, 100) - Locks acc1 \u2713 - Tries to lock acc2... WAITS Thread 2: transfer(acc2, acc1, 50) - Locks acc2 \u2713 - Tries to lock acc1... WAITS DEADLOCK! Both threads waiting for each other. Fix - Consistent lock ordering: public static boolean transfer(Account from, Account to, int amount) { // Always lock in consistent order (e.g., by account ID) Account first = from.id < to.id ? from : to; Account second = from.id < to.id ? to : from; first.lock.lock(); try { second.lock.lock(); try { if (from.balance >= amount) { from.balance -= amount; to.balance += amount; return true; } return false; } finally { second.lock.unlock(); } } finally { first.lock.unlock(); } } Why it works: All threads acquire locks in the same order (by ID), preventing circular wait. Challenge 3: Visibility Problem (Missing volatile) \u00b6 /** * This shutdown mechanism might not work! * Worker thread may never see the updated flag. */ public class BrokenShutdown { private boolean stopRequested = false; // Background worker thread public void backgroundWork() { long count = 0; while (!stopRequested) { // May never see true! count++; // CPU might cache stopRequested = false forever } System.out.println(\"Stopped after \" + count + \" iterations\"); } // Main thread public void requestStop() { stopRequested = true; // Write might not be visible! } } Your debugging: Bug location: [Which line?] Bug explanation: [What's the visibility problem?] Why does it fail? [CPU caching? Compiler optimization?] Symptoms: Expected: Worker stops immediately when requestStop() is called Actual: [What happens?] Bug fix: [How to ensure visibility?] Click to verify your answer Bug: Missing volatile keyword causes visibility problem. Why it fails: Each CPU core has its own cache Worker thread caches stopRequested = false Main thread writes stopRequested = true to its cache Worker thread never sees the update (reads from its stale cache) Loop runs forever! Fix Option 1 - volatile: private volatile boolean stopRequested = false; volatile ensures writes are visible to all threads Prevents CPU caching Adds memory barrier (flush to main memory) Fix Option 2 - synchronized: private boolean stopRequested = false; public synchronized void requestStop() { stopRequested = true; } public synchronized boolean isStopRequested() { return stopRequested; } public void backgroundWork() { long count = 0; while (!isStopRequested()) { count++; } } Fix Option 3 - AtomicBoolean: private final AtomicBoolean stopRequested = new AtomicBoolean(false); public void backgroundWork() { long count = 0; while (!stopRequested.get()) { count++; } } public void requestStop() { stopRequested.set(true); } Challenge 4: Thread Pool Starvation \u00b6 /** * This code can cause thread pool starvation! * All threads block waiting for tasks that can't execute. */ public class StarvationExample { private final ExecutorService executor = Executors.newFixedThreadPool(2); public void processWithSubtasks() throws Exception { executor.submit(() -> { System.out.println(\"Task 1: Starting\"); // This task submits subtasks and waits for them Future<String> subtask1 = executor.submit(() -> \"Subtask 1\"); Future<String> subtask2 = executor.submit(() -> \"Subtask 2\"); try { subtask1.get(); // Waits forever if pool is full subtask2.get(); } catch (Exception e) { e.printStackTrace(); } System.out.println(\"Task 1: Done\"); }); executor.submit(() -> { System.out.println(\"Task 2: Starting\"); Future<String> subtask3 = executor.submit(() -> \"Subtask 3\"); Future<String> subtask4 = executor.submit(() -> \"Subtask 4\"); try { subtask3.get(); subtask4.get(); } catch (Exception e) { e.printStackTrace(); } System.out.println(\"Task 2: Done\"); }); } } Your debugging: Bug explanation: [Why does it deadlock?] Thread pool state: Pool size: 2 threads Thread 1 executing: ___ Thread 2 executing: ___ Queued subtasks: ___ Why can't subtasks run? [Fill in] Bug fix: [How to prevent starvation?] Click to verify your answer Bug: Thread pool starvation - workers block waiting for work that can't execute. Deadlock scenario: Pool size: 2 threads Thread 1: Executing Task 1, blocked on subtask1.get() Thread 2: Executing Task 2, blocked on subtask3.get() Queue: [subtask1, subtask2, subtask3, subtask4] All workers blocked, no thread available to run queued subtasks! DEADLOCK! Fix Option 1 - Separate thread pools: private final ExecutorService mainExecutor = Executors.newFixedThreadPool(2); private final ExecutorService subtaskExecutor = Executors.newFixedThreadPool(4); // Submit main tasks to mainExecutor // Submit subtasks to subtaskExecutor Fix Option 2 - Larger pool: // Pool must be large enough for all parallel tasks + their subtasks private final ExecutorService executor = Executors.newFixedThreadPool(10); Fix Option 3 - Don't block on subtasks in worker thread: // Use callbacks/CompletableFuture instead of blocking get() CompletableFuture.supplyAsync(() -> \"Subtask 1\", executor) .thenCombine( CompletableFuture.supplyAsync(() -> \"Subtask 2\", executor), (s1, s2) -> s1 + s2 ) .thenAccept(result -> System.out.println(result)); Challenge 5: ABA Problem (Lock-Free Bug) \u00b6 /** * This lock-free stack has the ABA problem! * CAS can succeed incorrectly when value cycles back. */ public class ABAStack<T> { private static class Node<T> { final T value; Node<T> next; Node(T value) { this.value = value; } } private final AtomicReference<Node<T>> head = new AtomicReference<>(); public void push(T value) { Node<T> newNode = new Node<>(value); while (true) { Node<T> current = head.get(); newNode.next = current; if (head.compareAndSet(current, newNode)) { return; } } } public T pop() { while (true) { Node<T> current = head.get(); // Read A if (current == null) return null; Node<T> next = current.next; // Between here and CAS, another thread could: // 1. Pop A (head = B) // 2. Pop B (head = C) // 3. Push A back (head = A again!) // Now CAS succeeds but we're pointing to recycled node! if (head.compareAndSet(current, next)) { // CAS sees A == A \u2713 return current.value; } } } } Your debugging: Bug explanation: [What is the ABA problem?] ABA scenario: Thread 1 reads head = A Thread 2 pops A, pops B, pushes A back Thread 1's CAS succeeds (A == A) but... Problem: [What's wrong?] When does this cause issues? [What if nodes are reused?] Bug fix: [How to prevent ABA?] Click to verify your answer Bug: ABA problem - CAS succeeds when it shouldn't because value cycled back. ABA scenario: Initial: head -> A -> B -> C Thread 1: Read head = A, next = B (Gets preempted before CAS) Thread 2: - Pop A (head = B) - Pop B (head = C) - Push A back (head = A, but A.next = C now!) Thread 1 resumes: - CAS(head, A, B) succeeds! (head was A) - But now head points to B, which was already popped! - B might be reused/freed \u2192 CORRUPTION! Fix Option 1 - AtomicStampedReference: // Add version number to prevent ABA private final AtomicStampedReference<Node<T>> head = new AtomicStampedReference<>(null, 0); public void push(T value) { Node<T> newNode = new Node<>(value); int[] stampHolder = new int[1]; while (true) { Node<T> current = head.get(stampHolder); int stamp = stampHolder[0]; newNode.next = current; if (head.compareAndSet(current, newNode, stamp, stamp + 1)) { return; } } } Fix Option 2 - Hazard pointers (prevent reuse while in use) Fix Option 3 - Use Java's ConcurrentLinkedStack (already handles this) Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found race condition causing lost updates Found deadlock due to lock ordering Found visibility problem (missing volatile) Found thread pool starvation Understood ABA problem in lock-free algorithms Could explain each bug's root cause Learned how to prevent each type of bug Common concurrency bugs you discovered: [Race conditions: read-modify-write, check-then-act] [Deadlocks: circular lock dependencies] [Visibility: CPU caching, missing volatile/synchronized] [Starvation: blocking on tasks in same pool] [ABA problem: CAS with recycled values] How to prevent these bugs: [Use locks for compound operations] [Always acquire locks in consistent order] [Use volatile for flags, Atomic for counters] [Separate thread pools for dependent tasks] [Use stamped references or hazard pointers] Decision Framework \u00b6 Your task: Build decision trees for when to use each concurrency pattern. Question 1: When to use locks vs lock-free algorithms? \u00b6 Answer after implementation: Use Locks when: Complex operations: [Multiple steps that must be atomic] Simple reasoning: [Lock-based code is easier to understand] Fairness needed: [Locks can guarantee FIFO ordering] Most use cases: [Locks are sufficient for 90% of scenarios] Use Lock-Free when: High contention: [Many threads competing for same resource] Low-latency critical: [Cannot afford lock overhead] Progress guarantees: [At least one thread makes progress] Simple operations: [Increment, swap, stack push/pop] Question 2: Which thread pool to use? \u00b6 FixedThreadPool when: Known workload: [Consistent number of tasks] Resource limiting: [Don't want unbounded thread creation] CPU-bound tasks: [Pool size = CPU cores] CachedThreadPool when: Unpredictable load: [Varying number of tasks] I/O-bound tasks: [Threads spend time waiting] Short-lived tasks: [Quick execution, many tasks] ScheduledThreadPool when: Delayed execution: [Run after delay] Periodic tasks: [Cron-like scheduling] Background jobs: [Cleanup, monitoring, etc.] ForkJoinPool when: Recursive tasks: [Divide-and-conquer algorithms] Parallel algorithms: [Parallel sort, sum, map-reduce] Work stealing: [Balance load across threads] Question 3: Synchronized vs ReentrantLock? \u00b6 Synchronized when: Simple use case: [Just need mutual exclusion] Less code: [synchronized(this) { ... }] No advanced features needed: [Try-lock, interruptible, timeouts not needed] ReentrantLock when: Try-lock needed: [Attempt lock without blocking] Timeouts: [Give up after waiting] Interruptible: [Can interrupt waiting thread] Fairness: [FIFO lock acquisition] Condition variables: [Complex waiting conditions] Question 4: When to use async/non-blocking patterns? \u00b6 Use Non-Blocking I/O (Event Loop) when: Extreme concurrency: [10K+ connections] I/O-bound: [Network I/O, minimal CPU work] Single-threaded efficiency: [Want event loop pattern] Examples: [Chat servers, proxies, API gateways] Use Virtual Threads (Java 21+) when: Modern Java: [Java 21+] High concurrency + readability: [Want blocking-style code] Blocking I/O: [HTTP, database, file I/O] Avoid callback hell: [Simpler than reactive] Use Coroutines (Kotlin/Python/Go) when: Kotlin/Python/Go: [Language supports coroutines] Async/await model: [Want structured concurrency] Mobile apps: [Android Kotlin] Lightweight concurrency: [Million+ concurrent operations] Use Reactive Streams when: Streaming pipelines: [Kafka, WebSockets, event streams] Backpressure critical: [Prevent consumer overload] Complex transformations: [Chain operators, error handling] Spring WebFlux: [Reactive microservices] Question 5: Traditional Threads vs Modern Async? \u00b6 Platform Threads when: True parallelism: [CPU-bound work] Simple requirements: [Few concurrent tasks] Blocking libraries: [Can't avoid blocking calls] Thread count acceptable: [< 1000 threads] Virtual Threads/Coroutines when: High concurrency: [Thousands+ operations] I/O-bound: [Waiting for network/disk] Modern platforms: [Java 21+, Kotlin, Go] Readable code: [No callbacks needed] Event Loop when: Extreme scale: [10K+ connections] JavaScript-style: [Node.js, single-threaded] Callback acceptable: [Team comfortable with callbacks] Reactive when: Streaming data: [Real-time pipelines] Backpressure: [Producer-consumer rate control] Complex flows: [Many transformations] Team expertise: [Learning curve steep] Your Decision Tree \u00b6 Build this after solving practice scenarios: flowchart LR Start[\"Concurrency Pattern Selection\"] Q1{\"What's the workload?\"} Start --> Q1 N2[\"Use thread pool\"] Q1 -->|\"Many independent tasks\"| N2 N3[\"Need synchronization\"] Q1 -->|\"Shared mutable state\"| N3 N4[\"Use BlockingQueue\"] Q1 -->|\"Message passing\"| N4 Q5{\"Need synchronization?\"} Start --> Q5 N6[\"synchronized\"] Q5 -->|\"Simple critical section\"| N6 N7[\"ReentrantLock\"] Q5 -->|\"Advanced features\"| N7 N8[\"ReadWriteLock\"] Q5 -->|\"Read-heavy\"| N8 N9[\"Lock-free<br/>(Atomic)\"] Q5 -->|\"High contention\"| N9 Q10{\"Thread pool sizing?\"} Start --> Q10 N11[\"cores + 1\"] Q10 -->|\"CPU-bound\"| N11 N12[\"cores * 2 or more\"] Q10 -->|\"I/O-bound\"| N12 N13[\"Test and measure\"] Q10 -->|\"Mixed\"| N13 Q14{\"Queue sizing?\"} Start --> Q14 N15[\"Prevent memory exhaustion\"] Q14 -->|\"Bounded\"| N15 N16[\"Risk OOM, but never blocks producers\"] Q14 -->|\"Unbounded\"| N16 N17[\"Direct handoff, no buffering\"] Q14 -->|\"SynchronousQueue\"| N17 Practice \u00b6 Scenario 1: Web Server Request Handler \u00b6 Requirements: Handle 10,000 concurrent HTTP requests Each request: parse, validate, database query, response Database pool: 20 connections max CPU cores: 8 Must handle bursts (spike to 50K requests) Your design: Thread pool configuration: Pool type: [Fixed, Cached, or Custom?] Core threads: [How many?] Max threads: [How many?] Queue size: [Bounded or unbounded?] Rejection policy: [What happens when queue full?] Reasoning: [Why this pool type?] [How did you calculate thread counts?] [How does it handle bursts?] [What about database connection pool coordination?] Scenario 2: Real-Time Analytics Pipeline \u00b6 Requirements: Ingest 1M events/second Processing stages: validate \u2192 enrich \u2192 aggregate \u2192 store Each stage is CPU-intensive (10ms per event) Must maintain order within same user_id Latency target: p99 < 100ms Your design: Pipeline architecture: Number of stages: [How many?] Queue between stages: [Bounded or unbounded?] Threads per stage: [How many?] Backpressure handling: [What to do when backed up?] Ordering guarantee: How to maintain order per user_id: [Your approach] Trade-off: [Ordering vs throughput] Scenario 3: Distributed Cache \u00b6 Requirements: In-memory cache with 1M entries Operations: get (90%), put (9%), delete (1%) Concurrent access: 1000 threads Must track hit rate, eviction stats LRU eviction policy Your design: Data structure: Base structure: [ConcurrentHashMap? Why?] Synchronization: [Where do you need locks?] Statistics tracking: [Atomic variables?] LRU implementation: How to track access order: [Your approach] Thread-safe eviction: [How to handle concurrent evictions?] Read-write coordination: [ReadWriteLock? Why or why not?] Trade-offs: [Accuracy vs performance] [Lock-free vs locked] [Memory overhead] Review Checklist \u00b6 Before moving to the next topic: Implementation ReentrantLock counter works correctly ReadWriteLock cache works correctly Bank transfer with lock ordering prevents deadlocks Producer-consumer with BlockingQueue works ConcurrentHashMap cache operations work Lock-free counter with CAS works Thread pool executors work correctly Non-blocking echo server handles concurrent connections Virtual threads handle 10K+ concurrent operations All client code runs successfully Understanding Filled in all ELI5 explanations Understand difference between synchronized and ReentrantLock Understand read-write lock semantics Understand why lock ordering prevents deadlocks Understand BlockingQueue blocking behavior Understand CAS and ABA problem Understand thread pool sizing principles Understand event loop vs threaded I/O Understand virtual threads vs platform threads Understand coroutine suspend/resume mechanism Understand reactive backpressure Concurrency Principles Always release locks in finally blocks Lock ordering prevents deadlocks Use bounded queues to prevent OOM Properly handle InterruptedException Shutdown thread pools gracefully Prefer higher-level abstractions (BlockingQueue, Atomic) Measure contention before optimizing Choose async pattern based on workload Event loops don't block on CPU work Virtual threads excellent for blocking I/O Decision Making Know when to use locks vs lock-free Know how to size thread pools Know when to use event loops vs threads Know when virtual threads better than reactive Know when coroutines appropriate Know when reactive streams needed Completed practice scenarios Can explain synchronization trade-offs Can explain async pattern trade-offs Mastery Check Could implement thread-safe cache from memory Could design thread pool configuration for given workload Could build non-blocking server with NIO Could migrate app to virtual threads Understand concurrency bugs (deadlock, race conditions, starvation) Know how to debug concurrency issues Can choose appropriate async pattern for requirements Mastery Certification \u00b6 I certify that I can: Implement thread-safe data structures with locks Implement lock-free algorithms with CAS Design producer-consumer pipelines with BlockingQueue Configure and use thread pools appropriately Build non-blocking I/O servers with NIO Use virtual threads for high-concurrency applications Write coroutines with async/await patterns Build reactive streams with backpressure control Debug race conditions, deadlocks, and visibility issues Size thread pools based on workload characteristics Choose between sync and async patterns for requirements Compare trade-offs: locks vs lock-free vs immutable Compare trade-offs: threads vs virtual threads vs event loops vs reactive Explain concurrency concepts to others","title":"10. Concurrency Patterns"},{"location":"systems/10-concurrency-patterns/#concurrency-patterns","text":"Locks, thread pools, synchronization, and async patterns - From threads to coroutines to reactive streams","title":"Concurrency Patterns"},{"location":"systems/10-concurrency-patterns/#eli5-explain-like-im-5","text":"Your task: After implementing concurrency patterns, explain them simply. Prompts to guide you: What is a lock in one sentence? Your answer: [Fill in after implementation] Why do we need locks in concurrent programs? Your answer: [Fill in after implementation] Real-world analogy for ReentrantLock: Example: \"A ReentrantLock is like a bathroom key that you can use multiple times...\" Your analogy: [Fill in] What is a thread pool in one sentence? Your answer: [Fill in after implementation] Why use a thread pool instead of creating threads directly? Your answer: [Fill in after implementation] Real-world analogy for BlockingQueue: Example: \"A BlockingQueue is like a conveyor belt in a factory...\" Your analogy: [Fill in] What is an event loop in one sentence? Your answer: [Fill in after learning] How are virtual threads different from platform threads? Your answer: [Fill in after learning] Real-world analogy for virtual threads: Example: \"Virtual threads are like lightweight workers that...\" Your analogy: [Fill in] What is backpressure in reactive streams? Your answer: [Fill in after learning]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/10-concurrency-patterns/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition about concurrency without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/10-concurrency-patterns/#beforeafter-why-these-patterns-matter","text":"Your task: Compare unsafe vs synchronized vs lock-free approaches to understand the impact.","title":"Before/After: Why These Patterns Matter"},{"location":"systems/10-concurrency-patterns/#case-studies-concurrency-in-the-wild","text":"","title":"Case Studies: Concurrency in the Wild"},{"location":"systems/10-concurrency-patterns/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/10-concurrency-patterns/#debugging-challenges","text":"Your task: Find and fix concurrency bugs in broken implementations. This tests your understanding of thread safety.","title":"Debugging Challenges"},{"location":"systems/10-concurrency-patterns/#decision-framework","text":"Your task: Build decision trees for when to use each concurrency pattern.","title":"Decision Framework"},{"location":"systems/10-concurrency-patterns/#practice","text":"","title":"Practice"},{"location":"systems/10-concurrency-patterns/#review-checklist","text":"Before moving to the next topic: Implementation ReentrantLock counter works correctly ReadWriteLock cache works correctly Bank transfer with lock ordering prevents deadlocks Producer-consumer with BlockingQueue works ConcurrentHashMap cache operations work Lock-free counter with CAS works Thread pool executors work correctly Non-blocking echo server handles concurrent connections Virtual threads handle 10K+ concurrent operations All client code runs successfully Understanding Filled in all ELI5 explanations Understand difference between synchronized and ReentrantLock Understand read-write lock semantics Understand why lock ordering prevents deadlocks Understand BlockingQueue blocking behavior Understand CAS and ABA problem Understand thread pool sizing principles Understand event loop vs threaded I/O Understand virtual threads vs platform threads Understand coroutine suspend/resume mechanism Understand reactive backpressure Concurrency Principles Always release locks in finally blocks Lock ordering prevents deadlocks Use bounded queues to prevent OOM Properly handle InterruptedException Shutdown thread pools gracefully Prefer higher-level abstractions (BlockingQueue, Atomic) Measure contention before optimizing Choose async pattern based on workload Event loops don't block on CPU work Virtual threads excellent for blocking I/O Decision Making Know when to use locks vs lock-free Know how to size thread pools Know when to use event loops vs threads Know when virtual threads better than reactive Know when coroutines appropriate Know when reactive streams needed Completed practice scenarios Can explain synchronization trade-offs Can explain async pattern trade-offs Mastery Check Could implement thread-safe cache from memory Could design thread pool configuration for given workload Could build non-blocking server with NIO Could migrate app to virtual threads Understand concurrency bugs (deadlock, race conditions, starvation) Know how to debug concurrency issues Can choose appropriate async pattern for requirements","title":"Review Checklist"},{"location":"systems/11-database-scaling/","text":"Database Scaling \u00b6 Strategies for handling growing data and traffic through sharding, replication, and partitioning ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing database scaling strategies, explain them simply. Prompts to guide you: What is database scaling in one sentence? Your answer: [Fill in after implementation] Why do databases need to scale? Your answer: [Fill in after implementation] Real-world analogy for sharding: Example: \"Sharding is like having multiple filing cabinets where...\" Your analogy: [Fill in] What is sharding in one sentence? Your answer: [Fill in after implementation] How is replication different from sharding? Your answer: [Fill in after implementation] Real-world analogy for replication: Example: \"Replication is like photocopying important documents where...\" Your analogy: [Fill in] What is partitioning in one sentence? Your answer: [Fill in after implementation] When would you use horizontal vs vertical scaling? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition about database scaling without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Single database serving reads and writes: Bottleneck: [Your guess: CPU/Memory/Disk/Network?] Verified after learning: [Actual bottleneck] Master-slave replication with 3 read replicas: Read capacity increase: [Your guess: 2x/3x/4x?] Write capacity increase: [Your guess: No change/2x/3x?] Verified: [Actual capacity changes] Sharding 1TB database across 10 shards: Data per shard: [Calculate: _____ GB] If one shard fails, data lost: [Yes/No/Depends?] Speedup for single-key lookups: [10x/No change/Slower?] Scenario Predictions \u00b6 Scenario 1: Social media app with 100M users, 90% reads, 10% writes Best scaling strategy: [Sharding/Replication/Both - Why?] If using replication, how many read replicas? [2/5/10 - Reasoning?] Main challenge: [Fill in your prediction] Scenario 2: E-commerce site storing user profiles, each 5KB You need to shard by user_id. What happens to: Single user lookups? [Faster/Same/Slower - Why?] Cross-user analytics queries? [Faster/Same/Slower - Why?] Adding new shards? [Easy/Hard - Why?] Scenario 3: Time-series IoT data, queries by timestamp ranges Best sharding strategy: [Hash/Range/Consistent - Why?] Shard key should be: [user_id/timestamp/device_id - Why?] What's the risk? [Fill in your prediction] Trade-off Quiz \u00b6 Question: When would vertical partitioning be BETTER than adding more RAM? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN downside of sharding vs replication? Higher cost Complex queries across shards Slower single-key lookups Requires more DBAs Verify after implementation: [Which one(s) and why?] Question: Replication lag is 2 seconds. A user updates their profile, then immediately views it. What do they see? Always new data (master read) Always old data (slave lag) Old data if load balancer picks slave Random Your answer: [Fill in] Verified: [Fill in after understanding replication] Before/After: Why Database Scaling Matters \u00b6 Your task: Compare unscaled vs scaled approaches to understand the impact. Example: E-Commerce User Lookup \u00b6 Problem: Product catalog with 10M items, receiving 10,000 read requests/sec and 100 write requests/sec. Approach 1: Single Database (No Scaling) \u00b6 Architecture: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Clients \u2502 \u2500\u2500\u2500\u2500\u2500> 10,000 reads/sec + 100 writes/sec \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Single DB \u2502 \u2190 All traffic hits one server \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Analysis: All reads and writes hit one server Database becomes CPU and I/O bottleneck At ~1,000 req/sec: Response time = 50ms At ~5,000 req/sec: Response time = 200ms (degraded) At ~10,000 req/sec: Database crashes or times out Maximum throughput: ~3,000-5,000 req/sec (hardware limit) Breaking point: 10,000 req/sec target vs 5,000 req/sec capacity = 2x overloaded Approach 2: Master-Slave Replication (Read Scaling) \u00b6 Architecture: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Clients \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u251c\u2500\u2500\u2500\u2500\u2500> 100 writes/sec \u2500\u2500\u2500\u2500> \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 Master \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2502 (replicates to) \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2514\u2500> 10,000 reads/sec \u2500\u2500> \u2502 \u2502 \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Slave 1 \u2502 \u2502 Slave 2 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 5,000 reads/sec 5,000 reads/sec Analysis: Writes: 100 writes/sec to master (well under capacity) Reads: 10,000 reads/sec distributed across 2 slaves = 5,000 each Master handles: 100 writes + replication = ~200 ops/sec Each slave handles: 5,000 reads/sec (within capacity) Read capacity: 2x-3x improvement per replica added Write capacity: No improvement (still single master) Result: Can now handle 10,000 reads/sec + 100 writes/sec comfortably Approach 3: Sharding (Write + Data Scaling) \u00b6 Architecture: \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Clients \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 (Shard by user_id % 4) \u2502 \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u25bc \u25bc \u25bc \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502Shard0\u2502\u2502Shard1\u2502\u2502Shard2\u2502\u2502Shard3\u2502 \u2502 2.5M \u2502\u2502 2.5M \u2502\u2502 2.5M \u2502\u2502 2.5M \u2502 items each \u2502items \u2502\u2502items \u2502\u2502items \u2502\u2502items \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 2,500 2,500 2,500 2,500 reads/sec each 25 25 25 25 writes/sec each Analysis: Data per shard: 10M items / 4 = 2.5M items each Reads per shard: 10,000 / 4 = 2,500 reads/sec (well under capacity) Writes per shard: 100 / 4 = 25 writes/sec (well under capacity) Each shard operates at ~25% capacity (lots of headroom) Can scale writes (unlike replication) Can add more shards as data grows Trade-off: Cross-shard queries become complex (e.g., \"find all items > $100\") Performance Comparison \u00b6 Metric Single DB Replication (2 slaves) Sharding (4 shards) Read Capacity 5,000 req/sec 15,000 req/sec 20,000 req/sec Write Capacity 500 req/sec 500 req/sec 2,000 req/sec Data Capacity 1TB max 1TB max 4TB+ (linear) Latency (reads) 50ms @ load 50ms @ load 50ms @ load Latency (writes) 50ms 50ms + replication 50ms Single-key lookup 1 query 1 query 1 query (1 shard) Cross-entity query 1 query 1 query 4 queries (all shards) Cost 1x 3x (1M + 2S) 4x (4 shards) Complexity Low Medium High Your calculation: For 50,000 read req/sec, you'd need _ read replicas OR ___ shards. Real-World Impact Example \u00b6 Instagram's scaling journey (simplified): 2010: Single PostgreSQL database - 10K users - Single server - Cost: $500/month 2011: Master-slave replication - 1M users - 1 master + 3 read replicas - Can't scale writes fast enough - Cost: $5K/month 2012: Sharded by user_id - 10M users - 100+ shards - Custom sharding logic - Cost: $50K/month 2015: Cassandra (distributed database) - 500M users - Automatic sharding + replication - No single point of failure - Cost: $500K/month Why Does Replication Help Reads? \u00b6 Key insight to understand: Single Database (1000 reads/sec capacity): Request 1 \u2500\u2500\u2510 Request 2 \u2500\u2500\u2524 Request 3 \u2500\u2500\u2524\u2500\u2500> \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 ... \u2502 \u2502 DB \u2502 \u2190 Bottleneck Request 999 \u2500\u2500\u2524 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Request 1000\u2500\u2500\u2518 Request 1001 \u2717 (rejected/timeout) Replication with 3 slaves (3000 reads/sec total): Request 1-333 \u2500\u2500> \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Slave 1\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Request 334-666 \u2500\u2500> \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Slave 2\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Request 667-1000\u2500\u2500> \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Slave 3\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 All 1000 requests handled, with 2000 req/sec headroom After implementing, explain in your own words: Why does replication not help writes? [Your answer] Why does sharding help both reads and writes? [Your answer] When would you combine replication + sharding? [Your answer] Case Studies: Database Scaling in the Wild \u00b6 Facebook's Social Graph: Sharding at Massive Scale \u00b6 Pattern: Horizontal Sharding (by User ID). How it works: Facebook's social graph is far too large for a single database. They partition their data, storing a user and all their related data (posts, friends, messages) on a specific database server, or shard . The application logic hashes a user's ID to determine which shard contains their data. This allows Facebook to scale almost infinitely by simply adding more shards. Key Takeaway: For applications with a massive, growing dataset that can be logically partitioned (e.g., by user, by geography), sharding is the key to horizontal scalability. The main challenge becomes managing the complexity of routing queries to the correct shard and handling cross-shard operations. Instagram's Early Scaling: Read Replicas \u00b6 Pattern: Primary-Replica Replication. How it works: In its earlier days, Instagram scaled its PostgreSQL database to handle millions of users by using read replicas. All writes (new photos, comments, likes) went to a single powerful primary database. This primary database then asynchronously replicated all changes to dozens of read-only replica databases. The vast majority of user traffic (reading feeds, viewing photos) was served from these replicas, spreading the read load and keeping the primary free to handle writes. Key Takeaway: For workloads that are heavily skewed towards reads, primary-replica replication is a simple and highly effective scaling strategy. It's often the first and most impactful step companies take to scale their database. Slack: Scaling with Vitess \u00b6 Pattern: Horizontal Sharding via a Database Middleware (Vitess). How it works: Slack needed to scale its MySQL databases to handle explosive growth in users, messages, and channels. They adopted Vitess, a clustering system that sits between their application and their MySQL servers. Vitess automatically shards the data and routes queries, making a large cluster of small databases look like one single, massive database to the application. This allowed them to scale horizontally without significant changes to their application code. Key Takeaway: Database middleware like Vitess can abstract away the complexity of sharding. It provides the scalability benefits of a sharded architecture while minimizing the impact on application development, offering a powerful path for scaling existing SQL databases. Core Implementation \u00b6 Part 1: Hash-Based Sharding \u00b6 Your task: Implement hash-based sharding for distributing data. import java.util.*; /** * Hash-Based Sharding: Distribute data across shards using hash function * * Key principles: * - Hash key determines shard * - Even distribution of data * - Simple and predictable * - Resharding is expensive */ public class HashBasedSharding { private final List<DatabaseShard> shards; /** * Initialize hash-based sharding * * @param numShards Number of database shards * * TODO: Initialize sharding * - Create list of shards * - Initialize each shard */ public HashBasedSharding(int numShards) { // TODO: Initialize shards list // TODO: Create numShards DatabaseShard instances this.shards = null; // Replace } /** * Get shard for a given key * * @param key Record key (e.g., user ID) * @return Shard that should store this key * * TODO: Implement shard selection * 1. Hash the key * 2. Modulo by number of shards * 3. Return shard at that index */ public DatabaseShard getShard(String key) { // TODO: Hash key to integer // TODO: Get shard index using modulo // index = abs(hash) % shards.size() // TODO: Return shard at index return null; // Replace } /** * Insert record * * TODO: Route to correct shard and insert */ public void insert(String key, String value) { // TODO: Get shard for key // TODO: Insert into shard } /** * Get record * * TODO: Route to correct shard and retrieve */ public String get(String key) { // TODO: Get shard for key // TODO: Get from shard return null; // Replace } /** * Delete record * * TODO: Route to correct shard and delete */ public void delete(String key) { // TODO: Get shard for key // TODO: Delete from shard } /** * Get statistics for all shards */ public Map<Integer, Integer> getStats() { Map<Integer, Integer> stats = new HashMap<>(); for (int i = 0; i < shards.size(); i++) { stats.put(i, shards.get(i).getRecordCount()); } return stats; } /** * Hash function */ private int hash(String key) { // TODO: Hash key to integer // Hint: key.hashCode() & 0x7FFFFFFF return 0; // Replace } static class DatabaseShard { int shardId; Map<String, String> data; public DatabaseShard(int shardId) { this.shardId = shardId; this.data = new HashMap<>(); } public void insert(String key, String value) { data.put(key, value); } public String get(String key) { return data.get(key); } public void delete(String key) { data.remove(key); } public int getRecordCount() { return data.size(); } } } Part 2: Range-Based Sharding \u00b6 Your task: Implement range-based sharding for ordered data. /** * Range-Based Sharding: Distribute data by key ranges * * Key principles: * - Continuous key ranges per shard * - Good for range queries * - Risk of hotspots * - Easier to add shards */ public class RangeBasedSharding { private final TreeMap<String, DatabaseShard> rangeMap; private final List<DatabaseShard> shards; /** * Initialize range-based sharding * * @param ranges List of range boundaries (sorted) * * TODO: Initialize range sharding * - Create TreeMap for range lookup * - Assign shard to each range * * Example: ranges = [\"M\", \"Z\"] creates 3 shards * Shard 0: keys < \"M\" * Shard 1: keys >= \"M\" and < \"Z\" * Shard 2: keys >= \"Z\" */ public RangeBasedSharding(List<String> ranges) { // TODO: Initialize rangeMap and shards // TODO: Create shard for each range this.rangeMap = null; // Replace this.shards = null; // Replace } /** * Get shard for a given key * * @param key Record key * @return Shard that should store this key * * TODO: Implement range lookup * 1. Find first range >= key (ceilingEntry) * 2. If null, use last shard * 3. Return shard */ public HashBasedSharding.DatabaseShard getShard(String key) { // TODO: Look up range in TreeMap // entry = rangeMap.ceilingEntry(key) // TODO: Implement iteration/conditional logic // TODO: Otherwise return last shard (for keys >= last boundary) return null; // Replace } /** * Insert record */ public void insert(String key, String value) { // TODO: Get shard for key and insert } /** * Get record */ public String get(String key) { // TODO: Get shard for key and retrieve return null; // Replace } /** * Range query (scan multiple shards if needed) * * TODO: Find all shards in range and query them */ public List<String> rangeQuery(String startKey, String endKey) { // TODO: Find first shard containing startKey // TODO: Query all shards until endKey // TODO: Combine results return null; // Replace } /** * Get statistics */ public Map<Integer, Integer> getStats() { Map<Integer, Integer> stats = new HashMap<>(); for (int i = 0; i < shards.size(); i++) { stats.put(i, shards.get(i).getRecordCount()); } return stats; } } Part 3: Master-Slave Replication \u00b6 Your task: Implement master-slave replication for read scaling. /** * Master-Slave Replication: One writer, multiple readers * * Key principles: * - Master handles all writes * - Slaves replicate data from master * - Slaves handle reads * - Eventual consistency */ public class MasterSlaveReplication { private final Database master; private final List<Database> slaves; private int readIndex; // For round-robin read distribution /** * Initialize master-slave replication * * @param numSlaves Number of read replicas * * TODO: Initialize replication * - Create master database * - Create slave databases * - Initialize read index */ public MasterSlaveReplication(int numSlaves) { // TODO: Create master // TODO: Create slaves // TODO: Initialize readIndex to 0 this.master = null; // Replace this.slaves = null; // Replace } /** * Write operation (goes to master) * * TODO: Implement write * 1. Write to master * 2. Replicate to all slaves * * Note: In production, replication is async */ public void write(String key, String value) { // TODO: Write to master // TODO: Replicate to all slaves } /** * Read operation (load balanced across slaves) * * TODO: Implement read from slaves * - Use round robin to select slave * - Read from selected slave * - Fallback to master if slave fails */ public synchronized String read(String key) { // TODO: Select slave using round robin // slave = slaves.get(readIndex) // readIndex = (readIndex + 1) % slaves.size() // TODO: Read from slave // TODO: Implement iteration/conditional logic return null; // Replace } /** * Delete operation (goes to master) */ public void delete(String key) { // TODO: Delete from master // TODO: Replicate deletion to slaves } /** * Check replication lag * * TODO: Compare master and slave data * - Count keys that differ * - Return lag metrics */ public ReplicationStats getReplicationStats() { // TODO: Compare master with each slave return null; // Replace } static class Database { String id; Map<String, String> data; public Database(String id) { this.id = id; this.data = new HashMap<>(); } public void write(String key, String value) { data.put(key, value); } public String read(String key) { return data.get(key); } public void delete(String key) { data.remove(key); } public int size() { return data.size(); } } static class ReplicationStats { int totalKeys; Map<String, Integer> slaveKeyCount; public ReplicationStats() { this.slaveKeyCount = new HashMap<>(); } } } Part 4: Vertical Partitioning \u00b6 Your task: Implement vertical partitioning (column splitting). /** * Vertical Partitioning: Split tables by columns * * Key principles: * - Frequently accessed columns in one partition * - Rarely accessed columns in another * - Reduces I/O for common queries * - Requires joins for full records */ public class VerticalPartitioning { private final Map<String, HotData> hotStore; // Frequently accessed private final Map<String, ColdData> coldStore; // Rarely accessed /** * Initialize vertical partitioning * * TODO: Initialize hot and cold stores */ public VerticalPartitioning() { // TODO: Initialize both stores this.hotStore = null; // Replace this.coldStore = null; // Replace } /** * Insert full record * * TODO: Split record into hot and cold parts * - Store frequently accessed fields in hot store * - Store rarely accessed fields in cold store */ public void insert(String id, String name, String email, String bio, byte[] largeData) { // TODO: Create HotData with id, name, email // TODO: Create ColdData with id, bio, largeData // TODO: Store in respective stores } /** * Get hot data only (fast, common query) * * TODO: Retrieve from hot store only */ public HotData getHotData(String id) { // TODO: Get from hotStore return null; // Replace } /** * Get full record (requires join) * * TODO: Retrieve from both stores and merge */ public FullRecord getFullRecord(String id) { // TODO: Get hot data // TODO: Get cold data // TODO: Combine into FullRecord return null; // Replace } /** * Update hot data (fast) */ public void updateHotData(String id, String name, String email) { // TODO: Update hotStore only } /** * Update cold data (infrequent) */ public void updateColdData(String id, String bio, byte[] largeData) { // TODO: Update coldStore only } /** * Get statistics */ public PartitionStats getStats() { return new PartitionStats(hotStore.size(), coldStore.size()); } static class HotData { String id; String name; String email; public HotData(String id, String name, String email) { this.id = id; this.name = name; this.email = email; } } static class ColdData { String id; String bio; byte[] largeData; public ColdData(String id, String bio, byte[] largeData) { this.id = id; this.bio = bio; this.largeData = largeData; } } static class FullRecord { String id; String name; String email; String bio; byte[] largeData; public FullRecord(HotData hot, ColdData cold) { this.id = hot.id; this.name = hot.name; this.email = hot.email; this.bio = cold.bio; this.largeData = cold.largeData; } } static class PartitionStats { int hotRecords; int coldRecords; public PartitionStats(int hotRecords, int coldRecords) { this.hotRecords = hotRecords; this.coldRecords = coldRecords; } } } Part 5: Consistent Hashing for Dynamic Sharding \u00b6 Your task: Implement consistent hashing for easier resharding. /** * Consistent Hashing Sharding: Minimal data movement on resharding * * Key principles: * - Uses hash ring (from load balancing) * - Adding/removing shards affects limited keys * - Virtual nodes for better distribution * - Popular for distributed databases */ public class ConsistentHashSharding { private final TreeMap<Integer, HashBasedSharding.DatabaseShard> ring; private final Map<Integer, HashBasedSharding.DatabaseShard> shards; private final int virtualNodesPerShard; private int nextShardId; /** * Initialize consistent hash sharding * * @param initialShards Number of initial shards * @param virtualNodesPerShard Virtual nodes per physical shard * * TODO: Initialize hash ring * - Create TreeMap for ring * - Add initial shards with virtual nodes */ public ConsistentHashSharding(int initialShards, int virtualNodesPerShard) { // TODO: Initialize structures // TODO: Add initial shards this.ring = null; // Replace this.shards = null; // Replace this.virtualNodesPerShard = 0; } /** * Get shard for key * * TODO: Use consistent hashing to find shard */ public HashBasedSharding.DatabaseShard getShard(String key) { // TODO: Hash key // TODO: Find next shard on ring (ceilingEntry) // TODO: Implement iteration/conditional logic return null; // Replace } /** * Add new shard * * TODO: Add shard with virtual nodes * - Place virtual nodes on ring * - Migrate data from affected keys */ public void addShard() { // TODO: Create new shard // TODO: Add virtual nodes to ring // TODO: In production: migrate affected data } /** * Remove shard * * TODO: Remove shard and virtual nodes * - Remove from ring * - Migrate data to other shards */ public void removeShard(int shardId) { // TODO: Get shard // TODO: Remove all virtual nodes from ring // TODO: In production: migrate data } /** * Insert/Get/Delete operations */ public void insert(String key, String value) { getShard(key).insert(key, value); } public String get(String key) { return getShard(key).get(key); } public void delete(String key) { getShard(key).delete(key); } /** * Hash function */ private int hash(String key) { return key.hashCode() & 0x7FFFFFFF; } /** * Get statistics */ public Map<Integer, Integer> getStats() { Map<Integer, Integer> stats = new HashMap<>(); for (Map.Entry<Integer, HashBasedSharding.DatabaseShard> entry : shards.entrySet()) { stats.put(entry.getKey(), entry.getValue().getRecordCount()); } return stats; } } Client Code \u00b6 import java.util.*; public class DatabaseScalingClient { public static void main(String[] args) { testHashBasedSharding(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testRangeBasedSharding(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testMasterSlaveReplication(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testVerticalPartitioning(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testConsistentHashSharding(); } static void testHashBasedSharding() { System.out.println(\"=== Hash-Based Sharding Test ===\\n\"); HashBasedSharding db = new HashBasedSharding(3); // Insert data String[] users = {\"user1\", \"user2\", \"user3\", \"user4\", \"user5\", \"user6\", \"user7\", \"user8\", \"user9\", \"user10\"}; System.out.println(\"Inserting 10 users:\"); for (String user : users) { db.insert(user, user + \"_data\"); System.out.println(user + \" -> Shard \" + db.getShard(user).shardId); } System.out.println(\"\\nShard distribution:\"); System.out.println(db.getStats()); // Test retrieval System.out.println(\"\\nRetrieving user3:\"); System.out.println(db.get(\"user3\")); } static void testRangeBasedSharding() { System.out.println(\"=== Range-Based Sharding Test ===\\n\"); // Ranges: A-M, M-Z, Z+ List<String> ranges = Arrays.asList(\"M\", \"Z\"); RangeBasedSharding db = new RangeBasedSharding(ranges); // Insert data String[] names = {\"Alice\", \"Bob\", \"Charlie\", \"Mike\", \"Nancy\", \"Oscar\", \"Peter\", \"Zoe\", \"Zachary\"}; System.out.println(\"Inserting names (range-based):\"); for (String name : names) { db.insert(name, name + \"_data\"); System.out.println(name + \" -> Shard \" + db.getShard(name).shardId); } System.out.println(\"\\nShard distribution:\"); System.out.println(db.getStats()); // Test range query System.out.println(\"\\nRange query: M-P\"); List<String> results = db.rangeQuery(\"M\", \"P\"); System.out.println(\"Results: \" + results); } static void testMasterSlaveReplication() { System.out.println(\"=== Master-Slave Replication Test ===\\n\"); MasterSlaveReplication db = new MasterSlaveReplication(2); // Test writes (go to master, replicate to slaves) System.out.println(\"Writing to master:\"); db.write(\"key1\", \"value1\"); db.write(\"key2\", \"value2\"); db.write(\"key3\", \"value3\"); // Test reads (distributed across slaves) System.out.println(\"\\nReading from slaves (round-robin):\"); for (int i = 0; i < 6; i++) { String value = db.read(\"key\" + (i % 3 + 1)); System.out.println(\"Read \" + (i+1) + \": \" + value); } // Check replication System.out.println(\"\\nReplication stats:\"); System.out.println(db.getReplicationStats()); } static void testVerticalPartitioning() { System.out.println(\"=== Vertical Partitioning Test ===\\n\"); VerticalPartitioning db = new VerticalPartitioning(); // Insert records System.out.println(\"Inserting records (hot+cold data):\"); db.insert(\"user1\", \"Alice\", \"alice@example.com\", \"Long bio...\", new byte[1000]); db.insert(\"user2\", \"Bob\", \"bob@example.com\", \"Long bio...\", new byte[1000]); // Fast query (hot data only) System.out.println(\"\\nFast query (hot data only):\"); VerticalPartitioning.HotData hot = db.getHotData(\"user1\"); System.out.println(\"Name: \" + hot.name + \", Email: \" + hot.email); // Full query (requires join) System.out.println(\"\\nFull query (hot + cold data):\"); VerticalPartitioning.FullRecord full = db.getFullRecord(\"user1\"); System.out.println(\"Name: \" + full.name); System.out.println(\"Bio length: \" + full.bio.length()); System.out.println(\"Data size: \" + full.largeData.length + \" bytes\"); // Stats System.out.println(\"\\nPartition stats:\"); VerticalPartitioning.PartitionStats stats = db.getStats(); System.out.println(\"Hot: \" + stats.hotRecords + \", Cold: \" + stats.coldRecords); } static void testConsistentHashSharding() { System.out.println(\"=== Consistent Hash Sharding Test ===\\n\"); ConsistentHashSharding db = new ConsistentHashSharding(3, 3); // Insert data String[] keys = {\"key1\", \"key2\", \"key3\", \"key4\", \"key5\"}; System.out.println(\"Initial distribution (3 shards):\"); for (String key : keys) { db.insert(key, key + \"_data\"); System.out.println(key + \" -> Shard \" + db.getShard(key).shardId); } System.out.println(\"\\nStats: \" + db.getStats()); // Add shard (minimal redistribution) System.out.println(\"\\nAdding 4th shard:\"); db.addShard(); System.out.println(\"New distribution:\"); for (String key : keys) { System.out.println(key + \" -> Shard \" + db.getShard(key).shardId); } System.out.println(\"\\nStats: \" + db.getStats()); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken database scaling implementations. This tests your understanding of scaling pitfalls. Challenge 1: Broken Hash Sharding (Uneven Distribution) \u00b6 /** * This hash-based sharding causes hotspots * Find the BUG that creates uneven distribution! */ public class BrokenHashSharding { private final List<DatabaseShard> shards; public DatabaseShard getShard(String key) { int hash = key.length() % shards.size(); return shards.get(hash); } public void insert(String key, String value) { getShard(key).insert(key, value); } } // Test with user IDs: user1, user2, user3, ..., user100 // All have same key length! All go to same shard! Your debugging: Bug: [What\\'s the bug?] Trace through example: Keys: \"user1\", \"user2\", \"user99\" (all length 5) With 3 shards, where do they go? [All to shard ___] Expected: [Should be distributed across all shards] Click to verify your answer Bug: Using key.length() as hash creates terrible distribution. All keys with same length go to same shard. Fix: public DatabaseShard getShard(String key) { // Use proper hash function int hash = Math.abs(key.hashCode()) % shards.size(); return shards.get(hash); } Why: hashCode() produces different values for different strings, even with same length. The Math.abs() handles negative hash codes. Challenge 2: Replication Lag Bug (Read-After-Write Consistency) \u00b6 /** * User updates profile, immediately views it, sees OLD data * Find the BUG causing stale reads! */ public class BrokenReplication { private Database master; private List<Database> slaves; private int readIndex = 0; public void updateProfile(String userId, String newName) { // Write to master master.write(userId, newName); // Slaves don't have new data yet } public String getProfile(String userId) { Database slave = slaves.get(readIndex); readIndex = (readIndex + 1) % slaves.size(); return slave.read(userId); } } // Scenario: // 1. User updates name to \"John\" // 2. Master has \"John\" immediately // 3. Replication takes 200ms // 4. User views profile 50ms later // 5. Read goes to slave (still has old name \"Johnny\") // 6. User sees \"Johnny\" instead of \"John\" - CONFUSION! Your debugging: Bug location: [What's the problem?] Bug explanation: [Why do users see stale data?] Fix option 1: [How to guarantee read-after-write consistency?] Fix option 2: [Alternative approach?] Click to verify your answers Bug: Reading from slaves immediately after writing to master causes stale reads due to replication lag. Fix Option 1 - Read-Your-Writes (sticky sessions): public void updateProfile(String userId, String newName) { master.write(userId, newName); // Mark this session to read from master for next N seconds markSessionForMasterReads(userId, Duration.ofSeconds(5)); } public String getProfile(String userId) { // Check if user recently wrote if (shouldReadFromMaster(userId)) { return master.read(userId); // Read from master } // Otherwise read from slave Database slave = slaves.get(readIndex); readIndex = (readIndex + 1) % slaves.size(); return slave.read(userId); } Fix Option 2 - Always read from master after writes: public String getProfile(String userId, boolean afterWrite) { if (afterWrite) { return master.read(userId); // Guarantee consistency } // Normal read from slave Database slave = slaves.get(readIndex); readIndex = (readIndex + 1) % slaves.size(); return slave.read(userId); } Trade-off: Both fixes reduce read scalability by routing some reads to master. Challenge 3: Cross-Shard Query Disaster \u00b6 /** * Analytics query needs to scan all shards * This implementation has PERFORMANCE and CORRECTNESS bugs! */ public class BrokenCrossShardQuery { private List<DatabaseShard> shards; public List<User> findAllUsersOver21() { List<User> results = new ArrayList<>(); for (DatabaseShard shard : shards) { List<User> shardResults = shard.query(\"age > 21\"); results.addAll(shardResults); } // If 1M results * 1KB each = 1GB memory return results; } } // Scenario: 10 shards, each takes 2 seconds to query // Total time: 10 * 2 = 20 seconds! // If shard 5 is slow (10 seconds), entire query takes 28 seconds! Your debugging: Bug 1 (Performance): [Why is serial query slow?] Bug 1 fix: [How to parallelize?] Bug 2 (Memory): [What happens with millions of results?] Bug 2 fix: [How to handle large result sets?] Bug 3 (Reliability): [What if one shard hangs?] Bug 3 fix: [How to add timeout?] Click to verify your answers Fixed version with all bugs addressed: public List<User> findAllUsersOver21(int limit, int offset) { List<CompletableFuture<List<User>>> futures = new ArrayList<>(); // FIX 1: Parallel queries across shards for (DatabaseShard shard : shards) { CompletableFuture<List<User>> future = CompletableFuture.supplyAsync(() -> { // FIX 2: Per-shard pagination return shard.query(\"age > 21\", limit / shards.size(), offset / shards.size()); }); // FIX 3: Add timeout per shard future = future.orTimeout(5, TimeUnit.SECONDS) .exceptionally(ex -> { // Log error, return empty for failed shard System.err.println(\"Shard query failed: \" + ex); return Collections.emptyList(); }); futures.add(future); } // Wait for all shards (with timeout) List<User> results = new ArrayList<>(); for (CompletableFuture<List<User>> future : futures) { try { results.addAll(future.get(10, TimeUnit.SECONDS)); } catch (Exception e) { // Handle timeout or failure System.err.println(\"Shard timeout: \" + e); } } // FIX 2 continued: Apply global limit return results.stream().limit(limit).collect(Collectors.toList()); } Performance improvement: Before: 10 shards \u00d7 2 seconds = 20 seconds After: max(2 seconds) = 2 seconds (parallel) 10x faster! Challenge 4: Shard Imbalance (Hotspot) \u00b6 /** * Celebrity user causes ONE shard to be overloaded * Find the DESIGN BUG! */ public class BrokenSharding { // Sharding by user_id public DatabaseShard getShard(String userId) { int hash = Math.abs(userId.hashCode()) % shards.size(); return shards.get(hash); } // Social media queries public List<Post> getUserPosts(String userId) { return getShard(userId).queryPosts(userId); } public List<Follower> getUserFollowers(String userId) { return getShard(userId).queryFollowers(userId); } } // Scenario: Celebrity \"user123\" has 100M followers // - user123's shard stores: 100M follower records // - Other shards: ~1000 follower records each // - Shard 3 (celebrity's shard): 99.9% of queries! // - Shard 3: CPU 100%, disk full, crashing // - Other shards: CPU 5%, mostly idle // This is a HOTSPOT or HOT SHARD problem! Your debugging: Design bug: [Why does one user cause shard overload?] When does this happen? [What data pattern causes hotspots?] Fix option 1: [How to split celebrity data?] Fix option 2: [How to cache celebrity data?] Fix option 3: [Different sharding strategy?] Click to verify your answers Design bug: Sharding by user_id groups all of a user's data on one shard. For celebrities with massive data/traffic, that shard becomes a hotspot. Fix Option 1 - Split entity sharding: // Shard users and their posts by user_id (small data) public DatabaseShard getUserShard(String userId) { return shards.get(hash(userId) % shards.size()); } // Shard followers by follower_id, not celebrity_id (distributes load) public DatabaseShard getFollowerShard(String followerId) { return followerShards.get(hash(followerId) % followerShards.size()); } // Now celebrity's 100M followers distributed across ALL shards Fix Option 2 - Caching layer: // Cache celebrity data in Redis/Memcached public List<Post> getUserPosts(String userId) { // Check if celebrity (cached list) if (isCelebrity(userId)) { return cache.get(\"posts:\" + userId); } // Normal user -> query shard return getShard(userId).queryPosts(userId); } Fix Option 3 - Consistent hashing with detection: // Detect hot shards and split them if (shard.requestRate() > threshold) { splitShard(shard); // Create two shards from one rehashKeys(shard); // Redistribute keys } Prevention: Monitor shard metrics (CPU, request rate, data size) and set alerts for imbalance. Challenge 5: Split-Brain (Replication Failure) \u00b6 /** * Master-slave replication during network partition * Find the CATASTROPHIC bug! */ public class BrokenMasterFailover { private Database master; private List<Database> slaves; // Master goes down, promote slave to master public void handleMasterFailure() { System.out.println(\"Master failed! Promoting slave to master...\"); master = slaves.get(0); // Promote first slave slaves.remove(0); // Now we have NEW master accepting writes } // Meanwhile, OLD master recovers after network partition // OLD master thinks it's still master! // NEW master is also accepting writes! // TWO MASTERS = SPLIT BRAIN! // Writes to OLD master: user updates email to \"alice@new.com\" // Writes to NEW master: user updates email to \"alice@old.com\" // CONFLICT! Which is correct? } // Timeline: // T=0: Master fails (network partition) // T=10: Slave promoted to new master // T=20: Clients write to new master // T=30: Old master recovers, still thinks it's master // T=40: Some clients write to old master (split brain!) // T=50: Networks merge - DATA CONFLICTS! Your debugging: Bug: [What's the split-brain problem?] Why it's catastrophic: [What happens to data?] Fix option 1: [How to prevent old master from accepting writes?] Fix option 2: [How to detect split brain?] Real-world solution: [What do production systems do?] Click to verify your answers Bug: Split-brain occurs when two nodes both think they're master, accepting conflicting writes. This causes data divergence and conflicts. Fix Option 1 - Fencing (prevent old master from accepting writes): public void handleMasterFailure() { // Step 1: FENCE old master (disable it) oldMaster.fence(); // Prevent further writes // Step 2: Wait for in-flight writes to complete Thread.sleep(5000); // Step 3: Promote slave master = slaves.get(0); slaves.remove(0); // Step 4: Configure slaves to replicate from new master for (Database slave : slaves) { slave.replicateFrom(master); } } Fix Option 2 - Consensus protocol (Raft/Paxos): // Use leader election with quorum // - Only ONE master elected at a time // - Master must have quorum (majority votes) // - Old master can't get quorum if network partitioned public void electMaster() { int votes = 0; int requiredVotes = (nodes.size() / 2) + 1; // Majority for (Node node : nodes) { if (node.voteFor(thisNode)) { votes++; } } if (votes >= requiredVotes) { thisNode.becomeMaster(); // Safe - have quorum } } Fix Option 3 - Epoch/term numbers: class Database { int epoch = 0; // Incremented on each master change public void write(String key, String value, int writeEpoch) { if (writeEpoch < this.epoch) { throw new StaleEpochException(\"Old master, reject write\"); } // Accept write } } Real-world solutions: PostgreSQL: Uses fencing + watchdog MySQL: Group Replication with consensus MongoDB: Replica sets with election Distributed databases: Raft/Paxos consensus algorithms Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found hotspot bug in hash sharding Understood replication lag and read-after-write consistency Fixed cross-shard query performance issues Identified and solved hot shard problem Understood split-brain problem in replication Could explain each fix to someone else Common scaling bugs you discovered: [Poor hash functions cause hotspots] [Replication lag causes stale reads] [Cross-shard queries need parallelization and pagination] [Celebrity/popular entity data causes hot shards] [Split-brain in failover causes data conflicts] Your takeaways: Which bug surprised you most? [Fill in] Which bug is hardest to detect in production? [Fill in] Which bug has the worst consequences? [Fill in] Decision Framework \u00b6 Questions to answer after implementation: 1. Scaling Strategy Selection \u00b6 When to use Hash-Based Sharding? Your scenario: [Fill in] Key factors: [Fill in] When to use Range-Based Sharding? Your scenario: [Fill in] Key factors: [Fill in] When to use Master-Slave Replication? Your scenario: [Fill in] Key factors: [Fill in] When to use Vertical Partitioning? Your scenario: [Fill in] Key factors: [Fill in] When to use Consistent Hash Sharding? Your scenario: [Fill in] Key factors: [Fill in] 2. Trade-offs \u00b6 Hash-Based Sharding: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Range-Based Sharding: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Master-Slave Replication: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Vertical Partitioning: Pros: [Fill in after understanding] Cons: [Fill in after understanding] 3. Your Decision Tree \u00b6 Build your decision tree after practicing: flowchart LR Start[\"What is your bottleneck?\"] N1[\"?\"] Start -->|\"Read traffic\"| N1 N2[\"?\"] Start -->|\"Write traffic\"| N2 N3[\"?\"] Start -->|\"Data size\"| N3 N4[\"?\"] Start -->|\"Query patterns\"| N4 N5[\"?\"] Start -->|\"Operational complexity\"| N5 Practice \u00b6 Scenario 1: Scale read-heavy application \u00b6 Requirements: 90% reads, 10% writes Single database becoming bottleneck Need to scale to 10x traffic Can tolerate slight staleness Your design: Which strategy would you choose? [Fill in] Why? [Fill in] How many replicas? [Fill in] Consistency guarantees? [Fill in] Scenario 2: Scale social media platform \u00b6 Requirements: 500M users User profiles, posts, followers Need to distribute data Want fast user lookups Your design: Which sharding strategy? [Fill in] What's the shard key? [Fill in] How to handle hot users (celebrities)? [Fill in] Cross-shard queries? [Fill in] Scenario 3: Time-series data storage \u00b6 Requirements: IoT sensor data Queries by time range Recent data accessed frequently Old data rarely accessed Your design: Which partitioning strategy? [Fill in] How to partition? [Fill in] Archival strategy? [Fill in] Query optimization? [Fill in] Review Checklist \u00b6 Hash-based sharding implemented Range-based sharding implemented Master-slave replication implemented Vertical partitioning implemented Consistent hash sharding implemented Understand when to use each strategy Can explain trade-offs between strategies Built decision tree for strategy selection Completed practice scenarios Mastery Certification \u00b6 I certify that I can: Explain all scaling strategies to non-technical stakeholders Draw architecture diagrams for each strategy from memory Choose the correct strategy for different scenarios Calculate capacity requirements and costs Debug common scaling issues (lag, hotspots, split-brain) Analyze trade-offs between different approaches Handle production incidents involving sharding/replication Design database layer for large-scale systems Teach these concepts to others","title":"11. Database Scaling"},{"location":"systems/11-database-scaling/#database-scaling","text":"Strategies for handling growing data and traffic through sharding, replication, and partitioning","title":"Database Scaling"},{"location":"systems/11-database-scaling/#eli5-explain-like-im-5","text":"Your task: After implementing database scaling strategies, explain them simply. Prompts to guide you: What is database scaling in one sentence? Your answer: [Fill in after implementation] Why do databases need to scale? Your answer: [Fill in after implementation] Real-world analogy for sharding: Example: \"Sharding is like having multiple filing cabinets where...\" Your analogy: [Fill in] What is sharding in one sentence? Your answer: [Fill in after implementation] How is replication different from sharding? Your answer: [Fill in after implementation] Real-world analogy for replication: Example: \"Replication is like photocopying important documents where...\" Your analogy: [Fill in] What is partitioning in one sentence? Your answer: [Fill in after implementation] When would you use horizontal vs vertical scaling? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/11-database-scaling/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition about database scaling without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/11-database-scaling/#beforeafter-why-database-scaling-matters","text":"Your task: Compare unscaled vs scaled approaches to understand the impact.","title":"Before/After: Why Database Scaling Matters"},{"location":"systems/11-database-scaling/#case-studies-database-scaling-in-the-wild","text":"","title":"Case Studies: Database Scaling in the Wild"},{"location":"systems/11-database-scaling/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/11-database-scaling/#client-code","text":"import java.util.*; public class DatabaseScalingClient { public static void main(String[] args) { testHashBasedSharding(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testRangeBasedSharding(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testMasterSlaveReplication(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testVerticalPartitioning(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testConsistentHashSharding(); } static void testHashBasedSharding() { System.out.println(\"=== Hash-Based Sharding Test ===\\n\"); HashBasedSharding db = new HashBasedSharding(3); // Insert data String[] users = {\"user1\", \"user2\", \"user3\", \"user4\", \"user5\", \"user6\", \"user7\", \"user8\", \"user9\", \"user10\"}; System.out.println(\"Inserting 10 users:\"); for (String user : users) { db.insert(user, user + \"_data\"); System.out.println(user + \" -> Shard \" + db.getShard(user).shardId); } System.out.println(\"\\nShard distribution:\"); System.out.println(db.getStats()); // Test retrieval System.out.println(\"\\nRetrieving user3:\"); System.out.println(db.get(\"user3\")); } static void testRangeBasedSharding() { System.out.println(\"=== Range-Based Sharding Test ===\\n\"); // Ranges: A-M, M-Z, Z+ List<String> ranges = Arrays.asList(\"M\", \"Z\"); RangeBasedSharding db = new RangeBasedSharding(ranges); // Insert data String[] names = {\"Alice\", \"Bob\", \"Charlie\", \"Mike\", \"Nancy\", \"Oscar\", \"Peter\", \"Zoe\", \"Zachary\"}; System.out.println(\"Inserting names (range-based):\"); for (String name : names) { db.insert(name, name + \"_data\"); System.out.println(name + \" -> Shard \" + db.getShard(name).shardId); } System.out.println(\"\\nShard distribution:\"); System.out.println(db.getStats()); // Test range query System.out.println(\"\\nRange query: M-P\"); List<String> results = db.rangeQuery(\"M\", \"P\"); System.out.println(\"Results: \" + results); } static void testMasterSlaveReplication() { System.out.println(\"=== Master-Slave Replication Test ===\\n\"); MasterSlaveReplication db = new MasterSlaveReplication(2); // Test writes (go to master, replicate to slaves) System.out.println(\"Writing to master:\"); db.write(\"key1\", \"value1\"); db.write(\"key2\", \"value2\"); db.write(\"key3\", \"value3\"); // Test reads (distributed across slaves) System.out.println(\"\\nReading from slaves (round-robin):\"); for (int i = 0; i < 6; i++) { String value = db.read(\"key\" + (i % 3 + 1)); System.out.println(\"Read \" + (i+1) + \": \" + value); } // Check replication System.out.println(\"\\nReplication stats:\"); System.out.println(db.getReplicationStats()); } static void testVerticalPartitioning() { System.out.println(\"=== Vertical Partitioning Test ===\\n\"); VerticalPartitioning db = new VerticalPartitioning(); // Insert records System.out.println(\"Inserting records (hot+cold data):\"); db.insert(\"user1\", \"Alice\", \"alice@example.com\", \"Long bio...\", new byte[1000]); db.insert(\"user2\", \"Bob\", \"bob@example.com\", \"Long bio...\", new byte[1000]); // Fast query (hot data only) System.out.println(\"\\nFast query (hot data only):\"); VerticalPartitioning.HotData hot = db.getHotData(\"user1\"); System.out.println(\"Name: \" + hot.name + \", Email: \" + hot.email); // Full query (requires join) System.out.println(\"\\nFull query (hot + cold data):\"); VerticalPartitioning.FullRecord full = db.getFullRecord(\"user1\"); System.out.println(\"Name: \" + full.name); System.out.println(\"Bio length: \" + full.bio.length()); System.out.println(\"Data size: \" + full.largeData.length + \" bytes\"); // Stats System.out.println(\"\\nPartition stats:\"); VerticalPartitioning.PartitionStats stats = db.getStats(); System.out.println(\"Hot: \" + stats.hotRecords + \", Cold: \" + stats.coldRecords); } static void testConsistentHashSharding() { System.out.println(\"=== Consistent Hash Sharding Test ===\\n\"); ConsistentHashSharding db = new ConsistentHashSharding(3, 3); // Insert data String[] keys = {\"key1\", \"key2\", \"key3\", \"key4\", \"key5\"}; System.out.println(\"Initial distribution (3 shards):\"); for (String key : keys) { db.insert(key, key + \"_data\"); System.out.println(key + \" -> Shard \" + db.getShard(key).shardId); } System.out.println(\"\\nStats: \" + db.getStats()); // Add shard (minimal redistribution) System.out.println(\"\\nAdding 4th shard:\"); db.addShard(); System.out.println(\"New distribution:\"); for (String key : keys) { System.out.println(key + \" -> Shard \" + db.getShard(key).shardId); } System.out.println(\"\\nStats: \" + db.getStats()); } }","title":"Client Code"},{"location":"systems/11-database-scaling/#debugging-challenges","text":"Your task: Find and fix bugs in broken database scaling implementations. This tests your understanding of scaling pitfalls.","title":"Debugging Challenges"},{"location":"systems/11-database-scaling/#decision-framework","text":"Questions to answer after implementation:","title":"Decision Framework"},{"location":"systems/11-database-scaling/#practice","text":"","title":"Practice"},{"location":"systems/11-database-scaling/#review-checklist","text":"Hash-based sharding implemented Range-based sharding implemented Master-slave replication implemented Vertical partitioning implemented Consistent hash sharding implemented Understand when to use each strategy Can explain trade-offs between strategies Built decision tree for strategy selection Completed practice scenarios","title":"Review Checklist"},{"location":"systems/12-message-queues/","text":"Message Queues \u00b6 Asynchronous communication patterns for decoupled, scalable systems ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing different message queue patterns, explain them simply. Prompts to guide you: What is a message queue in one sentence? Your answer: [Fill in after implementation] Why do we need message queues? Your answer: [Fill in after implementation] Real-world analogy for simple queue: Example: \"A simple queue is like a line at a store where...\" Your analogy: [Fill in] What is the producer-consumer pattern in one sentence? Your answer: [Fill in after implementation] How is pub-sub different from producer-consumer? Your answer: [Fill in after implementation] Real-world analogy for pub-sub: Example: \"Pub-sub is like a newsletter subscription where...\" Your analogy: [Fill in] What is a priority queue in one sentence? Your answer: [Fill in after implementation] When would you use a dead letter queue? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Synchronous API call to process 100 tasks: Time if each task takes 1 second: [Your guess: ?] Verified after learning: [Actual: ?] Message queue with 3 workers processing 100 tasks: Time if each task takes 1 second: [Your guess: ?] Speedup factor: [Your guess: ?x faster] Verified: [Actual] Memory usage prediction: Simple queue with 1000 messages: [Your guess: O(?)] Pub-sub with 5 subscribers and 100 messages: [Your guess: O(?)] Verified: [Actual] Scenario Predictions \u00b6 Scenario 1: Image upload service - users upload photos that need resizing Should you use message queue? [Yes/No - Why?] Pattern to use: [Simple queue/Producer-consumer/Pub-sub/Priority] Why that pattern? [Fill in your reasoning] What happens without queue? [Fill in] Scenario 2: Notification system - send email AND SMS AND push notification Should you use message queue? [Yes/No - Why?] Pattern to use: [Simple queue/Producer-consumer/Pub-sub/Priority] Why that pattern? [Fill in your reasoning] How many times is each message delivered? [Fill in] Scenario 3: Payment processing - some customers are VIP, need faster processing Should you use message queue? [Yes/No - Why?] Pattern to use: [Simple queue/Producer-consumer/Pub-sub/Priority] Why that pattern? [Fill in your reasoning] How do you prevent low-priority starvation? [Fill in] Trade-off Quiz \u00b6 Question: When is a message queue WORSE than direct synchronous calls? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN difference between a queue and pub-sub? Queue is faster Queue stores messages longer Queue delivers to one consumer, pub-sub to many Queue supports priorities Verify after implementation: [Which one(s)?] Question: What does \"at-least-once delivery\" mean? Your answer: [Fill in before implementation] What problem does it cause? [Fill in] Verified: [Actual answer after learning] Question: When should you use a dead letter queue? Your answer: [Fill in before implementation] Verified: [Fill in after implementation] Before/After: Why Message Queues Matter \u00b6 Your task: Compare synchronous vs message queue vs pub-sub approaches to understand the impact. Example: Image Processing Service \u00b6 Problem: Users upload images that need to be resized, compressed, and thumbnailed. Each operation takes 2 seconds. Approach 1: Synchronous Processing (No Queue) \u00b6 // Naive approach - Process immediately in request handler public class SynchronousImageService { public UploadResponse uploadImage(Image image) { // User waits for all processing to complete resize(image); // 2 seconds compress(image); // 2 seconds thumbnail(image); // 2 seconds return new UploadResponse(\"success\"); } // Total user wait time: 6 seconds! } Analysis: User wait time: 6 seconds for each upload Scalability: Limited by processing capacity Failure handling: User sees error immediately For 100 uploads: 600 seconds (10 minutes) Problems: Poor user experience (slow response) Request timeout on slow operations No retry mechanism Server blocked during processing Approach 2: Simple Message Queue (Async Processing) \u00b6 // Better approach - Queue work for background processing public class QueuedImageService { private final SimpleMessageQueue queue; private final List<Worker> workers; // 3 worker threads public UploadResponse uploadImage(Image image) { // Queue the work immediately queue.send(new ImageProcessingTask(image)); // Return immediately - user doesn't wait! return new UploadResponse(\"processing\"); } // Workers process in background // Total user wait time: <100ms (just queue operation) } Analysis: User wait time: <100ms (instant response) Scalability: Can add more workers Failure handling: Automatic retry with DLQ For 100 uploads with 3 workers: ~200 seconds (parallelized) Benefits: Fast user response Decoupled processing Horizontal scaling Retry mechanism Approach 3: Pub-Sub (Multiple Subscribers) \u00b6 // Best approach - Multiple services process independently public class PubSubImageService { private final PubSubMessageQueue pubsub; public UploadResponse uploadImage(Image image) { // Publish once to \"image.uploaded\" topic pubsub.publish(\"image.uploaded\", new ImageEvent(image)); // Multiple subscribers receive: // - Resize service // - Analytics service // - Notification service return new UploadResponse(\"processing\"); } // Each service processes independently! } Analysis: User wait time: <100ms (instant response) Scalability: Each subscriber scales independently Extensibility: Add new subscribers without code changes Loose coupling: Services don't know about each other Performance Comparison \u00b6 Approach Upload Time 100 Uploads Scalability Failure Handling Synchronous 6 sec 600 sec (10 min) Poor User sees error Queue (3 workers) <100ms ~200 sec (3 min) Good Auto retry + DLQ Pub-Sub <100ms ~200 sec (3 min) Excellent Per-service retry Your calculation: For 1,000 uploads with 10 workers: Synchronous: _____ seconds Queue: _____ seconds Speedup: _____ times faster Why Does Message Queue Work? \u00b6 Key insight to understand: Without Queue: User \u2192 [Upload + Process] \u2192 Response \u2191 6 seconds wait \u2191 With Queue: User \u2192 [Upload] \u2192 Response (instant) Queue \u2192 [Worker 1] Process (async) \u2192 [Worker 2] Process (async) \u2192 [Worker 3] Process (async) After implementing, explain in your own words: Why does async processing improve user experience? [Your answer] What happens if a worker fails? [Your answer] When would you NOT use a message queue? [Your answer] Queue vs Pub-Sub: When to Use Each \u00b6 Use Queue when: One consumer should process each message Work distribution across workers Task processing (jobs, background work) Example: [Fill in your example] Use Pub-Sub when: Multiple consumers need each message Broadcasting events Event-driven architecture Example: [Fill in your example] After implementing, explain the difference: How does message delivery differ? [Your answer] Which one for image processing? Why? [Your answer] Which one for notifications? Why? [Your answer] Case Studies: Message Queues in the Wild \u00b6 Uber Ride Requests: Decoupling with a Task Queue \u00b6 Pattern: Point-to-Point (Task Queue) using a system like RabbitMQ or Apache Kafka. How it works: When a user requests a ride, the mobile application's API call doesn't wait for a driver to be found. Instead, it publishes a RideRequested message to a queue and immediately returns a response to the user. A separate pool of \"dispatcher\" microservices consumes tasks from this queue. These workers are responsible for the heavy lifting: finding nearby drivers, calculating ETAs, and sending notifications, all happening asynchronously. Key Takeaway: Message queues are fundamental for creating responsive and resilient systems. By decoupling the initial request from the complex backend processing, Uber's app feels fast and can handle massive bursts of requests, even if the backend services are temporarily slow. Twitter's Fan-out Service: The Power of Pub/Sub \u00b6 Pattern: Publish-Subscribe (Pub/Sub) for fanning out events. How it works: When a user tweets, that action is published as a single message to a \"Tweets\" topic in a system like Kafka. Many different downstream services subscribe to this topic. A \"fan-out\" service consumes the tweet and injects it into the home timeline caches of all the user's followers. A \"notifications\" service consumes it to send push notifications. A \"search\" service consumes it to index the tweet. Key Takeaway: Pub/sub is incredibly powerful for building extensible, loosely-coupled systems. The original tweeting service doesn't need to know about all the other services that care about new tweets. Teams can add new subscribers to the topic to build new features without ever changing the original service. Netflix Conductor: Orchestrating Workflows with Events \u00b6 Pattern: Pub/Sub for complex workflow orchestration. How it works: Encoding and processing a new movie is a complex, multi-step workflow. Netflix uses an event-driven orchestrator called Conductor. When a video upload is complete, a VideoUploaded event is published. A service consumes this and starts encoding, publishing a VideoEncodingSucceeded event on completion. This new event triggers multiple parallel actions: one service starts generating thumbnails, another runs quality control, and a third updates the catalog. Key Takeaway: Complex business processes can be modeled as a series of events and subscribers. This makes the system more resilient (a failed thumbnail generator doesn't stop the whole workflow) and easier to reason about than a single, monolithic application. Core Implementation \u00b6 Part 1: Simple Message Queue \u00b6 Your task: Implement a basic FIFO message queue. import java.util.*; import java.util.concurrent.*; /** * Simple Message Queue: FIFO with blocking operations * * Key principles: * - First In First Out ordering * - Blocking when empty (wait for messages) * - Thread-safe operations * - Decouples producers and consumers */ public class SimpleMessageQueue { private final Queue<Message> queue; private final int capacity; private final Object lock = new Object(); /** * Initialize simple message queue * * @param capacity Maximum queue size * * TODO: Initialize queue * - Create LinkedList for messages * - Set capacity limit */ public SimpleMessageQueue(int capacity) { // TODO: Initialize queue (LinkedList) // TODO: Store capacity this.queue = null; // Replace this.capacity = 0; } /** * Send message to queue (producer) * * @param message Message to send * @throws InterruptedException if interrupted while waiting * * TODO: Implement send * 1. Wait if queue is full * 2. Add message to queue * 3. Notify waiting consumers * * Hint: Use wait() and notifyAll() with synchronized block */ public void send(Message message) throws InterruptedException { synchronized (lock) { // TODO: Implement iteration/conditional logic // TODO: Add message to queue // TODO: Notify all waiting consumers // lock.notifyAll() } } /** * Receive message from queue (consumer) * * @return Next message from queue * @throws InterruptedException if interrupted while waiting * * TODO: Implement receive * 1. Wait if queue is empty * 2. Remove and return message * 3. Notify waiting producers */ public Message receive() throws InterruptedException { synchronized (lock) { // TODO: Implement iteration/conditional logic // TODO: Remove message from queue // TODO: Notify all waiting producers // lock.notifyAll() // TODO: Return message return null; // Replace } } /** * Try to receive with timeout * * @param timeoutMs Timeout in milliseconds * @return Message or null if timeout */ public Message receive(long timeoutMs) throws InterruptedException { synchronized (lock) { long deadline = System.currentTimeMillis() + timeoutMs; // TODO: Wait until message available or timeout // TODO: Implement iteration/conditional logic return null; // Replace (or message) } } /** * Get queue size */ public synchronized int size() { return queue.size(); } /** * Check if queue is empty */ public synchronized boolean isEmpty() { return queue.isEmpty(); } static class Message { String id; String content; long timestamp; public Message(String id, String content) { this.id = id; this.content = content; this.timestamp = System.currentTimeMillis(); } @Override public String toString() { return \"Message{id='\" + id + \"', content='\" + content + \"'}\"; } } } Part 2: Producer-Consumer Pattern \u00b6 Your task: Implement producer-consumer with multiple workers. /** * Producer-Consumer: Multiple producers and consumers processing work * * Key principles: * - Work distribution across consumers * - Load balancing * - Backpressure handling * - Graceful shutdown */ public class ProducerConsumer { private final SimpleMessageQueue queue; private final List<Thread> consumerThreads; private volatile boolean running; /** * Initialize producer-consumer system * * @param queueCapacity Queue size * @param numConsumers Number of consumer threads * * TODO: Initialize system * - Create message queue * - Create consumer threads * - Set running flag */ public ProducerConsumer(int queueCapacity, int numConsumers) { // TODO: Create SimpleMessageQueue // TODO: Initialize consumer threads list // TODO: Track state this.queue = null; // Replace this.consumerThreads = null; // Replace } /** * Start all consumers * * TODO: Start consumer threads * - Each consumer polls queue and processes messages * - Handle InterruptedException * - Check running flag */ public void start() { // TODO: Implement iteration/conditional logic } /** * Produce message (called by producers) * * TODO: Send message to queue */ public void produce(String messageId, String content) throws InterruptedException { // TODO: Create Message and send to queue } /** * Process message (override in subclass for custom logic) * * TODO: Implement message processing * - Extract message content * - Perform work * - Handle errors */ protected void processMessage(SimpleMessageQueue.Message message) { // TODO: Process message (simulated work) System.out.println(Thread.currentThread().getName() + \" processing: \" + message); // TODO: Simulate work try { Thread.sleep(100); } catch (InterruptedException e) { Thread.currentThread().interrupt(); } } /** * Shutdown system * * TODO: Graceful shutdown * - Set running to false * - Wait for consumers to finish */ public void shutdown() throws InterruptedException { // TODO: Track state // TODO: Interrupt all consumer threads // TODO: Wait for all threads to finish (join) } /** * Get queue statistics */ public QueueStats getStats() { return new QueueStats(queue.size(), consumerThreads.size()); } static class QueueStats { int queueSize; int activeConsumers; public QueueStats(int queueSize, int activeConsumers) { this.queueSize = queueSize; this.activeConsumers = activeConsumers; } } } Part 3: Publish-Subscribe Pattern \u00b6 Your task: Implement pub-sub for multiple subscribers. /** * Publish-Subscribe: Broadcast messages to multiple subscribers * * Key principles: * - One message delivered to all subscribers * - Topic-based routing * - Decoupled publishers and subscribers * - Each subscriber has own queue */ public class PubSubMessageQueue { private final Map<String, List<Subscriber>> topicSubscribers; private final Object lock = new Object(); /** * Initialize pub-sub system * * TODO: Initialize topic mapping */ public PubSubMessageQueue() { // TODO: Initialize topicSubscribers map (ConcurrentHashMap) this.topicSubscribers = null; // Replace } /** * Subscribe to topic * * @param topic Topic name * @param subscriber Subscriber to register * * TODO: Register subscriber * - Create topic if doesn't exist * - Add subscriber to topic list */ public void subscribe(String topic, Subscriber subscriber) { synchronized (lock) { // TODO: Get or create subscriber list for topic // TODO: Add subscriber to list System.out.println(subscriber.name + \" subscribed to \" + topic); } } /** * Unsubscribe from topic * * TODO: Remove subscriber from topic */ public void unsubscribe(String topic, Subscriber subscriber) { synchronized (lock) { // TODO: Get subscriber list for topic // TODO: Remove subscriber } } /** * Publish message to topic * * @param topic Topic to publish to * @param message Message to publish * * TODO: Deliver to all subscribers * - Get all subscribers for topic * - Send message to each subscriber's queue */ public void publish(String topic, SimpleMessageQueue.Message message) { synchronized (lock) { // TODO: Get subscribers for topic // TODO: Implement iteration/conditional logic System.out.println(\"Published to \" + topic + \": \" + message); } } /** * Get topic statistics */ public Map<String, Integer> getTopicStats() { Map<String, Integer> stats = new HashMap<>(); synchronized (lock) { for (Map.Entry<String, List<Subscriber>> entry : topicSubscribers.entrySet()) { stats.put(entry.getKey(), entry.getValue().size()); } } return stats; } static class Subscriber { String name; Queue<SimpleMessageQueue.Message> queue; public Subscriber(String name) { this.name = name; this.queue = new LinkedList<>(); } public void deliver(SimpleMessageQueue.Message message) { queue.offer(message); } public SimpleMessageQueue.Message receive() { return queue.poll(); } public int getQueueSize() { return queue.size(); } } } Part 4: Priority Message Queue \u00b6 Your task: Implement priority queue for urgent messages. /** * Priority Message Queue: Process high-priority messages first * * Key principles: * - Priority levels (HIGH, MEDIUM, LOW) * - Higher priority processed first * - FIFO within same priority * - Prevents starvation of low priority */ public class PriorityMessageQueue { private final PriorityQueue<PriorityMessage> queue; private final Object lock = new Object(); private final int capacity; /** * Initialize priority queue * * @param capacity Maximum queue size * * TODO: Initialize priority queue * - Create PriorityQueue with comparator * - Sort by priority then timestamp */ public PriorityMessageQueue(int capacity) { // TODO: Create PriorityQueue with comparator // Comparator: First by priority (descending), then timestamp (ascending) // TODO: Store capacity this.queue = null; // Replace this.capacity = 0; } /** * Send message with priority * * TODO: Add message to priority queue * - Wait if queue is full * - Add message * - Notify consumers */ public void send(PriorityMessage message) throws InterruptedException { synchronized (lock) { // TODO: Wait while queue is full // TODO: Add message to queue // TODO: Notify waiting consumers } } /** * Receive highest priority message * * TODO: Get message with highest priority * - Wait if queue is empty * - Remove highest priority message * - Notify producers */ public PriorityMessage receive() throws InterruptedException { synchronized (lock) { // TODO: Wait while queue is empty // TODO: Poll highest priority message // TODO: Notify waiting producers return null; // Replace } } /** * Get queue size */ public synchronized int size() { return queue.size(); } static class PriorityMessage implements Comparable<PriorityMessage> { String id; String content; Priority priority; long timestamp; public PriorityMessage(String id, String content, Priority priority) { this.id = id; this.content = content; this.priority = priority; this.timestamp = System.currentTimeMillis(); } @Override public int compareTo(PriorityMessage other) { // TODO: Compare by priority first (higher priority first) // Hint: // int priorityCompare = other.priority.value - this.priority.value; // if (priorityCompare != 0) return priorityCompare; // return Long.compare(this.timestamp, other.timestamp); return 0; // Replace } @Override public String toString() { return \"PriorityMessage{id='\" + id + \"', priority=\" + priority + \"}\"; } } enum Priority { LOW(1), MEDIUM(2), HIGH(3); final int value; Priority(int value) { this.value = value; } } } Part 5: Dead Letter Queue \u00b6 Your task: Implement dead letter queue for failed messages. /** * Dead Letter Queue: Handle messages that fail processing * * Key principles: * - Retry failed messages * - Max retry limit * - Move to DLQ after max retries * - Allows manual inspection/reprocessing */ public class DeadLetterQueue { private final SimpleMessageQueue mainQueue; private final SimpleMessageQueue dlq; private final int maxRetries; private final Map<String, Integer> retryCount; /** * Initialize dead letter queue system * * @param capacity Queue capacity * @param maxRetries Maximum retry attempts * * TODO: Initialize queues * - Create main queue * - Create DLQ * - Initialize retry counter */ public DeadLetterQueue(int capacity, int maxRetries) { // TODO: Create main queue // TODO: Create DLQ // TODO: Store maxRetries // TODO: Initialize retry counter map this.mainQueue = null; // Replace this.dlq = null; // Replace this.maxRetries = 0; this.retryCount = null; // Replace } /** * Send message to main queue */ public void send(SimpleMessageQueue.Message message) throws InterruptedException { // TODO: Send to main queue // Initialize retry count to 0 } /** * Process message with retry logic * * @param processor Message processor * @return true if processed successfully * * TODO: Process with retries * 1. Receive message from main queue * 2. Try to process * 3. If fails, check retry count * 4. If under limit, requeue with incremented count * 5. If over limit, move to DLQ */ public boolean processWithRetry(MessageProcessor processor) throws InterruptedException { // TODO: Receive message from main queue // TODO: Try to process message try { // processor.process(message) // return true if successful } catch (Exception e) { // TODO: Get current retry count // TODO: Implement iteration/conditional logic // TODO: Implement iteration/conditional logic } return false; // Replace } /** * Get message from DLQ for manual processing */ public SimpleMessageQueue.Message receiveDLQ() throws InterruptedException { return dlq.receive(); } /** * Reprocess message from DLQ (manual retry) */ public void reprocessFromDLQ(SimpleMessageQueue.Message message) throws InterruptedException { // TODO: Reset retry count and send to main queue } /** * Get statistics */ public DLQStats getStats() { return new DLQStats( mainQueue.size(), dlq.size(), retryCount.size() ); } interface MessageProcessor { void process(SimpleMessageQueue.Message message) throws Exception; } static class DLQStats { int mainQueueSize; int dlqSize; int messagesWithRetries; public DLQStats(int mainQueueSize, int dlqSize, int messagesWithRetries) { this.mainQueueSize = mainQueueSize; this.dlqSize = dlqSize; this.messagesWithRetries = messagesWithRetries; } @Override public String toString() { return \"DLQStats{main=\" + mainQueueSize + \", dlq=\" + dlqSize + \", retrying=\" + messagesWithRetries + \"}\"; } } } Client Code \u00b6 import java.util.*; public class MessageQueuesClient { public static void main(String[] args) throws Exception { testSimpleQueue(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testProducerConsumer(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testPubSub(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testPriorityQueue(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testDeadLetterQueue(); } static void testSimpleQueue() throws InterruptedException { System.out.println(\"=== Simple Message Queue Test ===\\n\"); SimpleMessageQueue queue = new SimpleMessageQueue(5); // Test: Producer thread Thread producer = new Thread(() -> { try { for (int i = 1; i <= 5; i++) { SimpleMessageQueue.Message msg = new SimpleMessageQueue.Message(\"msg\" + i, \"Content \" + i); queue.send(msg); System.out.println(\"Sent: \" + msg); Thread.sleep(100); } } catch (InterruptedException e) { Thread.currentThread().interrupt(); } }); // Test: Consumer thread Thread consumer = new Thread(() -> { try { for (int i = 1; i <= 5; i++) { SimpleMessageQueue.Message msg = queue.receive(); System.out.println(\"Received: \" + msg); } } catch (InterruptedException e) { Thread.currentThread().interrupt(); } }); producer.start(); consumer.start(); producer.join(); consumer.join(); System.out.println(\"\\nFinal queue size: \" + queue.size()); } static void testProducerConsumer() throws InterruptedException { System.out.println(\"=== Producer-Consumer Test ===\\n\"); ProducerConsumer pc = new ProducerConsumer(10, 3); pc.start(); // Produce messages System.out.println(\"Producing 10 messages...\"); for (int i = 1; i <= 10; i++) { pc.produce(\"msg\" + i, \"Task \" + i); Thread.sleep(50); } // Let consumers process Thread.sleep(2000); System.out.println(\"\\nStats: \" + pc.getStats()); pc.shutdown(); } static void testPubSub() throws InterruptedException { System.out.println(\"=== Pub-Sub Test ===\\n\"); PubSubMessageQueue pubsub = new PubSubMessageQueue(); // Create subscribers PubSubMessageQueue.Subscriber sub1 = new PubSubMessageQueue.Subscriber(\"User1\"); PubSubMessageQueue.Subscriber sub2 = new PubSubMessageQueue.Subscriber(\"User2\"); PubSubMessageQueue.Subscriber sub3 = new PubSubMessageQueue.Subscriber(\"User3\"); // Subscribe to topics pubsub.subscribe(\"news\", sub1); pubsub.subscribe(\"news\", sub2); pubsub.subscribe(\"sports\", sub2); pubsub.subscribe(\"sports\", sub3); System.out.println(\"\\nTopic stats: \" + pubsub.getTopicStats()); // Publish messages System.out.println(\"\\nPublishing messages:\"); pubsub.publish(\"news\", new SimpleMessageQueue.Message(\"n1\", \"Breaking news!\")); pubsub.publish(\"sports\", new SimpleMessageQueue.Message(\"s1\", \"Game update!\")); // Check subscriber queues System.out.println(\"\\nSubscriber queues:\"); System.out.println(\"User1 queue size: \" + sub1.getQueueSize()); System.out.println(\"User2 queue size: \" + sub2.getQueueSize()); System.out.println(\"User3 queue size: \" + sub3.getQueueSize()); // Receive messages System.out.println(\"\\nUser1 receives: \" + sub1.receive()); System.out.println(\"User2 receives: \" + sub2.receive()); System.out.println(\"User2 receives: \" + sub2.receive()); } static void testPriorityQueue() throws InterruptedException { System.out.println(\"=== Priority Queue Test ===\\n\"); PriorityMessageQueue queue = new PriorityMessageQueue(10); // Send messages with different priorities System.out.println(\"Sending messages:\"); queue.send(new PriorityMessageQueue.PriorityMessage( \"m1\", \"Low priority\", PriorityMessageQueue.Priority.LOW)); queue.send(new PriorityMessageQueue.PriorityMessage( \"m2\", \"High priority\", PriorityMessageQueue.Priority.HIGH)); queue.send(new PriorityMessageQueue.PriorityMessage( \"m3\", \"Medium priority\", PriorityMessageQueue.Priority.MEDIUM)); queue.send(new PriorityMessageQueue.PriorityMessage( \"m4\", \"High priority 2\", PriorityMessageQueue.Priority.HIGH)); System.out.println(\"Queue size: \" + queue.size()); // Receive in priority order System.out.println(\"\\nReceiving messages (priority order):\"); while (queue.size() > 0) { PriorityMessageQueue.PriorityMessage msg = queue.receive(); System.out.println(\"Received: \" + msg); } } static void testDeadLetterQueue() throws InterruptedException { System.out.println(\"=== Dead Letter Queue Test ===\\n\"); DeadLetterQueue dlq = new DeadLetterQueue(10, 3); // Create failing processor DeadLetterQueue.MessageProcessor failingProcessor = message -> { System.out.println(\"Processing: \" + message.id); if (message.id.equals(\"msg2\")) { throw new Exception(\"Simulated failure\"); } }; // Send messages System.out.println(\"Sending messages:\"); dlq.send(new SimpleMessageQueue.Message(\"msg1\", \"Good message\")); dlq.send(new SimpleMessageQueue.Message(\"msg2\", \"Bad message\")); dlq.send(new SimpleMessageQueue.Message(\"msg3\", \"Good message\")); // Process messages System.out.println(\"\\nProcessing messages:\"); for (int i = 0; i < 3; i++) { boolean success = dlq.processWithRetry(failingProcessor); System.out.println(\"Process attempt \" + (i+1) + \": \" + (success ? \"SUCCESS\" : \"FAILED\")); System.out.println(\"Stats: \" + dlq.getStats()); } // Try more times to move bad message to DLQ System.out.println(\"\\nRetrying failed message:\"); for (int i = 0; i < 3; i++) { dlq.processWithRetry(failingProcessor); System.out.println(\"Stats: \" + dlq.getStats()); } } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken message queue implementations. This tests your understanding of common message queue pitfalls. Challenge 1: Lost Messages \u00b6 /** * This message queue is losing messages under load. * It has 2 BUGS. Find them! */ public class BuggyMessageQueue { private final Queue<Message> queue; private final int capacity; public BuggyMessageQueue(int capacity) { this.queue = new LinkedList<>(); this.capacity = capacity; } public void send(Message message) throws InterruptedException { if (queue.size() >= capacity) { Thread.sleep(100); // Wait for space } queue.offer(message); } public Message receive() throws InterruptedException { if (queue.isEmpty()) { return null; // What happens to waiting consumers? } return queue.poll(); } } Your debugging: Bug 1: [What\\'s the bug?] Scenario that exposes bug: [Fill in] Bug 2: [What\\'s the bug?] Why does it lose messages? [Fill in] Click to verify your answers Bug 1 (send method): Missing synchronization. Multiple threads can check queue.size() simultaneously, both see space available, both add messages, exceeding capacity. Race condition on queue operations. Fix: public synchronized void send(Message message) throws InterruptedException { while (queue.size() >= capacity) { wait(); // Wait until space available } queue.offer(message); notifyAll(); // Wake up waiting receivers } Bug 2 (receive method): Returns null instead of waiting. Consumers poll repeatedly or miss messages. No coordination between producers and consumers. Fix: public synchronized Message receive() throws InterruptedException { while (queue.isEmpty()) { wait(); // Wait until message available } Message msg = queue.poll(); notifyAll(); // Wake up waiting senders return msg; } Why messages are lost: Without synchronization, concurrent operations can corrupt the queue state. Without wait/notify, producers may overwrite or consumers may miss messages. Challenge 2: Duplicate Processing \u00b6 /** * This consumer processes some messages twice! * It has 1 CRITICAL BUG related to at-least-once delivery. */ public class BuggyConsumer { private final SimpleMessageQueue queue; public void processMessages() { while (true) { try { Message msg = queue.receive(); processMessage(msg); // Long operation - 5 seconds // Message was already removed from queue! } catch (Exception e) { // Message lost or processed twice? System.err.println(\"Error: \" + e.getMessage()); } } } private void processMessage(Message msg) throws Exception { // Simulate processing if (msg.content.contains(\"fail\")) { throw new Exception(\"Processing failed\"); } // Save to database } } Your debugging: Bug: [What's the problem with this pattern?] At-least-once delivery issue: [Explain the problem] Exactly-once challenge: [Why is this hard to solve?] Fix option 1: [How to make it safe?] Fix option 2: [Alternative approach?] Click to verify your answer Bug: Message is removed from queue BEFORE successful processing. If processMessage fails, the message is lost. If we add retry logic, we might process it multiple times. At-least-once delivery problem: 1. Receive message (removed from queue) 2. Start processing 3. Process fails halfway 4. Message is lost - neither in queue nor processed! Exactly-once challenge: Very hard to guarantee. You need: Idempotent processing (safe to process twice) Transactional processing Deduplication mechanism Fix Option 1 - Use acknowledgment pattern: public void processMessages() { while (true) { Message msg = null; try { msg = queue.receive(); processMessage(msg); queue.acknowledge(msg.id); // Explicit ACK } catch (Exception e) { if (msg != null) { queue.nack(msg.id); // Negative ACK - requeue } } } } Fix Option 2 - Use idempotent processing: private void processMessage(Message msg) throws Exception { // Check if already processed (deduplication) if (database.isProcessed(msg.id)) { return; // Skip duplicate } // Process and mark as processed in same transaction database.transaction(() -> { doActualProcessing(msg); database.markProcessed(msg.id); }); } Key insight: At-least-once delivery requires idempotent consumers! Challenge 3: Message Ordering Violation \u00b6 /** * This pub-sub system delivers messages OUT OF ORDER. * It has 1 SUBTLE BUG in concurrent delivery. */ public class BuggyPubSub { private final Map<String, List<Subscriber>> topicSubscribers; private final ExecutorService executor; public BuggyPubSub() { this.topicSubscribers = new ConcurrentHashMap<>(); this.executor = Executors.newFixedThreadPool(10); } public void publish(String topic, Message message) { List<Subscriber> subscribers = topicSubscribers.get(topic); if (subscribers == null) return; for (Subscriber sub : subscribers) { executor.submit(() -> { sub.deliver(message); }); } // Messages may arrive out of order to each subscriber } } Your debugging: Bug: [What causes out-of-order delivery?] Example scenario: [When does this fail?] Why is ordering important? [Give examples] Fix for ordered delivery: [How to guarantee order?] Trade-off: [What do you sacrifice?] Click to verify your answer Bug: Using thread pool for parallel delivery breaks message ordering. Messages sent to thread pool may execute in any order. Example scenario: Publisher sends: msg1, msg2, msg3 Thread pool: - Thread 2 picks up msg1 (delayed) - Thread 1 picks up msg2 (fast) - Thread 3 picks up msg3 (fast) Subscriber receives: msg2, msg3, msg1 \u274c Why ordering matters: Database updates (create \u2192 update \u2192 delete) State transitions (pending \u2192 processing \u2192 completed) Financial transactions (debit must follow credit) Event sourcing (events must be ordered) Fix Option 1 - Serial delivery per subscriber: public void publish(String topic, Message message) { List<Subscriber> subscribers = topicSubscribers.get(topic); if (subscribers == null) return; // Each subscriber gets messages in order for (Subscriber sub : subscribers) { sub.deliverInOrder(message); // Subscriber queues internally } } class Subscriber { private final Queue<Message> orderedQueue; private final SingleThreadExecutor executor; // One thread per subscriber void deliverInOrder(Message msg) { orderedQueue.offer(msg); executor.execute(() -> { Message m = orderedQueue.poll(); process(m); // Processed in order }); } } Fix Option 2 - Partition-based ordering: // Only guarantee order within partition key (like Kafka) public void publish(String topic, Message message, String partitionKey) { int partition = Math.abs(partitionKey.hashCode()) % numPartitions; // Messages with same key go to same partition (ordered) partitions[partition].deliver(message); } Trade-off: Serial processing is slower than parallel. Must choose between throughput and ordering guarantees. Challenge 4: Dead Letter Queue Overflow \u00b6 /** * This DLQ system fills up and loses messages. * It has 2 BUGS in retry logic. */ public class BuggyDLQ { private final SimpleMessageQueue mainQueue; private final SimpleMessageQueue dlq; private final int maxRetries; private final Map<String, Integer> retryCount; public boolean processWithRetry(MessageProcessor processor) throws InterruptedException { Message msg = mainQueue.receive(); try { processor.process(msg); return true; } catch (Exception e) { int count = retryCount.getOrDefault(msg.id, 0); count++; retryCount.put(msg.id, count); if (count < maxRetries) { mainQueue.send(msg); return false; } else { dlq.send(msg); retryCount.remove(msg.id); return false; } } } } Your debugging: Bug 1: [What's wrong with immediate retry?] Bug 1 impact: [What happens to the system?] Bug 1 fix: [How to fix?] Bug 2: [What if DLQ is full?] Bug 2 impact: [Messages lost or blocked?] Bug 2 fix: [How to handle full DLQ?] Additional questions: Retry timing strategy: [Constant vs exponential backoff?] When to stop retrying: [How to decide max retries?] DLQ monitoring: [How to detect problems?] Click to verify your answers Bug 1: Immediate retry causes rapid retry loop. If a message consistently fails (e.g., bad data), it will be retried maxRetries times in rapid succession, wasting CPU and blocking other messages. Bug 1 fix - Add exponential backoff: if (count < maxRetries) { long delay = (long) Math.pow(2, count) * 1000; // 1s, 2s, 4s, 8s... Thread.sleep(delay); mainQueue.send(msg); return false; } Or better, use a delayed queue: if (count < maxRetries) { long delayMs = (long) Math.pow(2, count) * 1000; delayedQueue.send(msg, delayMs); // Reprocess after delay return false; } Bug 2: If DLQ is full, dlq.send(msg) will block forever (if blocking queue) or throw exception (if non-blocking). Message processing stalls. Bug 2 fix - Handle DLQ overflow: } else { // Try to send to DLQ with timeout boolean sent = dlq.sendWithTimeout(msg, 5000); if (!sent) { // DLQ full - critical alert! alertOps(\"DLQ full! Message: \" + msg.id); // Option 1: Drop message (with logging) logDroppedMessage(msg); // Option 2: Write to disk/database as backup persistToBackup(msg); } retryCount.remove(msg.id); return false; } Retry strategies: Constant backoff: Fixed delay (1s, 1s, 1s) Pro: Simple Con: Doesn't give system time to recover Exponential backoff: Increasing delay (1s, 2s, 4s, 8s) Pro: Reduces load during problems Con: Messages delayed longer Max retry limits: Transient errors (network): High retries (10+) Permanent errors (validation): Low retries (3) Timeout errors: Medium retries (5) DLQ monitoring: Alert when DLQ size > threshold Alert when DLQ growth rate is high Dashboard showing DLQ trends Automated DLQ processing for known issues Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Understood message loss causes (synchronization, race conditions) Understood duplicate processing (at-least-once vs exactly-once) Understood ordering violations (concurrent delivery) Understood DLQ overflow (backoff, capacity planning) Could explain each bug to someone else Learned common message queue mistakes to avoid Common mistakes you discovered: [List the patterns you noticed] [Fill in] [Fill in] [Fill in] Key takeaways: Concurrency: [What did you learn?] Delivery guarantees: [What did you learn?] Error handling: [What did you learn?] Monitoring: [What did you learn?] Decision Framework \u00b6 Questions to answer after implementation: 1. Pattern Selection \u00b6 When to use Simple Queue? Your scenario: [Fill in] Key factors: [Fill in] When to use Producer-Consumer? Your scenario: [Fill in] Key factors: [Fill in] When to use Pub-Sub? Your scenario: [Fill in] Key factors: [Fill in] When to use Priority Queue? Your scenario: [Fill in] Key factors: [Fill in] When to use Dead Letter Queue? Your scenario: [Fill in] Key factors: [Fill in] 2. Trade-offs \u00b6 Simple Queue: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Producer-Consumer: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Pub-Sub: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Priority Queue: Pros: [Fill in after understanding] Cons: [Fill in after understanding] 3. Your Decision Tree \u00b6 Build your decision tree after practicing: flowchart LR Start[\"What is your communication pattern?\"] N1[\"?\"] Start -->|\"One-to-one async processing\"| N1 N2[\"?\"] Start -->|\"Multiple workers needed\"| N2 N3[\"?\"] Start -->|\"Broadcast to multiple consumers\"| N3 N4[\"?\"] Start -->|\"Urgent messages need priority\"| N4 N5[\"?\"] Start -->|\"Need retry and failure handling\"| N5 Practice \u00b6 Scenario 1: Process uploaded images \u00b6 Requirements: Users upload images Need to resize, compress, generate thumbnails Processing takes 5-10 seconds Want fast upload response Handle processing failures Your design: Which pattern would you choose? [Fill in] Why? [Fill in] How many workers? [Fill in] Failure handling strategy? [Fill in] Scenario 2: Notification system \u00b6 Requirements: Send notifications via email, SMS, push Users subscribe to notification types Some notifications are urgent Must deliver to all channels Track delivery failures Your design: Which pattern would you choose? [Fill in] Why? [Fill in] How to handle different channels? [Fill in] Priority strategy? [Fill in] Scenario 3: Order processing system \u00b6 Requirements: Process orders from multiple sources Some orders need priority (VIP customers) Payment processing might fail Need retry logic Monitor failed orders Your design: Which pattern would you choose? [Fill in] Why? [Fill in] How to handle priorities? [Fill in] Retry strategy? [Fill in] Review Checklist \u00b6 Simple message queue implemented with blocking operations Producer-consumer implemented with multiple workers Pub-sub implemented with topic-based routing Priority queue implemented with priority ordering Dead letter queue implemented with retry logic Understand when to use each pattern Can explain trade-offs between patterns Built decision tree for pattern selection Completed practice scenarios Mastery Certification \u00b6 I certify that I can: Implement all message queue patterns from memory Explain when and why to use each pattern Identify the correct pattern for new scenarios Analyze delivery guarantees and trade-offs Debug common message queue problems Design scalable message-based systems Explain at-least-once vs exactly-once delivery Implement retry logic with dead letter queues Compare queue vs pub-sub architectures Teach these concepts to someone else","title":"12. Message Queues"},{"location":"systems/12-message-queues/#message-queues","text":"Asynchronous communication patterns for decoupled, scalable systems","title":"Message Queues"},{"location":"systems/12-message-queues/#eli5-explain-like-im-5","text":"Your task: After implementing different message queue patterns, explain them simply. Prompts to guide you: What is a message queue in one sentence? Your answer: [Fill in after implementation] Why do we need message queues? Your answer: [Fill in after implementation] Real-world analogy for simple queue: Example: \"A simple queue is like a line at a store where...\" Your analogy: [Fill in] What is the producer-consumer pattern in one sentence? Your answer: [Fill in after implementation] How is pub-sub different from producer-consumer? Your answer: [Fill in after implementation] Real-world analogy for pub-sub: Example: \"Pub-sub is like a newsletter subscription where...\" Your analogy: [Fill in] What is a priority queue in one sentence? Your answer: [Fill in after implementation] When would you use a dead letter queue? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/12-message-queues/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/12-message-queues/#beforeafter-why-message-queues-matter","text":"Your task: Compare synchronous vs message queue vs pub-sub approaches to understand the impact.","title":"Before/After: Why Message Queues Matter"},{"location":"systems/12-message-queues/#case-studies-message-queues-in-the-wild","text":"","title":"Case Studies: Message Queues in the Wild"},{"location":"systems/12-message-queues/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/12-message-queues/#client-code","text":"import java.util.*; public class MessageQueuesClient { public static void main(String[] args) throws Exception { testSimpleQueue(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testProducerConsumer(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testPubSub(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testPriorityQueue(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testDeadLetterQueue(); } static void testSimpleQueue() throws InterruptedException { System.out.println(\"=== Simple Message Queue Test ===\\n\"); SimpleMessageQueue queue = new SimpleMessageQueue(5); // Test: Producer thread Thread producer = new Thread(() -> { try { for (int i = 1; i <= 5; i++) { SimpleMessageQueue.Message msg = new SimpleMessageQueue.Message(\"msg\" + i, \"Content \" + i); queue.send(msg); System.out.println(\"Sent: \" + msg); Thread.sleep(100); } } catch (InterruptedException e) { Thread.currentThread().interrupt(); } }); // Test: Consumer thread Thread consumer = new Thread(() -> { try { for (int i = 1; i <= 5; i++) { SimpleMessageQueue.Message msg = queue.receive(); System.out.println(\"Received: \" + msg); } } catch (InterruptedException e) { Thread.currentThread().interrupt(); } }); producer.start(); consumer.start(); producer.join(); consumer.join(); System.out.println(\"\\nFinal queue size: \" + queue.size()); } static void testProducerConsumer() throws InterruptedException { System.out.println(\"=== Producer-Consumer Test ===\\n\"); ProducerConsumer pc = new ProducerConsumer(10, 3); pc.start(); // Produce messages System.out.println(\"Producing 10 messages...\"); for (int i = 1; i <= 10; i++) { pc.produce(\"msg\" + i, \"Task \" + i); Thread.sleep(50); } // Let consumers process Thread.sleep(2000); System.out.println(\"\\nStats: \" + pc.getStats()); pc.shutdown(); } static void testPubSub() throws InterruptedException { System.out.println(\"=== Pub-Sub Test ===\\n\"); PubSubMessageQueue pubsub = new PubSubMessageQueue(); // Create subscribers PubSubMessageQueue.Subscriber sub1 = new PubSubMessageQueue.Subscriber(\"User1\"); PubSubMessageQueue.Subscriber sub2 = new PubSubMessageQueue.Subscriber(\"User2\"); PubSubMessageQueue.Subscriber sub3 = new PubSubMessageQueue.Subscriber(\"User3\"); // Subscribe to topics pubsub.subscribe(\"news\", sub1); pubsub.subscribe(\"news\", sub2); pubsub.subscribe(\"sports\", sub2); pubsub.subscribe(\"sports\", sub3); System.out.println(\"\\nTopic stats: \" + pubsub.getTopicStats()); // Publish messages System.out.println(\"\\nPublishing messages:\"); pubsub.publish(\"news\", new SimpleMessageQueue.Message(\"n1\", \"Breaking news!\")); pubsub.publish(\"sports\", new SimpleMessageQueue.Message(\"s1\", \"Game update!\")); // Check subscriber queues System.out.println(\"\\nSubscriber queues:\"); System.out.println(\"User1 queue size: \" + sub1.getQueueSize()); System.out.println(\"User2 queue size: \" + sub2.getQueueSize()); System.out.println(\"User3 queue size: \" + sub3.getQueueSize()); // Receive messages System.out.println(\"\\nUser1 receives: \" + sub1.receive()); System.out.println(\"User2 receives: \" + sub2.receive()); System.out.println(\"User2 receives: \" + sub2.receive()); } static void testPriorityQueue() throws InterruptedException { System.out.println(\"=== Priority Queue Test ===\\n\"); PriorityMessageQueue queue = new PriorityMessageQueue(10); // Send messages with different priorities System.out.println(\"Sending messages:\"); queue.send(new PriorityMessageQueue.PriorityMessage( \"m1\", \"Low priority\", PriorityMessageQueue.Priority.LOW)); queue.send(new PriorityMessageQueue.PriorityMessage( \"m2\", \"High priority\", PriorityMessageQueue.Priority.HIGH)); queue.send(new PriorityMessageQueue.PriorityMessage( \"m3\", \"Medium priority\", PriorityMessageQueue.Priority.MEDIUM)); queue.send(new PriorityMessageQueue.PriorityMessage( \"m4\", \"High priority 2\", PriorityMessageQueue.Priority.HIGH)); System.out.println(\"Queue size: \" + queue.size()); // Receive in priority order System.out.println(\"\\nReceiving messages (priority order):\"); while (queue.size() > 0) { PriorityMessageQueue.PriorityMessage msg = queue.receive(); System.out.println(\"Received: \" + msg); } } static void testDeadLetterQueue() throws InterruptedException { System.out.println(\"=== Dead Letter Queue Test ===\\n\"); DeadLetterQueue dlq = new DeadLetterQueue(10, 3); // Create failing processor DeadLetterQueue.MessageProcessor failingProcessor = message -> { System.out.println(\"Processing: \" + message.id); if (message.id.equals(\"msg2\")) { throw new Exception(\"Simulated failure\"); } }; // Send messages System.out.println(\"Sending messages:\"); dlq.send(new SimpleMessageQueue.Message(\"msg1\", \"Good message\")); dlq.send(new SimpleMessageQueue.Message(\"msg2\", \"Bad message\")); dlq.send(new SimpleMessageQueue.Message(\"msg3\", \"Good message\")); // Process messages System.out.println(\"\\nProcessing messages:\"); for (int i = 0; i < 3; i++) { boolean success = dlq.processWithRetry(failingProcessor); System.out.println(\"Process attempt \" + (i+1) + \": \" + (success ? \"SUCCESS\" : \"FAILED\")); System.out.println(\"Stats: \" + dlq.getStats()); } // Try more times to move bad message to DLQ System.out.println(\"\\nRetrying failed message:\"); for (int i = 0; i < 3; i++) { dlq.processWithRetry(failingProcessor); System.out.println(\"Stats: \" + dlq.getStats()); } } }","title":"Client Code"},{"location":"systems/12-message-queues/#debugging-challenges","text":"Your task: Find and fix bugs in broken message queue implementations. This tests your understanding of common message queue pitfalls.","title":"Debugging Challenges"},{"location":"systems/12-message-queues/#decision-framework","text":"Questions to answer after implementation:","title":"Decision Framework"},{"location":"systems/12-message-queues/#practice","text":"","title":"Practice"},{"location":"systems/12-message-queues/#review-checklist","text":"Simple message queue implemented with blocking operations Producer-consumer implemented with multiple workers Pub-sub implemented with topic-based routing Priority queue implemented with priority ordering Dead letter queue implemented with retry logic Understand when to use each pattern Can explain trade-offs between patterns Built decision tree for pattern selection Completed practice scenarios","title":"Review Checklist"},{"location":"systems/13-stream-processing/","text":"Stream Processing \u00b6 Real-time data processing with windowing, watermarks, and stateful operations ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing stream processing patterns, explain them simply. Prompts to guide you: What is stream processing in one sentence? Your answer: [Fill in after implementation] What is a window in stream processing? Your answer: [Fill in after implementation] Real-world analogy for tumbling window: Example: \"A tumbling window is like counting cars that pass every 5 minutes...\" Your analogy: [Fill in] What are watermarks in one sentence? Your answer: [Fill in after implementation] What is the difference between event time and processing time? Your answer: [Fill in after implementation] Real-world analogy for late data handling: Example: \"Late data is like receiving a postcard that was sent last week...\" Your analogy: [Fill in] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition about stream processing. Answer these, then verify after implementation. Complexity Predictions \u00b6 Tumbling window processing: Time complexity per event: [Your guess: O(?)] Space complexity for K keys over W windows: [Your guess: O(?)] Verified after learning: [Actual] Sliding window vs tumbling window: If sliding window size = 10s, slide = 2s, how many windows per event? [Guess] Space overhead compared to tumbling: [Guess: X times larger] Verified: [Actual] State size calculation: If you have 100K unique keys, each storing 1KB of state Total memory needed: [Calculate] After 1 hour with TTL = 5 minutes: [Will it grow unbounded?] Scenario Predictions \u00b6 Scenario 1: Events arriving: timestamps [100, 200, 150, 300] (out of order) Tumbling window (size=100ms): Which windows do they belong to? Event@100ms \u2192 Window [0-100? 100-200?] Event@200ms \u2192 Window [Fill in] Event@150ms \u2192 Window [Fill in] Event@300ms \u2192 Window [Fill in] If watermark = 250ms and allowed lateness = 50ms: Event@150ms arrives when watermark=250ms: [Accept or Drop?] Event@100ms arrives when watermark=250ms: [Accept or Drop?] Why? [Fill in your reasoning] Scenario 2: Session window with 3-second gap Events for user1: [1000ms, 2000ms, 3000ms, 7000ms, 8000ms] How many sessions? [Guess] Session boundaries: [Fill in] If event@4500ms arrives late, what happens? [New session or merge?] Scenario 3: Processing 100K events/second Without state: Memory usage [Constant? Growing?] With state (no TTL): Memory usage [Constant? Growing?] With state (TTL=5min): Memory usage [Constant? Growing?] Your reasoning: [Fill in] Watermark Quiz \u00b6 Question: Watermark = 1000ms, allowed lateness = 200ms For a tumbling window [0-1000ms]: When does the window start computing? [Fill in] When does the window close and stop accepting data? [Fill in] Event@900ms arrives at processing time 1500ms: [Accepted?] Event@900ms arrives at processing time 1300ms: [Accepted?] Question: What happens if you set allowed lateness = 0? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Trade-off Quiz \u00b6 Question: When would batch processing be BETTER than stream processing? Your answer: [Fill in] Verified: [Fill in after implementation] Question: What's the MAIN trade-off of exactly-once processing? Uses more CPU Requires more memory Increases latency All of the above Verify after implementation: [Which one(s)?] Question: Event time vs Processing time Event occurs at 10:00:00 but arrives at system at 10:00:05: Event time = [Fill in] Processing time = [Fill in] Which one should windowing use? [Why?] Before/After: Why Stream Processing Matters \u00b6 Your task: Compare batch processing vs stream processing to understand the impact. Example: Real-Time Analytics \u00b6 Problem: Calculate page views per minute for a website getting 10K events/second. Approach 1: Batch Processing (Traditional) \u00b6 // Batch processing - Process accumulated data every minute public class BatchAnalytics { private List<Event> eventBuffer = new ArrayList<>(); public void collectEvent(Event event) { eventBuffer.add(event); } // Runs every 60 seconds public Map<String, Long> computePageViews() { Map<String, Long> counts = new HashMap<>(); // Process all accumulated events for (Event event : eventBuffer) { counts.merge(event.page, 1L, Long::sum); } // Clear buffer for next batch eventBuffer.clear(); return counts; } } Analysis: Latency: 30-60 seconds average (must wait for batch to complete) Memory: All events in 1 minute = 10K/sec \u00d7 60 = 600K events in memory Throughput: High (process all at once) Real-time: No - results delayed by up to 60 seconds Use case: Reports, ETL jobs, historical analysis Timeline visualization: Events: |-------- 60 seconds of collection --------| Processing: [Compute] \u2192 Results at T+60s User sees: \u2191 Results 60s old Approach 2: Stream Processing (Real-Time) \u00b6 // Stream processing - Continuous windowing public class StreamAnalytics { private Map<Long, Map<String, Long>> windows = new TreeMap<>(); private long windowSize = 60_000; // 60 seconds public void processEvent(Event event) { // Immediate assignment to window long windowStart = (event.timestamp / windowSize) * windowSize; // Update count immediately windows.computeIfAbsent(windowStart, k -> new HashMap<>()) .merge(event.page, 1L, Long::sum); // Emit results when window closes long currentTime = System.currentTimeMillis(); closeCompletedWindows(currentTime); } private void closeCompletedWindows(long currentTime) { // Windows that ended more than watermark delay ago long watermark = currentTime - 5000; // 5s delay tolerance windows.entrySet().removeIf(entry -> { long windowEnd = entry.getKey() + windowSize; if (windowEnd < watermark) { emitResults(entry.getKey(), entry.getValue()); return true; // Remove closed window } return false; }); } private void emitResults(long windowStart, Map<String, Long> counts) { // Results available immediately when window closes System.out.println(\"Window [\" + windowStart + \"]: \" + counts); } } Analysis: Latency: 5-10 seconds (watermark delay + processing) Memory: Only active windows = ~2 windows \u00d7 events = ~100K events in memory Throughput: Same (10K events/second) Real-time: Yes - results within seconds Use case: Dashboards, alerting, fraud detection Timeline visualization: Events: |--10s--|--10s--|--10s--|--10s--|--10s--|--10s--| Windows: [-------- Window 0-60s --------] Processing: \u2191 Results: Results at T+5s User sees: \u2191 Results ~5s old Performance Comparison \u00b6 Metric Batch (60s) Stream (Real-Time) Improvement Latency to see results 30-60 seconds 5-10 seconds 6-10x faster Memory (peak) 600K events 100K events 6x less Staleness of data Up to 60s old Up to 5s old 12x fresher Throughput 10K events/sec 10K events/sec Same Real-World Impact: Fraud Detection Example \u00b6 Batch approach: 10:00:00 - Fraudulent transaction occurs 10:00:05 - 3 more suspicious transactions 10:00:45 - 5 more transactions (pattern clear) 10:01:00 - Batch job runs, detects fraud 10:01:05 - Alert sent, account frozen Total: 9 fraudulent transactions, $4,500 loss Detection delay: 65 seconds Stream approach: 10:00:00 - Fraudulent transaction occurs 10:00:05 - 3 more suspicious transactions 10:00:10 - Pattern detected (window closed at watermark) 10:00:11 - Alert sent, account frozen Total: 4 fraudulent transactions, $2,000 loss Detection delay: 11 seconds Impact: Stream processing caught fraud 54 seconds faster , preventing $2,500 in losses . Why Does Stream Processing Win? \u00b6 Key insight to understand: Batch processing treats time in discrete chunks: Batch 1: [0s \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 60s] \u2192 Process \u2192 Wait Batch 2: [60s \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 120s] \u2192 Process \u2192 Wait Stream processing treats time continuously: Events: \u2500\u2022\u2500\u2500\u2022\u2500\u2022\u2500\u2500\u2500\u2022\u2500\u2500\u2022\u2500\u2022\u2500\u2500\u2022\u2500\u2022\u2500\u2500\u2022\u2500\u2192 (continuous) Windows: [\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500] [\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500] Results: \u2191 \u2191 (immediate) (immediate) Your calculation: For 1M events/second, batch processing with 5-minute windows needs _____ GB memory Stream processing with 1-minute windows needs _____ GB memory Memory savings: _____ times less When Batch Processing Is Still Better \u00b6 Batch wins when: Historical analysis: Processing months of data Complex joins: Multiple large datasets Cost-sensitive: Pay per compute hour (batch cheaper) No urgency: Daily reports, weekly summaries After implementing, explain in your own words: Why does stream processing use less memory? [Your answer] What's the trade-off between latency and accuracy with watermarks? [Your answer] When would you still choose batch processing? [Your answer] Case Studies: Stream Processing in the Wild \u00b6 Netflix: Real-time Streaming Analytics with Flink \u00b6 Pattern: Windowed Aggregations on event streams. How it works: Netflix's infrastructure generates a massive stream of events: every \"play,\" \"pause,\" \"buffer,\" and \"finish\" action from millions of viewers is published to Apache Kafka. They use Apache Flink to process this stream in real-time. For example, a Flink job might use a 10-second tumbling window to count the number of playback errors per region, allowing engineers to spot and react to regional outages instantly. Key Takeaway: Stream processing is essential for real-time operational monitoring at scale. By using windowed aggregations, raw event streams can be transformed into meaningful, actionable metrics for dashboards and alerting systems. LinkedIn Feed Updates: Real-time Content Delivery \u00b6 Pattern: Stream-Table Joins. How it works: LinkedIn's feed is a combination of real-time activity and user profile data. When a user you follow shares an article, that's a real-time event on a stream. To render the feed, their stream processing system (Apache Samza) must join this event stream with a table stream containing user profile data (like the user's name and headline). The result is a fully enriched feed item, delivered in near real-time. Key Takeaway: Stream processing isn't just about counting events. It's often about enriching real-time events with static or slow-moving data from tables to provide context and create a complete picture for the end-user. Cloudflare: DDoS Detection with Sliding Windows \u00b6 Pattern: Sliding Window analysis for anomaly detection. How it works: Cloudflare protects websites from DDoS attacks by analyzing vast streams of network request data. A stream processor might use a 1-minute sliding window , evaluated every 5 seconds, to track the request count per IP address. If the count for any IP suddenly spikes and crosses a predefined threshold within that window, the system automatically identifies it as a potential attack and can block the IP at the edge. Key Takeaway: Sliding windows are perfect for detecting anomalies in real-time data. By continuously analyzing recent activity, systems can identify and react to security threats or performance issues much faster than traditional batch-based analysis would allow. Core Implementation \u00b6 Pattern 1: Windowing (Tumbling, Sliding, Session) \u00b6 Concept: Group streaming data into finite chunks for aggregation. Use case: Real-time analytics, metrics aggregation, event counting. import java.util.*; /** * Stream Windowing: Group events into time-based windows * * Window Types: * - Tumbling: Fixed-size, non-overlapping (e.g., every 5 minutes) * - Sliding: Fixed-size, overlapping (e.g., last 5 minutes, updated every 1 minute) * - Session: Dynamic size based on inactivity gaps */ public class StreamWindow<K, V> { static class Event<K, V> { K key; V value; long timestamp; // Event time Event(K key, V value, long timestamp) { this.key = key; this.value = value; this.timestamp = timestamp; } } static class WindowResult<K> { K key; long windowStart; long windowEnd; long count; WindowResult(K key, long windowStart, long windowEnd, long count) { this.key = key; this.windowStart = windowStart; this.windowEnd = windowEnd; this.count = count; } @Override public String toString() { return String.format(\"Window[%d-%d] key=%s count=%d\", windowStart, windowEnd, key, count); } } /** * Tumbling Window: Fixed, non-overlapping time buckets * Time: O(1) per event, Space: O(W*K) where W=windows, K=keys * * TODO: Implement tumbling window * 1. Determine which window the event belongs to * 2. windowStart = (timestamp / windowSize) * windowSize * 3. Aggregate events in the same window * 4. Emit results when window closes */ public static <K, V> Map<Long, Map<K, Long>> tumblingWindow( List<Event<K, V>> events, long windowSizeMs) { Map<Long, Map<K, Long>> windows = new TreeMap<>(); // TODO: Process each event // // // Get or create window // Map<K, Long> window = windows.computeIfAbsent(windowStart, k -> new HashMap<>()); // // // Aggregate (count in this case) // window.merge(event.key, 1L, Long::sum); // } return windows; // Replace } /** * Sliding Window: Overlapping windows * Time: O(N) per event where N=num overlapping windows, Space: O(W*K) * * TODO: Implement sliding window * 1. For each event, determine ALL windows it belongs to * 2. An event at time T belongs to windows: * [T-windowSize+slide, T-windowSize+2*slide, ..., T] * 3. Update all overlapping windows */ public static <K, V> Map<Long, Map<K, Long>> slidingWindow( List<Event<K, V>> events, long windowSizeMs, long slideMs) { Map<Long, Map<K, Long>> windows = new TreeMap<>(); // TODO: Process each event // // // Add event to all overlapping windows // for (long windowStart = firstWindowStart; // windowStart <= lastWindowStart; // windowStart += slideMs) { // Map<K, Long> window = windows.computeIfAbsent(windowStart, k -> new HashMap<>()); // window.merge(event.key, 1L, Long::sum); // } // } return windows; // Replace } /** * Session Window: Group events with inactivity gap * Time: O(log N) per event, Space: O(S*K) where S=sessions * * TODO: Implement session window * 1. Sort events by key and timestamp * 2. For each key, group events within gap threshold * 3. Start new session if gap > threshold * 4. Merge sessions if events arrive late */ public static <K, V> List<WindowResult<K>> sessionWindow( List<Event<K, V>> events, long gapMs) { List<WindowResult<K>> results = new ArrayList<>(); // TODO: Group events by key // TODO: Implement iteration/conditional logic // // // Sort by timestamp // keyEvents.sort(Comparator.comparingLong(e -> e.timestamp)); // // // Create sessions // long sessionStart = keyEvents.get(0).timestamp; // long lastTimestamp = sessionStart; // long count = 0; // // for (Event<K, V> event : keyEvents) { // if (event.timestamp - lastTimestamp > gapMs) { // // Close current session // results.add(new WindowResult<>(key, sessionStart, lastTimestamp, count)); // // // Start new session // sessionStart = event.timestamp; // count = 0; // } // // lastTimestamp = event.timestamp; // count++; // } // // // Close final session // results.add(new WindowResult<>(key, sessionStart, lastTimestamp, count)); // } return results; // Replace } /** * Helper: Print window results */ public static <K> void printWindows(Map<Long, Map<K, Long>> windows) { for (Map.Entry<Long, Map<K, Long>> entry : windows.entrySet()) { long windowStart = entry.getKey(); System.out.println(\"Window [\" + windowStart + \"]:\"); for (Map.Entry<K, Long> keyCount : entry.getValue().entrySet()) { System.out.println(\" \" + keyCount.getKey() + \": \" + keyCount.getValue()); } } } } Runnable Client Code: import java.util.*; public class StreamWindowClient { public static void main(String[] args) { System.out.println(\"=== Stream Windowing ===\\n\"); // Create sample events (userId, action, timestamp) List<StreamWindow.Event<String, String>> events = Arrays.asList( new StreamWindow.Event<>(\"user1\", \"click\", 1000L), new StreamWindow.Event<>(\"user2\", \"click\", 2000L), new StreamWindow.Event<>(\"user1\", \"click\", 3000L), new StreamWindow.Event<>(\"user1\", \"click\", 6000L), new StreamWindow.Event<>(\"user2\", \"click\", 7000L), new StreamWindow.Event<>(\"user1\", \"click\", 11000L), new StreamWindow.Event<>(\"user2\", \"click\", 12000L), new StreamWindow.Event<>(\"user1\", \"click\", 15000L), new StreamWindow.Event<>(\"user2\", \"click\", 16000L) ); // Test 1: Tumbling Window (5 second windows) System.out.println(\"--- Test 1: Tumbling Window (5s) ---\"); Map<Long, Map<String, Long>> tumbling = StreamWindow.tumblingWindow(events, 5000L); StreamWindow.printWindows(tumbling); // Test 2: Sliding Window (5s window, 2s slide) System.out.println(\"\\n--- Test 2: Sliding Window (5s window, 2s slide) ---\"); Map<Long, Map<String, Long>> sliding = StreamWindow.slidingWindow(events, 5000L, 2000L); StreamWindow.printWindows(sliding); // Test 3: Session Window (3s gap) System.out.println(\"\\n--- Test 3: Session Window (3s gap) ---\"); List<StreamWindow.WindowResult<String>> sessions = StreamWindow.sessionWindow(events, 3000L); for (StreamWindow.WindowResult<String> result : sessions) { System.out.println(result); } // Test 4: Different gap threshold System.out.println(\"\\n--- Test 4: Session Window (5s gap) ---\"); List<StreamWindow.WindowResult<String>> sessions2 = StreamWindow.sessionWindow(events, 5000L); for (StreamWindow.WindowResult<String> result : sessions2) { System.out.println(result); } } } Pattern 2: Watermarks and Late Data \u00b6 Concept: Handle out-of-order events and determine when to close windows. Use case: Distributed systems, network delays, mobile data sync. import java.util.*; /** * Watermarks: Track event time progress in the stream * * Watermark Properties: * - Monotonically increasing timestamp * - Indicates \"all events before this time have been seen\" * - Allows system to close windows and emit results * - Late data: events arriving after watermark */ public class WatermarkProcessor<K, V> { static class Event<K, V> { K key; V value; long eventTime; // When event actually occurred long processingTime; // When event was processed Event(K key, V value, long eventTime, long processingTime) { this.key = key; this.value = value; this.eventTime = eventTime; this.processingTime = processingTime; } } static class WindowState<K> { K key; long windowStart; long windowEnd; long count; boolean closed; WindowState(K key, long windowStart, long windowEnd) { this.key = key; this.windowStart = windowStart; this.windowEnd = windowEnd; this.count = 0; this.closed = false; } } private final long windowSize; private final long allowedLateness; private long currentWatermark; // Active windows: windowStart -> key -> state private Map<Long, Map<K, WindowState<K>>> windows; // Late data count private long lateEventCount; public WatermarkProcessor(long windowSize, long allowedLateness) { this.windowSize = windowSize; this.allowedLateness = allowedLateness; this.currentWatermark = 0; this.windows = new TreeMap<>(); this.lateEventCount = 0; } /** * Process event with watermark tracking * Time: O(log W) where W=windows, Space: O(W*K) * * TODO: Implement event processing with watermarks * 1. Update watermark based on event time * 2. Assign event to window * 3. Check if event is late (eventTime < watermark - allowedLateness) * 4. Close windows when watermark passes windowEnd + allowedLateness */ public void processEvent(Event<K, V> event) { // TODO: Update watermark (typically: eventTime - maxDelay) // TODO: Calculate window for this event // TODO: Check if event is too late // TODO: Get or create window state // TODO: Update state if window not closed // TODO: Close windows that are ready } /** * Close windows that have passed watermark + allowedLateness * Time: O(W*K), Space: O(1) * * TODO: Implement window closing logic */ private void closeCompletedWindows() { List<Long> toRemove = new ArrayList<>(); // TODO: Check each window // // // Close if watermark passed windowEnd + allowedLateness // if (currentWatermark >= windowEnd + allowedLateness) { // Map<K, WindowState<K>> window = entry.getValue(); // // // Emit results for each key in window // for (WindowState<K> state : window.values()) { // if (!state.closed) { // emitResult(state); // state.closed = true; // } // } // // toRemove.add(windowStart); // } // } // TODO: Remove closed windows } /** * Emit window result (in production: send to output stream) */ private void emitResult(WindowState<K> state) { System.out.printf(\"EMIT: Window[%d-%d] key=%s count=%d watermark=%d%n\", state.windowStart, state.windowEnd, state.key, state.count, currentWatermark); } /** * Generate periodic watermark (for idle streams) * Time: O(1), Space: O(1) * * TODO: Implement periodic watermark generation */ public void generatePeriodicWatermark(long timestamp) { // TODO: Advance watermark } /** * Get current watermark */ public long getWatermark() { return currentWatermark; } /** * Get late event count */ public long getLateEventCount() { return lateEventCount; } /** * Get active window count */ public int getActiveWindowCount() { return windows.size(); } } Runnable Client Code: import java.util.*; public class WatermarkClient { public static void main(String[] args) { System.out.println(\"=== Watermarks and Late Data ===\\n\"); WatermarkProcessor<String, String> processor = new WatermarkProcessor<>(5000L, 2000L); // 5s window, 2s late // Test 1: In-order events System.out.println(\"--- Test 1: In-Order Events ---\"); List<WatermarkProcessor.Event<String, String>> events1 = Arrays.asList( new WatermarkProcessor.Event<>(\"user1\", \"click\", 1000L, 1000L), new WatermarkProcessor.Event<>(\"user1\", \"click\", 2000L, 2000L), new WatermarkProcessor.Event<>(\"user2\", \"click\", 3000L, 3000L) ); for (WatermarkProcessor.Event<String, String> event : events1) { processor.processEvent(event); } System.out.println(\"Watermark: \" + processor.getWatermark()); // Test 2: Advance watermark to close window System.out.println(\"\\n--- Test 2: Close Window ---\"); processor.generatePeriodicWatermark(8000L); System.out.println(\"Active windows: \" + processor.getActiveWindowCount()); // Test 3: Out-of-order events (within allowed lateness) System.out.println(\"\\n--- Test 3: Out-of-Order (Within Lateness) ---\"); WatermarkProcessor.Event<String, String> lateEvent = new WatermarkProcessor.Event<>(\"user2\", \"click\", 4000L, 9000L); processor.processEvent(lateEvent); // Test 4: Very late event (outside allowed lateness) System.out.println(\"\\n--- Test 4: Very Late Event (Dropped) ---\"); WatermarkProcessor.Event<String, String> veryLateEvent = new WatermarkProcessor.Event<>(\"user1\", \"click\", 500L, 10000L); processor.processEvent(veryLateEvent); System.out.println(\"Late event count: \" + processor.getLateEventCount()); // Test 5: Multiple windows with different keys System.out.println(\"\\n--- Test 5: Multiple Windows ---\"); WatermarkProcessor<String, String> processor2 = new WatermarkProcessor<>(5000L, 1000L); List<WatermarkProcessor.Event<String, String>> events2 = Arrays.asList( new WatermarkProcessor.Event<>(\"A\", \"x\", 1000L, 1000L), new WatermarkProcessor.Event<>(\"B\", \"y\", 2000L, 2000L), new WatermarkProcessor.Event<>(\"A\", \"x\", 3000L, 3000L), new WatermarkProcessor.Event<>(\"A\", \"x\", 7000L, 7000L), new WatermarkProcessor.Event<>(\"B\", \"y\", 8000L, 8000L) ); for (WatermarkProcessor.Event<String, String> event : events2) { processor2.processEvent(event); } // Close all windows processor2.generatePeriodicWatermark(15000L); } } Pattern 3: Stateful Stream Processing \u00b6 Concept: Maintain state across events for aggregations, joins, and enrichment. Use case: Running totals, user sessions, stream joins, enrichment. import java.util.*; /** * Stateful Stream Processing * * State Types: * - Value State: Single value per key * - List State: List of values per key * - Map State: Nested key-value map per key * * State Backends: * - In-memory (fast, not fault-tolerant) * - RocksDB (persistent, fault-tolerant) */ public class StatefulProcessor<K, V> { static class Event<K, V> { K key; V value; long timestamp; Event(K key, V value, long timestamp) { this.key = key; this.value = value; this.timestamp = timestamp; } } static class StateDescriptor<S> { String name; Class<S> stateType; long ttlMs; // Time-to-live for state cleanup StateDescriptor(String name, Class<S> stateType, long ttlMs) { this.name = name; this.stateType = stateType; this.ttlMs = ttlMs; } } /** * Value State: Single value per key * Time: O(1), Space: O(K) */ static class ValueState<K, S> { private Map<K, S> state; private Map<K, Long> lastAccess; // For TTL private long ttlMs; ValueState(long ttlMs) { this.state = new HashMap<>(); this.lastAccess = new HashMap<>(); this.ttlMs = ttlMs; } /** * Get state for key * * TODO: Implement get with TTL check * 1. Check if key exists * 2. Check if state expired (current time - lastAccess > ttl) * 3. If expired, remove and return null * 4. Otherwise return value */ public S get(K key, long currentTime) { // TODO: Check expiration return null; // Replace } /** * Update state for key * * TODO: Implement update with TTL tracking */ public void update(K key, S value, long currentTime) { // TODO: Update state and last access time } /** * Clear state for key */ public void clear(K key) { state.remove(key); lastAccess.remove(key); } } /** * List State: Append-only list per key * Time: O(1) append, O(N) iterate, Space: O(K*N) */ static class ListState<K, S> { private Map<K, List<S>> state; private Map<K, Long> lastAccess; private long ttlMs; ListState(long ttlMs) { this.state = new HashMap<>(); this.lastAccess = new HashMap<>(); this.ttlMs = ttlMs; } /** * Append value to list * * TODO: Implement append */ public void append(K key, S value, long currentTime) { // TODO: Get or create list and append } /** * Get all values for key * * TODO: Implement get with TTL check */ public List<S> get(K key, long currentTime) { // TODO: Check expiration similar to ValueState return new ArrayList<>(); // Replace } } /** * Example: Running sum aggregation * Time: O(1) per event, Space: O(K) * * TODO: Implement stateful aggregation */ public static class RunningSumProcessor { private ValueState<String, Long> sumState; public RunningSumProcessor(long ttlMs) { this.sumState = new ValueState<>(ttlMs); } /** * Process event and update running sum * * TODO: Implement running sum * 1. Get current sum for key * 2. Add new value * 3. Update state * 4. Return new sum */ public long process(Event<String, Long> event) { // TODO: Get current sum // TODO: Add new value // TODO: Update state // TODO: Return result return 0L; // Replace } public Long getCurrentSum(String key, long timestamp) { return sumState.get(key, timestamp); } } /** * Example: Stream-Stream Join * Time: O(1) per event, Space: O(K*W) where W=window size * * TODO: Implement stream join */ public static class StreamJoinProcessor { private ListState<String, Event<String, String>> leftState; private ListState<String, Event<String, String>> rightState; private long joinWindowMs; public StreamJoinProcessor(long joinWindowMs, long stateTtl) { this.leftState = new ListState<>(stateTtl); this.rightState = new ListState<>(stateTtl); this.joinWindowMs = joinWindowMs; } /** * Process left stream event * * TODO: Implement left stream processing * 1. Store event in left state * 2. Look for matching events in right state within join window * 3. Emit joined results */ public List<String> processLeft(Event<String, String> event) { List<String> results = new ArrayList<>(); // TODO: Store in left state // TODO: Find matches in right state return results; // Replace } /** * Process right stream event * * TODO: Implement right stream processing (symmetric to left) */ public List<String> processRight(Event<String, String> event) { List<String> results = new ArrayList<>(); // TODO: Similar to processLeft but reversed return results; // Replace } } /** * State cleanup: Remove expired state * Time: O(K), Space: O(1) * * TODO: Implement periodic state cleanup */ public static void cleanupExpiredState(ValueState<?, ?> state, long currentTime) { // TODO: Iterate through all keys and remove expired entries // In production: RocksDB handles this with compaction } } Runnable Client Code: import java.util.*; public class StatefulProcessorClient { public static void main(String[] args) { System.out.println(\"=== Stateful Stream Processing ===\\n\"); // Test 1: Running sum System.out.println(\"--- Test 1: Running Sum ---\"); StatefulProcessor.RunningSumProcessor sumProcessor = new StatefulProcessor.RunningSumProcessor(10000L); List<StatefulProcessor.Event<String, Long>> sumEvents = Arrays.asList( new StatefulProcessor.Event<>(\"user1\", 10L, 1000L), new StatefulProcessor.Event<>(\"user1\", 20L, 2000L), new StatefulProcessor.Event<>(\"user2\", 5L, 2500L), new StatefulProcessor.Event<>(\"user1\", 15L, 3000L), new StatefulProcessor.Event<>(\"user2\", 10L, 3500L) ); for (StatefulProcessor.Event<String, Long> event : sumEvents) { long sum = sumProcessor.process(event); System.out.printf(\"Key=%s Value=%d RunningSum=%d%n\", event.key, event.value, sum); } // Test 2: Stream join System.out.println(\"\\n--- Test 2: Stream Join ---\"); StatefulProcessor.StreamJoinProcessor joinProcessor = new StatefulProcessor.StreamJoinProcessor(2000L, 10000L); // Left stream events System.out.println(\"Processing left stream:\"); StatefulProcessor.Event<String, String> left1 = new StatefulProcessor.Event<>(\"order1\", \"LeftA\", 1000L); List<String> joined1 = joinProcessor.processLeft(left1); System.out.println(\" \" + left1.key + \": \" + joined1); // Right stream events System.out.println(\"Processing right stream:\"); StatefulProcessor.Event<String, String> right1 = new StatefulProcessor.Event<>(\"order1\", \"RightX\", 1500L); List<String> joined2 = joinProcessor.processRight(right1); System.out.println(\" \" + right1.key + \": \" + joined2); // More events StatefulProcessor.Event<String, String> left2 = new StatefulProcessor.Event<>(\"order2\", \"LeftB\", 2000L); List<String> joined3 = joinProcessor.processLeft(left2); System.out.println(\" \" + left2.key + \": \" + joined3); // Test 3: State TTL System.out.println(\"\\n--- Test 3: State TTL ---\"); StatefulProcessor.ValueState<String, String> ttlState = new StatefulProcessor.ValueState<>(2000L); // 2s TTL ttlState.update(\"key1\", \"value1\", 1000L); System.out.println(\"Stored at t=1000\"); String val1 = ttlState.get(\"key1\", 2000L); System.out.println(\"Get at t=2000: \" + val1); // Should exist String val2 = ttlState.get(\"key1\", 4000L); System.out.println(\"Get at t=4000: \" + val2); // Should be expired // Test 4: List state System.out.println(\"\\n--- Test 4: List State ---\"); StatefulProcessor.ListState<String, String> listState = new StatefulProcessor.ListState<>(10000L); listState.append(\"user1\", \"event1\", 1000L); listState.append(\"user1\", \"event2\", 2000L); listState.append(\"user1\", \"event3\", 3000L); List<String> events = listState.get(\"user1\", 3500L); System.out.println(\"Events for user1: \" + events); } } Pattern 4: Exactly-Once Semantics \u00b6 Concept: Ensure each event is processed exactly once, even with failures. Use case: Financial transactions, billing, critical business logic. import java.util.*; /** * Exactly-Once Processing * * Techniques: * - Idempotent operations (safe to retry) * - Two-phase commit for sinks * - Transaction markers in stream * - Deduplication with state */ public class ExactlyOnceProcessor<K, V> { static class Event<K, V> { String eventId; // Unique ID for deduplication K key; V value; long timestamp; Event(String eventId, K key, V value, long timestamp) { this.eventId = eventId; this.key = key; this.value = value; this.timestamp = timestamp; } } static class Transaction { String transactionId; long timestamp; List<String> processedEventIds; boolean committed; Transaction(String transactionId, long timestamp) { this.transactionId = transactionId; this.timestamp = timestamp; this.processedEventIds = new ArrayList<>(); this.committed = false; } } /** * Deduplication State: Track processed events * Time: O(1) per check, Space: O(N) where N=events in window */ static class DeduplicationState { private Set<String> processedEventIds; private long oldestEventTime; private long retentionMs; DeduplicationState(long retentionMs) { this.processedEventIds = new HashSet<>(); this.retentionMs = retentionMs; this.oldestEventTime = Long.MAX_VALUE; } /** * Check if event already processed * * TODO: Implement deduplication check * 1. Check if eventId exists in set * 2. If yes, return true (duplicate) * 3. If no, add to set and return false */ public boolean isDuplicate(String eventId) { // TODO: Check and add return false; // Replace } /** * Cleanup old event IDs * * TODO: Implement periodic cleanup * In production: use time-based eviction or Bloom filter */ public void cleanup(long currentTime) { // TODO: Remove event IDs older than retention period // Simplified: in reality need timestamps per event if (currentTime - oldestEventTime > retentionMs) { processedEventIds.clear(); oldestEventTime = currentTime; } } public int size() { return processedEventIds.size(); } } /** * Idempotent Aggregator: Safe to process same event multiple times * Time: O(1) per event, Space: O(K) * * TODO: Implement idempotent aggregation */ static class IdempotentAggregator<K> { // Store: key -> (value, eventId) private Map<K, Long> values; private Map<K, String> lastEventIds; IdempotentAggregator() { this.values = new HashMap<>(); this.lastEventIds = new HashMap<>(); } /** * Process event idempotently * * TODO: Implement idempotent update * 1. Check if this exact event was already processed * 2. If same eventId, skip (idempotent) * 3. If new eventId, update value */ public void process(Event<K, Long> event) { // TODO: Check if already processed // TODO: Update value (idempotent SET operation) } public Long getValue(K key) { return values.get(key); } } /** * Two-Phase Commit Sink: Transactional output * Time: O(1) per event, O(N) per commit * * TODO: Implement 2PC for sink */ static class TransactionalSink<T> { private Transaction currentTransaction; private List<T> pendingWrites; private Set<String> committedTransactions; TransactionalSink() { this.pendingWrites = new ArrayList<>(); this.committedTransactions = new HashSet<>(); } /** * Begin new transaction * * TODO: Implement transaction begin */ public void beginTransaction(String txnId) { // TODO: Create new transaction } /** * Write to transaction (not committed yet) * * TODO: Implement transactional write */ public void write(T value, String eventId) { // TODO: Add to pending writes } /** * Commit transaction (make writes visible) * * TODO: Implement commit * 1. Check if transaction already committed (idempotent) * 2. Flush all pending writes * 3. Mark transaction as committed */ public void commit() { // TODO: Commit if not already done // // if (committedTransactions.contains(currentTransaction.transactionId)) { // // Already committed (idempotent) // return; // } // // // Flush writes (in production: write to external system) // System.out.println(\"COMMIT: \" + pendingWrites.size() + \" writes\"); // // // Mark as committed // committedTransactions.add(currentTransaction.transactionId); // currentTransaction.committed = true; } /** * Abort transaction (discard writes) * * TODO: Implement abort */ public void abort() { // TODO: Clear pending writes } public int getPendingCount() { return pendingWrites.size(); } } /** * Checkpoint coordinator: Manage checkpoints for fault tolerance * Time: O(S) where S=state size, Space: O(S) * * TODO: Implement checkpointing */ static class CheckpointCoordinator { private long lastCheckpointId; private Map<Long, Map<String, Object>> checkpoints; CheckpointCoordinator() { this.lastCheckpointId = 0; this.checkpoints = new HashMap<>(); } /** * Trigger checkpoint * * TODO: Implement checkpoint trigger * 1. Generate checkpoint ID * 2. Snapshot all state * 3. Store checkpoint * 4. Return checkpoint ID */ public long triggerCheckpoint(Map<String, Object> state) { // TODO: Create checkpoint return 0; // Replace } /** * Restore from checkpoint * * TODO: Implement restore */ public Map<String, Object> restore(long checkpointId) { // TODO: Restore state return null; // Replace } } } Runnable Client Code: import java.util.*; public class ExactlyOnceClient { public static void main(String[] args) { System.out.println(\"=== Exactly-Once Processing ===\\n\"); // Test 1: Deduplication System.out.println(\"--- Test 1: Deduplication ---\"); ExactlyOnceProcessor.DeduplicationState dedup = new ExactlyOnceProcessor.DeduplicationState(10000L); String[] eventIds = {\"evt1\", \"evt2\", \"evt1\", \"evt3\", \"evt2\"}; for (String id : eventIds) { boolean isDup = dedup.isDuplicate(id); System.out.println(\"Event \" + id + \": \" + (isDup ? \"DUPLICATE\" : \"NEW\")); } System.out.println(\"Unique events tracked: \" + dedup.size()); // Test 2: Idempotent aggregation System.out.println(\"\\n--- Test 2: Idempotent Operations ---\"); ExactlyOnceProcessor.IdempotentAggregator<String> aggregator = new ExactlyOnceProcessor.IdempotentAggregator<>(); List<ExactlyOnceProcessor.Event<String, Long>> events = Arrays.asList( new ExactlyOnceProcessor.Event<>(\"e1\", \"user1\", 100L, 1000L), new ExactlyOnceProcessor.Event<>(\"e2\", \"user1\", 200L, 2000L), new ExactlyOnceProcessor.Event<>(\"e1\", \"user1\", 100L, 2500L), // Duplicate new ExactlyOnceProcessor.Event<>(\"e3\", \"user2\", 50L, 3000L) ); for (ExactlyOnceProcessor.Event<String, Long> event : events) { System.out.println(\"Processing: \" + event.eventId); aggregator.process(event); System.out.println(\" user1: \" + aggregator.getValue(\"user1\")); System.out.println(\" user2: \" + aggregator.getValue(\"user2\")); } // Test 3: Transactional sink System.out.println(\"\\n--- Test 3: Transactional Sink ---\"); ExactlyOnceProcessor.TransactionalSink<String> sink = new ExactlyOnceProcessor.TransactionalSink<>(); // Transaction 1 sink.beginTransaction(\"txn1\"); sink.write(\"value1\", \"e1\"); sink.write(\"value2\", \"e2\"); System.out.println(\"Pending writes: \" + sink.getPendingCount()); sink.commit(); // Transaction 2 (with abort) sink.beginTransaction(\"txn2\"); sink.write(\"value3\", \"e3\"); System.out.println(\"Pending writes: \" + sink.getPendingCount()); sink.abort(); System.out.println(\"After abort: \" + sink.getPendingCount()); // Test 4: Checkpointing System.out.println(\"\\n--- Test 4: Checkpointing ---\"); ExactlyOnceProcessor.CheckpointCoordinator coordinator = new ExactlyOnceProcessor.CheckpointCoordinator(); Map<String, Object> state1 = new HashMap<>(); state1.put(\"counter\", 100); state1.put(\"sum\", 500L); long cp1 = coordinator.triggerCheckpoint(state1); System.out.println(\"Created checkpoint: \" + cp1); // Modify state state1.put(\"counter\", 200); long cp2 = coordinator.triggerCheckpoint(state1); System.out.println(\"Created checkpoint: \" + cp2); // Restore Map<String, Object> restored = coordinator.restore(cp1); System.out.println(\"Restored state: \" + restored); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken stream processing implementations. This tests your understanding. Challenge 1: Broken Tumbling Window \u00b6 /** * This tumbling window implementation has 2 BUGS. * It's supposed to count events per key per window. */ public static Map<Long, Map<String, Long>> tumblingWindow_Buggy( List<Event<String, String>> events, long windowSize) { Map<Long, Map<String, Long>> windows = new TreeMap<>(); for (Event<String, String> event : events) { long windowStart = event.timestamp / windowSize; Map<String, Long> window = windows.computeIfAbsent(windowStart, k -> new HashMap<>()); window.put(event.key, 1L); } return windows; } Your debugging: Bug 1: [What\\'s the bug?] Current output: [What windows?] Expected output: [What windows?] Bug 2: [What\\'s the bug?] Current count: [What do you get?] Expected count: [Should be?] Click to verify your answers Bug 1: Window start should be (event.timestamp / windowSize) * windowSize to align to window boundaries. Current: windowStart = 1500 / 1000 = 1 (wrong - just the window index) Fixed: windowStart = (1500 / 1000) * 1000 = 1000 (correct - actual timestamp) Bug 2: Using put overwrites the count instead of incrementing it. // Wrong: Always sets count to 1 window.put(event.key, 1L); // Correct: Increment existing count or start at 1 window.merge(event.key, 1L, Long::sum); Challenge 2: Watermark Calculation Bug \u00b6 /** * This watermark processor has CRITICAL BUGS with late data handling. * Test with: Event@100ms arrives when watermark=300ms, allowedLateness=50ms */ public void processEvent_Buggy(Event<String, String> event) { // Update watermark if (event.eventTime > currentWatermark) { currentWatermark = event.eventTime; } // Calculate window long windowStart = (event.eventTime / windowSize) * windowSize; long windowEnd = windowStart + windowSize; if (event.eventTime < currentWatermark) { System.out.println(\"LATE DATA: \" + event.key); lateEventCount++; return; // Drop event } if (currentWatermark > windowEnd) { // Window already closed, don't accept event return; } // Add to window Map<String, WindowState<String>> window = windows.computeIfAbsent(windowStart, k -> new HashMap<>()); WindowState<String> state = window.computeIfAbsent(event.key, k -> new WindowState<>(event.key, windowStart, windowEnd)); state.count++; } Your debugging: Bug 1: Late data check Problem: [What's wrong with the late data check?] Scenario: Event@100ms, watermark=300ms, allowedLateness=50ms Should be: [Accepted or Dropped?] Currently: [What happens?] Fix: [Correct condition?] Bug 2: Window closing Problem: [What's wrong with window closing logic?] Scenario: Event@250ms, windowEnd=300ms, watermark=310ms, allowedLateness=50ms Should be: [Accepted or Dropped?] Currently: [What happens?] Fix: [How to properly check if window is still open?] Test trace: Window [0-1000ms], allowedLateness=200ms - Event@900ms arrives, watermark=900ms \u2192 ? - Event@800ms arrives, watermark=1100ms \u2192 ? - Event@800ms arrives, watermark=1300ms \u2192 ? Your predictions: [Fill in for each event] Click to verify your answers Bug 1: Late data check doesn't account for allowed lateness. // Wrong: Drops ANY late event if (event.eventTime < currentWatermark) { return; } // Correct: Only drop if REALLY late if (event.eventTime < currentWatermark - allowedLateness) { System.out.println(\"LATE DATA: \" + event.key); lateEventCount++; return; } Bug 2: Window closing doesn't account for allowed lateness. // Wrong: Closes window as soon as watermark passes windowEnd if (currentWatermark > windowEnd) { return; } // Correct: Keep window open for allowed lateness period if (currentWatermark >= windowEnd + allowedLateness) { // Window truly closed now return; } Test trace answers: Event@900ms, watermark=900ms \u2192 Accepted (in window, on time) Event@800ms, watermark=1100ms \u2192 Accepted (late but within 200ms) Event@800ms, watermark=1300ms \u2192 Dropped (too late: 1300 - 800 = 500ms > 200ms) Challenge 3: State Management Memory Leak \u00b6 /** * This stateful processor has a MEMORY LEAK. * State grows unbounded even with TTL configured! */ public class ValueState_Buggy<K, S> { private Map<K, S> state = new HashMap<>(); private Map<K, Long> lastAccess = new HashMap<>(); private long ttlMs; ValueState_Buggy(long ttlMs) { this.ttlMs = ttlMs; } public S get(K key, long currentTime) { if (state.containsKey(key)) { Long lastTime = lastAccess.get(key); if (currentTime - lastTime > ttlMs) { // State expired, return null return null; } return state.get(key); } return null; } public void update(K key, S value, long currentTime) { state.put(key, value); lastAccess.put(key, currentTime); } } Your debugging: Memory leak scenario: Process 1M unique keys over 1 hour TTL = 5 minutes Expected memory: [How many keys should remain?] Actual memory: [What happens?] Bug location: [Which method and line?] Bug explanation: [Why does state grow unbounded?] Bug fix: [What code is missing?] Performance impact: After 1 hour: [How many expired keys still in memory?] Memory waste: [Calculate] Performance degradation: [Why does this hurt performance?] Click to verify your answer Bug: When state expires, we return null but never remove the expired entries from the maps. // Wrong: Returns null but leaves garbage in memory if (currentTime - lastTime > ttlMs) { return null; // Memory leak! } // Correct: Clean up expired state if (currentTime - lastTime > ttlMs) { // Remove expired state state.remove(key); lastAccess.remove(key); return null; } Performance impact: After 1 hour with 1M unique keys and TTL=5min: Expected: ~100K keys (5min worth at constant rate) Actual: 1M keys (ALL keys ever seen) Memory waste: 900K entries \u00d7 (state size + 2 map entries) HashMap performance degrades with size (more collisions, slower lookups) Challenge 4: Window Boundary Bug \u00b6 /** * Session window merger has a SUBTLE BUG. * Sometimes creates gaps, sometimes creates wrong sessions. */ public static List<WindowResult<String>> sessionWindow_Buggy( List<Event<String, String>> events, long gapMs) { List<WindowResult<String>> results = new ArrayList<>(); Map<String, List<Event<String, String>>> eventsByKey = new HashMap<>(); // Group by key for (Event<String, String> event : events) { eventsByKey.computeIfAbsent(event.key, k -> new ArrayList<>()).add(event); } // Create sessions per key for (Map.Entry<String, List<Event<String, String>>> entry : eventsByKey.entrySet()) { String key = entry.getKey(); List<Event<String, String>> keyEvents = entry.getValue(); // keyEvents.sort(Comparator.comparingLong(e -> e.timestamp)); long sessionStart = keyEvents.get(0).timestamp; long lastTimestamp = sessionStart; long count = 0; for (Event<String, String> event : keyEvents) { if (event.timestamp - lastTimestamp >= gapMs) { // Close current session results.add(new WindowResult<>(key, sessionStart, lastTimestamp, count)); // Start new session sessionStart = event.timestamp; count = 0; } lastTimestamp = event.timestamp; count++; } // Close final session results.add(new WindowResult<>(key, sessionStart, lastTimestamp, count)); } return results; } Your debugging: Bug 1: Missing sort Scenario: Events arrive [3000ms, 1000ms, 2000ms], gap=1000ms Expected sessions: [Fill in after sorting] Actual sessions: [What do you get without sorting?] Why is this wrong? [Explain] Bug 2: Gap boundary condition Scenario: Events at [0ms, 1000ms, 2000ms], gap=1000ms Current logic: gap check uses >= Event@1000ms: gap = 1000 - 0 = 1000ms \u2192 [New session or same?] Event@2000ms: gap = 2000 - 1000 = 1000ms \u2192 [New session or same?] Is this correct? [Should gap of exactly 1000ms create new session?] Fix: [Use > or >=?] Bug 3: Edge case - Empty events What happens if keyEvents is empty? [Will it crash?] Fix: [Add what check?] Click to verify your answers Bug 1: Missing sort // Without sort: [3000, 1000, 2000] // Session 1: [3000], Session 2: [1000], Session 3: [2000] // Wrong! Events are scattered across sessions // With sort: [1000, 2000, 3000] // Session 1: [1000, 2000, 3000] (all within gap) // Correct! // Fix: ALWAYS sort by timestamp keyEvents.sort(Comparator.comparingLong(e -> e.timestamp)); Bug 2: Gap boundary // Question: Is 1000ms gap exactly equal to gapMs=1000ms a new session? // Typically: gap GREATER THAN threshold \u2192 new session // At exactly the gap threshold \u2192 still same session // Wrong: >= creates new session at boundary if (event.timestamp - lastTimestamp >= gapMs) { // Correct: > allows events exactly at gap boundary if (event.timestamp - lastTimestamp > gapMs) { Bug 3: Empty list // Add check before accessing keyEvents.get(0) if (keyEvents.isEmpty()) { continue; } Challenge 5: Stream Join Race Condition \u00b6 /** * Stream join has a RACE CONDITION and MEMORY ISSUE. */ public List<String> processLeft_Buggy(Event<String, String> event) { List<String> results = new ArrayList<>(); List<Event<String, String>> rightEvents = rightState.get(event.key, event.timestamp); if (rightEvents != null) { for (Event<String, String> right : rightEvents) { // Join window check if (Math.abs(event.timestamp - right.timestamp) <= joinWindowMs) { results.add(event.value + \"+\" + right.value); } } } leftState.append(event.key, event, event.timestamp); return results; } Your debugging: Bug: Join order matters Scenario: Left event L1 arrives at T=0 Right event R1 arrives at T=1 (within join window) Expected: L1+R1 joined What happens? processLeft(L1): Looks in rightState, empty, no match \u2192 stores L1 processRight(R1): Looks in leftState, finds L1, match! \u2192 stores R1 Problem: [Is this symmetric? Do we emit join twice or once?] Scenario 2: Right event R1 arrives first at T=0 Left event L1 arrives at T=1 What happens? processRight(R1): Looks in leftState, empty, no match \u2192 stores R1 processLeft(L1): Looks in rightState, finds R1, match! \u2192 stores L1 Problem: [Same or different from scenario 1?] Question: Should the join be emitted by processLeft, processRight, or both? Your answer: [Fill in] Memory issue: Without TTL on state, what happens? [Fill in] With join window = 5 seconds, what TTL should you use? [Fill in] Click to verify your answers Bug explanation: The current code stores left events AFTER checking right state. This is fine, but the symmetric operation in processRight must also store right events AFTER checking left state. The join will be emitted only once (either by left or right, whichever processes second). Correct pattern: Store BEFORE matching (then match both directions): // CORRECT: Store first, then match leftState.append(event.key, event, event.timestamp); // Then look for matches in right state List<Event<String, String>> rightEvents = rightState.get(event.key, event.timestamp); // ... matching logic ... This ensures both sides can find each other. The join will be emitted by whichever event arrives second. Memory issue: Without TTL: All events stay in state forever \u2192 memory leak With join window = 5s: Set TTL = 2 \u00d7 join window = 10s (safety margin) Why 2\u00d7? Events might arrive late, need extra retention Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all window calculation bugs Understood watermark and late data handling Fixed memory leaks in state management Corrected session window boundary issues Resolved stream join race conditions Learned common stream processing mistakes Common mistakes you discovered: [List the patterns - e.g., \"Not accounting for allowed lateness\"] [Fill in] [Fill in] [Fill in] Prevention checklist: Always align windows to proper boundaries Always check: eventTime vs (watermark - allowedLateness) Always remove expired state in TTL checks Always sort events by timestamp for session windows Always set TTL for stateful joins Always test with out-of-order events Decision Framework \u00b6 Your task: Build decision trees for when to use each stream processing pattern. Question 1: What type of windowing do you need? \u00b6 Answer after implementation: Use Tumbling Window when: Fixed time boundaries: [Every hour, every day] Non-overlapping: [Each event in exactly one window] Simple aggregation: [Count, sum per time period] Example: [Hourly sales reports, daily active users] Use Sliding Window when: Moving average: [Last N minutes] Overlapping periods: [Need smooth transitions] Real-time dashboards: [Updated frequently] Example: [5-minute average updated every 30 seconds] Use Session Window when: User activity: [Group by engagement sessions] Variable length: [Based on inactivity] Burst detection: [Cluster related events] Example: [User browsing sessions, click streams] Question 2: How do you handle late data? \u00b6 Use Watermarks when: Bounded lateness: [Most events arrive within X seconds] Completeness needed: [Want accurate results] Can tolerate delay: [Results can wait for late data] Allow Lateness when: Some late arrivals: [Network delays, mobile sync] Update results: [Can emit corrections] Balance accuracy/latency: [Wait a bit, not forever] Drop Late Data when: Strict latency: [Need real-time results] Rare late arrivals: [< 1% of events] Approximate OK: [Metrics, dashboards] Question 3: Do you need state? \u00b6 Stateless processing when: Pure transformations: [map, filter] No aggregation: [Just routing events] No joins: [Single stream] Maximum throughput: [No state overhead] Stateful processing when: Aggregations: [count, sum, average] Joins: [Combine multiple streams] Enrichment: [Add reference data] Session tracking: [User state across events] Question 4: What consistency level? \u00b6 At-most-once when: Monitoring/Metrics: [Losing some data OK] Maximum throughput: [No overhead] Non-critical: [Dashboards, alerts] At-least-once when: Idempotent operations: [Safe to retry] Can deduplicate: [Downstream handles duplicates] Good balance: [Performance + reliability] Exactly-once when: Financial: [Money, billing, payments] Critical business logic: [Inventory, orders] Compliance: [Audit trails] Your Decision Tree \u00b6 Build this after solving practice scenarios: flowchart LR Start[\"Stream Processing Pattern Selection\"] Q1{\"What's the data arrival pattern?\"} Start --> Q1 N2[\"Simple windowing\"] Q1 -->|\"In-order,<br/>low latency\"| N2 N3[\"Watermarks + allowed lateness\"] Q1 -->|\"Out-of-order,<br/>bounded\"| N3 N4[\"Session windows or approximation\"] Q1 -->|\"Out-of-order,<br/>unbounded\"| N4 Q5{\"What operations do you need?\"} Start --> Q5 N6[\"Windowing only\"] Q5 -->|\"Simple aggregation\"| N6 N7[\"Stateful processing\"] Q5 -->|\"Cross-event logic\"| N7 N8[\"Stream joins\"] Q5 -->|\"Multiple streams\"| N8 N9[\"Stateful with reference data\"] Q5 -->|\"Enrichment\"| N9 Q10{\"What's the consistency requirement?\"} Start --> Q10 N11[\"At-most-once\"] Q10 -->|\"Best effort\"| N11 N12[\"At-least-once + dedup\"] Q10 -->|\"No duplicates OK\"| N12 N13[\"Full transactional processing\"] Q10 -->|\"Exactly-once\"| N13 Q14{\"What's the latency requirement?\"} Start --> Q14 N15[\"Drop late data, smaller windows\"] Q14 -->|\"Sub-second\"| N15 N16[\"Watermarks with small lateness\"] Q14 -->|\"Seconds\"| N16 N17[\"Large lateness window, accurate results\"] Q14 -->|\"Minutes\"| N17 Practice \u00b6 Scenario 1: Real-Time Analytics Dashboard \u00b6 Requirements: Track page views per minute (updated every 10 seconds) Show top pages in last 5 minutes Handle 100K events/second Mobile apps may sync late data (up to 30s delay) Display updated immediately Your design: Windowing strategy: [Tumbling, Sliding, or Session?] Reasoning: Window type: [Fill in] Window size: [Fill in] Slide interval: [Fill in] Why this choice: [Fill in] Late data handling: [How to handle 30s delayed mobile events?] Watermark strategy: [Fill in] Allowed lateness: [Fill in] Trade-offs: [Fill in] State requirements: [What state do you need?] Per-key state: [Fill in] State backend: [In-memory or RocksDB?] TTL: [Fill in] Scenario 2: Fraud Detection System \u00b6 Requirements: Detect suspicious patterns in real-time Multiple failed logins within 1 minute Transactions from different countries < 10 minutes apart Must detect within 2 seconds of last event No false negatives (can't miss fraud) Your design: Pattern detection: [How to detect patterns across events?] Windowing: [Fill in] State needed: [Fill in] Join strategy: [Fill in] Consistency: [At-most-once, at-least-once, or exactly-once?] Choice: [Fill in] Why: [Fill in] Implementation: [Deduplication? Transactions?] Latency: [How to meet 2-second requirement?] Watermark strategy: [Fill in] Trade-offs: [Accuracy vs speed] Scenario 3: IoT Sensor Aggregation \u00b6 Requirements: 10K sensors sending readings every 10 seconds Compute average, min, max per sensor per minute Sensors have unreliable networks (late data common) Some sensors offline for hours, then send batch Store aggregates in database (no duplicates) Your design: Windowing: [Which type and why?] Window type: [Fill in] Size: [Fill in] Reasoning: [Fill in] Late data: [How to handle hours-late data?] Watermark strategy: [Fill in] Allowed lateness: [Fill in] Very late data: [Drop or reprocess?] State management: [How to manage state for 10K sensors?] State size: [Estimate per sensor] TTL: [How long to keep state?] Cleanup: [When to purge old state?] Output: [How to avoid duplicate writes to database?] Strategy: [Idempotent writes? Deduplication? Transactions?] Implementation: [Fill in] Review Checklist \u00b6 Before moving to the next topic: Implementation Tumbling window implementation works Sliding window implementation works Session window implementation works Watermark processing works Late data handling works Stateful processing works Deduplication works All client code runs successfully Understanding Filled in all ELI5 explanations Understand difference between window types Know event time vs processing time Understand watermarks and their purpose Know how state works and TTL Understand exactly-once semantics Can explain checkpointing Decision Making Know when to use each window type Can design watermark strategy Know when to use state Can choose consistency level Completed all practice scenarios Can justify design choices Performance Understand time/space complexity Know state size implications Can estimate resource needs Know throughput vs latency trade-offs Mastery Check Could implement basic stream processor Could design windowing strategy Could handle late data appropriately Could implement exactly-once processing Know common streaming pitfalls Can debug watermark issues Mastery Certification \u00b6 I certify that I can: Implement tumbling, sliding, and session windows from memory Configure watermarks and handle late data correctly Design state management with appropriate TTL Implement exactly-once processing Explain event time vs processing time Debug watermark and window closing issues Choose appropriate consistency levels Estimate memory and performance requirements Design complete stream processing pipelines Teach these concepts to someone else","title":"13. Stream Processing"},{"location":"systems/13-stream-processing/#stream-processing","text":"Real-time data processing with windowing, watermarks, and stateful operations","title":"Stream Processing"},{"location":"systems/13-stream-processing/#eli5-explain-like-im-5","text":"Your task: After implementing stream processing patterns, explain them simply. Prompts to guide you: What is stream processing in one sentence? Your answer: [Fill in after implementation] What is a window in stream processing? Your answer: [Fill in after implementation] Real-world analogy for tumbling window: Example: \"A tumbling window is like counting cars that pass every 5 minutes...\" Your analogy: [Fill in] What are watermarks in one sentence? Your answer: [Fill in after implementation] What is the difference between event time and processing time? Your answer: [Fill in after implementation] Real-world analogy for late data handling: Example: \"Late data is like receiving a postcard that was sent last week...\" Your analogy: [Fill in]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/13-stream-processing/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition about stream processing. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/13-stream-processing/#beforeafter-why-stream-processing-matters","text":"Your task: Compare batch processing vs stream processing to understand the impact.","title":"Before/After: Why Stream Processing Matters"},{"location":"systems/13-stream-processing/#case-studies-stream-processing-in-the-wild","text":"","title":"Case Studies: Stream Processing in the Wild"},{"location":"systems/13-stream-processing/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/13-stream-processing/#debugging-challenges","text":"Your task: Find and fix bugs in broken stream processing implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"systems/13-stream-processing/#decision-framework","text":"Your task: Build decision trees for when to use each stream processing pattern.","title":"Decision Framework"},{"location":"systems/13-stream-processing/#practice","text":"","title":"Practice"},{"location":"systems/13-stream-processing/#review-checklist","text":"Before moving to the next topic: Implementation Tumbling window implementation works Sliding window implementation works Session window implementation works Watermark processing works Late data handling works Stateful processing works Deduplication works All client code runs successfully Understanding Filled in all ELI5 explanations Understand difference between window types Know event time vs processing time Understand watermarks and their purpose Know how state works and TTL Understand exactly-once semantics Can explain checkpointing Decision Making Know when to use each window type Can design watermark strategy Know when to use state Can choose consistency level Completed all practice scenarios Can justify design choices Performance Understand time/space complexity Know state size implications Can estimate resource needs Know throughput vs latency trade-offs Mastery Check Could implement basic stream processor Could design windowing strategy Could handle late data appropriately Could implement exactly-once processing Know common streaming pitfalls Can debug watermark issues","title":"Review Checklist"},{"location":"systems/14-observability/","text":"Observability \u00b6 Metrics, Logging, Tracing - Understanding what your distributed system is doing ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing observability patterns, explain them simply. Prompts to guide you: What is observability in one sentence? Your answer: [Fill in after implementation] What are the three pillars of observability? Your answer: [Fill in after implementation] Real-world analogy for metrics: Example: \"Metrics are like a car's dashboard showing speed, fuel, temperature...\" Your analogy: [Fill in] Real-world analogy for logs: Example: \"Logs are like a detailed diary of everything that happened...\" Your analogy: [Fill in] Real-world analogy for traces: Example: \"Traces are like following a package through the postal system...\" Your analogy: [Fill in] When should you add metrics vs logs vs traces? Your answer: [Fill in after practice] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Storing all request details in memory: Space complexity: [Your guess: O(?)] Verified after learning: [Actual: O(?)] Recording a metric counter increment: Time complexity: [Your guess: O(?)] Space complexity: [Your guess: O(?)] Verified: [Actual] Cost calculation: If you log every request at 10K req/sec = _____ logs/day If you sample traces at 1% = _____ traces/day Storage reduction factor: _____ times less Scenario Predictions \u00b6 Scenario 1: Your API has 99.5% success rate with a 99.9% SLO Is this within SLO? [Yes/No - Why?] Error budget remaining: [Calculate] Should you alert? [Yes/No - Why?] How many more failures can you have? [Fill in] Scenario 2: Users report \"slow checkout\" but avg latency looks fine Which observability tool helps most? [Metrics/Logs/Traces - Why?] What metric might you be missing? [Fill in] What percentile should you check? [P50/P95/P99 - Why?] Scenario 3: Metric label has user_id with 1M unique values Is this a good metric label? [Yes/No - Why?] What problem does this cause? [Fill in] What should you do instead? [Fill in] Trade-off Quiz \u00b6 Question: When would structured logs be BETTER than traces for debugging? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN difference between metrics and logs? Metrics are numbers, logs are text Metrics are aggregated, logs are individual events Metrics are faster, logs are slower Metrics are free, logs cost money Verify after implementation: [Which one(s)?] Question: Why sample traces instead of capturing 100%? Your answer: [Fill in reasoning] Verified: [Fill in after learning about performance impact] Before/After: Why This Pattern Matters \u00b6 Your task: Compare blind systems vs observable systems to understand the impact. Example: Debugging a Slow API \u00b6 Problem: Users report checkout API is slow, but you don't know why. Approach 1: No Observability (Flying Blind) \u00b6 // No instrumentation - just the business logic public class CheckoutService { public Order checkout(Cart cart) { validateCart(cart); chargePayment(cart); createOrder(cart); sendEmail(cart); return order; } } What you can see: Nothing! You have to guess what's slow Add println statements and redeploy Wait for complaints to narrow down the issue Time to debug: Hours to days Analysis: Debugging time: Multiple deploy cycles Mean time to resolution: 4-8 hours Customer impact: High (prolonged issues) Approach 2: With Full Observability \u00b6 // Instrumented with metrics, logs, and traces public class CheckoutService { private final MetricsCollector metrics; private final StructuredLogger logger; private final DistributedTracer tracer; public Order checkout(Cart cart) { // Start trace Span span = tracer.startSpan(\"checkout\"); span.setTag(\"cart_id\", cart.getId()); span.setTag(\"items_count\", cart.getItems().size()); // Add context for logs logger.addContext(\"trace_id\", span.traceId); logger.addContext(\"cart_id\", cart.getId()); long startTime = System.nanoTime(); try { // Validate cart Span validateSpan = tracer.startChildSpan(\"validate_cart\"); validateCart(cart); tracer.finishSpan(); // Charge payment Span paymentSpan = tracer.startChildSpan(\"charge_payment\"); chargePayment(cart); tracer.finishSpan(); logger.info(\"Payment charged\", Map.of(\"amount\", cart.getTotal())); // Create order Span orderSpan = tracer.startChildSpan(\"create_order\"); Order order = createOrder(cart); tracer.finishSpan(); // Send email Span emailSpan = tracer.startChildSpan(\"send_email\"); sendEmail(cart); tracer.finishSpan(); // Record success metrics long duration = System.nanoTime() - startTime; metrics.recordRequest(duration / 1_000_000_000.0); logger.info(\"Checkout completed\", Map.of(\"order_id\", order.getId())); return order; } catch (Exception e) { metrics.recordError((System.nanoTime() - startTime) / 1_000_000_000.0); logger.error(\"Checkout failed\", e); throw e; } finally { tracer.finishSpan(); logger.clearContext(); } } } What you can see: Metrics: P99 latency is 3s (P50 is 100ms) - it's a tail latency issue! Logs: Search by trace_id shows payment gateway timeouts Traces: Visualization shows 95% of time spent in charge_payment span Analysis: Debugging time: 5 minutes (query dashboards) Root cause: Payment gateway timeout for 1% of requests Solution: Add timeout + retry logic Time to resolution: 30 minutes Performance Comparison \u00b6 Scenario No Observability With Observability Improvement Time to detect issue 30+ minutes (user reports) 30 seconds (alert fired) 60x faster Time to identify root cause 2-4 hours (trial/error) 5 minutes (query traces) 24x faster Deploy cycles needed 3-5 deploys 1 deploy 3-5x fewer Customer impact High (hours) Low (minutes) 10x better Your calculation: If you have 10 incidents per month, observability saves approximately _____ engineering hours. Why Does Observability Work? \u00b6 Key insight to understand: Without observability, debugging is like: Finding a needle in a haystack... blindfolded... in the dark With observability, you can: Metrics: Quickly identify that there IS a problem (P99 spike) Logs: Find specific failing requests (search by error, user_id, trace_id) Traces: See exactly where time is spent (payment span = 2.9s of 3s total) No observability: \"Users say checkout is slow\" \u2192 Try things \u2192 Deploy \u2192 Wait \u2192 Repeat With observability: \"Users say checkout is slow\" \u2192 Check metrics (P99 = 3s) \u2192 Check traces (payment = 2.9s) \u2192 Check logs (gateway timeout) \u2192 Fix + deploy \u2192 Done Why can you skip trial-and-error? Traces show the exact bottleneck (no guessing) Logs provide context (what failed and why) Metrics prove the fix worked (P99 drops to 200ms) After implementing, explain in your own words: How do the three pillars work together? [Your answer] What questions can you answer with each type? [Your answer] Why is context propagation (trace_id) important? [Your answer] Case Studies: Observability in the Wild \u00b6 Datadog: The Three Pillars in One Platform \u00b6 Pattern: Unified collection of Metrics, Traces, and Logs. How it works: A company using Datadog instruments its applications to send all three types of telemetry data. When a user reports a slow API endpoint, an engineer can start with a dashboard showing a spike in latency for that endpoint ( metric ). From there, they can drill down to a specific slow trace for that endpoint. The trace will show that the database query took 3 seconds. The engineer can then pivot directly to the logs from the database server at that exact time, which reveal a \"slow query\" log line, identifying the exact SQL query that needs optimization. Key Takeaway: The power of observability comes from correlating the three pillars. By seamlessly moving between metrics, traces, and logs, engineers can diagnose problems orders of magnitude faster than by looking at each data source in isolation. Netflix's Distributed Tracing: Debugging Microservices \u00b6 Pattern: Distributed Tracing with a unique Request ID. How it works: When a user's request enters the Netflix ecosystem, it's assigned a unique ID (e.g., Netflix-Request-Id ). This ID is passed in the header of every subsequent internal network call as the request travels through dozens of microservices. If any service encounters an error, it logs the error along with this Request ID. Engineers can then use this single ID to search their centralized logging platform (like ELK) and instantly retrieve all logs and traces related to that specific request from every service it touched. Key Takeaway: In a complex microservices architecture, simple log messages are not enough. Distributed tracing is essential for understanding the full lifecycle of a request and quickly pinpointing which service in a long chain is the source of an error or latency. Uber's M3: High-Cardinality Metrics at Scale \u00b6 Pattern: High-cardinality time-series metrics for business and system monitoring. How it works: Uber needed to monitor millions of unique entities in real-time (drivers, riders, trips). Traditional metrics systems struggle with this \"high cardinality.\" They built M3, a metrics platform designed to handle this scale. It allows them to ask questions like \"What is the average wait time for riders in downtown San Francisco right now?\" ( metrics.riders.wait_time.avg{region=sf, district=downtown} ). Key Takeaway: Metrics are not just for CPU and memory. They are a powerful tool for real-time business intelligence. However, monitoring high-cardinality dimensions (like individual users or orders) requires a specialized time-series database built to handle the metric explosion. Core Implementation \u00b6 Pattern 1: Metrics Collection \u00b6 Concept: Time-series measurements for monitoring system health and performance. Use case: API monitoring, resource utilization, business metrics. import java.util.*; import java.util.concurrent.*; import java.util.concurrent.atomic.*; /** * Metrics Collection: Counter, Gauge, Histogram * * Metric types: * - Counter: monotonically increasing (requests, errors) * - Gauge: point-in-time value (CPU, memory, queue size) * - Histogram: distribution of values (latencies, sizes) * * Methods: * - RED: Rate, Errors, Duration * - USE: Utilization, Saturation, Errors */ public class MetricsCollector { /** * Counter: Monotonically increasing value * Time: O(1), Space: O(1) * * Use for: request counts, error counts, bytes processed */ static class Counter { private final AtomicLong value = new AtomicLong(0); private final String name; private final Map<String, String> labels; Counter(String name, Map<String, String> labels) { this.name = name; this.labels = labels; } /** * Increment counter by 1 * Time: O(1) * * TODO: Implement increment */ public void inc() { // TODO: Increment value atomically } /** * Increment counter by delta * Time: O(1) * * TODO: Implement increment by delta */ public void inc(long delta) { // TODO: Add delta to value atomically } public long get() { return value.get(); } } /** * Gauge: Point-in-time value that can increase or decrease * Time: O(1), Space: O(1) * * Use for: CPU usage, memory usage, active connections */ static class Gauge { private final AtomicDouble value = new AtomicDouble(0.0); private final String name; private final Map<String, String> labels; Gauge(String name, Map<String, String> labels) { this.name = name; this.labels = labels; } /** * Set gauge to specific value * Time: O(1) * * TODO: Implement gauge set */ public void set(double val) { // TODO: Set value atomically } /** * Increment gauge * Time: O(1) * * TODO: Implement increment */ public void inc(double delta) { // TODO: Add delta atomically } public void dec(double delta) { inc(-delta); } public double get() { return value.get(); } } /** * Histogram: Distribution of observed values * Time: O(1) per observation, Space: O(B) where B = buckets * * Use for: request latencies, response sizes, batch sizes */ static class Histogram { private final String name; private final double[] buckets; // Upper bounds private final AtomicLongArray counts; // Count per bucket private final AtomicLong sum = new AtomicLong(0); private final AtomicLong count = new AtomicLong(0); /** * Create histogram with specific buckets * Example: [0.01, 0.05, 0.1, 0.5, 1.0, 5.0] * * TODO: Initialize histogram */ Histogram(String name, double[] buckets) { this.name = name; this.buckets = buckets; this.counts = new AtomicLongArray(buckets.length + 1); // +1 for +Inf } /** * Observe a value * Time: O(log B) with binary search * * TODO: Implement observation * 1. Find which bucket this value falls into * 2. Increment that bucket's count * 3. Update sum and count */ public void observe(double value) { // TODO: Find bucket using binary search // TODO: Update sum and count } /** * Helper: Find bucket index for value * * TODO: Implement binary search */ private int findBucket(double value) { // TODO: Binary search in buckets array return 0; // Replace } /** * Calculate percentile from histogram * Time: O(B) * * TODO: Implement percentile calculation */ public double getPercentile(double percentile) { long totalCount = count.get(); if (totalCount == 0) return 0.0; // TODO: Find bucket containing percentile return 0.0; // Replace } public double getAverage() { long c = count.get(); return c > 0 ? (sum.get() / 1000.0) / c : 0.0; } } /** * RED Method: Rate, Errors, Duration * Time: O(1) per metric update * * TODO: Implement RED method tracking */ static class REDMetrics { private final Counter requestCount; private final Counter errorCount; private final Histogram duration; REDMetrics(String service) { Map<String, String> labels = Map.of(\"service\", service); this.requestCount = new Counter(\"requests_total\", labels); this.errorCount = new Counter(\"errors_total\", labels); this.duration = new Histogram(\"request_duration_seconds\", new double[]{0.01, 0.05, 0.1, 0.5, 1.0, 5.0}); } /** * Record successful request * Time: O(log B) * * TODO: Update RED metrics */ public void recordRequest(double durationSeconds) { // TODO: Increment request count // TODO: Record duration } /** * Record failed request * * TODO: Update error metrics */ public void recordError(double durationSeconds) { // TODO: Increment request and error counts // TODO: Record duration } public double getErrorRate() { long requests = requestCount.get(); return requests > 0 ? (double)errorCount.get() / requests : 0.0; } public double getP50Latency() { return duration.getPercentile(0.50); } public double getP99Latency() { return duration.getPercentile(0.99); } } /** * USE Method: Utilization, Saturation, Errors * Time: O(1) per metric update * * TODO: Implement USE method tracking */ static class USEMetrics { private final Gauge utilization; // % of resource used private final Gauge saturation; // Amount of queued work private final Counter errors; // Error events USEMetrics(String resource) { Map<String, String> labels = Map.of(\"resource\", resource); this.utilization = new Gauge(\"resource_utilization\", labels); this.saturation = new Gauge(\"resource_saturation\", labels); this.errors = new Counter(\"resource_errors\", labels); } /** * Update resource utilization (0.0 to 1.0) * * TODO: Set utilization gauge */ public void setUtilization(double percent) { // TODO: Set gauge value } /** * Update saturation (queue depth) * * TODO: Set saturation gauge */ public void setSaturation(double queueDepth) { // TODO: Set gauge value } /** * Record resource error * * TODO: Increment error counter */ public void recordError() { // TODO: Increment errors } } // Helper class for atomic double operations static class AtomicDouble { private final AtomicLong bits = new AtomicLong(); public void set(double value) { bits.set(Double.doubleToLongBits(value)); } public double get() { return Double.longBitsToDouble(bits.get()); } public void addAndGet(double delta) { while (true) { long current = bits.get(); double currentVal = Double.longBitsToDouble(current); double newVal = currentVal + delta; long newBits = Double.doubleToLongBits(newVal); if (bits.compareAndSet(current, newBits)) { return; } } } } } Runnable Client Code: import java.util.*; public class MetricsClient { public static void main(String[] args) throws InterruptedException { System.out.println(\"=== Metrics Collection ===\\n\"); // Test 1: Counter System.out.println(\"--- Test 1: Counter ---\"); MetricsCollector.Counter requests = new MetricsCollector.Counter( \"http_requests_total\", Map.of(\"method\", \"GET\", \"endpoint\", \"/api/users\") ); for (int i = 0; i < 100; i++) { requests.inc(); } System.out.println(\"Total requests: \" + requests.get()); // Test 2: Gauge System.out.println(\"\\n--- Test 2: Gauge ---\"); MetricsCollector.Gauge cpuUsage = new MetricsCollector.Gauge( \"cpu_usage_percent\", Map.of(\"core\", \"0\") ); cpuUsage.set(45.5); System.out.println(\"CPU usage: \" + cpuUsage.get() + \"%\"); cpuUsage.inc(10.2); System.out.println(\"CPU usage after increase: \" + cpuUsage.get() + \"%\"); // Test 3: Histogram System.out.println(\"\\n--- Test 3: Histogram ---\"); MetricsCollector.Histogram latency = new MetricsCollector.Histogram( \"request_duration_seconds\", new double[]{0.01, 0.05, 0.1, 0.5, 1.0, 5.0} ); Random rand = new Random(42); for (int i = 0; i < 1000; i++) { double duration = rand.nextGaussian() * 0.2 + 0.3; // Mean 300ms latency.observe(Math.max(0, duration)); } System.out.println(\"Average latency: \" + latency.getAverage() + \"s\"); System.out.println(\"P50 latency: \" + latency.getPercentile(0.50) + \"s\"); System.out.println(\"P99 latency: \" + latency.getPercentile(0.99) + \"s\"); // Test 4: RED Method System.out.println(\"\\n--- Test 4: RED Method ---\"); MetricsCollector.REDMetrics red = new MetricsCollector.REDMetrics(\"user-service\"); for (int i = 0; i < 95; i++) { double duration = rand.nextDouble() * 0.5; red.recordRequest(duration); } for (int i = 0; i < 5; i++) { double duration = rand.nextDouble() * 2.0; red.recordError(duration); } System.out.println(\"Error rate: \" + (red.getErrorRate() * 100) + \"%\"); System.out.println(\"P50 latency: \" + red.getP50Latency() + \"s\"); System.out.println(\"P99 latency: \" + red.getP99Latency() + \"s\"); // Test 5: USE Method System.out.println(\"\\n--- Test 5: USE Method ---\"); MetricsCollector.USEMetrics use = new MetricsCollector.USEMetrics(\"database_pool\"); use.setUtilization(0.75); // 75% of connections in use use.setSaturation(12.0); // 12 requests waiting use.recordError(); use.recordError(); System.out.println(\"Pool utilization: 75%\"); System.out.println(\"Queue saturation: 12 waiting requests\"); System.out.println(\"Errors recorded: 2\"); } } Pattern 2: Structured Logging \u00b6 Concept: JSON-formatted logs with consistent fields for parsing and aggregation. Use case: Application logs, audit trails, debugging distributed systems. import java.util.*; import java.time.*; import com.fasterxml.jackson.databind.ObjectMapper; /** * Structured Logging: JSON logs with context * * Log levels: TRACE, DEBUG, INFO, WARN, ERROR, FATAL * Context: Trace ID, User ID, Request ID * Fields: timestamp, level, message, context, fields */ public class StructuredLogger { enum LogLevel { TRACE(0), DEBUG(1), INFO(2), WARN(3), ERROR(4), FATAL(5); final int priority; LogLevel(int priority) { this.priority = priority; } } private final String service; private final LogLevel minLevel; private final ObjectMapper mapper; private final ThreadLocal<Map<String, Object>> context; public StructuredLogger(String service, LogLevel minLevel) { this.service = service; this.minLevel = minLevel; this.mapper = new ObjectMapper(); this.context = ThreadLocal.withInitial(HashMap::new); } /** * Log entry structure * * TODO: Define log entry format */ static class LogEntry { public String timestamp; public String level; public String service; public String message; public Map<String, Object> context; public Map<String, Object> fields; LogEntry(String service, LogLevel level, String message, Map<String, Object> context, Map<String, Object> fields) { this.timestamp = Instant.now().toString(); this.level = level.name(); this.service = service; this.message = message; this.context = new HashMap<>(context); this.fields = fields; } } /** * Add context to current thread (trace ID, user ID, etc.) * Time: O(1) * * TODO: Implement context addition */ public void addContext(String key, Object value) { // TODO: Add to thread-local context } /** * Clear context for current thread * Time: O(1) * * TODO: Implement context clearing */ public void clearContext() { // TODO: Clear thread-local context } /** * Log at INFO level * Time: O(1) * * TODO: Implement info logging */ public void info(String message) { log(LogLevel.INFO, message, Map.of()); } public void info(String message, Map<String, Object> fields) { log(LogLevel.INFO, message, fields); } /** * Log at WARN level * * TODO: Implement warn logging */ public void warn(String message) { log(LogLevel.WARN, message, Map.of()); } public void warn(String message, Map<String, Object> fields) { log(LogLevel.WARN, message, fields); } /** * Log at ERROR level * * TODO: Implement error logging */ public void error(String message) { log(LogLevel.ERROR, message, Map.of()); } public void error(String message, Throwable t) { Map<String, Object> fields = new HashMap<>(); fields.put(\"error\", t.getClass().getName()); fields.put(\"error_message\", t.getMessage()); fields.put(\"stack_trace\", getStackTrace(t)); log(LogLevel.ERROR, message, fields); } public void error(String message, Map<String, Object> fields) { log(LogLevel.ERROR, message, fields); } /** * Core logging method * Time: O(1) + O(JSON serialization) * * TODO: Implement core logging * 1. Check if level is enabled * 2. Create log entry with context * 3. Serialize to JSON * 4. Write to output */ private void log(LogLevel level, String message, Map<String, Object> fields) { // TODO: Check if level should be logged // TODO: Create log entry // TODO: Serialize to JSON and output } /** * Helper: Get stack trace as string * * TODO: Implement stack trace extraction */ private String getStackTrace(Throwable t) { // TODO: Convert stack trace to string return null; // Replace } /** * Log aggregation helper: Parse JSON logs * * TODO: Implement log parsing for aggregation */ public static LogEntry parseLog(String json) { // TODO: Parse JSON string back to LogEntry return null; // Replace } /** * Filter logs by level * Time: O(N) where N = logs * * TODO: Implement log filtering */ public static List<LogEntry> filterByLevel(List<LogEntry> logs, LogLevel level) { // TODO: Filter logs by minimum level return null; // Replace } /** * Find logs by context (e.g., trace_id) * Time: O(N) * * TODO: Implement context-based search */ public static List<LogEntry> findByContext(List<LogEntry> logs, String key, Object value) { // TODO: Filter logs by context field return null; // Replace } } Runnable Client Code: import java.util.*; public class StructuredLoggerClient { public static void main(String[] args) { System.out.println(\"=== Structured Logging ===\\n\"); StructuredLogger logger = new StructuredLogger( \"order-service\", StructuredLogger.LogLevel.INFO ); // Test 1: Basic logging System.out.println(\"--- Test 1: Basic Logging ---\"); logger.info(\"Service started\"); logger.info(\"Processing order\", Map.of(\"order_id\", \"12345\", \"amount\", 99.99)); logger.warn(\"High memory usage\", Map.of(\"memory_percent\", 85)); // Test 2: Context propagation System.out.println(\"\\n--- Test 2: Context Propagation ---\"); logger.addContext(\"trace_id\", \"abc-123-def\"); logger.addContext(\"user_id\", \"user_456\"); logger.info(\"User logged in\"); logger.info(\"Fetching user data\", Map.of(\"user_id\", \"user_456\")); logger.info(\"Order created\", Map.of(\"order_id\", \"67890\")); logger.clearContext(); // Test 3: Error logging System.out.println(\"\\n--- Test 3: Error Logging ---\"); try { throw new RuntimeException(\"Database connection failed\"); } catch (Exception e) { logger.error(\"Failed to process order\", e); } // Test 4: Different log levels System.out.println(\"\\n--- Test 4: Log Levels ---\"); StructuredLogger debugLogger = new StructuredLogger( \"debug-service\", StructuredLogger.LogLevel.DEBUG ); debugLogger.info(\"This appears\"); // debugLogger.debug(\"This also appears\"); // Would need debug method debugLogger.warn(\"Warning message\"); debugLogger.error(\"Error message\"); // Test 5: Structured fields System.out.println(\"\\n--- Test 5: Structured Fields ---\"); Map<String, Object> orderFields = new HashMap<>(); orderFields.put(\"order_id\", \"ORDER-123\"); orderFields.put(\"customer_id\", \"CUST-456\"); orderFields.put(\"total\", 149.99); orderFields.put(\"items_count\", 3); logger.info(\"Order completed\", orderFields); } } Pattern 3: Distributed Tracing \u00b6 Concept: Track requests across multiple services with parent-child relationships. Use case: Debugging microservices, understanding request flow, finding bottlenecks. import java.util.*; import java.time.*; import java.util.concurrent.*; /** * Distributed Tracing: Track requests across services * * Concepts: * - Trace: End-to-end request flow * - Span: Single operation within a trace * - Context: Trace ID + Span ID propagated across services */ public class DistributedTracer { /** * Trace context: Propagated across service boundaries * * TODO: Define trace context structure */ static class TraceContext { String traceId; // Unique ID for entire trace String spanId; // Current span ID String parentSpanId; // Parent span ID (null for root) TraceContext(String traceId, String spanId, String parentSpanId) { this.traceId = traceId; this.spanId = spanId; this.parentSpanId = parentSpanId; } /** * Create root trace context * * TODO: Generate new trace ID */ public static TraceContext createRoot() { // TODO: Generate unique trace ID return null; // Replace } /** * Create child context for new span * * TODO: Generate child span ID */ public TraceContext createChild() { // TODO: Keep same trace ID, new span ID return null; // Replace } } /** * Span: Represents single operation * Time: O(1) for all operations * * TODO: Define span structure */ static class Span { String traceId; String spanId; String parentSpanId; String operationName; long startTimeMicros; long endTimeMicros; Map<String, String> tags; List<String> logs; Span(TraceContext context, String operationName) { this.traceId = context.traceId; this.spanId = context.spanId; this.parentSpanId = context.parentSpanId; this.operationName = operationName; this.startTimeMicros = System.nanoTime() / 1000; this.tags = new HashMap<>(); this.logs = new ArrayList<>(); } /** * Add tag to span (metadata) * * TODO: Add span tag */ public void setTag(String key, String value) { // TODO: Add to tags map } /** * Log event within span * * TODO: Add log entry */ public void log(String message) { // TODO: Add timestamped log } /** * Finish span (record end time) * * TODO: Record end time */ public void finish() { // TODO: Set end time } public long getDurationMicros() { return endTimeMicros - startTimeMicros; } } /** * Tracer: Creates and manages spans */ private final String serviceName; private final ThreadLocal<Deque<Span>> activeSpans; private final List<Span> completedSpans; public DistributedTracer(String serviceName) { this.serviceName = serviceName; this.activeSpans = ThreadLocal.withInitial(ArrayDeque::new); this.completedSpans = new CopyOnWriteArrayList<>(); } /** * Start new root span * Time: O(1) * * TODO: Create root span */ public Span startSpan(String operationName) { // TODO: Create root context and span return null; // Replace } /** * Start child span of current active span * Time: O(1) * * TODO: Create child span */ public Span startChildSpan(String operationName) { // TODO: Get current span, create child context // // Span parent = stack.peek(); // TraceContext parentContext = new TraceContext( // parent.traceId, parent.spanId, parent.parentSpanId // ); // TraceContext childContext = parentContext.createChild(); // Span span = new Span(childContext, operationName); // span.setTag(\"service\", serviceName); // stack.push(span); // return span; return null; // Replace } /** * Finish current span * Time: O(1) * * TODO: Finish and record span */ public void finishSpan() { // TODO: Pop span from stack, finish it, store it } /** * Get current active span * Time: O(1) * * TODO: Return current span */ public Span getCurrentSpan() { // TODO: Peek at top of stack return null; // Replace } /** * Find all spans for a trace * Time: O(N) where N = total spans * * TODO: Filter spans by trace ID */ public List<Span> getTrace(String traceId) { // TODO: Filter completed spans by trace ID return null; // Replace } /** * Build trace tree for visualization * Time: O(N) where N = spans in trace * * TODO: Build parent-child tree */ public String visualizeTrace(String traceId) { // TODO: Build tree structure from parent-child relationships // // // Group by parent // for (Span span : spans) { // String parentId = span.parentSpanId != null ? span.parentSpanId : \"root\"; // childrenMap.computeIfAbsent(parentId, k -> new ArrayList<>()) // .add(span); // } // // // Find root and build tree // Span root = spans.stream() // .filter(s -> s.parentSpanId == null) // .findFirst() // .orElse(null); // // StringBuilder sb = new StringBuilder(); // buildTraceString(root, childrenMap, sb, 0); // return sb.toString(); return null; // Replace } private void buildTraceString(Span span, Map<String, List<Span>> childrenMap, StringBuilder sb, int depth) { // TODO: Recursively build tree string // // List<Span> children = childrenMap.get(span.spanId); // if (children != null) { // for (Span child : children) { // buildTraceString(child, childrenMap, sb, depth + 1); // } // } } /** * Calculate critical path (longest path in trace) * Time: O(N) where N = spans * * TODO: Find bottleneck in trace */ public List<Span> getCriticalPath(String traceId) { // TODO: DFS to find longest path return null; // Replace } } Runnable Client Code: import java.util.*; public class DistributedTracerClient { public static void main(String[] args) throws InterruptedException { System.out.println(\"=== Distributed Tracing ===\\n\"); DistributedTracer tracer = new DistributedTracer(\"order-service\"); // Test 1: Simple trace System.out.println(\"--- Test 1: Simple Trace ---\"); DistributedTracer.Span rootSpan = tracer.startSpan(\"process_order\"); rootSpan.setTag(\"order_id\", \"12345\"); rootSpan.log(\"Order validation started\"); Thread.sleep(10); rootSpan.log(\"Order validated\"); tracer.finishSpan(); System.out.println(\"Root span duration: \" + rootSpan.getDurationMicros() / 1000.0 + \"ms\"); // Test 2: Parent-child spans System.out.println(\"\\n--- Test 2: Parent-Child Spans ---\"); DistributedTracer.Span apiSpan = tracer.startSpan(\"api_request\"); apiSpan.setTag(\"endpoint\", \"/api/checkout\"); apiSpan.log(\"Request received\"); Thread.sleep(5); DistributedTracer.Span dbSpan = tracer.startChildSpan(\"database_query\"); dbSpan.setTag(\"query\", \"SELECT * FROM orders\"); dbSpan.log(\"Query started\"); Thread.sleep(15); dbSpan.log(\"Query completed\"); tracer.finishSpan(); // Finish DB span DistributedTracer.Span cacheSpan = tracer.startChildSpan(\"cache_lookup\"); cacheSpan.setTag(\"key\", \"user:123\"); Thread.sleep(2); tracer.finishSpan(); // Finish cache span apiSpan.log(\"Request completed\"); tracer.finishSpan(); // Finish API span String traceId = apiSpan.traceId; System.out.println(\"Trace ID: \" + traceId); System.out.println(\"API span duration: \" + apiSpan.getDurationMicros() / 1000.0 + \"ms\"); System.out.println(\"DB span duration: \" + dbSpan.getDurationMicros() / 1000.0 + \"ms\"); System.out.println(\"Cache span duration: \" + cacheSpan.getDurationMicros() / 1000.0 + \"ms\"); // Test 3: Trace visualization System.out.println(\"\\n--- Test 3: Trace Visualization ---\"); String visualization = tracer.visualizeTrace(traceId); if (visualization != null) { System.out.println(visualization); } else { System.out.println(\"(Trace visualization not yet implemented)\"); } // Test 4: Multiple traces System.out.println(\"\\n--- Test 4: Multiple Traces ---\"); DistributedTracer.Span trace1 = tracer.startSpan(\"operation_1\"); Thread.sleep(5); tracer.finishSpan(); DistributedTracer.Span trace2 = tracer.startSpan(\"operation_2\"); Thread.sleep(8); tracer.finishSpan(); System.out.println(\"Trace 1 ID: \" + trace1.traceId); System.out.println(\"Trace 2 ID: \" + trace2.traceId); System.out.println(\"Different traces: \" + !trace1.traceId.equals(trace2.traceId)); } } Pattern 4: SLOs and Alerting \u00b6 Concept: Define Service Level Objectives and alert on violations. Use case: Production monitoring, on-call alerting, capacity planning. import java.util.*; import java.time.*; /** * SLO Management and Alerting * * Definitions: * - SLI (Service Level Indicator): Actual measurement (e.g., 99.5% uptime) * - SLO (Service Level Objective): Target value (e.g., 99.9% uptime) * - SLA (Service Level Agreement): Contract with penalties (e.g., 99.95% uptime) * - Error Budget: Allowed failure amount (e.g., 0.1% = 43 minutes/month) */ public class SLOManager { /** * SLI: Service Level Indicator */ enum SLIType { AVAILABILITY, // % of successful requests LATENCY, // % of requests below threshold ERROR_RATE // % of failed requests } /** * SLO Definition * * TODO: Define SLO structure */ static class SLO { String name; SLIType type; double target; // Target value (e.g., 0.999 for 99.9%) Duration window; // Time window (e.g., 30 days) double alertThreshold; // When to alert (e.g., 0.5 = 50% error budget consumed) SLO(String name, SLIType type, double target, Duration window, double alertThreshold) { this.name = name; this.type = type; this.target = target; this.window = window; this.alertThreshold = alertThreshold; } } /** * SLI Measurement * * TODO: Track actual measurements */ static class SLIMeasurement { SLO slo; List<DataPoint> measurements; static class DataPoint { Instant timestamp; double value; DataPoint(Instant timestamp, double value) { this.timestamp = timestamp; this.value = value; } } SLIMeasurement(SLO slo) { this.slo = slo; this.measurements = new ArrayList<>(); } /** * Record measurement * Time: O(1) * * TODO: Add data point */ public void record(double value) { // TODO: Add measurement with timestamp } /** * Calculate current SLI value * Time: O(N) where N = measurements in window * * TODO: Calculate SLI for time window */ public double calculate() { // TODO: Filter measurements within window // // if (recent.isEmpty()) return 1.0; // // // Calculate based on SLI type // switch (slo.type) { // case AVAILABILITY: // // % of successful requests // return recent.stream() // .mapToDouble(dp -> dp.value) // .average() // .orElse(1.0); // // ... other types // } return 1.0; // Replace } /** * Calculate error budget remaining * Time: O(1) after calculate() * * TODO: Compute error budget * Error budget = (target - actual) / (1 - target) * Example: target=0.999, actual=0.998 * budget = (0.999 - 0.998) / (1 - 0.999) = 0.001 / 0.001 = 1.0 (100% remaining) */ public double getErrorBudget() { // TODO: Calculate error budget // // double allowed = 1.0 - slo.target; // Allowed failure rate // double actual = 1.0 - current; // Actual failure rate // double consumed = actual / allowed; // Fraction consumed // return Math.max(0.0, 1.0 - consumed); // Remaining budget return 1.0; // Replace } /** * Check if alert should fire * Time: O(1) * * TODO: Determine if alert needed */ public boolean shouldAlert() { // TODO: Check if error budget below threshold return false; // Replace } } /** * Alert Rule * * TODO: Define alert conditions */ static class AlertRule { String name; String query; // Metric query (e.g., \"error_rate > 0.01\") Duration duration; // Must be true for this long String severity; // CRITICAL, WARNING, INFO String message; List<String> notifyChannels; // slack, pagerduty, email AlertRule(String name, String query, Duration duration, String severity, String message, List<String> channels) { this.name = name; this.query = query; this.duration = duration; this.severity = severity; this.message = message; this.notifyChannels = channels; } } /** * Alert Manager * * TODO: Evaluate rules and fire alerts */ static class AlertManager { private List<AlertRule> rules; private Map<String, Instant> firingAlerts; // Alert name -> first fired time AlertManager() { this.rules = new ArrayList<>(); this.firingAlerts = new HashMap<>(); } /** * Add alert rule * * TODO: Register alert rule */ public void addRule(AlertRule rule) { // TODO: Add to rules list } /** * Evaluate all rules * Time: O(R) where R = rules * * TODO: Check all alert conditions */ public List<Alert> evaluate(Map<String, Double> metrics) { List<Alert> alerts = new ArrayList<>(); // TODO: Implement iteration/conditional logic // // if (condition) { // // Check if been firing long enough // Instant firstFired = firingAlerts.computeIfAbsent( // rule.name, k -> Instant.now() // ); // Duration firingFor = Duration.between(firstFired, Instant.now()); // // if (firingFor.compareTo(rule.duration) >= 0) { // alerts.add(new Alert(rule)); // } // } else { // // Clear if no longer firing // firingAlerts.remove(rule.name); // } // } return alerts; // Replace } /** * Helper: Evaluate query condition * * TODO: Parse and evaluate simple queries */ private boolean evaluateQuery(String query, Map<String, Double> metrics) { // TODO: Parse query like \"error_rate > 0.01\" // // String metric = parts[0]; // String operator = parts[1]; // double threshold = Double.parseDouble(parts[2]); // // Double value = metrics.get(metric); // if (value == null) return false; // // switch (operator) { // case \">\": return value > threshold; // case \"<\": return value < threshold; // case \">=\": return value >= threshold; // case \"<=\": return value <= threshold; // default: return false; // } return false; // Replace } } /** * Fired Alert * * TODO: Alert notification structure */ static class Alert { AlertRule rule; Instant firedAt; Alert(AlertRule rule) { this.rule = rule; this.firedAt = Instant.now(); } @Override public String toString() { return String.format(\"[%s] %s: %s\", rule.severity, rule.name, rule.message); } } /** * Runbook: Steps to resolve alert * * TODO: Define runbook structure */ static class Runbook { String alertName; String description; List<String> investigationSteps; List<String> resolutionSteps; Map<String, String> relatedDashboards; Runbook(String alertName) { this.alertName = alertName; this.investigationSteps = new ArrayList<>(); this.resolutionSteps = new ArrayList<>(); this.relatedDashboards = new HashMap<>(); } public void addInvestigationStep(String step) { investigationSteps.add(step); } public void addResolutionStep(String step) { resolutionSteps.add(step); } public void addDashboard(String name, String url) { relatedDashboards.put(name, url); } @Override public String toString() { StringBuilder sb = new StringBuilder(); sb.append(\"=== Runbook: \").append(alertName).append(\" ===\\n\\n\"); sb.append(description).append(\"\\n\\n\"); sb.append(\"Investigation Steps:\\n\"); for (int i = 0; i < investigationSteps.size(); i++) { sb.append((i + 1)).append(\". \").append(investigationSteps.get(i)).append(\"\\n\"); } sb.append(\"\\nResolution Steps:\\n\"); for (int i = 0; i < resolutionSteps.size(); i++) { sb.append((i + 1)).append(\". \").append(resolutionSteps.get(i)).append(\"\\n\"); } if (!relatedDashboards.isEmpty()) { sb.append(\"\\nRelated Dashboards:\\n\"); relatedDashboards.forEach((name, url) -> sb.append(\"- \").append(name).append(\": \").append(url).append(\"\\n\") ); } return sb.toString(); } } } Runnable Client Code: import java.util.*; import java.time.*; public class SLOManagerClient { public static void main(String[] args) throws InterruptedException { System.out.println(\"=== SLO Management and Alerting ===\\n\"); // Test 1: SLO Tracking System.out.println(\"--- Test 1: SLO Tracking ---\"); SLOManager.SLO availabilitySLO = new SLOManager.SLO( \"API Availability\", SLOManager.SLIType.AVAILABILITY, 0.999, // 99.9% target Duration.ofDays(30), 0.5 // Alert at 50% error budget ); SLOManager.SLIMeasurement measurement = new SLOManager.SLIMeasurement(availabilitySLO); // Simulate measurements (1 = success, 0 = failure) for (int i = 0; i < 1000; i++) { measurement.record(1.0); // Success } for (int i = 0; i < 2; i++) { measurement.record(0.0); // Failure } double currentSLI = measurement.calculate(); double errorBudget = measurement.getErrorBudget(); boolean shouldAlert = measurement.shouldAlert(); System.out.println(\"Target SLO: 99.9%\"); System.out.println(\"Current SLI: \" + (currentSLI * 100) + \"%\"); System.out.println(\"Error budget remaining: \" + (errorBudget * 100) + \"%\"); System.out.println(\"Should alert: \" + shouldAlert); // Test 2: Alert Rules System.out.println(\"\\n--- Test 2: Alert Rules ---\"); SLOManager.AlertManager alertManager = new SLOManager.AlertManager(); SLOManager.AlertRule highErrorRate = new SLOManager.AlertRule( \"HighErrorRate\", \"error_rate > 0.01\", Duration.ofMinutes(5), \"CRITICAL\", \"Error rate above 1% for 5+ minutes\", Arrays.asList(\"pagerduty\", \"slack\") ); SLOManager.AlertRule highLatency = new SLOManager.AlertRule( \"HighLatency\", \"p99_latency > 1.0\", Duration.ofMinutes(10), \"WARNING\", \"P99 latency above 1s for 10+ minutes\", Arrays.asList(\"slack\") ); alertManager.addRule(highErrorRate); alertManager.addRule(highLatency); // Simulate metrics Map<String, Double> metrics = new HashMap<>(); metrics.put(\"error_rate\", 0.015); // 1.5% errors metrics.put(\"p99_latency\", 0.8); // 800ms List<SLOManager.Alert> alerts = alertManager.evaluate(metrics); System.out.println(\"Alerts fired: \" + alerts.size()); for (SLOManager.Alert alert : alerts) { System.out.println(alert); } // Test 3: Error Budget Calculation System.out.println(\"\\n--- Test 3: Error Budget Calculation ---\"); double[] targets = {0.99, 0.999, 0.9999}; String[] names = {\"99%\", \"99.9%\", \"99.99%\"}; for (int i = 0; i < targets.length; i++) { double target = targets[i]; double allowedDowntime = (1.0 - target) * 30 * 24 * 60; // minutes per month System.out.println(\"\\nSLO: \" + names[i]); System.out.println(\"Allowed downtime: \" + allowedDowntime + \" minutes/month\"); System.out.println(\"That's \" + (allowedDowntime / 60) + \" hours/month\"); } // Test 4: Runbook System.out.println(\"\\n--- Test 4: Runbook ---\"); SLOManager.Runbook runbook = new SLOManager.Runbook(\"HighErrorRate\"); runbook.description = \"Error rate has exceeded 1% threshold\"; runbook.addInvestigationStep(\"Check error logs for patterns\"); runbook.addInvestigationStep(\"Review recent deployments\"); runbook.addInvestigationStep(\"Check downstream service health\"); runbook.addInvestigationStep(\"Verify database connection pool\"); runbook.addResolutionStep(\"Rollback recent deployment if needed\"); runbook.addResolutionStep(\"Scale up affected services\"); runbook.addResolutionStep(\"Enable circuit breakers\"); runbook.addResolutionStep(\"Update on-call ticket with findings\"); runbook.addDashboard(\"Error Dashboard\", \"https://grafana.example.com/errors\"); runbook.addDashboard(\"Service Health\", \"https://grafana.example.com/health\"); System.out.println(runbook); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in broken observability implementations. This tests your understanding. Challenge 1: Missing Metric Labels \u00b6 /** * This metrics collector is supposed to track requests per endpoint. * It has a CRITICAL DESIGN FLAW. Find it! */ public class EndpointMetrics { private Counter totalRequests = new Counter(\"http_requests_total\", Map.of()); public void recordRequest(String endpoint, String method, int statusCode) { totalRequests.inc(); } public long getRequestCount() { return totalRequests.get(); } } Your debugging: Bug: [What\\'s the bug?] Impact: [Why is this a problem in production?] Test scenario: 100 requests to /api/users (GET) 50 requests to /api/orders (POST) 10 requests to /api/users (DELETE) What can you query? [Fill in what you can and can't learn] Click to verify your answers Bug: No labels! All requests go into a single counter, so you can't distinguish: Which endpoint is getting traffic Which HTTP method is used Which status codes are returned Fix: public void recordRequest(String endpoint, String method, int statusCode) { Map<String, String> labels = Map.of( \"endpoint\", endpoint, \"method\", method, \"status_code\", String.valueOf(statusCode) ); Counter counter = new Counter(\"http_requests_total\", labels); counter.inc(); } Impact: Without labels, you can't: Alert on specific endpoint errors Identify which API is slow Track SLOs per endpoint Debug which endpoint is causing load Challenge 2: High-Cardinality Labels \u00b6 /** * This code tracks cache hits/misses per user. * It has a SCALABILITY BUG. Find it! */ public class CacheMetrics { private Map<String, Counter> cacheHitsByUser = new ConcurrentHashMap<>(); public void recordCacheHit(String userId) { cacheHitsByUser .computeIfAbsent(userId, k -> new Counter(\"cache_hits\", Map.of(\"user_id\", userId))) .inc(); } // 1M users = 1M unique metric series! } Your debugging: Bug: [What's the high-cardinality problem?] Memory impact: [How much memory with 1M users?] Query impact: [Why do queries become slow?] Fix: [What should you track instead?] Cardinality calculation: If 1M users, you create _____ unique time series If each series uses 10KB of memory = _____ GB total Query time grows from 10ms to _____ seconds Click to verify your answers Bug: user_id is a high-cardinality label (potentially millions of unique values). This causes: Memory explosion: Each unique label combination = new time series Slow queries: Database must scan millions of series Storage costs: Unbounded growth Metric system overload: Can crash Prometheus/etc. Fix: Track aggregated metrics instead: // GOOD: Low cardinality private Counter cacheHits = new Counter(\"cache_hits_total\", Map.of()); private Counter cacheMisses = new Counter(\"cache_misses_total\", Map.of()); public void recordCacheHit(boolean hit) { if (hit) { cacheHits.inc(); } else { cacheMisses.inc(); } } // If you need user-level detail, use logs or traces instead! logger.info(\"Cache hit\", Map.of(\"user_id\", userId, \"key\", key)); Rule: Metric labels should have bounded cardinality (< 100 unique values per label). Challenge 3: Slow Histogram Queries \u00b6 /** * This histogram implementation is correct but SLOW. * Why? How to fix it? */ public class LatencyHistogram { private List<Double> allLatencies = new ArrayList<>(); public void observe(double latency) { synchronized(allLatencies) { allLatencies.add(latency); } } public double getP99() { synchronized(allLatencies) { if (allLatencies.isEmpty()) return 0.0; Collections.sort(allLatencies); int index = (int)(allLatencies.size() * 0.99); return allLatencies.get(index); } } } Your debugging: Performance bug: [What operation is expensive?] Complexity: [What's the time complexity?] With 1M observations: [How long does getP99 take?] Fix: [What data structure should you use?] Performance impact: 1M observations in list Calling getP99 100 times per second Current: _____ ms per call Fixed: _____ ms per call Click to verify your answers Bug: Sorting entire list on every query is O(n log n). With 1M observations: 1M * log(1M) \u2248 20M operations At 100 calls/sec = 2 billion operations/sec! Fix: Use bucketed histogram (like Prometheus): public class LatencyHistogram { private final double[] buckets = {0.01, 0.05, 0.1, 0.5, 1.0, 5.0}; private final AtomicLongArray counts = new AtomicLongArray(buckets.length + 1); private final AtomicLong totalCount = new AtomicLong(0); public void observe(double latency) { int bucket = findBucket(latency); counts.incrementAndGet(bucket); totalCount.incrementAndGet(); } public double getP99() { long total = totalCount.get(); long target = (long)(total * 0.99); long cumulative = 0; for (int i = 0; i < counts.length(); i++) { cumulative += counts.get(i); if (cumulative >= target) { return i < buckets.length ? buckets[i] : Double.POSITIVE_INFINITY; } } return 0.0; } } Improvement: Before: O(n log n) = ~20M ops for 1M observations After: O(buckets) = ~7 ops regardless of observation count Speedup: ~3 million times faster! Trade-off: Approximate percentiles (bucket boundaries) vs exact values. Challenge 4: Broken Trace Sampling \u00b6 /** * This trace sampler is supposed to sample 10% of traces. * It has a CRITICAL BUG. Find it! */ public class TraceSampler { private Random random = new Random(); private double sampleRate = 0.10; // 10% public boolean shouldSample(String traceId) { return random.nextDouble() < sampleRate; } } // Usage: public Span startSpan(String operation) { if (sampler.shouldSample(currentTraceId)) { return tracer.startSpan(operation); } return null; } Your debugging: Bug: [What's inconsistent about this sampling?] Impact: [What happens to child spans?] Trace visualization: [Why are traces incomplete?] Fix: [How to ensure consistent sampling?] Test scenario: Parent span: API request (sampled = true) Child span: Database query (sampled = ???) Grandchild span: Cache lookup (sampled = ???) Problem: [What's broken about the trace?] Click to verify your answers Bug: Each span makes independent sampling decision! This causes: Parent sampled but children dropped \u2192 incomplete traces Children sampled but parent dropped \u2192 orphaned spans Can't reconstruct full request flow Fix: Sample based on trace ID (head-based sampling): public class TraceSampler { private double sampleRate = 0.10; public boolean shouldSample(String traceId) { // Hash trace ID to get consistent decision // All spans in same trace get same result long hash = Math.abs(traceId.hashCode()); return (hash % 100) < (sampleRate * 100); } } // Now all spans in trace have same sampling decision! Alternative: Tail-based sampling (sample AFTER seeing full trace): // Keep all traces in memory temporarily // Sample based on: errors, high latency, specific endpoints // Trade-off: More memory, but smarter sampling Key insight: Sampling decision must be consistent across entire trace. Challenge 5: Missing Log Context \u00b6 /** * These logs look fine individually but are USELESS for debugging. * What's missing? */ public class OrderService { private StructuredLogger logger = new StructuredLogger(\"order-service\"); public void processOrder(Order order) { logger.info(\"Processing order\"); validateOrder(order); logger.info(\"Order validated\"); chargePayment(order); logger.info(\"Payment charged\"); createShipment(order); logger.info(\"Shipment created\"); } } // Logs in production: // {\"timestamp\":\"...\", \"level\":\"INFO\", \"message\":\"Processing order\"} // {\"timestamp\":\"...\", \"level\":\"INFO\", \"message\":\"Payment charged\"} // {\"timestamp\":\"...\", \"level\":\"INFO\", \"message\":\"Order validated\"} // {\"timestamp\":\"...\", \"level\":\"INFO\", \"message\":\"Shipment created\"} Your debugging: Bug: [What's missing from every log?] Debugging scenario: [How do you find logs for order #12345?] Correlation: [How do you trace a request across services?] Fix: [What fields should every log have?] Click to verify your answers Bug: No context! Missing: order_id - can't find logs for specific order trace_id - can't correlate across services user_id - can't find user's journey Ordering - can't tell which log belongs to which request Fix: Add context to every log: public void processOrder(Order order) { // Set context once at start logger.addContext(\"order_id\", order.getId()); logger.addContext(\"user_id\", order.getUserId()); logger.addContext(\"trace_id\", getCurrentTraceId()); try { logger.info(\"Processing order\", Map.of(\"total\", order.getTotal())); validateOrder(order); logger.info(\"Order validated\"); chargePayment(order); logger.info(\"Payment charged\", Map.of(\"amount\", order.getTotal())); createShipment(order); logger.info(\"Shipment created\", Map.of(\"tracking\", shipment.getTrackingId())); } finally { logger.clearContext(); // Clean up } } // Now logs look like: // {\"timestamp\":\"...\", \"level\":\"INFO\", \"message\":\"Processing order\", // \"context\":{\"order_id\":\"12345\", \"user_id\":\"user_1\", \"trace_id\":\"abc-123\"}, // \"fields\":{\"total\":99.99}} Key insight: Logs without context are useless for debugging distributed systems. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found high-cardinality label issue Understood histogram performance trade-offs Fixed trace sampling consistency Added proper log context Avoided common observability pitfalls Common mistakes you discovered: [List the patterns you noticed] [Fill in] [Fill in] Decision Framework \u00b6 Your task: Build decision trees for observability patterns. Question 1: Metrics vs Logs vs Traces? \u00b6 Answer after implementation: Use Metrics when: Aggregated data: [Count of requests, average latency] Alerting: [Need to trigger alerts on thresholds] Dashboards: [Time-series graphs and trends] Low overhead: [Constant memory usage] Use Logs when: Debugging: [Need full context of what happened] Audit trail: [Who did what and when] Irregular events: [Errors, exceptions, business events] Flexible queries: [Search by any field] Use Traces when: Distributed systems: [Request flows across services] Performance analysis: [Find bottlenecks in request path] Dependencies: [Understand service relationships] Latency debugging: [Which service is slow] Question 2: When to add observability? \u00b6 During development: Add metrics: [Core business operations, API endpoints] Add logs: [Error paths, state changes, important decisions] Add traces: [Service boundaries, external calls] During incidents: Add metrics: [Missing visibility into problem area] Add logs: [Need more context for debugging] Add traces: [Don't understand request flow] Question 3: How much is too much? \u00b6 Metrics: Too few: [Can't understand system health] Too many: [Storage costs, query performance] Sweet spot: [RED/USE for each service, key business metrics] Logs: Too few: [Can't debug issues] Too many: [Storage costs, signal-to-noise ratio] Sweet spot: [WARN+ always, INFO for business events, DEBUG on-demand] Traces: Too few: [Can't understand distributed requests] Too many: [Storage costs, performance impact] Sweet spot: [Sample based on traffic volume (1-10%)] Your Decision Tree \u00b6 Build this after solving practice scenarios: flowchart LR Start[\"Observability Pattern Selection\"] Q1{\"What are you trying to understand?\"} Start --> Q1 N2[\"Metrics<br/>(RED/USE)\"] Q1 -->|\"System health\"| N2 N3[\"Logs + Traces\"] Q1 -->|\"Why something failed\"| N3 N4[\"Traces + Metrics\"] Q1 -->|\"Performance bottleneck\"| N4 N5[\"Metrics + Logs\"] Q1 -->|\"Business analytics\"| N5 Q6{\"What's the cardinality?\"} Start --> Q6 N7[\"Metric labels\"] Q6 -->|\"Low (< 100 unique values)\"| N7 N8[\"Logs with indexing\"] Q6 -->|\"Medium (100-10K)\"| N8 N9[\"Sampling + traces\"] Q6 -->|\"High (> 10K)\"| N9 Q10{\"What's the query pattern?\"} Start --> Q10 N11[\"Metrics\"] Q10 -->|\"Time-series aggregation\"| N11 N12[\"Logs\"] Q10 -->|\"Full-text search\"| N12 N13[\"Traces\"] Q10 -->|\"Causality tracking\"| N13 N14[\"Logs + Traces\"] Q10 -->|\"Ad-hoc exploration\"| N14 Q15{\"What's the retention need?\"} Start --> Q15 N16[\"Metrics<br/>(short retention)\"] Q15 -->|\"Real-time only\"| N16 N17[\"Logs + Traces\"] Q15 -->|\"Debugging (days)\"| N17 N18[\"Logs<br/>(archive)\"] Q15 -->|\"Compliance (years)\"| N18 N19[\"Metrics<br/>(downsampled)\"] Q15 -->|\"Trending (months)\"| N19 Practice \u00b6 Scenario 1: Monitor E-commerce API \u00b6 Requirements: REST API: /checkout, /orders, /products Traffic: 10K requests/sec peak SLO: 99.9% availability, P99 < 500ms Team of 5 engineers on-call rotation Your observability design: Metrics to collect: [What RED metrics for each endpoint?] [What USE metrics for infrastructure?] [What business metrics (orders, revenue)?] Logs to capture: [What should be logged at each level?] [How to add trace IDs to logs?] [What fields in structured logs?] Traces to implement: [Where to start/end spans?] [What sampling rate?] [What tags on spans?] Alerts to configure: [SLO violation alerts?] [Error budget alerts?] [Runbook for each alert?] Scenario 2: Debug Distributed Payment System \u00b6 Context: Payment service calls: auth-service, fraud-service, payment-gateway Users reporting \"payment hangs\" (no error, just slow) Happens for 1% of requests Can't reproduce in staging Your debugging approach: Using traces: [What would you look for first?] [How to find the slow requests?] [How to identify which service is slow?] Using logs: [What log queries would you run?] [How to correlate logs across services?] [What might you be missing?] Using metrics: [What metrics would show the problem?] [How to narrow down the time window?] [What percentiles to examine?] Root cause: How would you prove your hypothesis? [Your answer] What would you change to fix it? [Your answer] What observability would you add? [Your answer] Scenario 3: Capacity Planning for Growth \u00b6 Situation: Current: 1K requests/sec Growth: Expected 10K requests/sec in 6 months Need to plan infrastructure scaling Your analysis approach: Metrics analysis: [What metrics show current capacity?] [How to extrapolate to 10x load?] [What are the bottlenecks?] Load testing: [What metrics to collect during load test?] [How to identify breaking points?] [What percentiles matter most?] Planning: [When will current capacity be exceeded?] [What needs to be scaled (compute, db, cache)?] [What are the cost implications?] Review Checklist \u00b6 Before moving to the next topic: Implementation Counter, Gauge, Histogram work correctly RED and USE metrics implemented Structured logging with JSON output works Distributed tracing with spans works SLO tracking and error budget calculation works Alert rule evaluation works All client code runs successfully Understanding Filled in all ELI5 explanations Understand difference between metrics/logs/traces Know when to use RED vs USE method Understand trace context propagation Know how to calculate error budget Can design alert rules Decision Making Know when to use each observability pattern Understand sampling strategies Can design SLOs for a service Completed practice scenarios Can explain observability trade-offs Mastery Check Could implement metrics collector from memory Could add observability to new service Understand observability costs and trade-offs Know how to debug with observability data Can write runbooks for alerts Mastery Certification \u00b6 I certify that I can: Implement metrics (Counter, Gauge, Histogram) from memory Design structured logging with proper context Implement distributed tracing with span propagation Calculate SLOs and error budgets Choose the right observability tool for each problem Identify and fix high-cardinality issues Debug systems using metrics, logs, and traces Design observability for a new service","title":"14. Observability"},{"location":"systems/14-observability/#observability","text":"Metrics, Logging, Tracing - Understanding what your distributed system is doing","title":"Observability"},{"location":"systems/14-observability/#eli5-explain-like-im-5","text":"Your task: After implementing observability patterns, explain them simply. Prompts to guide you: What is observability in one sentence? Your answer: [Fill in after implementation] What are the three pillars of observability? Your answer: [Fill in after implementation] Real-world analogy for metrics: Example: \"Metrics are like a car's dashboard showing speed, fuel, temperature...\" Your analogy: [Fill in] Real-world analogy for logs: Example: \"Logs are like a detailed diary of everything that happened...\" Your analogy: [Fill in] Real-world analogy for traces: Example: \"Traces are like following a package through the postal system...\" Your analogy: [Fill in] When should you add metrics vs logs vs traces? Your answer: [Fill in after practice]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/14-observability/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/14-observability/#beforeafter-why-this-pattern-matters","text":"Your task: Compare blind systems vs observable systems to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"systems/14-observability/#case-studies-observability-in-the-wild","text":"","title":"Case Studies: Observability in the Wild"},{"location":"systems/14-observability/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/14-observability/#debugging-challenges","text":"Your task: Find and fix bugs in broken observability implementations. This tests your understanding.","title":"Debugging Challenges"},{"location":"systems/14-observability/#decision-framework","text":"Your task: Build decision trees for observability patterns.","title":"Decision Framework"},{"location":"systems/14-observability/#practice","text":"","title":"Practice"},{"location":"systems/14-observability/#review-checklist","text":"Before moving to the next topic: Implementation Counter, Gauge, Histogram work correctly RED and USE metrics implemented Structured logging with JSON output works Distributed tracing with spans works SLO tracking and error budget calculation works Alert rule evaluation works All client code runs successfully Understanding Filled in all ELI5 explanations Understand difference between metrics/logs/traces Know when to use RED vs USE method Understand trace context propagation Know how to calculate error budget Can design alert rules Decision Making Know when to use each observability pattern Understand sampling strategies Can design SLOs for a service Completed practice scenarios Can explain observability trade-offs Mastery Check Could implement metrics collector from memory Could add observability to new service Understand observability costs and trade-offs Know how to debug with observability data Can write runbooks for alerts","title":"Review Checklist"},{"location":"systems/15-distributed-transactions/","text":"Distributed Transactions \u00b6 Maintaining consistency across multiple services and databases ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing distributed transaction patterns, explain them simply. Prompts to guide you: What is a distributed transaction in one sentence? Your answer: [Fill in after implementation] Why are distributed transactions hard? Your answer: [Fill in after implementation] Real-world analogy for 2PC: Example: \"Two-phase commit is like a wedding ceremony where...\" Your analogy: [Fill in] What is the Saga pattern in one sentence? Your answer: [Fill in after implementation] How is orchestration different from choreography? Your answer: [Fill in after implementation] Real-world analogy for Saga choreography: Example: \"Saga choreography is like a relay race where...\" Your analogy: [Fill in] What is compensation in one sentence? Your answer: [Fill in after implementation] When would you use event sourcing? Your answer: [Fill in after implementation] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Two-phase commit with 3 participants: Network round trips: [Your guess: ?] Verified after learning: [Actual: ?] Saga with 5 steps: If 3rd step fails, how many compensations? [Your guess: ?] Verified: [Actual: ?] Consistency guarantees: 2PC provides: [Strong/Eventual consistency?] Saga provides: [Strong/Eventual consistency?] Verified: [Actual answers] Scenario Predictions \u00b6 Scenario 1: Transfer $100 from Account A to Account B across different databases Can you use 2PC? [Yes/No - Why?] What if one database is down during commit phase? [What happens?] Is the transaction atomic? [Yes/No - Explain] Scenario 2: Order processing: Reserve inventory \u2192 Charge payment \u2192 Ship order Can you use 2PC? [Yes/No - Why?] What if payment fails after inventory is reserved? [How to handle?] Should you use orchestration or choreography? [Fill in reasoning] Scenario 3: Payment service charged customer but shipping service is down With 2PC, can this happen? [Yes/No - Why?] With Saga, can this happen? [Yes/No - Why?] How would you recover? [Fill in your approach] Trade-off Quiz \u00b6 Question: When would 2PC be WORSE than Saga for distributed transactions? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN challenge with compensation in Saga? Compensation must be idempotent Compensation can fail Order matters (reverse order) All of the above Verify after implementation: [Which one(s)?] Question: Why does 2PC block while Saga doesn't? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Before/After: Why This Pattern Matters \u00b6 Your task: Compare naive vs optimized approaches to understand the impact. Example: E-Commerce Order Processing \u00b6 Problem: Process an order involving inventory, payment, and shipping across 3 microservices. Approach 1: Naive Distributed Operations (No Transaction Management) \u00b6 // Naive approach - Just call services, hope for the best public class NaiveOrderProcessing { public boolean processOrder(Order order) { // Step 1: Reserve inventory inventoryService.reserve(order.items); // Step 2: Charge payment paymentService.charge(order.customerId, order.amount); // Step 3: Create shipment shippingService.createShipment(order); return true; } } Problems: No rollback: If payment fails, inventory stays reserved forever Partial failures: Customer charged but shipment never created No consistency: Services can be in inconsistent states No retry logic: Transient failures cause permanent data corruption Debugging nightmare: Can't tell which step failed or what state the system is in Real failure scenario: Step 1: Inventory reserved \u2713 Step 2: Payment charged \u2713 Step 3: Shipping service crashes \u2717 Result: Customer charged, inventory locked, no shipment! Approach 2: Two-Phase Commit (Strong Consistency) \u00b6 // 2PC approach - Coordinated commit across all services public class TwoPhaseCommitOrderProcessing { public boolean processOrder(Order order) { String txId = generateTransactionId(); // PHASE 1: PREPARE - Ask all services if they can commit boolean inventoryReady = inventoryService.prepare(txId, order.items); boolean paymentReady = paymentService.prepare(txId, order.customerId, order.amount); boolean shippingReady = shippingService.prepare(txId, order); // PHASE 2: COMMIT or ABORT if (inventoryReady && paymentReady && shippingReady) { // All ready - commit everywhere inventoryService.commit(txId); paymentService.commit(txId); shippingService.commit(txId); return true; } else { // Someone can't commit - abort everywhere inventoryService.abort(txId); paymentService.abort(txId); shippingService.abort(txId); return false; } } } Benefits: Atomic: Either all commit or all abort Strong consistency: No partial states Locks held: Resources reserved during prepare Drawbacks: Blocking: If coordinator crashes, participants locked Latency: 2 network round trips minimum Availability: Any participant down = whole transaction fails Not suitable for: Long-running operations, high-latency networks Approach 3: Saga Pattern (Eventual Consistency) \u00b6 // Saga approach - Sequential steps with compensation public class SagaOrderProcessing { public boolean processOrder(Order order) { List<CompletedStep> completed = new ArrayList<>(); try { // Step 1: Reserve inventory inventoryService.reserve(order.items); completed.add(new CompletedStep(\"inventory\", () -> inventoryService.releaseReservation(order.items))); // Step 2: Charge payment paymentService.charge(order.customerId, order.amount); completed.add(new CompletedStep(\"payment\", () -> paymentService.refund(order.customerId, order.amount))); // Step 3: Create shipment shippingService.createShipment(order); completed.add(new CompletedStep(\"shipping\", () -> shippingService.cancelShipment(order))); return true; } catch (Exception e) { // Compensate in reverse order for (int i = completed.size() - 1; i >= 0; i--) { completed.get(i).compensate(); } return false; } } } Benefits: Non-blocking: No locks held between steps Eventual consistency: System reaches consistent state Resilient: Can handle long-running operations Suitable for: Microservices, distributed systems Drawbacks: Compensation complexity: Must implement reverse operations Intermediate states visible: Brief inconsistency possible Idempotency required: Compensations may retry No isolation: Other transactions may see partial state Performance Comparison \u00b6 Aspect Naive 2PC Saga Consistency None Strong (ACID) Eventual Availability High Low (any down = fail) High Latency 1 round trip 2+ round trips 1 round trip per step Blocking No Yes (locks held) No Partial failures Corrupt data All abort Compensate Complexity Low Medium High Best for Nothing (unsafe) Small, fast operations Long-running, microservices Latency Example (3 services, 50ms network latency each): Naive: ~150ms (3 sequential calls) - but leaves data corrupt on failure 2PC: ~300ms (prepare phase 150ms + commit phase 150ms) Saga: ~150ms (3 sequential calls) + compensation time if needed Real-World Impact \u00b6 Scenario: E-commerce site processing 1000 orders/minute With 2PC: 1 stuck participant locks 100s of transactions Average latency: 300-500ms Failure of any service stops all orders Good for: Financial transfers, small critical updates With Saga: Failures isolated per order Average latency: 150-200ms Partial failures compensated automatically Good for: Order processing, user onboarding, booking systems Your calculation: For 5 microservices with 100ms latency each, 2PC takes approximately _____ ms Saga with same services takes approximately _____ ms If one service has 10% failure rate, 2PC success rate: ___ _% Saga can still complete (with compensation) in: ___ % of cases Why Does Saga Work? \u00b6 Key insight to understand: In a long-running order process with potential failures: Saga: Reserve \u2192 Charge \u2192 Ship (if any fails, undo previous) Why eventual consistency is acceptable: Customer doesn't see intermediate states (happens in seconds) If compensation needed, appears as \"order canceled\" to user Each service independently consistent Auditability: Full history of actions + compensations After implementing, explain in your own words: Why is compensation in reverse order critical? [Your answer] What makes a good compensation operation? [Your answer] When would you need orchestration vs choreography? [Your answer] Case Studies: Distributed Transactions in the Wild \u00b6 E-commerce Order Processing: The Saga Pattern \u00b6 Pattern: Saga (Choreography-based). How it works: When you place an order on a site like Amazon, multiple microservices are involved. This is not a single ACID transaction. Instead, it's a Saga : The Orders Service creates an order and saves it with a PENDING status. It then publishes an OrderCreated event. The Payments Service listens for OrderCreated , processes the payment, and publishes a PaymentSucceeded event. The Inventory Service listens for PaymentSucceeded , decrements the product stock, and publishes InventoryUpdated . Compensation: If the payment fails, the Payments Service publishes PaymentFailed . The Orders Service listens for this and runs a compensating action to cancel the order. Key Takeaway: For long-running business processes that span multiple services, Sagas provide a way to achieve eventual consistency without using slow, blocking distributed locks. The key is defining a compensating action for every step that could fail. Distributed Databases (Google Spanner, CockroachDB): Two-Phase Commit \u00b6 Pattern: Two-Phase Commit (2PC) integrated with a consensus protocol like Paxos or Raft. How it works: Modern distributed SQL databases like Spanner and CockroachDB provide ACID transactions across multiple machines. When you BEGIN TRANSACTION and update records that live on different nodes (shards), the database uses a 2PC-like protocol. A transaction coordinator first asks all participating nodes if they are ready to commit (Phase 1: Prepare). If all nodes agree, the coordinator tells them all to commit (Phase 2: Commit). This ensures the transaction is atomic, even across a global cluster. Key Takeaway: While brittle in traditional applications, 2PC is a core component of modern distributed databases. When tightly integrated with consensus protocols, it provides the strong consistency guarantees that developers expect from a SQL database, even in a distributed environment. Ride-Sharing Apps (Uber, Lyft): Sagas for Trip Management \u00b6 Pattern: Saga (Orchestration-based). How it works: A single ride is a long-running process managed by a Saga orchestrator. Request Ride: The orchestrator calls the MatchingService to find a driver. Driver Found: The orchestrator calls the NotificationService to inform the user. Trip Completed: The driver's app signals the end of the trip. The orchestrator calls the BillingService to calculate the fare and charge the user. Payment Processed: The orchestrator calls the RatingsService to prompt the user and driver for a rating. Key Takeaway: An orchestrator provides a centralized way to manage a complex workflow. It's easier to see the state of the entire process, but it can become a single point of failure and a bottleneck if not designed carefully. This contrasts with the choreography approach where services react to each other's events. Core Implementation \u00b6 Part 1: Two-Phase Commit (2PC) \u00b6 Your task: Implement basic 2PC protocol. import java.util.*; /** * Two-Phase Commit: Atomic commit across multiple participants * * Key principles: * - Phase 1: Prepare (voting) * - Phase 2: Commit/Abort (decision) * - Coordinator manages protocol * - All or nothing semantics */ public class TwoPhaseCommit { private final List<Participant> participants; private final TransactionLog log; /** * Initialize 2PC coordinator * * @param participants List of transaction participants * * TODO: Initialize coordinator * - Store participants * - Create transaction log */ public TwoPhaseCommit(List<Participant> participants) { // TODO: Store participants list // TODO: Create transaction log this.participants = null; // Replace this.log = null; // Replace } /** * Execute distributed transaction * * @param transactionId Transaction identifier * @param operations Operations to execute * @return Transaction result * * TODO: Implement 2PC protocol * Phase 1: Prepare * - Send prepare to all participants * - Collect votes (YES/NO) * - If any NO, abort * Phase 2: Commit/Abort * - If all YES, send commit to all * - If any NO, send abort to all * - Wait for acknowledgments */ public TransactionResult executeTransaction(String transactionId, Map<Participant, String> operations) { // TODO: Log transaction start log.write(\"START \" + transactionId); // PHASE 1: PREPARE System.out.println(\"Phase 1: Prepare\"); // TODO: Send prepare to all participants Map<Participant, Vote> votes = new HashMap<>(); for (Map.Entry<Participant, String> entry : participants) { // TODO: Send prepare request // Vote vote = participant.prepare(transactionId, operation) // Store vote } // TODO: Check if all voted YES boolean allYes = true; // Calculate this // PHASE 2: COMMIT or ABORT if (allYes) { System.out.println(\"Phase 2: Commit\"); // TODO: Send commit to all participants // TODO: Log commit // TODO: Return success } else { System.out.println(\"Phase 2: Abort\"); // TODO: Send abort to all participants // TODO: Log abort // TODO: Return failure } return null; // Replace } /** * Participant in distributed transaction */ static class Participant { String id; Map<String, String> preparedTransactions; // transactionId -> data public Participant(String id) { this.id = id; this.preparedTransactions = new HashMap<>(); } /** * Prepare phase: Can you commit? * * TODO: Prepare transaction * - Check if can commit (resources available, no conflicts) * - If yes, lock resources and save state * - Return YES or NO vote */ public Vote prepare(String transactionId, String operation) { System.out.println(id + \" preparing: \" + operation); // TODO: Check if can commit (simulate) // Simulate: random failure 20% of time if (Math.random() < 0.2) { System.out.println(id + \" votes NO\"); return Vote.NO; } preparedTransactions.put(transactionId, operation); System.out.println(id + \" votes YES\"); return Vote.YES; } /** * Commit phase: Execute the transaction * * TODO: Commit transaction * - Apply prepared changes * - Release locks * - Clean up prepared state */ public void commit(String transactionId) { System.out.println(id + \" committing\"); // TODO: Apply changes // TODO: Clean up prepared state preparedTransactions.remove(transactionId); } /** * Abort phase: Rollback the transaction * * TODO: Abort transaction * - Discard prepared changes * - Release locks * - Clean up prepared state */ public void abort(String transactionId) { System.out.println(id + \" aborting\"); // TODO: Rollback changes // TODO: Clean up prepared state preparedTransactions.remove(transactionId); } } enum Vote { YES, NO } static class TransactionLog { List<String> entries; public TransactionLog() { this.entries = new ArrayList<>(); } public void write(String entry) { entries.add(System.currentTimeMillis() + \": \" + entry); } } static class TransactionResult { boolean success; String message; public TransactionResult(boolean success, String message) { this.success = success; this.message = message; } } } Part 2: Saga Pattern - Orchestration \u00b6 Your task: Implement Saga with centralized orchestrator. /** * Saga Orchestration: Centralized coordinator manages workflow * * Key principles: * - Orchestrator controls flow * - Sequential steps with compensation * - Rollback on failure * - Each step has compensating action */ public class SagaOrchestrator { private final List<SagaStep> steps; /** * Initialize Saga orchestrator * * TODO: Initialize step list */ public SagaOrchestrator() { // TODO: Initialize steps list this.steps = null; // Replace } /** * Add step to saga * * @param step Saga step with transaction and compensation */ public void addStep(SagaStep step) { // TODO: Add step to list } /** * Execute saga * * TODO: Execute all steps sequentially * 1. Execute each step's transaction * 2. If any step fails: * - Execute compensation for completed steps * - Return failure * 3. If all succeed, return success */ public SagaResult execute(SagaContext context) { List<SagaStep> completedSteps = new ArrayList<>(); System.out.println(\"Starting Saga execution\"); // TODO: Execute each step for (SagaStep step : steps) { try { System.out.println(\"Executing: \" + step.getName()); // TODO: Execute step transaction // step.execute(context) // TODO: Add to completed steps } catch (Exception e) { System.out.println(\"Step failed: \" + step.getName()); // TODO: Compensate completed steps in reverse order System.out.println(\"Starting compensation\"); // for (int i = completedSteps.size() - 1; i >= 0; i--): // completedSteps.get(i).compensate(context) // TODO: Return failure return new SagaResult(false, \"Failed at: \" + step.getName()); } } // TODO: All steps succeeded System.out.println(\"Saga completed successfully\"); return new SagaResult(true, \"Success\"); } /** * Saga step with transaction and compensation */ static abstract class SagaStep { String name; public SagaStep(String name) { this.name = name; } public String getName() { return name; } /** * Execute forward transaction */ public abstract void execute(SagaContext context) throws Exception; /** * Execute compensating transaction */ public abstract void compensate(SagaContext context); } /** * Saga execution context (shared state) */ static class SagaContext { Map<String, Object> data; public SagaContext() { this.data = new HashMap<>(); } public void put(String key, Object value) { data.put(key, value); } public Object get(String key) { return data.get(key); } } static class SagaResult { boolean success; String message; public SagaResult(boolean success, String message) { this.success = success; this.message = message; } } } Part 3: Saga Pattern - Choreography \u00b6 Your task: Implement Saga with event-based choreography. /** * Saga Choreography: Event-driven with no central coordinator * * Key principles: * - Services listen for events * - Each service knows next step * - Decentralized control * - Event-driven compensation */ public class SagaChoreography { private final Map<String, List<EventHandler>> eventHandlers; private final EventBus eventBus; /** * Initialize choreography * * TODO: Initialize event system * - Create event bus * - Create handler registry */ public SagaChoreography() { // TODO: Initialize eventHandlers map // TODO: Create event bus this.eventHandlers = null; // Replace this.eventBus = null; // Replace } /** * Register event handler * * @param eventType Event type to listen for * @param handler Handler to execute * * TODO: Register handler for event type */ public void registerHandler(String eventType, EventHandler handler) { // TODO: Get or create handler list for event type // TODO: Add handler to list } /** * Publish event * * TODO: Publish event to all registered handlers * - Get handlers for event type * - Execute each handler * - Handlers may publish new events */ public void publishEvent(Event event) { System.out.println(\"Event published: \" + event.type); // TODO: Get handlers for event type // TODO: Execute each handler } /** * Start saga by publishing initial event */ public void startSaga(Event initialEvent) { // TODO: Publish initial event publishEvent(initialEvent); } /** * Event handler interface */ interface EventHandler { void handle(Event event, EventBus eventBus); } /** * Event bus for publishing events */ static class EventBus { SagaChoreography choreography; public EventBus(SagaChoreography choreography) { this.choreography = choreography; } public void publish(Event event) { choreography.publishEvent(event); } } /** * Event in the saga */ static class Event { String type; Map<String, Object> data; public Event(String type) { this.type = type; this.data = new HashMap<>(); } public void put(String key, Object value) { data.put(key, value); } public Object get(String key) { return data.get(key); } } } Part 4: Compensation Pattern \u00b6 Your task: Implement compensating transactions. /** * Compensation: Undo completed operations on failure * * Key principles: * - Each operation has compensating action * - Compensation executed in reverse order * - Semantic rollback (not physical) * - Eventually consistent */ public class CompensationHandler { private final Stack<CompensatingAction> completedActions; /** * Initialize compensation handler * * TODO: Initialize action stack */ public CompensationHandler() { // TODO: Create stack for completed actions this.completedActions = null; // Replace } /** * Execute action and record for compensation * * @param action Action to execute * @return true if successful * * TODO: Execute and record action * - Try to execute action * - If success, push to stack * - If failure, return false */ public boolean executeWithCompensation(CompensatingAction action) { try { System.out.println(\"Executing: \" + action.getName()); // TODO: Execute action // action.execute() // TODO: Push to stack for potential compensation return true; } catch (Exception e) { System.out.println(\"Action failed: \" + action.getName()); return false; } } /** * Compensate all completed actions * * TODO: Execute compensating actions in reverse order * - Pop actions from stack * - Execute compensation for each * - Handle compensation failures */ public void compensateAll() { System.out.println(\"Starting compensation\"); // TODO: Implement iteration/conditional logic while (!completedActions.isEmpty()) { CompensatingAction action = completedActions.pop(); try { System.out.println(\"Compensating: \" + action.getName()); // TODO: Execute compensation // action.compensate() } catch (Exception e) { System.out.println(\"Compensation failed: \" + action.getName()); // TODO: Log failure but continue compensating } } } /** * Clear compensation stack (after successful completion) */ public void clear() { // TODO: Clear the stack } /** * Action with compensating logic */ static abstract class CompensatingAction { String name; public CompensatingAction(String name) { this.name = name; } public String getName() { return name; } /** * Execute forward action */ public abstract void execute() throws Exception; /** * Execute compensating action */ public abstract void compensate() throws Exception; } } Part 5: Event Sourcing \u00b6 Your task: Implement event sourcing for transaction history. /** * Event Sourcing: Store events instead of current state * * Key principles: * - All changes stored as events * - Current state derived from events * - Complete audit trail * - Time travel (replay to any point) */ public class EventSourcedAggregate { private final String aggregateId; private final List<DomainEvent> events; private int version; /** * Initialize event sourced aggregate * * @param aggregateId Unique identifier * * TODO: Initialize aggregate * - Store aggregate ID * - Create events list * - Set version to 0 */ public EventSourcedAggregate(String aggregateId) { // TODO: Store aggregateId // TODO: Initialize events list // TODO: Track state this.aggregateId = null; // Replace this.events = null; // Replace this.version = 0; } /** * Apply and record event * * @param event Event to apply * * TODO: Apply event * - Add event to list * - Increment version * - Apply state change */ public void applyEvent(DomainEvent event) { // TODO: Set event version // TODO: Add to events list // TODO: Increment version // TODO: Apply state change (handled by subclass) System.out.println(\"Event applied: \" + event); } /** * Replay events to reconstruct state * * @param events Historical events * * TODO: Replay all events * - Clear current state * - Apply each event in order * - Reconstruct current state */ public void replayEvents(List<DomainEvent> events) { System.out.println(\"Replaying \" + events.size() + \" events\"); // TODO: Implement iteration/conditional logic } /** * Get events after specific version * * TODO: Filter events by version */ public List<DomainEvent> getEventsSince(int version) { // TODO: Filter events where event.version > version return null; // Replace } /** * Get all events */ public List<DomainEvent> getAllEvents() { return new ArrayList<>(events); } /** * Get current version */ public int getVersion() { return version; } /** * Domain event */ static class DomainEvent { String aggregateId; String eventType; int version; long timestamp; Map<String, Object> data; public DomainEvent(String aggregateId, String eventType) { this.aggregateId = aggregateId; this.eventType = eventType; this.timestamp = System.currentTimeMillis(); this.data = new HashMap<>(); } public void put(String key, Object value) { data.put(key, value); } public Object get(String key) { return data.get(key); } @Override public String toString() { return \"Event{type='\" + eventType + \"', version=\" + version + \"}\"; } } } Client Code \u00b6 import java.util.*; public class DistributedTransactionsClient { public static void main(String[] args) { testTwoPhaseCommit(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testSagaOrchestration(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testSagaChoreography(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testCompensation(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testEventSourcing(); } static void testTwoPhaseCommit() { System.out.println(\"=== Two-Phase Commit Test ===\\n\"); // Create participants List<TwoPhaseCommit.Participant> participants = Arrays.asList( new TwoPhaseCommit.Participant(\"Database-A\"), new TwoPhaseCommit.Participant(\"Database-B\"), new TwoPhaseCommit.Participant(\"Database-C\") ); TwoPhaseCommit coordinator = new TwoPhaseCommit(participants); // Execute transaction Map<TwoPhaseCommit.Participant, String> operations = new HashMap<>(); for (TwoPhaseCommit.Participant p : participants) { operations.put(p, \"UPDATE balance SET amount = amount - 100\"); } TwoPhaseCommit.TransactionResult result = coordinator.executeTransaction(\"tx123\", operations); System.out.println(\"\\nResult: \" + result.message); } static void testSagaOrchestration() { System.out.println(\"=== Saga Orchestration Test ===\\n\"); SagaOrchestrator saga = new SagaOrchestrator(); // Define saga steps saga.addStep(new SagaOrchestrator.SagaStep(\"Reserve Inventory\") { @Override public void execute(SagaOrchestrator.SagaContext context) throws Exception { System.out.println(\" -> Reserving inventory\"); context.put(\"inventoryReserved\", true); // Simulate occasional failure if (Math.random() < 0.3) { throw new Exception(\"Out of stock\"); } } @Override public void compensate(SagaOrchestrator.SagaContext context) { System.out.println(\" -> Releasing inventory\"); context.put(\"inventoryReserved\", false); } }); saga.addStep(new SagaOrchestrator.SagaStep(\"Process Payment\") { @Override public void execute(SagaOrchestrator.SagaContext context) throws Exception { System.out.println(\" -> Processing payment\"); context.put(\"paymentProcessed\", true); } @Override public void compensate(SagaOrchestrator.SagaContext context) { System.out.println(\" -> Refunding payment\"); context.put(\"paymentProcessed\", false); } }); saga.addStep(new SagaOrchestrator.SagaStep(\"Ship Order\") { @Override public void execute(SagaOrchestrator.SagaContext context) throws Exception { System.out.println(\" -> Shipping order\"); context.put(\"orderShipped\", true); } @Override public void compensate(SagaOrchestrator.SagaContext context) { System.out.println(\" -> Canceling shipment\"); context.put(\"orderShipped\", false); } }); // Execute saga SagaOrchestrator.SagaContext context = new SagaOrchestrator.SagaContext(); SagaOrchestrator.SagaResult result = saga.execute(context); System.out.println(\"\\nResult: \" + result.message); } static void testSagaChoreography() { System.out.println(\"=== Saga Choreography Test ===\\n\"); SagaChoreography choreography = new SagaChoreography(); // Register event handlers choreography.registerHandler(\"OrderCreated\", (event, bus) -> { System.out.println(\" -> Handling OrderCreated\"); System.out.println(\" -> Reserving inventory\"); // Publish next event SagaChoreography.Event inventoryReserved = new SagaChoreography.Event(\"InventoryReserved\"); inventoryReserved.put(\"orderId\", event.get(\"orderId\")); bus.publish(inventoryReserved); }); choreography.registerHandler(\"InventoryReserved\", (event, bus) -> { System.out.println(\" -> Handling InventoryReserved\"); System.out.println(\" -> Processing payment\"); // Publish next event SagaChoreography.Event paymentProcessed = new SagaChoreography.Event(\"PaymentProcessed\"); paymentProcessed.put(\"orderId\", event.get(\"orderId\")); bus.publish(paymentProcessed); }); choreography.registerHandler(\"PaymentProcessed\", (event, bus) -> { System.out.println(\" -> Handling PaymentProcessed\"); System.out.println(\" -> Shipping order\"); SagaChoreography.Event orderShipped = new SagaChoreography.Event(\"OrderShipped\"); orderShipped.put(\"orderId\", event.get(\"orderId\")); System.out.println(\" -> Saga complete!\"); }); // Start saga SagaChoreography.Event orderCreated = new SagaChoreography.Event(\"OrderCreated\"); orderCreated.put(\"orderId\", \"order123\"); choreography.startSaga(orderCreated); } static void testCompensation() { System.out.println(\"=== Compensation Test ===\\n\"); CompensationHandler handler = new CompensationHandler(); // Define compensating actions boolean success = true; success = handler.executeWithCompensation( new CompensationHandler.CompensatingAction(\"Deduct Balance\") { @Override public void execute() throws Exception { System.out.println(\" -> Balance deducted\"); } @Override public void compensate() throws Exception { System.out.println(\" -> Balance restored\"); } } ); if (!success) return; success = handler.executeWithCompensation( new CompensationHandler.CompensatingAction(\"Send Email\") { @Override public void execute() throws Exception { System.out.println(\" -> Email sent\"); // Simulate failure if (Math.random() < 0.5) { throw new Exception(\"Email service down\"); } } @Override public void compensate() throws Exception { System.out.println(\" -> Cancellation email sent\"); } } ); if (!success) { System.out.println(\"\\nOperation failed, compensating...\"); handler.compensateAll(); } else { System.out.println(\"\\nAll operations successful\"); handler.clear(); } } static void testEventSourcing() { System.out.println(\"=== Event Sourcing Test ===\\n\"); EventSourcedAggregate account = new EventSourcedAggregate(\"account123\"); // Apply events System.out.println(\"Applying events:\"); EventSourcedAggregate.DomainEvent created = new EventSourcedAggregate.DomainEvent(\"account123\", \"AccountCreated\"); created.put(\"initialBalance\", 1000); account.applyEvent(created); EventSourcedAggregate.DomainEvent deposited = new EventSourcedAggregate.DomainEvent(\"account123\", \"MoneyDeposited\"); deposited.put(\"amount\", 500); account.applyEvent(deposited); EventSourcedAggregate.DomainEvent withdrawn = new EventSourcedAggregate.DomainEvent(\"account123\", \"MoneyWithdrawn\"); withdrawn.put(\"amount\", 200); account.applyEvent(withdrawn); System.out.println(\"\\nCurrent version: \" + account.getVersion()); System.out.println(\"Total events: \" + account.getAllEvents().size()); // Replay events System.out.println(\"\\nReplaying events:\"); EventSourcedAggregate newAccount = new EventSourcedAggregate(\"account123\"); newAccount.replayEvents(account.getAllEvents()); System.out.println(\"Reconstructed version: \" + newAccount.getVersion()); } } Debugging Challenges \u00b6 Your task: Find and fix bugs in distributed transaction implementations. This tests your understanding of distributed systems pitfalls. Challenge 1: Broken Saga Compensation Order \u00b6 /** * This Saga implementation has a CRITICAL compensation bug. * It will leave the system in an inconsistent state on failure. */ public class BrokenSagaCompensation { public SagaResult executeOrderSaga(Order order) { List<SagaStep> completedSteps = new ArrayList<>(); try { // Step 1: Reserve inventory inventoryService.reserve(order.items); completedSteps.add(new InventoryStep()); // Step 2: Charge payment paymentService.charge(order.amount); completedSteps.add(new PaymentStep()); // Step 3: Create shipment (this might fail) shippingService.createShipment(order); completedSteps.add(new ShippingStep()); return SagaResult.success(); } catch (Exception e) { for (SagaStep step : completedSteps) { step.compensate(); // What's wrong with this? } return SagaResult.failure(); } } } Your debugging: Bug location: [Which line?] Bug explanation: [Why is the compensation order wrong?] Real-world impact: [What could happen?] Fix: [How to correct it?] Scenario to trace: Inventory reserved (step 1) Payment charged (step 2) Shipping fails (step 3) Compensation runs in forward order... What happens if refund depends on shipment being canceled first? Click to verify your answer Bug: Compensating in forward order instead of reverse order (line with for (SagaStep step : completedSteps) ). Why it's wrong: Step 3 depends on step 2, step 2 depends on step 1 Must undo in reverse dependency order Forward compensation can violate business rules Example failure: Forward order: Compensate inventory \u2192 compensate payment \u2192 compensate shipping Problem: Refund issued before shipment canceled (business rule violation) Reverse order: Compensate shipping \u2192 compensate payment \u2192 compensate inventory Correct: Cancel shipment first, then refund, then release inventory Fix: for (int i = completedSteps.size() - 1; i >= 0; i--) { completedSteps.get(i).compensate(); } Challenge 2: Non-Idempotent Compensation \u00b6 /** * This compensation is NOT idempotent - running it twice causes problems! * In distributed systems, compensations may retry due to network failures. */ public class NonIdempotentCompensation extends CompensatingAction { @Override public void execute() throws Exception { // Deduct inventory inventory.reduce(productId, quantity); } @Override public void compensate() throws Exception { inventory.add(productId, quantity); } } Your debugging: Bug: [What happens if compensate() is called twice?] Scenario: [Network timeout causes retry - what's the result?] Impact: [What's wrong with the inventory now?] Fix: [How to make it idempotent?] Test case: Initial inventory: 100 units Transaction reserves: 10 units (inventory now 90) Compensation called first time: adds back 10 (inventory now 100) \u2713 Network timeout, retry... Compensation called second time: adds back 10 again! Current inventory: [Fill in - is this correct?] Click to verify your answer Bug: Compensation adds inventory without checking if it was already compensated. Running twice adds 20 units instead of 10. Impact: Inventory count becomes incorrect (ghost inventory) Overselling possible Accounting mismatch Fix - Make it idempotent: @Override public void compensate() throws Exception { // Use idempotency key String compensationId = transactionId + \"-inventory-compensate\"; if (compensationLog.isAlreadyProcessed(compensationId)) { System.out.println(\"Already compensated, skipping\"); return; } inventory.add(productId, quantity); compensationLog.markProcessed(compensationId); } Alternative fix - Use state machine: if (reservation.status == COMPENSATED) { return; // Already done } inventory.add(productId, quantity); reservation.status = COMPENSATED; Challenge 3: Orphaned Saga (Lost Coordinator) \u00b6 /** * The saga coordinator crashes after step 2 completes. * Steps 1 and 2 are done, but step 3 never executes. * How do you detect and recover from this? */ public class OrphanedSaga { public void executeSaga(String sagaId) { sagaLog.write(sagaId, \"STARTED\"); // Step 1: Reserve inventory inventoryService.reserve(sagaId, items); sagaLog.write(sagaId, \"INVENTORY_RESERVED\"); // Step 2: Charge payment paymentService.charge(sagaId, amount); sagaLog.write(sagaId, \"PAYMENT_CHARGED\"); // CRASH HERE! Coordinator dies before step 3 // Step 3 never executes, saga never completes // Step 3: Create shipment shippingService.createShipment(sagaId, order); sagaLog.write(sagaId, \"COMPLETED\"); } } Your debugging: Problem: [What's the system state after crash?] Detection: [How do you detect this orphaned saga?] Recovery strategy: [Should you complete it or compensate?] Prevention: [How to prevent this?] Scenario: Saga started at 10:00:00 Step 1 completed at 10:00:01 Step 2 completed at 10:00:02 Coordinator crashed at 10:00:02.5 Current time: 10:05:00 (5 minutes later) Saga status: [What does the log show?] Customer charged: [Yes/No?] Order shipped: [Yes/No?] Click to verify your answer Problem: Saga is stuck in partial state - inventory reserved, payment charged, but never shipped. Detection strategies: Timeout monitoring: // Background job finds incomplete sagas List<Saga> stuck = sagaLog.findSagasOlderThan(5 minutes, status != COMPLETED); for (Saga saga : stuck) { recoverOrCompensate(saga); } Health check pattern: // Coordinator sends heartbeats coordinator.recordHeartbeat(sagaId, timestamp); // Monitor detects missing heartbeats if (timeSinceLastHeartbeat > threshold) { recoverSaga(sagaId); } Recovery strategy: // Check saga log to determine recovery action SagaState state = sagaLog.getCurrentState(sagaId); if (state == \"PAYMENT_CHARGED\") { // Decision point: complete or compensate? // Option 1: Complete the saga (forward recovery) shippingService.createShipment(sagaId, order); sagaLog.write(sagaId, \"COMPLETED\"); // Option 2: Compensate (backward recovery) paymentService.refund(sagaId); inventoryService.release(sagaId); sagaLog.write(sagaId, \"COMPENSATED\"); } Prevention: Persist saga state before each step Use durable message queue for step execution Implement saga recovery service Set timeouts for each step Challenge 4: Compensation Failure \u00b6 /** * What happens when compensation itself fails? * This is one of the hardest distributed transaction problems! */ public class CompensationFailure { public void executeSagaWithFailingCompensation() { try { // Step 1: Charge payment - succeeds paymentService.charge(customerId, 100); // Step 2: Reserve inventory - FAILS inventoryService.reserve(items); throw new Exception(\"Out of stock\"); } catch (Exception e) { // Try to compensate step 1 try { paymentService.refund(customerId, 100); throw new Exception(\"Payment gateway timeout\"); } catch (Exception compensationError) { // NOW WHAT? Customer charged, no order, refund failed! // Your code here: <span class=\"fill-in\">[How do you handle this?]</span> } } } } Your debugging: Problem: [What's the current state?] Can you retry? [What if retry also fails?] Manual intervention? [How to flag for human review?] Customer impact: [What does the customer see?] Failure tree: Transaction: Charge $100 \u2192 Reserve inventory \u251c\u2500 Charge succeeds \u2713 \u251c\u2500 Reserve fails \u2717 \u2514\u2500 Compensation: Refund $100 \u2514\u2500 Refund FAILS \u2717 (payment gateway down) Current state: - Customer charged: $100 \u2713 - Inventory reserved: No \u2717 - Refund processed: No \u2717 - System state: INCONSISTENT! Click to verify your answer This is a real problem with no perfect solution. Here are the strategies: Strategy 1: Retry with exponential backoff catch (Exception compensationError) { // Add to retry queue retryQueue.add(new CompensationRetry( sagaId, \"refund\", maxRetries: 10, backoff: EXPONENTIAL )); // Alert monitoring alerting.criticalError(\"Compensation failed for \" + sagaId); } Strategy 2: Dead letter queue + manual intervention catch (Exception compensationError) { // Move to dead letter queue after max retries if (retryCount > MAX_RETRIES) { deadLetterQueue.add(new FailedCompensation( sagaId, customerId, amount: 100, reason: compensationError.getMessage() )); // Create support ticket ticketSystem.create( priority: HIGH, title: \"Manual refund needed\", details: \"Customer \" + customerId + \" needs $100 refund\" ); } } Strategy 3: Eventual consistency with monitoring catch (Exception compensationError) { // Mark saga as \"COMPENSATION_PENDING\" sagaLog.write(sagaId, \"COMPENSATION_PENDING\", { action: \"refund\", amount: 100, customerId: customerId, failureReason: compensationError.getMessage(), retryAfter: now + 5.minutes }); // Background job will retry // Dashboard shows pending compensations // Alert if pending > 1 hour } Key principle: Compensation failures require: Persistent retry mechanism Monitoring and alerting Manual intervention workflow Clear audit trail Customer communication plan There is no automatic fix for this - it's a fundamental distributed systems problem! Challenge 5: Partial Failure in 2PC Commit Phase \u00b6 /** * In 2PC, what happens if coordinator crashes DURING commit phase? * Some participants committed, others still waiting! */ public class TwoPhaseCommitPartialFailure { public void executeTransaction(String txId) { // PHASE 1: PREPARE - All vote YES List<Vote> votes = new ArrayList<>(); for (Participant p : participants) { votes.add(p.prepare(txId)); // All return YES } // Decision: COMMIT transactionLog.write(txId, \"COMMIT_DECISION\"); // PHASE 2: Send commit to all participants.get(0).commit(txId); // \u2713 Committed participants.get(1).commit(txId); // \u2713 Committed // CRASH HERE! Coordinator dies // participants.get(2).commit(txId); // Never called! // participants.get(3).commit(txId); // Never called! // NOW: 2 participants committed, 2 still waiting! // What happens to the waiting participants? } } Your debugging: Problem: [What state are the waiting participants in?] Blocking: [Are they holding locks?] Recovery: [How do they know to commit or abort?] Data consistency: [Is data consistent across participants?] Participant states: Participant 0: COMMITTED \u2713 Participant 1: COMMITTED \u2713 Participant 2: PREPARED (waiting...) \u23f3 Participant 3: PREPARED (waiting...) \u23f3 Time passes... Participant 2: Still holding locks on resources! Participant 3: Still holding locks on resources! Other transactions: BLOCKED waiting for locks! Click to verify your answer Problem: This is the classic \"blocking\" problem of 2PC! What happens: Participants 2 & 3 are in PREPARED state They're holding locks, waiting for commit/abort Coordinator is dead, can't send decision They can't commit on their own (might not be safe) They can't abort on their own (others might have committed) They're STUCK! Recovery using transaction log: // Participant timeout handler if (waitingTime > TIMEOUT) { // Ask coordinator for decision Decision decision = coordinator.getDecision(txId); if (decision == COMMIT) { this.commit(txId); } else if (decision == ABORT) { this.abort(txId); } else { // Coordinator unreachable - contact other participants Decision consensus = askOtherParticipants(txId); if (consensus == COMMITTED) { // Someone committed - we must commit too this.commit(txId); } else if (consensus == ALL_PREPARED) { // Everyone waiting - check coordinator log Decision loggedDecision = coordinatorLog.read(txId); if (loggedDecision == COMMIT) { this.commit(txId); } } } } Why coordinator log is critical: // Coordinator MUST log decision BEFORE sending commits transactionLog.write(txId, \"COMMIT_DECISION\"); // Durable write! // Now even if coordinator crashes, recovery can read log // and complete the transaction This is why 2PC is considered \"blocking\": Participants can be stuck if coordinator fails Requires timeout + recovery protocol Modern systems prefer non-blocking alternatives (3PC, Paxos, Raft) Prevention: Use coordinator replicas (HA) Implement participant timeout and recovery Use 3-phase commit (reduces blocking window) Consider Saga pattern instead (no blocking) Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Understood why compensation order matters Can implement idempotent operations Know how to detect and recover orphaned sagas Understand compensation failure handling Recognize 2PC blocking scenarios Can design recovery mechanisms Common distributed transaction bugs you discovered: [List the patterns you noticed] [Fill in] [Fill in] Real-world war stories (fill in after implementation): Most surprising bug: [Fill in] Hardest to debug: [Fill in] Most dangerous if missed: [Fill in] Decision Framework \u00b6 Questions to answer after implementation: 1. Pattern Selection \u00b6 When to use Two-Phase Commit? Your scenario: [Fill in] Key factors: [Fill in] When to use Saga (Orchestration)? Your scenario: [Fill in] Key factors: [Fill in] When to use Saga (Choreography)? Your scenario: [Fill in] Key factors: [Fill in] When to use Event Sourcing? Your scenario: [Fill in] Key factors: [Fill in] 2. Trade-offs \u00b6 Two-Phase Commit: Pros: [Fill in after understanding] Cons: [Fill in after understanding] Saga (Orchestration): Pros: [Fill in after understanding] Cons: [Fill in after understanding] Saga (Choreography): Pros: [Fill in after understanding] Cons: [Fill in after understanding] Event Sourcing: Pros: [Fill in after understanding] Cons: [Fill in after understanding] 3. Your Decision Tree \u00b6 Build your decision tree after practicing: flowchart LR Start[\"What consistency do you need?\"] N1[\"?\"] Start -->|\"Strong consistency (ACID)\"| N1 N2[\"?\"] Start -->|\"Eventual consistency acceptable\"| N2 N3[\"?\"] Start -->|\"Long-running transactions\"| N3 N4[\"?\"] Start -->|\"Need audit trail\"| N4 N5[\"?\"] Start -->|\"Highly distributed services\"| N5 Practice \u00b6 Scenario 1: Banking transfer \u00b6 Requirements: Transfer money between accounts Accounts in different databases Must be atomic (all or nothing) Low latency required Rare failures acceptable Your design: Which pattern would you choose? [Fill in] Why? [Fill in] How to handle failures? [Fill in] Consistency guarantees? [Fill in] Scenario 2: E-commerce order \u00b6 Requirements: Order involves: inventory, payment, shipping Each service is independent Long-running process (minutes) Need to handle partial failures Must be eventually consistent Your design: Which pattern would you choose? [Fill in] Why? [Fill in] Compensation strategy? [Fill in] How to monitor progress? [Fill in] Scenario 3: Account audit system \u00b6 Requirements: Need complete history of all changes Regulatory compliance Ability to replay transactions Time-based queries High write volume Your design: Which pattern would you choose? [Fill in] Why? [Fill in] Storage strategy? [Fill in] Query optimization? [Fill in] Review Checklist \u00b6 Two-phase commit implemented with prepare and commit phases Saga orchestration implemented with central coordinator Saga choreography implemented with event-driven flow Compensation handler implemented for rollback Event sourcing implemented with event replay Understand when to use each pattern Can explain trade-offs between patterns Built decision tree for pattern selection Completed practice scenarios Mastery Certification \u00b6 I certify that I can: Explain 2PC, Saga (both types), and Event Sourcing Identify which pattern to use for different scenarios Implement basic Saga orchestrator from memory Design compensation strategies Handle failure scenarios (coordinator crash, participant failure, compensation failure) Implement idempotent operations Understand CAP theorem trade-offs Debug distributed transaction issues Teach these concepts to someone else","title":"15. Distributed Transactions"},{"location":"systems/15-distributed-transactions/#distributed-transactions","text":"Maintaining consistency across multiple services and databases","title":"Distributed Transactions"},{"location":"systems/15-distributed-transactions/#eli5-explain-like-im-5","text":"Your task: After implementing distributed transaction patterns, explain them simply. Prompts to guide you: What is a distributed transaction in one sentence? Your answer: [Fill in after implementation] Why are distributed transactions hard? Your answer: [Fill in after implementation] Real-world analogy for 2PC: Example: \"Two-phase commit is like a wedding ceremony where...\" Your analogy: [Fill in] What is the Saga pattern in one sentence? Your answer: [Fill in after implementation] How is orchestration different from choreography? Your answer: [Fill in after implementation] Real-world analogy for Saga choreography: Example: \"Saga choreography is like a relay race where...\" Your analogy: [Fill in] What is compensation in one sentence? Your answer: [Fill in after implementation] When would you use event sourcing? Your answer: [Fill in after implementation]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/15-distributed-transactions/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/15-distributed-transactions/#beforeafter-why-this-pattern-matters","text":"Your task: Compare naive vs optimized approaches to understand the impact.","title":"Before/After: Why This Pattern Matters"},{"location":"systems/15-distributed-transactions/#case-studies-distributed-transactions-in-the-wild","text":"","title":"Case Studies: Distributed Transactions in the Wild"},{"location":"systems/15-distributed-transactions/#core-implementation","text":"","title":"Core Implementation"},{"location":"systems/15-distributed-transactions/#client-code","text":"import java.util.*; public class DistributedTransactionsClient { public static void main(String[] args) { testTwoPhaseCommit(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testSagaOrchestration(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testSagaChoreography(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testCompensation(); System.out.println(\"\\n\" + \"=\".repeat(50) + \"\\n\"); testEventSourcing(); } static void testTwoPhaseCommit() { System.out.println(\"=== Two-Phase Commit Test ===\\n\"); // Create participants List<TwoPhaseCommit.Participant> participants = Arrays.asList( new TwoPhaseCommit.Participant(\"Database-A\"), new TwoPhaseCommit.Participant(\"Database-B\"), new TwoPhaseCommit.Participant(\"Database-C\") ); TwoPhaseCommit coordinator = new TwoPhaseCommit(participants); // Execute transaction Map<TwoPhaseCommit.Participant, String> operations = new HashMap<>(); for (TwoPhaseCommit.Participant p : participants) { operations.put(p, \"UPDATE balance SET amount = amount - 100\"); } TwoPhaseCommit.TransactionResult result = coordinator.executeTransaction(\"tx123\", operations); System.out.println(\"\\nResult: \" + result.message); } static void testSagaOrchestration() { System.out.println(\"=== Saga Orchestration Test ===\\n\"); SagaOrchestrator saga = new SagaOrchestrator(); // Define saga steps saga.addStep(new SagaOrchestrator.SagaStep(\"Reserve Inventory\") { @Override public void execute(SagaOrchestrator.SagaContext context) throws Exception { System.out.println(\" -> Reserving inventory\"); context.put(\"inventoryReserved\", true); // Simulate occasional failure if (Math.random() < 0.3) { throw new Exception(\"Out of stock\"); } } @Override public void compensate(SagaOrchestrator.SagaContext context) { System.out.println(\" -> Releasing inventory\"); context.put(\"inventoryReserved\", false); } }); saga.addStep(new SagaOrchestrator.SagaStep(\"Process Payment\") { @Override public void execute(SagaOrchestrator.SagaContext context) throws Exception { System.out.println(\" -> Processing payment\"); context.put(\"paymentProcessed\", true); } @Override public void compensate(SagaOrchestrator.SagaContext context) { System.out.println(\" -> Refunding payment\"); context.put(\"paymentProcessed\", false); } }); saga.addStep(new SagaOrchestrator.SagaStep(\"Ship Order\") { @Override public void execute(SagaOrchestrator.SagaContext context) throws Exception { System.out.println(\" -> Shipping order\"); context.put(\"orderShipped\", true); } @Override public void compensate(SagaOrchestrator.SagaContext context) { System.out.println(\" -> Canceling shipment\"); context.put(\"orderShipped\", false); } }); // Execute saga SagaOrchestrator.SagaContext context = new SagaOrchestrator.SagaContext(); SagaOrchestrator.SagaResult result = saga.execute(context); System.out.println(\"\\nResult: \" + result.message); } static void testSagaChoreography() { System.out.println(\"=== Saga Choreography Test ===\\n\"); SagaChoreography choreography = new SagaChoreography(); // Register event handlers choreography.registerHandler(\"OrderCreated\", (event, bus) -> { System.out.println(\" -> Handling OrderCreated\"); System.out.println(\" -> Reserving inventory\"); // Publish next event SagaChoreography.Event inventoryReserved = new SagaChoreography.Event(\"InventoryReserved\"); inventoryReserved.put(\"orderId\", event.get(\"orderId\")); bus.publish(inventoryReserved); }); choreography.registerHandler(\"InventoryReserved\", (event, bus) -> { System.out.println(\" -> Handling InventoryReserved\"); System.out.println(\" -> Processing payment\"); // Publish next event SagaChoreography.Event paymentProcessed = new SagaChoreography.Event(\"PaymentProcessed\"); paymentProcessed.put(\"orderId\", event.get(\"orderId\")); bus.publish(paymentProcessed); }); choreography.registerHandler(\"PaymentProcessed\", (event, bus) -> { System.out.println(\" -> Handling PaymentProcessed\"); System.out.println(\" -> Shipping order\"); SagaChoreography.Event orderShipped = new SagaChoreography.Event(\"OrderShipped\"); orderShipped.put(\"orderId\", event.get(\"orderId\")); System.out.println(\" -> Saga complete!\"); }); // Start saga SagaChoreography.Event orderCreated = new SagaChoreography.Event(\"OrderCreated\"); orderCreated.put(\"orderId\", \"order123\"); choreography.startSaga(orderCreated); } static void testCompensation() { System.out.println(\"=== Compensation Test ===\\n\"); CompensationHandler handler = new CompensationHandler(); // Define compensating actions boolean success = true; success = handler.executeWithCompensation( new CompensationHandler.CompensatingAction(\"Deduct Balance\") { @Override public void execute() throws Exception { System.out.println(\" -> Balance deducted\"); } @Override public void compensate() throws Exception { System.out.println(\" -> Balance restored\"); } } ); if (!success) return; success = handler.executeWithCompensation( new CompensationHandler.CompensatingAction(\"Send Email\") { @Override public void execute() throws Exception { System.out.println(\" -> Email sent\"); // Simulate failure if (Math.random() < 0.5) { throw new Exception(\"Email service down\"); } } @Override public void compensate() throws Exception { System.out.println(\" -> Cancellation email sent\"); } } ); if (!success) { System.out.println(\"\\nOperation failed, compensating...\"); handler.compensateAll(); } else { System.out.println(\"\\nAll operations successful\"); handler.clear(); } } static void testEventSourcing() { System.out.println(\"=== Event Sourcing Test ===\\n\"); EventSourcedAggregate account = new EventSourcedAggregate(\"account123\"); // Apply events System.out.println(\"Applying events:\"); EventSourcedAggregate.DomainEvent created = new EventSourcedAggregate.DomainEvent(\"account123\", \"AccountCreated\"); created.put(\"initialBalance\", 1000); account.applyEvent(created); EventSourcedAggregate.DomainEvent deposited = new EventSourcedAggregate.DomainEvent(\"account123\", \"MoneyDeposited\"); deposited.put(\"amount\", 500); account.applyEvent(deposited); EventSourcedAggregate.DomainEvent withdrawn = new EventSourcedAggregate.DomainEvent(\"account123\", \"MoneyWithdrawn\"); withdrawn.put(\"amount\", 200); account.applyEvent(withdrawn); System.out.println(\"\\nCurrent version: \" + account.getVersion()); System.out.println(\"Total events: \" + account.getAllEvents().size()); // Replay events System.out.println(\"\\nReplaying events:\"); EventSourcedAggregate newAccount = new EventSourcedAggregate(\"account123\"); newAccount.replayEvents(account.getAllEvents()); System.out.println(\"Reconstructed version: \" + newAccount.getVersion()); } }","title":"Client Code"},{"location":"systems/15-distributed-transactions/#debugging-challenges","text":"Your task: Find and fix bugs in distributed transaction implementations. This tests your understanding of distributed systems pitfalls.","title":"Debugging Challenges"},{"location":"systems/15-distributed-transactions/#decision-framework","text":"Questions to answer after implementation:","title":"Decision Framework"},{"location":"systems/15-distributed-transactions/#practice","text":"","title":"Practice"},{"location":"systems/15-distributed-transactions/#review-checklist","text":"Two-phase commit implemented with prepare and commit phases Saga orchestration implemented with central coordinator Saga choreography implemented with event-driven flow Compensation handler implemented for rollback Event sourcing implemented with event replay Understand when to use each pattern Can explain trade-offs between patterns Built decision tree for pattern selection Completed practice scenarios","title":"Review Checklist"},{"location":"systems/16-consensus-patterns/","text":"Consensus Patterns \u00b6 Leader election, Raft consensus, distributed locks, and quorum-based systems ELI5: Explain Like I'm 5 \u00b6 Your task: After implementing consensus patterns, explain them simply. Prompts to guide you: What is consensus in one sentence? Your answer: [Fill in after implementation] Why do distributed systems need consensus? Your answer: [Fill in after implementation] Real-world analogy for leader election: Example: \"Leader election is like choosing a class president where...\" Your analogy: [Fill in] What is the split-brain problem in one sentence? Your answer: [Fill in after implementation] Real-world analogy for distributed locks: Example: \"A distributed lock is like a bathroom key that...\" Your analogy: [Fill in] Why do we need quorums? Your answer: [Fill in after practice] Quick Quiz (Do BEFORE implementing) \u00b6 Your task: Test your intuition about distributed consensus without looking at code. Answer these, then verify after implementation. Complexity Predictions \u00b6 Leader election with N nodes (Bully algorithm): Time complexity: [Your guess: O(?)] Message complexity: [How many messages?] Verified after learning: [Actual: O(?)] Raft log replication to majority of N nodes: Time complexity: [Your guess: O(?)] When is an entry committed: [Your guess] Verified: [Actual] Quorum read with R nodes, N total nodes: Time complexity: [Your guess: O(?)] Space complexity per node: [Your guess: O(?)] Verified: [Actual] Scenario Predictions \u00b6 Scenario 1: 5-node cluster, leader fails during log replication What happens to uncommitted entries? [Lost/Preserved - Why?] How long until new leader elected? [Depends on what?] Can clients write during election? [Yes/No - Why?] Scenario 2: Network partition splits 5 nodes into {3 nodes, 2 nodes} Which partition can elect a leader? [3-node/2-node/Both - Why?] What happens to writes in minority partition? [Fill in] Is this a split-brain scenario? [Yes/No - Why?] Scenario 3: Distributed lock with 30-second TTL, holder crashes after 10 seconds When can another process acquire the lock? [Immediately/After 20s/Never] Why that timing? [Fill in your reasoning] What could go wrong? [Fill in] Trade-off Quiz \u00b6 Question: When would leaderless (quorum) be BETTER than Raft for consensus? Your answer: [Fill in before implementation] Verified answer: [Fill in after learning] Question: What's the MAIN requirement for achieving strong consistency with quorums? R + W > N (where N is replication factor) R + W = N R = W = majority R = N, W = 1 Verify after implementation: [Which one(s)? Why?] Question: Why use fencing tokens with distributed locks? Your answer: [Fill in before implementation] Verified answer: [Fill in after implementing Pattern 3] Before/After: Why Consensus Matters \u00b6 Your task: Compare naive distributed coordination vs proper consensus to understand the impact. Example: Leader Election Without Consensus \u00b6 Problem: Multiple nodes need to coordinate on a single leader for a distributed database. Approach 1: Naive Leader Election (No Consensus) \u00b6 // Naive approach - Highest ID claims leadership public class NaiveLeaderElection { private int myId; private int leaderId; public void electLeader(Set<Integer> visibleNodes) { // Just pick the highest ID we can see int maxId = myId; for (int nodeId : visibleNodes) { if (nodeId > maxId) { maxId = nodeId; } } leaderId = maxId; if (leaderId == myId) { System.out.println(\"I am the leader!\"); } } } What goes wrong: Network Partition Scenario Before partition: Cluster: [Node 1, Node 2, Node 3, Node 4, Node 5] Leader: Node 5 (highest ID) After network partition: Partition A: [Node 1, Node 2, Node 3] Partition B: [Node 4, Node 5] Partition A thinks: Node 3 is leader (highest visible) Partition B thinks: Node 5 is leader (highest visible) SPLIT-BRAIN: Two leaders accepting writes simultaneously! Result: - Data divergence (inconsistent state) - Lost updates when partition heals - Violated uniqueness guarantee Analysis: Time: O(N) to scan visible nodes Space: O(1) Problem: No consensus, split-brain during partition! Failure rate: ~50% in networks with partitions Approach 2: Raft Consensus (Safe Leader Election) \u00b6 // Raft approach - Majority vote required public class RaftLeaderElection { private int currentTerm; private int votedFor; private int myId; public boolean electLeader(Set<Integer> allNodes) { currentTerm++; votedFor = myId; int votesReceived = 1; // Vote for self int majoritySize = (allNodes.size() / 2) + 1; // Request votes from all nodes for (int nodeId : allNodes) { if (nodeId != myId && requestVote(nodeId, currentTerm)) { votesReceived++; } } // Only become leader if MAJORITY votes received if (votesReceived >= majoritySize) { System.out.println(\"I am leader with \" + votesReceived + \" votes\"); return true; } return false; } } Same network partition with Raft: After network partition: Partition A: [Node 1, Node 2, Node 3] - 3 nodes, majority = 2 Partition B: [Node 4, Node 5] - 2 nodes, majority = 2 Partition A attempts election: - Node 3 requests votes from Node 1, Node 2 (both visible) - Node 3 gets 3 votes total \u2192 SUCCESS (3 \u2265 2 majority) - Node 3 becomes leader \u2713 Partition B attempts election: - Node 5 requests votes from Node 4 (only visible node) - Node 5 gets 2 votes total \u2192 FAIL (2 < 3 majority of 5 total) - No leader elected \u2717 Result: - Only ONE leader (Node 3) - Partition B cannot accept writes (no leader) - Partition A continues operating safely - No split-brain! \u2713 - When partition heals, Node 5 recognizes Node 3 as leader Analysis: Time: O(N) to request votes Space: O(1) Safety: Prevents split-brain through majority requirement Availability: Minority partition cannot elect leader (trade-off for safety) Performance Comparison: Failure Scenarios \u00b6 Scenario Naive Election Raft Consensus Network partition Split-brain (2 leaders) Single leader in majority Node failure Immediate re-election Election only if leader fails Data consistency Violated during partition Preserved (CP in CAP) Write availability Both partitions accept Only majority partition Why Does Raft Work? \u00b6 Key insight: The Majority Principle With 5 nodes, majority = 3: Any two majorities must overlap by at least 1 node That overlapping node prevents conflicting decisions Example: {Node 1, 2, 3} and {Node 3, 4, 5} both contain Node 3 Election term visualization: Term 1: Node 5 is leader (got votes from 1, 3, 5) Network partition occurs Term 2: Node 3 attempts election - Gets votes from 1, 2, 3 (majority) \u2192 SUCCESS - Node 5 in minority cannot get majority \u2192 FAILS Term 3: When partition heals, Node 5 sees Node 3 has higher term \u2192 steps down After implementing, explain in your own words: Why does majority prevent split-brain? [Your answer] What's the trade-off between safety and availability? [Your answer] Why can't the minority partition elect a leader? [Your answer] Real-World Impact \u00b6 Without consensus (naive approach): Google Cloud DNS split-brain (2015): Traffic routed to wrong servers MongoDB 2.4 split-brain: Accepted conflicting writes, data corruption Recovery time: Hours to manually resolve conflicts With consensus (Raft/Paxos): etcd (Kubernetes): Thousands of clusters, zero split-brain incidents Consul: Service discovery with guaranteed consistency Recovery time: Seconds (automatic election) Your calculation: For a 7-node cluster with network partition into {4, 3}: Naive approach: _____ leaders elected (how many?) Raft consensus: _____ leader(s) elected (in which partition?) Which partition can serve writes: _____ Case Studies: Consensus in the Wild \u00b6 Kubernetes: Cluster Coordination with etcd (Raft) \u00b6 Pattern: Raft for consistent state replication. How it works: Kubernetes, the container orchestration system, needs to reliably store the state of the entire cluster: which nodes are active, what pods should be running, what secrets are available, etc. It uses etcd , a distributed key-value store, for this. etcd forms a small cluster (typically 3 or 5 nodes) and uses the Raft consensus algorithm to ensure that all nodes have a consistent, replicated log of all changes. The Raft leader receives all writes, and a write is only considered \"committed\" when it has been replicated to a majority of the nodes. Key Takeaway: Raft provides the safety and consistency needed for critical infrastructure components. By requiring a majority quorum for all decisions, it can tolerate node failures while preventing split-brain scenarios and maintaining a consistent view of the system state. Google's Chubby: Distributed Locking with Paxos \u00b6 Pattern: Paxos for distributed locking and leader election. How it works: Inside Google, many distributed systems need to elect a single primary or \"leader\" from a group of identical replicas. They use a service called Chubby . A group of service replicas will attempt to acquire an exclusive lock in Chubby. The one that succeeds becomes the leader. All other replicas become standbys, watching the lock. If the leader crashes and its session with Chubby expires, the lock is released, and the standby replicas are notified so they can attempt to acquire the lock and elect a new leader. Key Takeaway: Consensus algorithms provide the foundation for reliable leader election, a fundamental pattern in distributed systems. By using a consistent lock service, systems can ensure that there is only one active leader at any given time, preventing data corruption and split-brain issues. Apache Kafka: Cluster Management with ZooKeeper \u00b6 Pattern: Consensus for metadata management and leader election. How it works: Apache Kafka uses Apache ZooKeeper (which uses a Paxos-like protocol called Zab) to manage the state of the Kafka cluster. ZooKeeper is responsible for tracking which brokers are alive, which broker is the \" controller\" (the leader for the whole cluster), the configuration of all topics, and which replica is the leader for each topic partition. If a broker fails, the controller (elected via ZooKeeper) is responsible for electing new partition leaders from the available replicas. Key Takeaway: Many distributed data systems (like Kafka, Hadoop, and HBase) delegate the complex task of consensus to a dedicated coordination service like ZooKeeper. This separates the concern of data processing from the difficult problem of managing distributed state and leader election. Core Concepts \u00b6 Pattern 1: Leader Election \u00b6 Concept: Distributed algorithm to elect a single leader node from a cluster of nodes, ensuring only one leader exists at any time. Use case: Distributed databases, coordination services, master-worker systems. Key Properties: Safety : At most one leader at any time Liveness : Eventually a leader is elected if majority is available Agreement : All nodes agree on who the leader is Common Algorithms: 1. Bully Algorithm Highest ID node becomes leader Node contacts all higher-ID nodes; if no response, declares itself leader Simple but can cause message storms Time: O(N\u00b2) messages in worst case 2. Ring Algorithm Nodes organized in logical ring Election message passes around ring collecting IDs Node with highest ID becomes leader Time: O(N) messages, but slower latency 3. Raft Leader Election (see Pattern 2) Term-based elections with majority voting Prevents split-brain through quorum Production-ready (etcd, Consul) Simplified Example: // High-level API - implementation details abstracted public interface LeaderElection { // Start election process void startElection(int nodeId); // Get current leader (or -1 if none) int getLeader(); // Check if this node is the leader boolean isLeader(int nodeId); // Detect leader failure via heartbeat timeout void checkLeaderHealth(int nodeId); } // Typical usage LeaderElection election = new BullyAlgorithm(nodeIds, heartbeatTimeout); election.startElection(myNodeId); if (election.isLeader(myNodeId)) { // I'm the leader, handle writes handleWrites(); } else { // I'm a follower, forward to leader forwardToLeader(election.getLeader()); } Failure Handling: Initial state: Node 5 is leader Node 5 fails (heartbeat timeout) Node 4 detects failure \u2192 starts election Node 4 sends election messages to Node 5 (no response) Node 4 declares itself leader Node 4 broadcasts victory to all nodes New state: Node 4 is leader Pattern 2: Raft Consensus Algorithm \u00b6 Concept: Consensus algorithm that ensures replicated log consistency across distributed nodes through leader election and log replication. Use case: Distributed databases (etcd, Consul), replicated state machines, configuration management. Key Components: Leader Election with Terms Each election cycle has a term number Candidate requests votes from all nodes Requires majority to become leader Prevents split-brain through quorum Log Replication (AppendEntries RPC) Leader appends entries to local log Replicates to followers Commits when majority acknowledges Guarantees: committed entries never lost Safety Properties Election Safety : At most one leader per term Leader Append-Only : Leader never overwrites/deletes entries Log Matching : If two logs contain entry with same index/term, all preceding entries are identical Leader Completeness : If entry committed in term T, it will be present in leaders of all future terms State Machine Safety : If a server applies log entry at index i, no other server will apply different entry at i How Raft Works: Phase 1: Leader Election - Follower timeout \u2192 becomes Candidate - Candidate increments term, votes for self - Requests votes from all nodes - If majority grants votes \u2192 becomes Leader - If receives heartbeat from valid leader \u2192 becomes Follower - If election timeout \u2192 starts new election Phase 2: Log Replication - Client sends command to Leader - Leader appends to local log - Leader sends AppendEntries to all Followers - Followers append entries to their logs - Once majority acknowledges \u2192 Leader commits entry - Leader notifies Followers of commit via next AppendEntries Phase 3: Safety - New leader contains all committed entries (election restriction) - Leader never commits entries from previous terms directly - Only commits when majority has current-term entry Simplified API: // High-level Raft interface public interface RaftConsensus { // Start election (becomes candidate) void startElection(int nodeId); // Append command to replicated log boolean appendEntry(int leaderId, String command); // Get current leader int getLeader(); // Get committed log entries List<LogEntry> getCommittedEntries(int nodeId); } // Typical usage RaftConsensus raft = new RaftImpl(nodeIds); // Elect a leader raft.startElection(1); int leader = raft.getLeader(); // Replicate commands raft.appendEntry(leader, \"SET x=1\"); raft.appendEntry(leader, \"DELETE y\"); // All nodes will have same committed log List<LogEntry> node1Log = raft.getCommittedEntries(1); List<LogEntry> node2Log = raft.getCommittedEntries(2); // node1Log == node2Log (same order, same entries) Log Replication Flow: Client \u2192 Leader: \"SET x=1\" Leader state: term: 2 log: [...] commitIndex: 5 Step 1: Leader appends to local log log: [..., Entry(term=2, index=6, cmd=\"SET x=1\")] Step 2: Leader sends AppendEntries to Followers \u2192 Follower 2: AppendEntries(term=2, prevIndex=5, entries=[Entry(6)]) \u2192 Follower 3: AppendEntries(term=2, prevIndex=5, entries=[Entry(6)]) \u2192 Follower 4: AppendEntries(term=2, prevIndex=5, entries=[Entry(6)]) \u2192 Follower 5: AppendEntries(term=2, prevIndex=5, entries=[Entry(6)]) Step 3: Followers append and ACK Follower 2: \u2713 ACK Follower 3: \u2713 ACK Follower 4: \u2717 (down) Follower 5: \u2717 (partition) Step 4: Leader receives majority (Leader + 2 followers = 3/5) commitIndex: 6 (committed!) Step 5: Leader notifies followers of commit in next AppendEntries All nodes apply \"SET x=1\" to state machine Key Insight: Log Matching Property If two entries in different logs have same index and term: 1. They store the same command 2. All preceding entries are identical Why? Leader creates at most one entry per index per term, and entries are never moved or deleted (append-only). This property enables Raft to keep logs consistent with simple checks. Pattern 3: Distributed Locks \u00b6 Concept: Mechanism to ensure mutual exclusion across distributed systems, preventing concurrent access to shared resources. Use case: Job schedulers, resource allocation, preventing duplicate processing. Key Features: Time-To-Live (TTL): Locks automatically expire after timeout Prevents deadlock if lock holder crashes Trade-off: Too short = premature release, too long = delayed recovery Fencing Tokens: Monotonically increasing token per lock acquisition Prevents stale lock holders from corrupting data Resource validates token before accepting operations Lock Renewal: Extend lease before expiration Allows long-running operations Heartbeat mechanism to prove liveness How Distributed Locks Work: Lock Lifecycle: 1. Try Acquire: Client \u2192 Lock Service: \"Lock resource X for client A\" If unlocked or expired: Generate fencing token (counter++) Store: {resource: X, owner: A, token: 123, expires: now+TTL} Return: Lock{token: 123} If locked by another owner: Return: null (acquisition failed) 2. Hold Lock: Client performs work Optionally renew before expiration: Client \u2192 Lock Service: \"Renew X with token 123\" If valid: Update expires = now + TTL 3. Release Lock: Client \u2192 Lock Service: \"Release X with token 123\" Verify owner and token match Delete lock entry 4. Auto-Expiration (if crash): Lock expires at TTL Next client can acquire Fencing Token Pattern: Problem: Lock holder's work outlives the lock Scenario WITHOUT fencing tokens: t0: Client A acquires lock (TTL=10s), starts slow write t10: Lock expires (A still writing) t11: Client B acquires lock, starts fast write t12: Client B completes write t15: Client A completes write \u2190 Overwrites B's data! Solution WITH fencing tokens: t0: Client A acquires lock \u2192 token=100 t10: Lock expires t11: Client B acquires lock \u2192 token=101 t12: Client B writes with token=101 \u2192 SUCCESS t15: Client A writes with token=100 \u2192 REJECTED (stale token!) Resource checks: token >= last_accepted_token Example Flow: Job Scheduler with Distributed Locks: Job: \"Send daily email\" Schedulers: A, B, C (for redundancy) Scheduler A: 1. tryAcquire(\"job:daily-email\", \"scheduler-A\") \u2192 Lock{token: 456, expires: now+30s} 2. Execute job (15 seconds) - Fetch recipients - Generate emails - Send emails 3. release(\"job:daily-email\", token=456) \u2192 Success Scheduler B (parallel attempt): 1. tryAcquire(\"job:daily-email\", \"scheduler-B\") \u2192 null (already locked by A) 2. Skip job (A is handling it) Result: Email sent exactly once \u2713 High-Level API: // Simple distributed lock interface public interface DistributedLock { // Try to acquire lock immediately Lock tryAcquire(String resourceId, String ownerId); // Try with custom TTL Lock tryAcquire(String resourceId, String ownerId, long ttlMs); // Blocking acquire with timeout Lock acquire(String resourceId, String ownerId, long timeoutMs); // Release lock boolean release(String resourceId, String ownerId, long fencingToken); // Extend lease boolean renew(String resourceId, String ownerId, long fencingToken); // Check if locked boolean isLocked(String resourceId); } // Typical usage with try-finally DistributedLock lockService = new RedisLock(); Lock lock = lockService.tryAcquire(\"resource:123\", \"worker-1\", 30000); if (lock != null) { try { // Perform exclusive work processResource(); // Optionally renew if work takes longer if (needMoreTime()) { lockService.renew(\"resource:123\", \"worker-1\", lock.fencingToken); } } finally { lockService.release(\"resource:123\", \"worker-1\", lock.fencingToken); } } else { // Resource locked by another worker, skip or retry later System.out.println(\"Resource busy\"); } Lock Acquisition Strategies: 1. Non-blocking (tryAcquire): Use when: Fast failure preferred Pattern: Try once, fail immediately if unavailable lockService.tryAcquire(\"job:x\", \"worker-1\"); \u2192 null \u2192 Skip job, another worker handling it 2. Blocking with timeout (acquire): Use when: Willing to wait briefly Pattern: Retry with backoff until timeout lockService.acquire(\"job:x\", \"worker-1\", 5000); // 5s timeout \u2192 Retries every 50-100ms \u2192 Returns lock or null after timeout 3. Lease renewal: Use when: Long-running tasks Pattern: Periodic heartbeat to extend lease Lock lock = lockService.acquire(\"job:x\", \"worker-1\", 30000); ScheduledExecutorService renewalService = ...; renewalService.scheduleAtFixedRate(() -> { lockService.renew(\"job:x\", \"worker-1\", lock.fencingToken); }, 10, 10, TimeUnit.SECONDS); // Renew every 10s (TTL=30s) Common Pitfalls: 1. Deadlock from crashed holder: Problem: Client crashes while holding lock Without TTL: Lock held forever \u2192 Deadlock With TTL: Lock expires after 30s Next client can acquire System self-heals \u2713 2. Split-brain without fencing: Problem: GC pause longer than TTL Client A: Acquires lock GC pause (40s) \u2190 Longer than TTL! Resumes, thinks it still has lock Writes data \u2190 DANGER! Client B: Lock expired, acquires new lock Writes data Result: Conflicting writes! Solution: Use fencing tokens Client A write rejected (stale token) 3. Clock skew: Problem: Distributed clocks not synchronized Node 1 clock: 12:00:00 Node 2 clock: 12:00:30 (30s ahead!) Lock expires at: 12:00:20 Node 1: Still valid (20s left) Node 2: Expired! (in the past) Solution: - Use relative time (TTL in milliseconds, not absolute timestamps) - Single source of truth (lock service's clock) - Or use logical clocks (Lamport timestamps) Implementation Options: Redis (Redlock algorithm): SET resource:lock \"owner-id\" NX PX 30000 NX = only if not exists PX = expire after milliseconds Pros: Fast, simple Cons: Single point of failure (unless Redis cluster) ZooKeeper (ephemeral nodes): create /locks/resource-1 \"owner-id\" EPHEMERAL Node auto-deleted if client session ends Pros: Reliable, automatic cleanup Cons: Higher latency, more complex etcd (lease-based): 1. Create lease (TTL=30s) 2. Put key with lease 3. Keep-alive to renew lease Pros: Lease abstraction, Raft consensus Cons: More moving parts Time Complexity: Acquire: O(1) - single operation Release: O(1) - single operation Renew: O(1) - update expiration Cleanup: O(N) - scan expired locks (background) Pattern 4: Quorum-Based Consensus \u00b6 Concept: Achieve consistency by requiring a majority (quorum) of nodes to agree on reads and writes. Use case: Distributed databases (Cassandra, DynamoDB), multi-datacenter replication, high availability systems. Key Properties: Quorum Requirement (R + W > N): R = Read quorum (nodes to read from) W = Write quorum (nodes to write to) N = Replication factor (total copies) When R + W > N, guarantees strong consistency (overlap ensures latest value seen) Versioning: Each write tagged with version (timestamp or vector clock) Enables conflict detection and resolution Client receives latest version on read Tunable Consistency: Adjust R and W based on workload Read-heavy: R=1, W=N (fast reads, slow writes) Write-heavy: R=N, W=1 (fast writes, slow reads) Strong consistency: R+W > N (most common: R=W=majority) How Quorum Consensus Works: Cluster: 5 nodes, Replication Factor (N) = 3 Configuration: R=2, W=2 (R+W=4 > N=3, strong consistency) Key \"user:123\" stored on nodes: [1, 3, 5] (consistent hashing) Quorum Write Flow: Client \u2192 Coordinator: write(\"user:123\", \"Alice\") Step 1: Select W nodes for write Coordinator selects 2 of 3 replicas: [Node 1, Node 3] Step 2: Create versioned value value = { data: \"Alice\", version: {timestamp: 1234567890, vectorClock: v5} } Step 3: Write to W nodes concurrently \u2192 Node 1: write(key, value) \u2192 ACK \u2192 Node 3: write(key, value) \u2192 ACK Step 4: Wait for W=2 acknowledgments Received 2 ACKs \u2192 Write successful! (Even though Node 5 wasn't written, quorum satisfied) Step 5: Return success to client Write latency: max(Node 1, Node 3 latency) \u2248 10-50ms Quorum Read Flow with Conflict Resolution: Client \u2192 Coordinator: read(\"user:123\") Step 1: Select R nodes for read Coordinator selects 2 of 3 replicas: [Node 1, Node 5] Step 2: Read from R nodes concurrently \u2192 Node 1: value={data:\"Alice\", version:v5} \u2192 Node 5: value={data:\"Bob\", version:v3} (stale!) Step 3: Wait for R=2 responses Received responses from Node 1 and Node 5 Step 4: Resolve conflicts (pick latest version) Compare versions: Node 1: v5 (latest) Node 5: v3 (stale) Winner: Node 1's value \"Alice\" (version v5) Step 5: Optional read-repair Coordinator sends v5 to Node 5 to update stale data (Background operation, doesn't block client) Step 6: Return latest value to client Client receives: \"Alice\" Read latency: max(Node 1, Node 5 latency) \u2248 5-20ms Why R + W > N Guarantees Consistency: N = 3 replicas: [Node 1, Node 2, Node 3] R = 2, W = 2 (R + W = 4 > N = 3) Write to nodes [1, 2] \u2192 At least one has latest value Read from nodes [2, 3] \u2192 At least one has latest value Overlap: Node 2 appears in both sets! This guarantees reader sees latest write. If R + W \u2264 N: Write [1, 2], Read [3, 4] \u2192 No overlap! Stale read possible High-Level API: // Simple quorum-based data store interface public interface QuorumStore { // Write value with quorum boolean write(String key, String value); // Read value with quorum VersionedValue read(String key); // Configure quorum sizes void setQuorum(int readQuorum, int writeQuorum); // Check if value exists boolean exists(String key); } // Typical usage QuorumStore store = new QuorumStore( numNodes: 5, replicationFactor: 3, readQuorum: 2, writeQuorum: 2 ); // Write data store.write(\"session:abc123\", \"user-data\"); // Read data (gets latest version) VersionedValue value = store.read(\"session:abc123\"); System.out.println(\"Value: \" + value.data); System.out.println(\"Version: \" + value.version); Quorum Configurations: Configuration 1: Strong Consistency (R=2, W=2, N=3) Use case: Financial transactions, inventory management R + W = 4 > N = 3 \u2713 Trade-off: + Always read latest write + Tolerate 1 node failure - Higher latency (wait for 2 nodes) Configuration 2: Read-Optimized (R=1, W=3, N=3) Use case: Product catalog, content delivery R + W = 4 > N = 3 \u2713 Trade-off: + Fast reads (only 1 node) - Slow writes (all 3 nodes) + Still strongly consistent Configuration 3: Write-Optimized (R=3, W=1, N=3) Use case: Write-heavy logging, telemetry R + W = 4 > N = 3 \u2713 Trade-off: + Fast writes (only 1 node) - Slow reads (all 3 nodes) + Still strongly consistent Configuration 4: Eventual Consistency (R=1, W=1, N=3) Use case: Shopping cart, user preferences R + W = 2 \u2264 N = 3 \u2717 Trade-off: + Fastest reads and writes - May read stale data temporarily - Eventually consistent (not strongly consistent) Advanced Techniques: 1. Read Repair (Fix Stale Replicas): During quorum read, if coordinator detects stale replicas: Read responses: Node 1: version v5 (latest) Node 2: version v3 (stale) Coordinator triggers read-repair: \u2192 Node 2: write(key, value with v5) Next read will see consistent data across replicas Happens in background, doesn't block client 2. Hinted Handoff (Handle Temporary Failures): Write fails because target node is down: Intended replicas: [Node 1, Node 2, Node 3] Node 2 is down! Coordinator stores \"hint\" on Node 4: hint = { target: Node 2, key: \"user:123\", value: \"Alice\", version: v5 } When Node 2 recovers: Node 4 replays hint \u2192 Node 2 gets missing write System self-heals without manual intervention 3. Anti-Entropy (Periodic Synchronization): Background process compares replicas: Every 10 minutes: 1. Compare Merkle trees of replicas 2. Identify differences 3. Synchronize stale data Ensures eventual consistency even if read-repair missed Handles network partitions and prolonged node failures Implementation Examples: Cassandra (Apache): Configuration: CREATE KEYSPACE store WITH replication = { 'class': 'SimpleStrategy', 'replication_factor': 3 }; Write: INSERT INTO users (id, name) VALUES ('123', 'Alice') USING CONSISTENCY QUORUM; // W = majority Read: SELECT * FROM users WHERE id='123' USING CONSISTENCY QUORUM; // R = majority Features: - Tunable consistency per query - Automatic read-repair - Hinted handoff - Multi-datacenter replication DynamoDB (AWS): Configuration: Table: users Read Capacity: 100 RCUs Write Capacity: 50 WCUs Write: put_item( TableName='users', Item={'id': '123', 'name': 'Alice'}, ConsistentRead=True // W = majority ) Read: get_item( TableName='users', Key={'id': '123'}, ConsistentRead=True // R = majority (strong consistency) ) Features: - Eventually consistent reads by default - Strongly consistent reads on demand - Automatic scaling and replication - Global tables for multi-region Riak (Basho): Configuration: bucket-type create users '{\"props\":{\"n_val\":3}}' Write with custom quorum: PUT /types/users/buckets/sessions/keys/abc123 Header: X-Riak-W: 2 // Write quorum Read with custom quorum: GET /types/users/buckets/sessions/keys/abc123 Header: X-Riak-R: 2 // Read quorum Features: - Per-request quorum tuning - Vector clocks for versioning - Conflict resolution strategies - Active anti-entropy Time Complexity: Operation Complexity Notes Write O(W) W = write quorum, parallel writes Read O(R) R = read quorum, parallel reads Conflict resolution O(R) Compare R versions Node selection O(1) Consistent hashing lookup Read repair O(N - R) Update stale replicas in background Space Complexity: Per-node storage: O(K / N) where K = total keys, N = replication factor Version metadata: O(K) per node (small overhead) Hint storage: O(H) where H = pending hints (temporary) Debugging Challenges \u00b6 Your task: Find and fix bugs in broken consensus implementations. This tests your understanding of distributed systems failure modes. Challenge 1: Split-Brain in Leader Election \u00b6 /** * Leader election that's supposed to prevent split-brain. * This has 2 CRITICAL BUGS that allow multiple leaders. * Find them! */ public class BuggyLeaderElection { private Map<Integer, Node> nodes; private int majoritySize; public void startElection(int candidateId) { Node candidate = nodes.get(candidateId); candidate.role = NodeRole.CANDIDATE; candidate.currentTerm++; int votesReceived = 1; // Vote for self for (Map.Entry<Integer, Node> entry : nodes.entrySet()) { int nodeId = entry.getKey(); if (nodeId != candidateId) { boolean voteGranted = requestVote(nodeId, candidateId); votesReceived++; // Counting even if vote not granted! } } if (votesReceived > majoritySize) { // Should this be > or >= ? becomeLeader(candidateId); } } } Your debugging: Bug 1: [What\\'s the bug?] Bug 2: [What\\'s the bug?] Split-brain scenario: 5 nodes, majoritySize = 3 Network partition: {1, 2} and {3, 4, 5} Node 1 starts election, gets vote from Node 2 Node 3 starts election, gets votes from 4, 5 With bugs: [How many leaders? Why?] After fixes: [How many leaders? Why?] Click to verify your answers Bug 1 (Line 15): Increments votesReceived unconditionally, even when voteGranted is false. Should only increment when vote is granted. Fix: if (voteGranted) votesReceived++; Bug 2 (Line 19): Uses > instead of >= . With 5 nodes, majority is 3. If candidate gets exactly 3 votes, 3 > 3 is false, so no leader elected! Fix: if (votesReceived >= majoritySize) { With bugs: In partition {1, 2}, Node 1 gets 2 votes but bug counts as 3+ \u2192 becomes leader. In partition {3, 4, 5}, Node 3 gets 3 votes \u2192 becomes leader. Two leaders! After fixes: Node 1 gets 2 votes < 3 majority \u2192 no leader. Node 3 gets 3 votes \u2265 3 majority \u2192 becomes leader. One leader only. Challenge 2: Lost Commits in Raft \u00b6 /** * Raft log replication with a CRITICAL BUG. * Committed entries can be LOST after leader failure! */ public class BuggyRaftReplication { public boolean appendEntry(int leaderId, String command) { RaftNode leader = nodes.get(leaderId); if (leader.role != NodeRole.LEADER) return false; // Create log entry int newIndex = leader.getLastLogIndex() + 1; LogEntry entry = new LogEntry(leader.currentTerm, command, newIndex); leader.log.add(entry); int replicatedCount = 0; // Forgot to count leader! for (Map.Entry<Integer, RaftNode> e : nodes.entrySet()) { int nodeId = e.getKey(); if (nodeId != leaderId && nodeActive.get(nodeId)) { boolean success = sendAppendEntries(leaderId, nodeId); if (success) replicatedCount++; } } // Commit if majority replicated if (replicatedCount >= majoritySize) { leader.commitIndex = newIndex; return true; } return false; } } Your debugging: Bug: [What\\'s the bug?] Failure scenario: 5 nodes (Node 1 = leader), majoritySize = 3 Leader appends entry \"SET x=1\" Entry replicated to Node 2, Node 3 (2 nodes) Bug: replicatedCount = 2 < 3 majority \u2192 NOT committed Leader crashes before replicating to Node 4 With bug: Entry lost (never committed) Trace through: [Step by step, what happens?] Click to verify your answer Bug (Line 17): Initializes replicatedCount = 0 , forgetting that the leader already has the entry in its log. Should start at 1. Fix: int replicatedCount = 1; // Leader has it Why it matters: With 5 nodes, majority = 3. If leader + 2 followers have the entry, that's 3 copies (majority). But bug counts only 2 followers, thinks it's not committed, and entry could be lost if leader crashes. Correct behavior: Leader counts self + 2 followers = 3 \u2265 majority \u2192 committed. Entry is safe even if leader fails. Challenge 3: Term Confusion in Raft \u00b6 /** * Raft RequestVote RPC with TERM HANDLING BUG. * Can accept votes from candidates with STALE terms! */ public class BuggyRequestVote { private boolean requestVote(int voterId, int candidateId, int candidateTerm) { RaftNode voter = nodes.get(voterId); // Check term if (candidateTerm < voter.currentTerm) { return false; // Reject outdated candidate } voter.currentTerm = candidateTerm; // Missing: What should happen to voter.votedFor? // Grant vote if haven't voted if (voter.votedFor == -1) { voter.votedFor = candidateId; return true; } return false; } } Your debugging: Bug: [What\\'s the bug?] Failure scenario: Term 1: Node 3 votes for Node 5 Term 2: Node 1 starts election, requests vote from Node 3 Node 3's state: currentTerm=1, votedFor=5 Node 1's term: currentTerm=2 With bug: What happens to Node 3's votedFor? Expected: [Should vote be granted? Why?] Click to verify your answer Bug (After line 14): When updating term, must reset votedFor = -1 to allow voting in new term. Current code leaves old vote in place. Fix: if (candidateTerm > voter.currentTerm) { voter.currentTerm = candidateTerm; voter.votedFor = -1; // Reset vote for new term! } Why it matters: In new term, voter should be able to vote again. Without reset, voter stays committed to old vote, can't vote for anyone in new term, election may fail. Correct: When Node 3 sees candidateTerm=2 > currentTerm=1, it resets votedFor=-1, then can vote for Node 1. Challenge 4: Log Inconsistency in Raft \u00b6 /** * Raft AppendEntries with LOG CONSISTENCY BUG. * Follower can accept entries that create holes in log! */ public class BuggyAppendEntries { private boolean appendEntries(int followerId, int leaderTerm, int prevLogIndex, int prevLogTerm, List<LogEntry> entries) { RaftNode follower = nodes.get(followerId); // Check term if (leaderTerm < follower.currentTerm) { return false; } // Should verify follower has entry at prevLogIndex with prevLogTerm // Append entries for (LogEntry entry : entries) { follower.log.add(entry); } return true; } } Your debugging: Bug: [What\\'s the bug?] Failure scenario: Leader log: [e1(term=1), e2(term=1), e3(term=2)] Follower log: [e1(term=1), e2(term=2)] (e2 has wrong term!) Leader sends: prevLogIndex=2, prevLogTerm=1, entries=[e3] With bug: [What happens?] Expected behavior: [Should append be accepted?] Click to verify your answer Bug (After line 16): Missing log consistency check. Must verify that follower's log at prevLogIndex has term prevLogTerm . Fix: // Check log consistency if (prevLogIndex > 0) { if (prevLogIndex > follower.getLastLogIndex()) { return false; // Follower's log too short } if (follower.log.get(prevLogIndex - 1).term != prevLogTerm) { return false; // Term mismatch } } Why it matters: Raft requires logs to be consistent before appending. If follower has conflicting entry (different term at same index), must reject and let leader retry with earlier index. Correct: Leader's prevLogTerm=1, but follower has term=2 at index 2 \u2192 reject. Leader decrements prevLogIndex and retries until logs match. Challenge 5: Distributed Lock Deadlock \u00b6 /** * Distributed lock with DEADLOCK BUG. * Lock can remain held forever if holder crashes! */ public class BuggyDistributedLock { public Lock tryAcquire(String resourceId, String ownerId) { Lock existingLock = locks.get(resourceId); if (existingLock != null) { if (!existingLock.ownerId.equals(ownerId)) { return null; // Lock held by someone else } } // Acquire lock long fencingToken = tokenCounter.incrementAndGet(); Lock newLock = new Lock(resourceId, ownerId, fencingToken, ttl); locks.put(resourceId, newLock); return newLock; } } Your debugging: Bug: [What\\'s the bug?] Failure scenario: Client A acquires lock with 30s TTL Client A crashes after 5 seconds (doesn't release) Client B tries to acquire lock after 40 seconds With bug: [Can Client B acquire lock? Why?] Expected: [Should lock be available?] Click to verify your answer Bug (Line 11): Doesn't check if existing lock is expired. Should call existingLock.isExpired() before rejecting acquisition. Fix: if (existingLock != null && !existingLock.isExpired()) { if (!existingLock.ownerId.equals(ownerId)) { return null; // Lock still valid and held by someone else } } Why it matters: If lock holder crashes, lock expires after TTL. Without expiration check, lock remains held forever \u2192 deadlock. Other processes can never acquire. Correct: After TTL expires, lock is considered released, can be acquired by another process. Prevents deadlock from crashed holders. Challenge 6: Quorum Read Inconsistency \u00b6 /** * Quorum read with CONSISTENCY BUG. * Can return stale data even with proper R/W settings! */ public class BuggyQuorumRead { public VersionedValue read(String key) { List<Node> replicas = selectNodes(key, replicationFactor); List<VersionedValue> responses = new ArrayList<>(); // Read from R nodes int successCount = 0; for (Node node : replicas) { if (node.active && successCount < readQuorum) { VersionedValue value = node.get(key); if (value != null) { responses.add(value); successCount++; } } } if (!responses.isEmpty()) { return responses.get(0); // Wrong! Might be stale! } return null; } } Your debugging: Bug: [What\\'s the bug?] Inconsistency scenario: R=2, W=2, N=3 (strong consistency: R+W > N) Node 1: value=\"v1\" (version=1) Node 2: value=\"v2\" (version=2, latest) Node 3: value=\"v2\" (version=2) Read from Node 1, Node 2 (in that order) With bug: [What value is returned?] Expected: [What should be returned?] Click to verify your answer Bug (Line 23): Returns first response without comparing versions. First response might be stale (lower version). Fix: if (responses.size() >= readQuorum) { return resolveConflicts(responses); // Pick latest version } Conflict resolution: private VersionedValue resolveConflicts(List<VersionedValue> values) { VersionedValue latest = values.get(0); for (VersionedValue v : values) { if (v.version.vectorClock > latest.version.vectorClock) { latest = v; } } return latest; } Why it matters: Even with quorum, responses can have different versions. Must compare and return latest to guarantee consistency. Correct: Compare Node 1 (v1) and Node 2 (v2), return v2 (higher version). Client sees consistent, latest data. Your Debugging Scorecard \u00b6 After finding and fixing all bugs: Found all 8+ bugs across 6 challenges Understood consensus failure modes (split-brain, lost commits, inconsistency) Could explain WHY each bug violates safety properties Learned common distributed systems mistakes Common consensus bugs you discovered: [List patterns: vote counting, term handling, etc.] [Fill in] [Fill in] Safety properties that bugs violated: Agreement: [Which bugs caused nodes to disagree?] Validity: [Which bugs caused invalid states?] Termination: [Which bugs caused deadlock/livelock?] Decision Framework \u00b6 Your task: Build decision trees for when to use each consensus pattern. Question 1: Leader Election vs Leaderless? \u00b6 Answer after implementation: Use Leader Election when: Single coordinator needed: [One node must make decisions] Simplify operations: [Leader handles all writes] Strong consistency: [Leader ensures ordering] Examples: [Master-worker, coordinator services] Use Leaderless (Quorum) when: High availability: [No single point of failure] Multi-datacenter: [Local writes in each DC] Read/write balance: [Tune R/W for workload] Examples: [Cassandra, DynamoDB] Question 2: When to use Raft vs Paxos? \u00b6 Raft when: Understandability: [Easier to implement and reason about] Log replication: [Need ordered log of operations] Modern systems: [etcd, Consul use Raft] Paxos when: Proven formal correctness: [Mathematically proven] Legacy systems: [Google Chubby uses Paxos] Academic interest: [Understanding distributed consensus theory] Question 3: When to use distributed locks? \u00b6 Use distributed locks when: Mutual exclusion: [Only one process should access resource] Job scheduling: [Prevent duplicate job execution] Leader election: [Simple leader election mechanism] Avoid distributed locks when: Performance critical: [Locks add latency] Can use optimistic locking: [Version-based concurrency control] Idempotent operations: [Can safely retry without lock] Your Decision Tree \u00b6 Build this after solving practice scenarios: flowchart LR Start[\"Consensus Pattern Selection\"] Q1{\"Need single coordinator?\"} Start --> Q1 N2[\"Leader Election\"] Q1 -->|\"YES\"| N2 N3[\"Bully algorithm\"] Q1 -->|\"Simple cluster?\"| N3 N4[\"Raft\"] Q1 -->|\"Production system?\"| N4 N5[\"Continue to Q2\"] Q1 -->|\"NO\"| N5 Q6{\"What's the consistency requirement?\"} Start --> Q6 N7[\"Raft or Paxos\"] Q6 -->|\"Strong consistency\"| N7 N8[\"Quorum with R=1, W=1\"] Q6 -->|\"Eventual consistency\"| N8 N9[\"Quorum with configurable R/W\"] Q6 -->|\"Tunable consistency\"| N9 Q10{\"What's the failure scenario?\"} Start --> Q10 N11[\"Raft<br/>(CP in CAP)\"] Q10 -->|\"Network partitions\"| N11 N12[\"Quorum with hints\"] Q10 -->|\"Node failures\"| N12 N13[\"Leader election with fencing\"] Q10 -->|\"Split-brain\"| N13 Q14{\"What's the use case?\"} Start --> Q14 N15[\"Raft<br/>(etcd, ZooKeeper)\"] Q14 -->|\"Configuration management\"| N15 N16[\"Quorum consensus\"] Q14 -->|\"Distributed database\"| N16 N17[\"Distributed locks\"] Q14 -->|\"Job coordination\"| N17 N18[\"No consensus needed<br/>(best-effort)\"] Q14 -->|\"Cache invalidation\"| N18 Practice \u00b6 Scenario 1: Distributed Database Leader Election \u00b6 Requirements: 5-node database cluster Need single primary for writes Automatic failover on primary failure Must prevent split-brain Downtime < 5 seconds on failure Your design: Leader election algorithm: [Raft or Bully? Why?] Reasoning: Election speed: [Fill in] Split-brain prevention: [How?] Failure detection: [Heartbeat timeout?] Implementation details: [Heartbeat interval and timeout values] [How to handle network partition] [Fencing mechanism to prevent dual-primary] Scenario 2: Distributed Job Scheduler \u00b6 Requirements: 1000 scheduled jobs Must run exactly once Multiple scheduler instances for HA Jobs can take 1-60 minutes Must handle scheduler crashes Your design: Lock mechanism: [Distributed locks or leader election?] Why? [Exactly-once execution guarantee] [How to handle lock holder crash] [Lock timeout calculation] Lock implementation: TTL: [How long?] Renewal: [When and how often?] Fencing: [Prevent duplicate execution how?] Scenario 3: Multi-Region Key-Value Store \u00b6 Requirements: 3 datacenters (US, EU, Asia) Need low latency reads in each region Eventual consistency acceptable 1M writes/sec globally Must survive datacenter failure Your design: Consensus approach: [Quorum or leader?] Quorum configuration: Replication factor: [How many DCs?] Read quorum: [R = ?] Write quorum: [W = ?] Reasoning: [R + W > N?] Trade-offs: [Consistency vs availability] [Cross-DC write latency] [Conflict resolution strategy] Review Checklist \u00b6 Before moving to the next topic: Implementation Leader election works (Bully algorithm) Raft election and log replication work Distributed locks acquire, release, renew work Quorum reads and writes work All client code runs successfully Understanding Filled in all ELI5 explanations Understand split-brain problem Know how Raft achieves consensus Understand fencing tokens Know how quorums provide consistency Failure Scenarios Leader failure and re-election Lock holder crashes (deadlock prevention) Network partition (split-brain) Quorum not reachable Node recovery and catch-up Decision Making Know when to use leader vs leaderless Know when to use Raft vs Paxos Completed practice scenarios Can explain trade-offs in CAP theorem Mastery Check Could implement leader election from memory Could design consensus for new system Understand Raft log replication Know how to prevent split-brain Can calculate quorum sizes for requirements Mastery Certification \u00b6 I certify that I can: Implement leader election (Bully or Raft) from memory Explain split-brain problem and how consensus prevents it Design consensus strategy for new distributed system Identify correct consensus pattern for requirements Analyze failure modes and their impact Debug common consensus bugs (vote counting, term handling, etc.) Explain trade-offs between leader-based and leaderless Apply CAP theorem to consensus decisions Calculate quorum sizes for consistency requirements Teach consensus concepts to someone else","title":"16. Consensus Patterns"},{"location":"systems/16-consensus-patterns/#consensus-patterns","text":"Leader election, Raft consensus, distributed locks, and quorum-based systems","title":"Consensus Patterns"},{"location":"systems/16-consensus-patterns/#eli5-explain-like-im-5","text":"Your task: After implementing consensus patterns, explain them simply. Prompts to guide you: What is consensus in one sentence? Your answer: [Fill in after implementation] Why do distributed systems need consensus? Your answer: [Fill in after implementation] Real-world analogy for leader election: Example: \"Leader election is like choosing a class president where...\" Your analogy: [Fill in] What is the split-brain problem in one sentence? Your answer: [Fill in after implementation] Real-world analogy for distributed locks: Example: \"A distributed lock is like a bathroom key that...\" Your analogy: [Fill in] Why do we need quorums? Your answer: [Fill in after practice]","title":"ELI5: Explain Like I'm 5"},{"location":"systems/16-consensus-patterns/#quick-quiz-do-before-implementing","text":"Your task: Test your intuition about distributed consensus without looking at code. Answer these, then verify after implementation.","title":"Quick Quiz (Do BEFORE implementing)"},{"location":"systems/16-consensus-patterns/#beforeafter-why-consensus-matters","text":"Your task: Compare naive distributed coordination vs proper consensus to understand the impact.","title":"Before/After: Why Consensus Matters"},{"location":"systems/16-consensus-patterns/#case-studies-consensus-in-the-wild","text":"","title":"Case Studies: Consensus in the Wild"},{"location":"systems/16-consensus-patterns/#core-concepts","text":"","title":"Core Concepts"},{"location":"systems/16-consensus-patterns/#debugging-challenges","text":"Your task: Find and fix bugs in broken consensus implementations. This tests your understanding of distributed systems failure modes.","title":"Debugging Challenges"},{"location":"systems/16-consensus-patterns/#decision-framework","text":"Your task: Build decision trees for when to use each consensus pattern.","title":"Decision Framework"},{"location":"systems/16-consensus-patterns/#practice","text":"","title":"Practice"},{"location":"systems/16-consensus-patterns/#review-checklist","text":"Before moving to the next topic: Implementation Leader election works (Bully algorithm) Raft election and log replication work Distributed locks acquire, release, renew work Quorum reads and writes work All client code runs successfully Understanding Filled in all ELI5 explanations Understand split-brain problem Know how Raft achieves consensus Understand fencing tokens Know how quorums provide consistency Failure Scenarios Leader failure and re-election Lock holder crashes (deadlock prevention) Network partition (split-brain) Quorum not reachable Node recovery and catch-up Decision Making Know when to use leader vs leaderless Know when to use Raft vs Paxos Completed practice scenarios Can explain trade-offs in CAP theorem Mastery Check Could implement leader election from memory Could design consensus for new system Understand Raft log replication Know how to prevent split-brain Can calculate quorum sizes for requirements","title":"Review Checklist"}]}